---
title: MLIR Compiling Flow of Conv2D
date: 2025-03-04
permalink: /pages/000015/
---

# MLIR Compiling Flow of Conv2D

> This is generated by ChatGPT.

## 1. High-Level Dialect (TOSA or MHLO)
At the highest level, the convolution typically appears as a single-purpose op, e.g., TOSA’s tosa.conv2d or MHLO’s mhlo.convolution. Below is a TOSA-style example:

<details>
  <summary>Code</summary>
  
```mlir
# Simple 2D conv: stride=1, no padding, no dilation
# Input:  [N=4, H=32, W=32, C_in=3]
# Filter: [KH=3, KW=3, C_in=3, C_out=8]
# Output: [N=4, H=30, W=30, C_out=8]
tosa.conv2d(
  %input,    // tensor<4x32x32x3xf32>
  %filter,   // tensor<3x3x3x8xf32>
  %bias,     // tensor<8xf32>, optional
  strides = [1, 1],
  pad = [0, 0, 0, 0],
  dilation = [1, 1]
) : (tensor<4x32x32x3xf32>,
     tensor<3x3x3x8xf32>,
     tensor<8xf32>) -> tensor<4x30x30x8xf32>
```
</details>

At this stage, the operation is still “high level”: the compiler knows it’s a convolution with certain attributes but hasn’t expanded it into explicit loops yet.

## 2. Lowering to Linalg (Structured Ops)
A specialized Linalg op may replace the TOSA conv2d. MLIR has built-in named ops like linalg.conv_2d_nhwc_hwcf, but one can also lower to a more generic linalg.generic. Below is a named Linalg form that captures the same convolution:

<details>
  <summary>Code</summary>
  
```mlir
%output = linalg.conv_2d_nhwc_hwcf
    ins(%input, %filter : tensor<4x32x32x3xf32>, tensor<3x3x3x8xf32>)
    outs(%init : tensor<4x30x30x8xf32>) -> tensor<4x30x30x8xf32>
{
  // The internal region is typically auto-generated. 
  // The iteration spaces (N, H_out, W_out, C_out, KH, KW, C_in) 
  // are implied by this op’s definition.
}
```
</details>

Alternatively, you may see an expanded linalg.generic with explicit indexing maps that show the iteration space. Here’s a solid example: we define 7 iteration dimensions:
- 4 “parallel” dims: (n, oh, ow, co)
- 3 “reduction” dims: (kh, kw, ci)

We then set up affine maps to read from (n, oh + kh, ow + kw, ci) and (kh, kw, ci, co), accumulating into (n, oh, ow, co):

<details>
  <summary>Code</summary>
  
```mlir
%conv_result = linalg.generic
    // Define how to map the 7 loop indices to each tensor’s coordinates
    { indexing_maps = [
        // Input:  (n, oh, ow, co, kh, kw, ci) -> (n, oh + kh, ow + kw, ci)
        affine_map<(n, oh, ow, co, kh, kw, ci) -> (n, oh + kh, ow + kw, ci)>,

        // Filter: (n, oh, ow, co, kh, kw, ci) -> (kh, kw, ci, co)
        affine_map<(n, oh, ow, co, kh, kw, ci) -> (kh, kw, ci, co)>,

        // Output: (n, oh, ow, co, kh, kw, ci) -> (n, oh, ow, co)
        affine_map<(n, oh, ow, co, kh, kw, ci) -> (n, oh, ow, co)>
      ],
      iterator_types = [
        "parallel", "parallel", "parallel", "parallel", // n, oh, ow, co
        "reduction", "reduction", "reduction"           // kh, kw, ci
      ]
    }
    ins(%input, %filter : tensor<4x32x32x3xf32>, tensor<3x3x3x8xf32>)
    outs(%init_out : tensor<4x30x30x8xf32>) -> tensor<4x30x30x8xf32>
  {
    // This block is the "fused operation" over each point in the iteration space
    ^bb0(%in_val: f32, %fil_val: f32, %acc_val: f32):
      %mul = arith.mulf %in_val, %fil_val : f32
      %res = arith.addf %acc_val, %mul : f32
      linalg.yield %res : f32
  }
```
</details>

This explicitly shows the iteration space (n, oh, ow, co, kh, kw, ci) and how each tensor is indexed. The convolution is essentially a 7D loop nest: 4 parallel loops, 3 reduction loops.


## 3. Bufferization & MemRef Dialect
Before we can generate code for GPUs, we usually transform from “tensor forms” (value semantics) into memref forms (explicit pointers). For instance, each tensor might become a memref<4x32x32x3xf32> allocated in GPU memory. A simplified result might look like:

<details>
  <summary>Code</summary>
  
```mlir
// Example function using memrefs for input/output/filter:
func.func @conv2d_main(%input: memref<4x32x32x3xf32>, 
                       %filter: memref<3x3x3x8xf32>, 
                       %output: memref<4x30x30x8xf32>) {
  // The linalg op is now in buffer form:
  call @linalg_conv_2d_bufferized(%input, %filter, %output)

  func.return
}
```
</details>

Internally, linalg_conv_2d_bufferized will do loads and stores (memref.load, memref.store) or eventually llvm.load/llvm.store once further lowered. Bufferization clarifies memory addressing and layouts.

## 4. Tiling, Scheduling, and Vectorization
At this stage (still in Linalg land or in the GPU dialect), a series of passes will:

Tile the convolution (e.g., break the iteration space into tile sizes that fit GPU blocks/threads).
Schedule the loops to map them to GPU block IDs and thread IDs.
Optionally vectorize (using the vector dialect) or fuse elementwise ops (like a subsequent ReLU).
Conceptually, this transforms the single big convolution nest into multiple smaller “tiled” computations that run in parallel on the GPU.

## 5. Lowering to GPU Dialect (and then to LLVM Dialect)
After tiling and scheduling, we insert ops from the GPU dialect—these specify things like gpu.launch, gpu.block_id, gpu.thread_id. For example:

<details>
  <summary>Code</summary>
  
```mlir
gpu.func @conv2d_kernel(%input: memref<4x32x32x3xf32, 1>,
                        %filter: memref<3x3x3x8xf32, 1>,
                        %output: memref<4x30x30x8xf32, 1>) {
  %bx = gpu.block_id x
  %tx = gpu.thread_id x
  // Possibly compute global indices = bx * blockSize + tx, etc.

  // "scf.for" or "gpu.for" loops for the tile
  // Perform loads, FMAs, stores
  gpu.return
}
```
</details>

Once in the GPU dialect, a pass lowers it to NVVM or ROCDL dialect ops, which are then turned into the LLVM dialect. For instance, we might see:

<details>
  <summary>Code</summary>
  
```mlir
// NVVM dialect or final LLVM dialect snippet (illustrative)
llvm.func @conv2d_kernel_gpu(...) {
  %0 = llvm.getelementptr ...
  %1 = llvm.load %0 : !llvm.ptr<f32>
  %2 = llvm.fmul %1, %some_other_val : f32
  ...
  llvm.store %2, %dest_ptr : !llvm.ptr<f32>
  llvm.return
}
```
</details>

## 6. Final PTX Generation
Finally, the LLVM dialect is handed off to LLVM’s NVPTX backend, which emits PTX code. In practice, you might see (in textual PTX form):

<details>
  <summary>Code</summary>
  
```mlir
    // A tiny snippet of PTX from the compiled kernel
    // load r0, [rd0];
    // ...
    mul.f32  %f2, %f1, %f0;
    add.f32  %f3, %f2, %f4;
    // ...
    ret;
```
</details>

This PTX can be JIT-compiled by the CUDA driver into GPU microcode (cubin). Once this step completes, your conv2d kernel is ready to run on NVIDIA hardware.

## Putting It All Together (Conv2D → PTX)

**High-Level Dialect (TOSA, MHLO)**
- Single op: tosa.conv2d / mhlo.convolution.

**Linalg (Structured Ops)**
- Named op: linalg.conv_2d_nhwc_hwcf, or
- Expanded op: linalg.generic with explicit (n, oh, ow, co, kh, kw, ci) iteration.

**Bufferization → MemRef dialect**
- Turn tensors into explicit memory references (memref<...>).

**Tiling & Scheduling**
- Transform loops for GPU block/thread distribution, fuse ops, vectorize.

**GPU Dialect → NVVM / LLVM Dialect**
- Introduce gpu.launch, thread IDs, eventually become LLVM IR.

**LLVM NVPTX Backend → PTX**
- Final codegen to PTX, run on the NVIDIA GPU.


In short, each stage refines the representation from an abstract “conv2d” operation down to the PTX instructions that physically perform the multiply-accumulate loops on the GPU.