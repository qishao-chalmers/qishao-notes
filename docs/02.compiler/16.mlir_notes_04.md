---
title: MLIR Compiling Flow of Transformer-Decoder
date: 2025-03-04
permalink: /pages/000016/
---

# MLIR Compiling Flow of Transformer-Decoder

> This is generated by ChatGPT.

This document details the MLIR compilation flow for a **Transformer decoder**, focusing on key computations such as:

- **Self-attention** (QKV projections, matmul, softmax, output projection)
- **MLP** (Feed-forward layers with matmul, activation functions)
- **Layer Normalization** (Reduction and elementwise operations)

The compilation follows the pipeline:

1. **Frontend Import (Torch Dialect)**: Convert TorchScript to the `torch` dialect, preserving dynamic semantics.
2. **Computational Pattern Lowering (Linalg Dialect)**: Convert tensor computations (matmul, elementwise ops) to structured `linalg` operations.
3. **Iteration Space Optimization (Affine Dialect)**: Introduce affine transformations for loop nest optimization, tiling, and dependency analysis.
4. **Lowering to Vectorization and SCF**: Apply vectorization, loop transformations, and parallelize structured loops (`scf` dialect).
5. **Final GPU Lowering (GPU Dialect)**: Map computation to GPU blocks/threads, introduce shared memory, and lower to NVGPU/LLVM.

Each stage includes **MLIR code snippets** demonstrating how IR evolves during compilation.

## 1. Frontend Import: TorchScript to Torch Dialect

The Transformer decoder is first exported from PyTorch as a TorchScript model and converted into MLIR’s `torch` dialect. This preserves dynamic shapes and PyTorch semantics:

```mlir
%q = torch.aten.matmul %input, %weight_q : (!torch.vtensor<[?,64],f32>, !torch.vtensor<[64,64],f32>) -> !torch.vtensor<[?,64],f32>
%k = torch.aten.matmul %input, %weight_k : (!torch.vtensor<[?,64],f32>, !torch.vtensor<[64,64],f32>) -> !torch.vtensor<[?,64],f32>
%v = torch.aten.matmul %input, %weight_v : (!torch.vtensor<[?,64],f32>, !torch.vtensor<[64,64],f32>) -> !torch.vtensor<[?,64],f32>
```

At this stage, operations like matrix multiplications, softmax, and elementwise operations remain in `torch.aten` form.

## 2. Computational Pattern Lowering: Torch to Linalg Dialect

Next, `torch.aten` operations are lowered into `linalg` operations, which define structured computations over tensors. This allows for explicit loop optimizations.

```mlir
%q = linalg.matmul ins(%input, %weight_q) outs(%q_init) -> tensor<?x64xf32>
%k = linalg.matmul ins(%input, %weight_k) outs(%k_init) -> tensor<?x64xf32>
%v = linalg.matmul ins(%input, %weight_v) outs(%v_init) -> tensor<?x64xf32>
```

Additionally, elementwise functions (e.g., softmax, GeLU) are converted into `linalg.generic`:

```mlir
%softmax_out = linalg.generic { indexing_maps = [...], iterator_types = ["parallel"] }
  ins(%logits) outs(%softmax_init) {
    ^bb(%arg0: f32):
    %exp = math.exp %arg0 : f32
    linalg.yield %exp : f32
  } -> tensor<?x?xf32>
```

This explicit representation enables further transformations such as tiling and vectorization.

## 3. Iteration Space Optimization: Affine Dialect

Affine transformations introduce **loop nest optimization**, **tiling**, and **dependency analysis**:

```mlir
scf.for %i = 0 to %M step 16 {
  scf.for %j = 0 to %N step 16 {
    %tile_q = affine.load %Q[%i, %j] : memref<?x64xf32>
    %tile_k = affine.load %K[%i, %j] : memref<?x64xf32>
    %tile_v = affine.load %V[%i, %j] : memref<?x64xf32>
    %out = linalg.matmul %tile_q, %tile_k : tensor<16x64xf32>
  }
}
```

The affine dialect provides static scheduling guarantees, allowing aggressive compiler optimizations.

## 4. Lowering to Vectorization and SCF

At this stage, computations are **vectorized** and **parallelized** using structured loops (`scf.for`):

```mlir
vector.transfer_read %A[%i, %j] : memref<?x64xf32> -> vector<4xf32>
%result = vector.fma %vecA, %vecB, %vecC : vector<4xf32>
vector.transfer_write %result, %C[%i, %j] : memref<?x64xf32>
```

By vectorizing the inner loops, MLIR ensures that matrix multiplications and elementwise ops map efficiently to hardware vector units.

## 5. Final GPU Lowering: Mapping to GPU Blocks/Threads

The final stage maps computations to GPU thread blocks, introduces shared memory buffers, and replaces operations with hardware-specific intrinsics:

```mlir
gpu.launch blocks(%grid_x, %grid_y, %c1) threads(%block_x, %block_y, %c1) {
  %q_tile = memref.alloc() : memref<16x16xf32, #gpu.address_space<workgroup>>
  %k_tile = memref.alloc() : memref<16x16xf32, #gpu.address_space<workgroup>>
  nvgpu.device_async_copy %q_global, %q_tile
  nvgpu.device_async_wait
  %acc = nvgpu.mma.sync %q_tile, %k_tile, %acc_init : tensor<16x16xf32>
  gpu.barrier
}
```

Here, we:

- Allocate **shared memory tiles** (`memref.alloc` in `workgroup` memory space).
- **Insert async memory copies** from global to shared memory (`nvgpu.device_async_copy`).
- **Use Tensor Core instructions** (`nvgpu.mma.sync`) for efficient matrix multiplications.
- Introduce **synchronization** (`gpu.barrier`).

## Conclusion

This pipeline enables efficient compilation of Transformer decoder computations for GPUs. By leveraging MLIR’s **structured dialects**, **tiling**, **vectorization**, and **hardware mapping**, we systematically lower high-level computations into efficient GPU kernels. The final MLIR IR is ready for conversion into **LLVM/NVPTX** for execution on NVIDIA GPUs, ensuring high performance and optimized memory usage.

