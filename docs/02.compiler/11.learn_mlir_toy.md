---
title: MLIR TOY Tutorial
date: 2024-12-10
permalink: /pages/000011/
---

### Chapter 1. Toy Language and AST
Lexer and recursive descent parser construt AST

### Chapter 2. Emit Basic MLIR

Operations: instructions, globals(functions), modules, in LLVM

![image](https://github.com/user-attachments/assets/1547200e-1e13-4075-b202-77be37cf0b74)
From [link](https://medium.com/sniper-ai/mlir-tutorial-create-your-custom-dialect-lowering-to-llvm-ir-dialect-system-1-1f125a6a3008)

Transpose Operation

```
%t_tensor = "toy.transpose"(%tensor) {inplace = true} : (tensor<2x3xf64>) ->
tensor<3x2xf64> loc("example/file/path":12:1)
```

meaning of each part

```
result  = name of operation (input operands) dictionary of traits :
type of operations (input and output), location
```

- A name for the operation.
- A list of SSA operand values.
- A list of attributes.
- A list of types for result values.
- A source location for debugging purposes.
- A list of successors blocks (for branches, mostly).
- A list of regions (for structural operations like functions).

**Opaque API**


**Define a Toy Dialect**

dialect could be defined by c++ or tablegen(declarative specification).

after the definiation, it could be loaded to MLIR Context.
context.loadDialect<ToyDialect>();

**Defining Toy Operations**

```
class ConstantOp : public mlir::Op<
                     /// `mlir::Op` is a CRTP class, meaning that we provide the
                     /// derived class as a template parameter.
                     ConstantOp,
                     /// The ConstantOp takes zero input operands.
                     mlir::OpTrait::ZeroOperands,
                     /// The ConstantOp returns a single result.
                     mlir::OpTrait::OneResult,
                     /// We also provide a utility `getType` accessor that
                     /// returns the TensorType of the single result.
                     mlir::OpTraits::OneTypedResult<TensorType>::Impl> {

 public:
  /// Inherit the constructors from the base Op class.
  using Op::Op;
  ...
  static void build(mlir::OpBuilder &builder, mlir::OperationState &state,
                    mlir::Type result, mlir::DenseElementsAttr value);
```

register operation:

```
void ToyDialect::initialize() {
  addOperations<ConstantOp>();
}
```

**Op vs Operation: Using MLIR Operations**

**Using Operation Definition Specification Framwork**

base Toy_Op

```
class Toy_Op<string mnemonic, list<Trait> traits = []> :
    Op<Toy_Dialect, mnemonic, traits>;
```

ConstantOp

```
def ConstantOp : Toy_Op<"constant"> {
}
```

**Attaching build Methods**

In ConstantOp, it declared a list of build. ODS will generate the first build.\
As to other builds, we have to atttach.
```
def ConstantOp : Toy_Op<"constant"> {
  ...

  // Add custom build methods for the constant operation. These methods populate
  // the `state` that MLIR uses to create operations, i.e. these are used when
  // using `builder.create<ConstantOp>(...)`.
  let builders = [
    // Build a constant with a given constant tensor value.
    OpBuilder<(ins "DenseElementsAttr":$value), [{
      // Call into an autogenerated `build` method.
      build(builder, result, value.getType(), value);
    }]>,

    // Build a constant with a given constant floating-point value. This builder
    // creates a declaration for `ConstantOp::build` with the given parameters.
    OpBuilder<(ins "double":$value)>
  ];
}
```

**Specifying a Custom Assembly Format**

The printout version of IR has too much information.

We can strip out by implementing our owne oversion of print and parse function.

Take PrintOp as example.

```
void PrintOp::print(mlir::OpAsmPrinter &printer) {
  printer << "toy.print " << op.input();
  printer.printOptionalAttrDict(op.getAttrs());
  printer << " : " << op.input().getType();
}

mlir::ParseResult PrintOp::parse(mlir::OpAsmParser &parser,
                                 mlir::OperationState &result) {
...
}
```

### Chapter 3. High-level Language-Specific Analysis and Transformation

- Imperative, C++ Pattern match and Rewrite
- Decalrative, rule-based pattern-match and rewrite using table-driven

**C++**

```
/// Fold transpose(transpose(x)) -> x
struct SimplifyRedundantTranspose : public mlir::OpRewritePattern<TransposeOp> {
  SimplifyRedundantTranspose(mlir::MLIRContext *context)
      : OpRewritePattern<TransposeOp>(context, /*benefit=*/1) {}
  llvm::LogicalResult
  matchAndRewrite(TransposeOp op,
                  mlir::PatternRewriter &rewriter) const override {
    // Look through the input of the current transpose.
  ....
  }
};

// Register our patterns for rewrite by the Canonicalization framework.
void TransposeOp::getCanonicalizationPatterns(
    RewritePatternSet &results, MLIRContext *context) {
  results.add<SimplifyRedundantTranspose>(context);
}

// Add into Pass Manager
mlir::PassManager pm(module->getName());
pm.addNestedPass<mlir::toy::FuncOp>(mlir::createCanonicalizerPass());
```

**rule-based pattern-match and rewrite (DRR)**

```
def TypesAreIdentical : Constraint<CPred<"$0.getType() == $1.getType()">>;
def RedundantReshapeOptPattern : Pat<
  (ReshapeOp:$res $arg), (replaceWithValue $arg),
  [(TypesAreIdentical $res, $arg)]>;
```

### Chapter 4. Enabling Generic Transformation with Interfaces

If you still remember correctly, in last chapter, we register *SimplifyRedundantTranspose* into getCanonicalizationPatterns\
which *applies transformations defined by operations in a greedy, iterative manner*. 

```
void TransposeOp::getCanonicalizationPatterns(
    RewritePatternSet &results, MLIRContext *context) {
  results.add<SimplifyRedundantTranspose>(context);
}
```

This is not scalable.

**Shape Inference: Preparing for Code Generation**

This starts from an example of inlining all function calls to perform intraprocedural shape propagation.

**Inlining**

MLIR provide a generic inliner algorithm that dialects can plug into.

In toy, we need to provide interfaces so the inliner could hook into.

1. The first, we need to define constraints on inlining operations.

Define a interface, which is a class containing a set of virtual hooks which the dialects can override.

```
struct ToyInlinerInterface : public DialectInlinerInterface {
  using DialectInlinerInterface::DialectInlinerInterface;
  ...
  // check whether the region is able to inline
  isLegalToInline();
  ...
  void handleTerminator(Operation *op,
                        MutableArrayRef<Value> valuesToRepl)
  ...
}
```

2. Register dialect interface on to TOY dialect.

```
void ToyDialect::initialize() {
  addInterfaces<ToyInlinerInterface>();
}
```

3. We need a way to inform inliner that toy.generic_call is a *call* and toy.func is *function*.

We achieve this goal by operation interface, marking they are callable or callop.

```
include "mlir/Interfaces/CallInterfaces.td"

def FuncOp : Toy_Op<"func",
    [DeclareOpInterfaceMethods<CallableOpInterface>]> {
  ...
}

def GenericCallOp : Toy_Op<"generic_call",
    [DeclareOpInterfaceMethods<CallOpInterface>]> {
  ...
}
```

The above *DeclareOpInterfaceMethods* directive auto-declares all of the interface methods in the class declaration of GenericCallOp.

Then we need to fill in the definitions:

```
...
Region *FuncOp::getCallableRegion()
...
CallInterfaceCallable GenericCallOp::getCallableForCallee()
...
GenericCallOp::setCalleeFromCallable(CallInterfaceCallable callee)
```

4. Now the inliner has been informed about the Toy dialect.

add the inlinerpasser to pass manager
```
pm.addPass(mlir::createInlinerPass());
```

5. Considering the input of the transpose function is different from argument.

Then they define a cast Op to cast between different shapes.

```
def CastOp : Toy_Op<"cast", [
    DeclareOpInterfaceMethods<CastOpInterface>,
    Pure,
    SameOperandsAndResultShape]

 > {
...
}
```

Notice that CastOp add CastOpInterface into traits lists.


```
/// Returns true if the given set of input and result types are compatible with
/// this cast operation. This is required by the `CastOpInterface` to verify
/// this operation and provide other additional utilities.
bool CastOp::areCastCompatible(TypeRange inputs, TypeRange outputs) {
...
}
```

```
struct ToyInlinerInterface : public DialectInlinerInterface {
  ...

  /// Attempts to materialize a conversion for a type mismatch between a call
  /// from this dialect, and a callable region. This method should generate an
  /// operation that takes 'input' as the only operand, and produces a single
  /// result of 'resultType'. If a conversion can not be generated, nullptr
  /// should be returned.
  Operation *materializeCallConversion(OpBuilder &builder, Value input,
                                       Type resultType,
                                       Location conversionLoc) const final {
    return builder.create<CastOp>(conversionLoc, resultType, input);
  }
};
```

Then, the output is as expected.

```
toy.func @main() {
  %0 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>
  %1 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>
  %2 = toy.cast %1 : tensor<2x3xf64> to tensor<*xf64>
  %3 = toy.cast %0 : tensor<2x3xf64> to tensor<*xf64>
  %4 = toy.transpose(%2 : tensor<*xf64>) to tensor<*xf64>
  %5 = toy.transpose(%3 : tensor<*xf64>) to tensor<*xf64>
  %6 = toy.mul %4, %5 : tensor<*xf64>
  toy.print %6 : tensor<*xf64>
  toy.return
}
```

**Intraprocedural Shape Inference**

Current state: all function has been inlined and mixed with static and dynamic shaped operations.

The shape propagation should be generic to many dialects.

*Operation Interface*

We could define this by:

```
def ShapeInferenceOpInterface : OpInterface<"ShapeInference"> {
  let description = [{
    Interface to access a registered method to infer the return types for an
    operation that can be used during type inference.
  }];

  let methods = [
    InterfaceMethod<"Infer and set the output shape for the current operation.",
                    "void", "inferShapes">
  ];

}
```

Then we add interface to necessary Toy Operations.

This is simple to add *CallOpInterface* to GenericCallOp.

```
def MulOp : Toy_Op<"mul",
    [..., DeclareOpInterfaceMethods<ShapeInferenceOpInterface>]> {
  ...
}
```

As to the operations has been added the interface, we need to provide definition of the method.

```
void MulOp::inferShapes() { getResult().setType(getLhs().getType()); }
```

```
class ShapeInferencePass
    : public mlir::PassWrapper<ShapeInferencePass, OperationPass<FuncOp>> {
  void runOnOperation() override {
    FuncOp function = getOperation();
    ...
  }
};
```

*Helper method*
```
std::unique_ptr<mlir::Pass> mlir::toy::createShapeInferencePass() {
  return std::make_unique<ShapeInferencePass>();
}
```

```
pm.addPass(mlir::createShapeInferencePass());
```

Then the output will be like this:

```
toy.func @main() {
  %0 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>
  %1 = toy.transpose(%0 : tensor<2x3xf64>) to tensor<3x2xf64>
  %2 = toy.mul %1, %1 : tensor<3x2xf64>
  toy.print %2 : tensor<3x2xf64>
  toy.return
}
```

### Chapter 5. Partial Lowering to Lower-Level Dialects for Optimization

**Dialect Conversions**

1. Conversion Target
  - formal specification of what operations or dialects are legal for the conversion
  - Operations that arenâ€™t legal will require rewrite patterns to perform legalization.
2. Rewrite Patterns
  -  set of patterns used to convert illegal operations into a set of zero or more legal ones.
3. Type Convert

#### Conversion Target

Convert Toy Operations into combination of operations from:
- Affine
- Arith
- Func
- MemRef

Then we set *ToyDialect* to be illegal. So it will be lowered.

And print will be dynamically legal to keep it untouched.


```
void ToyToAffineLoweringPass::runOnOperation() {
  target.addLegalDialect<affine::AffineDialect,
                         arith::ArithDialect,
                         func::FuncDialect,
                         memref::MemRefDialect>();

  target.addIllegalDialect<ToyDialect>();
  target.addDynamicallyLegalOp<toy::PrintOp>([](toy::PrintOp op) {
    return llvm::none_of(op->getOperandTypes(),
                         [](Type type) { return type.isa<TensorType>(); });
  });
}
```

#### Conversion Patterns

For operator that needs lowering, define **matchAndRewrite**.

```
/// Lower the `toy.transpose` operation to an affine loop nest.
struct TransposeOpLowering : public mlir::ConversionPattern {
  TransposeOpLowering(mlir::MLIRContext *ctx)
  llvm::LogicalResult
  matchAndRewrite(mlir::Operation *op, ArrayRef<mlir::Value> operands,
                  mlir::ConversionPatternRewriter &rewriter) const final {
  ...
   return rewriter.create<mlir::AffineLoadOp>(loc, input, reverseIvs);
  }
}
```

```
void ToyToAffineLoweringPass::runOnOperation() {
  ...

  // Now that the conversion target has been defined, we just need to provide
  // the set of patterns that will lower the Toy operations.
  mlir::RewritePatternSet patterns(&getContext());
  patterns.add<..., TransposeOpLowering>(&getContext());

  ...
```


#### Partial Lowering

```
void ToyToAffineLoweringPass::runOnOperation() {
  ...
  mlir::applyPartialConversion(getOperation(), target, patterns));
  ...
}
```

**Design Considerations with Partial Lowering**

In this lowering, we lower from value-type(TensorType) to allcoate(or buffer)-type(MemRefType).

However, toy.print function is not designed for allocate-type from beginning.

Three ways to solve this:

- Generate load operations from the buffer
- Generate a new version of toy.print that operates on the lowered type
- Update toy.print to allow for operating on the lowered type

The third way is chosen due to its simplicity.

```
def PrintOp : Toy_Op<"print"> {
  ...

  // The print operation takes an input tensor to print.
  // We also allow a F64MemRef to enable interop during partial lowering.
  let arguments = (ins AnyTypeOf<[F64Tensor, F64MemRef]>:$input);
}
```

#### Complete Toy Example
<details>
<summary>Code</summary>
       
```
func.func @main() {
  %cst = arith.constant 1.000000e+00 : f64
  %cst_0 = arith.constant 2.000000e+00 : f64
  %cst_1 = arith.constant 3.000000e+00 : f64
  %cst_2 = arith.constant 4.000000e+00 : f64
  %cst_3 = arith.constant 5.000000e+00 : f64
  %cst_4 = arith.constant 6.000000e+00 : f64

  // Allocating buffers for the inputs and outputs.
  %0 = memref.alloc() : memref<3x2xf64>
  %1 = memref.alloc() : memref<3x2xf64>
  %2 = memref.alloc() : memref<2x3xf64>

  // Initialize the input buffer with the constant values.
  affine.store %cst, %2[0, 0] : memref<2x3xf64>
  affine.store %cst_0, %2[0, 1] : memref<2x3xf64>
  affine.store %cst_1, %2[0, 2] : memref<2x3xf64>
  affine.store %cst_2, %2[1, 0] : memref<2x3xf64>
  affine.store %cst_3, %2[1, 1] : memref<2x3xf64>
  affine.store %cst_4, %2[1, 2] : memref<2x3xf64>

  // Load the transpose value from the input buffer and store it into the
  // next input buffer.
  affine.for %arg0 = 0 to 3 {
    affine.for %arg1 = 0 to 2 {
      %3 = affine.load %2[%arg1, %arg0] : memref<2x3xf64>
      affine.store %3, %1[%arg0, %arg1] : memref<3x2xf64>
    }
  }

  // Multiply and store into the output buffer.
  affine.for %arg0 = 0 to 3 {
    affine.for %arg1 = 0 to 2 {
      %3 = affine.load %1[%arg0, %arg1] : memref<3x2xf64>
      %4 = affine.load %1[%arg0, %arg1] : memref<3x2xf64>
      %5 = arith.mulf %3, %4 : f64
      affine.store %5, %0[%arg0, %arg1] : memref<3x2xf64>
    }
  }

  // Print the value held by the buffer.
  toy.print %0 : memref<3x2xf64>
  memref.dealloc %2 : memref<2x3xf64>
  memref.dealloc %1 : memref<3x2xf64>
  memref.dealloc %0 : memref<3x2xf64>
  return
}
```
</details>

#### Taking Advantage of Affine Optimization

Add additional passes to the pipeline to reduce redundant loads.

- LoopFusion
- AffineScalarReplacement

### Chapter 6.Lowering to LLVM and CodeGeneration

#### Conversion Target

```
  mlir::ConversionTarget target(getContext());
  target.addLegalDialect<mlir::LLVMDialect>();
  target.addLegalOp<mlir::ModuleOp>();
```

#### Type Converter

This lowering transform the MemRef types which are currently being operated on into a representation in LLVM.

```
  LLVMTypeConverter typeConverter(&getContext());
```

#### Conversion Patterns 

*affine*, *arith* and *std* has provide set of patterns needed to lower them into llvm dialect

```
  mlir::RewritePatternSet patterns(&getContext());
  mlir::populateAffineToStdConversionPatterns(patterns, &getContext());
  mlir::cf::populateSCFToControlFlowConversionPatterns(patterns, &getContext());
  mlir::arith::populateArithToLLVMConversionPatterns(typeConverter,
                                                          patterns);
  mlir::populateFuncToLLVMConversionPatterns(typeConverter, patterns);
  mlir::cf::populateControlFlowToLLVMConversionPatterns(patterns, &getContext());

  // The only remaining operation, to lower from the `toy` dialect, is the
  // PrintOp.
  patterns.add<PrintOpLowering>(&getContext());
```

#### Full Lowering

```
mlir::ModuleOp module = getOperation();
if (mlir::failed(mlir::applyFullConversion(module, target, patterns)))
  signalPassFailure();
```

The generated llvm dialect:

<details>
<summary>Code</summary>
       
```
llvm.func @free(!llvm<"i8*">)
llvm.func @printf(!llvm<"i8*">, ...) -> i32
llvm.func @malloc(i64) -> !llvm<"i8*">
llvm.func @main() {
  %0 = llvm.mlir.constant(1.000000e+00 : f64) : f64
  %1 = llvm.mlir.constant(2.000000e+00 : f64) : f64

  ...

^bb16:
  %221 = llvm.extractvalue %25[0] : !llvm<"{ double*, i64, [2 x i64], [2 x i64] }">
  %222 = llvm.mlir.constant(0 : index) : i64
  %223 = llvm.mlir.constant(2 : index) : i64
  %224 = llvm.mul %214, %223 : i64
  %225 = llvm.add %222, %224 : i64
  %226 = llvm.mlir.constant(1 : index) : i64
  %227 = llvm.mul %219, %226 : i64
  %228 = llvm.add %225, %227 : i64
  %229 = llvm.getelementptr %221[%228] : (!llvm."double*">, i64) -> !llvm<"f64*">
  %230 = llvm.load %229 : !llvm<"double*">
  %231 = llvm.call @printf(%207, %230) : (!llvm<"i8*">, f64) -> i32
  %232 = llvm.add %219, %218 : i64
  llvm.br ^bb15(%232 : i64)

  ...

^bb18:
  %235 = llvm.extractvalue %65[0] : !llvm<"{ double*, i64, [2 x i64], [2 x i64] }">
  %236 = llvm.bitcast %235 : !llvm<"double*"> to !llvm<"i8*">
  llvm.call @free(%236) : (!llvm<"i8*">) -> ()
  %237 = llvm.extractvalue %45[0] : !llvm<"{ double*, i64, [2 x i64], [2 x i64] }">
  %238 = llvm.bitcast %237 : !llvm<"double*"> to !llvm<"i8*">
  llvm.call @free(%238) : (!llvm<"i8*">) -> ()
  %239 = llvm.extractvalue %25[0] : !llvm<"{ double*, i64, [2 x i64], [2 x i64] }">
  %240 = llvm.bitcast %239 : !llvm<"double*"> to !llvm<"i8*">
  llvm.call @free(%240) : (!llvm<"i8*">) -> ()
  llvm.return
}
```
</details>
