---
title: MLIR Linalg Dialect
date: 2025-03-04
permalink: /pages/000019/
---

# MLIR Linalg Dialect

> This is generated by ChatGPT.

The Linalg dialect in MLIR (Multi-Level Intermediate Representation) is a structured abstraction for expressing operations on tensors, buffers, and memories in a way that facilitates transformations and optimizations.

It is crucial in MLIR’s progressive lowering strategy, acting as a bridge between high-level tensor algebra and low-level machine-specific operations.

## 1. Function of the Linalg Dialect
The Linalg dialect serves as a powerful intermediate representation for tensor and buffer computations in MLIR. It provides a structured way to express and optimize linear algebra operations while facilitating progressive lowering to lower-level representations.

### 1. Core Operations
The dialect supports several high-level operations:
- Matrix operations (matmul, convolutions)
- Elementwise computations (add, multiply)
- Reduction operations (sum, min, max)
- Generic structured operations


```mlir
// Example of a generic LINALG operation
linalg.generic {
  indexing_maps = [
    affine_map<(i,j) -> (i,j)>,   // Input matrix
    affine_map<(i,j) -> (i,j)>    // Output matrix
  ],
  iterator_types = ["parallel", "parallel"]
} ins(%input : tensor<4x4xf32>) 
  outs(%output : tensor<4x4xf32>) {
  ^bb0(%in: f32, %out: f32):
    // Computation body
    %result = some_computation(%in)
    linalg.yield %result : f32
}
```

### 2. Key Features

#### 2.1 Optimization-Friendly Design
- Enables sophisticated transformations:
  - Tiling and fusion
  - Vectorization
  - Distribution across compute units
- Provides analyzable computation patterns
- Supports performance optimization strategies

#### 2.2 Progressive Lowering Support
- Systematic lowering to loops and vectors
- Conversion to hardware-specific instructions
- Flexible targeting of different architectures

#### 2.3 Rich Interoperability
- Seamless integration with other MLIR dialects:
  - Tensor dialect for tensor operations
  - MemRef dialect for memory operations
  - Affine dialect for loop transformations
  - SCF dialect for control flow

### 3. Design Principles

#### 3.1 Abstraction Level
- Captures high-level semantic intent
- Preserves optimization opportunities
- Maintains transformation flexibility

#### 3.2 Implementation Characteristics
- Parametric operation support
- Structured computation representation
- Transformation-friendly design
- Multi-level optimization capability


## 2. Principles of the Linalg Dialect
The design principles of the Linalg dialect revolve around structured operations and progressive lowering.

### 2.1. Structured Operations
Linalg operations are loop nests over multi-dimensional data structures.
They define explicit iteration spaces and access patterns.
Example operations include:
- `linalg.matmul`
- `linalg.conv_2d`
- `linalg.reduce`

### 2.2. Progressive Lowering Strategy
Linalg serves as an intermediate step in the lowering pipeline:

- High-Level IR (TOSA, Tensor, MHLO, etc.) → Lowered to Linalg for structured optimization.
- Linalg Transformations (tiling, fusion, etc.) → Optimized at the Linalg level.
- Lowering to Loops, SCF, and Vector Dialect → Further optimized and hardware-aware transformations applied.
- Lowering to LLVM Dialect and Machine Code → Final code generation via LLVM or other backends.

## 3. Cooperation with Other Dialects
Linalg works closely with multiple MLIR dialects:

### 3.1. Tensor Dialect
Linalg operates on tensor values.
Tensor-level transformations like bufferization convert tensor operations into memref operations.
Example:
```mlir
%A = tensor.from_memref %A_mem : memref<4x4xf32> -> tensor<4x4xf32>
```

### 3.2. MemRef Dialect
When moving to a memory-aware representation, Linalg lowers operations from tensor to memref.
Example:
```mlir
%A_mem = memref.alloc() : memref<4x4xf32>
```

### 3.3. SCF (Structured Control Flow) Dialect
Linalg operations can be lowered into explicit loops using SCF.
Example: Lowering `linalg.matmul` to SCF loops:
```mlir
scf.for %i = 0 to 4 {
  scf.for %j = 0 to 4 {
    scf.for %k = 0 to 4 {
      %prod = arith.mulf %A[%i, %k], %B[%k, %j] : f32
      %sum = arith.addf %C[%i, %j], %prod : f32
      memref.store %sum, %C[%i, %j] : memref<4x4xf32>
    }
  }
}
```

### 3.4. Affine Dialect
Affine dialect provides advanced loop and memory access transformations.

When lowering to hardware-specific memory layouts, Linalg may use Affine transformations for loop optimizations.

Example:
```mlir
affine.for %i = 0 to 4 {
  affine.for %j = 0 to 4 {
    affine.for %k = 0 to 4 {
      %prod = arith.mulf %A[%i, %k], %B[%k, %j] : f32
      %sum = arith.addf %C[%i, %j], %prod : f32
      affine.store %sum, %C[%i, %j] : memref<4x4xf32>
    }
  }
}
```

### 3.5. Vector Dialect
Linalg lowering can introduce vectorized operations.
Example: linalg.matmul lowering to vector.contract:
```mlir
%C = vector.contract {indexing_maps = [affine_map<(m, n, k) -> (m, k)>,
                                      affine_map<(m, n, k) -> (k, n)>,
                                      affine_map<(m, n, k) -> (m, n)>],
                      iterator_types = ["parallel", "parallel", "reduction"]}
```

## 4. Example: Linalg Dialect in Action
Here’s an example MLIR program using linalg.matmul:

```mlir
func.func @matmul(%A: tensor<4x4xf32>, %B: tensor<4x4xf32>, %C: tensor<4x4xf32>) -> tensor<4x4xf32> {
  %result = linalg.matmul ins(%A, %B : tensor<4x4xf32>, tensor<4x4xf32>)
                         outs(%C : tensor<4x4xf32>) -> tensor<4x4xf32>
  return %result : tensor<4x4xf32>
}
```

### Lowering Steps

- Linalg Dialect
  - Uses structured linalg.matmul for clear semantics.
- Lower to SCF (Loops)
  - Transforms linalg.matmul into explicit nested loops.
- Lower to Affine or Vector Dialect
  - Optimizes for CPU or GPU execution.
- Lower to LLVM Dialect
  - Converts into final machine-executable code.

## Conclusion
The Linalg dialect is a structured and optimization-friendly representation in MLIR, bridging high-level tensor computations and low-level execution.

It works alongside the `Tensor`, `MemRef`, `SCF`, `Affine`, and `Vector` dialects to progressively lower operations into efficient machine code.

By leveraging tiling, fusion, vectorization, and hardware mapping, Linalg plays a critical role in modern compiler optimization.