---
title: Learn TVM (1)
date: 2024-12-08
permalink: /pages/000009/
---

---
## 0. Feel the flow of TVM compilation

**Model Definition**
```
import tvm
from tvm import relay, te
import numpy as np

# Model parameters
batch_size, input_dim, output_dim = 32, 128, 64

# Relay model
x = relay.var("x", shape=(batch_size, input_dim), dtype="float32")
w = relay.var("w", shape=(input_dim, output_dim), dtype="float32")
y = relay.nn.dense(x, w)
model = relay.Function([x, w], y)

# Input data
x_data = np.random.rand(batch_size, input_dim).astype("float32")
w_data = np.random.rand(input_dim, output_dim).astype("float32")
params = {"w": w_data}
```

**Relay IR**
The relay.Function represents the high-level computational graph.

```
print(model)

# Simplified Relay IR:
# fn (%x: Tensor[(32, 128), float32], %w: Tensor[(128, 64), float32]) {
#   nn.dense(%x, %w)
# }

```
**Lowering to Tensor Expression (TE)**


```
with tvm.transform.PassContext(opt_level=3):
    mod, params = relay.build_module.bind_params_by_name(model, params)
    graph, lib, params = relay.build(mod, target="cuda", params=params)
```

In Tensor Expression (TE), computations are represented using tensor operations:
- Compute: C[i, j] = sum(A[i, k] * B[k, j] for k in range(input_dim))
- Schedule: Operations like tiling, thread binding, and vectorization are applied.


**Example TE for matrix multiplication:**
```
A = te.placeholder((batch_size, input_dim), name="A")
B = te.placeholder((input_dim, output_dim), name="B")
k = te.reduce_axis((0, input_dim), name="k")

# Compute definition
C = te.compute(
    (batch_size, output_dim),
    lambda i, j: te.sum(A[i, k] * B[k, j], axis=k),
    name="C"
)

```

**TIR (Tensor IR)**

After applying schedules, TE is lowered to TIR. TIR is a low-level representation focusing on loops and memory hierarchy.

Example TIR (simplified):
```
@tvm.script.ir_module
class MyModule:
    @tvm.tir.prim_func
    def main(A: tvm.tir.Buffer[(32, 128), "float32"],
             B: tvm.tir.Buffer[(128, 64), "float32"],
             C: tvm.tir.Buffer[(32, 64), "float32"]):
        for i in range(32):  # Outer loop for batch
            for j in range(64):  # Outer loop for output_dim
                with tvm.tir.block("C"):
                    C[i, j] = 0.0
                    for k in range(128):  # Reduction loop for input_dim
                        C[i, j] += A[i, k] * B[k, j]

```

**CUDA Code Generation**

Finally, TIR is compiled into CUDA code:
```
print(lib.imported_modules[0].get_source())

# Simplified CUDA Code:
# __global__ void fused_dense(float* __restrict__ A, float* __restrict__ B, float* __restrict__ C) {
#   int idx = threadIdx.x + blockIdx.x * blockDim.x;
#   if (idx < 2048) {  // 32 * 64 = batch_size * output_dim
#     int i = idx / 64;  // Batch index
#     int j = idx % 64;  // Output index
#     float result = 0.0;
#     for (int k = 0; k < 128; ++k) {  // Reduction loop
#       result += A[i * 128 + k] * B[k * 64 + j];
#     }
#     C[idx] = result;
#   }
# }

```
**Summary of Intermediate Representations**

1. Relay IR: High-level computational graph, defines operators like nn.dense.
2. TE: Abstracts computation using mathematical tensor operations and supports scheduling primitives.
3. TIR: Low-level, loop-based representation with explicit memory hierarchy.
4. CUDA Code: GPU kernel for matrix multiplication, including thread and block mappings.

---

### 0.1 Model Parsing and Relay IR Construction
- Input
Model in high-level frameworks like TensorFlow, PyTorch, or ONNX.
- Process:
  - TVM parses the input model and converts it into Relay IR, a hig-h-level intermediate representation.
  - The Relay IR describes the computational graph with operator-level abstractions. 
- Key Functions:
relay.frontend.from_pytorch(), relay.frontend.from_onnx() in src/relay/frontend/.

### 0.2 High-Level Optimizations in Relay
- Input: Relay IR.
- Process:
  - Optimize the Relay IR for performance and hardware compatibility through:
  - Operator Fusion: Fuse adjacent operations.
  - Constant Folding: Precompute static expressions.
  - Layout Transformation: Adjust data layouts (e.g., NCHW → NCHWc).
  - Quantization: Lower precision where applicable.
  - Common Subexpression Elimination.
Finalize the optimized Relay graph.
- Key Functions:\
src/relay/transforms/ for passes like fuse_ops.cc, alter_op_layout.cc.
### 0.3. Lowering to Tensor Expression (TE)
- Input: Optimized Relay IR.
- Process:
  - Translate high-level Relay operators into Tensor Expressions (TE).
  - TE represents computations as mathematical tensor operations and allows for:
    - Abstraction of computation patterns (e.g., matrix multiplication).
    - Introduction of scheduling primitives (e.g., tiling, unrolling, vectorization).
- Key Functions:
  - src/relay/backend/te_compiler.cc: Bridges Relay IR and TE.
  - src/te/tensor.cc: Constructs tensor expressions.
### 0.4. Scheduling in TE
- Input: Tensor Expressions.
- Process:
  - Apply scheduling primitives to improve performance:
  - Tiling: Divide tensors into smaller chunks for parallelism.
  - Unrolling: Optimize loops for instruction pipelining.
  - Thread/Block Mapping: Map computations to GPU threads and blocks.
  - Vectorization: Use SIMD instructions where applicable.
  - Refines Tensor Expressions into Tensor Intermediate Representation (TIR).
- Key Functions:
  - src/te/schedule/ for scheduling functions.
  - src/te/schedule/schedule_dataflow_rewrite.cc: Handles dataflow rewrite scheduling.
### 0.5. Lowering to TIR
- Input: Tensor Expressions with schedules.
- Process:
  - Convert TE into Tensor IR (TIR), a low-level IR closer to device execution.
  - Perform device-specific optimizations for CUDA (e.g., thread hierarchy mapping).
- Key Functions:
  - src/tir/transform/ for device-specific passes like loop unrolling and thread binding.
### 0.6. Code Generation
- Input: TIR optimized for CUDA.
- Process:
  - Code Generation:Translate TIR into CUDA kernels. Use TVM's built-in CUDA code generator.
  - Calling cuBLAS/cuDNN or CUTLASS:
      - For specific operations (e.g., GEMM), call external libraries.
      - Determine the sequence of library calls and parameters based on operator attributes.
  - Memory Allocation: Analyze dataflow to allocate memory efficiently on GPU.
- Key Functions:
  - CUDA Codegen:
src/target/source/codegen_cuda.cc: Generates CUDA source code.
  - External Libraries:
src/runtime/contrib/cublas.cc: Integrates with cuBLAS.
src/runtime/contrib/cudnn.cc: Integrates with cuDNN.
src/contrib/cutlass/: Integrates with CUTLASS.
### 0.7. Final Compilation and Deployment
- Input: CUDA source code.
- Process:
  - Compile CUDA source code using NVCC or the TVM runtime.
  - Deploy the compiled kernel and runtime modules.
- Key Functions:
  - src/target/source/: Handles code generation.
  - src/runtime/: Manages runtime execution and deployment.


---
## 1. Model Parsing and Relay IR Construction

In TVM, high-level optimization in the Relay IR phase includes several graph-level optimizations, data layout transformations, and other functional passes.

These optimizations are implemented in various source files under src/relay/transform and src/relay/op directories.

Below is a categorized list of these optimizations along with their corresponding source code files and functions:

### 1.1 Graph-Level Optimizations

Graph-level optimizations restructure or simplify the computation graph for better performance.

|Optimization	Source |File	| Key Functions/Classes|
|---|---|---|
|Constant Folding|	src/relay/transform/fold_constant.cc	|FoldConstant, ConstantFolder
|Operator Fusion|	src/relay/transform/fuse_ops.cc	|FuseOps, FuseMutator, PatternMatcher
|Dead Code Elimination (DCE)	|src/relay/transform/eliminate_common_subexpr.cc|	EliminateCommonSubexpr
|Common Subexpression Elimination	| src/relay/transform/eliminate_common_subexpr.cc	|EliminateCommonSubexpr
|Simplify Inference	|src/relay/transform/simplify_inference.cc	|SimplifyInference, SimplifyInferenceMutator
|Call Folding	|src/relay/transform/fold_call.cc	|FoldCall
|Inline Functions	|src/relay/transform/inline.cc	|Inline, InlineMutator
|Prune Unused Functions|	src/relay/transform/prune_unused_functions.cc	|PruneUnusedFunctions

### 1.2 Data Layout Transformations
These optimizations adjust the layout of tensors for better memory access patterns and compatibility with target hardware.
|Transformation |File	| Key Functions/Classes|
|---|---|---|
|Alter Layout|src/relay/transform/alter_op_layout.cc|AlterOpLayout, AlterOpLayoutRewriter|
|Convert Layout|s	src/relay/transform/convert_layout.cc|ConvertLayout|
|Fold Scale Axis|src/relay/transform/fold_scale_axis.cc|FoldScaleAxis, ScaleAxisSimplifier|
|Layout Optimization|src/relay/transform/layout_rewrite.cc|LayoutRewrite|

### 1.3 Quantization and Precision Management
TVM supports quantization optimizations for reduced precision operations.

|Optimization |File	| Key Functions/Classes|
|---|---|---|
|Quantize|src/relay/quantize/quantize.cc|Quantize, CreateQuantizePass|
|Dequantize|src/relay/quantize/dequantize.cc|Dequantize|
|SimplifyQuantize|src/relay/transform/simplify_quantize.cc|SimplifyQuantize, SimplifyQuantizeRewriter|

### 1.4 Automatic Differentiation
TVM includes an autodiff system for neural networks.

|Transformation |File	| Key Functions/Classes|
|---|---|---|
|Reverse Mode Autodiff|src/relay/transforms/gradient.cc	|AutomaticDifferentiation, ReverseAD|

### 1.5 High-Level Hardware-Aware Optimizations
These optimizations modify operations based on the target hardware.

|Optimization |File	| Key Functions/Classes|
|---|---|---|
|Annotate Target|src/relay/transform/annotate_target.cc|AnnotateTarget|
|Partition Graph|src/relay/transform/partition_graph.cc|PartitionGraph|
|Merge Compiler Regions|src/relay/transform/merge_compiler_regions.cc|MergeCompilerRegions|

### 1.6 Device Placement
These passes assign operations to devices for heterogeneous execution.
|Transformation |File	| Key Functions/Classes|
|---|---|---|
|Rewrite Annotated Ops|src/relay/transform/rewrite_annotated_ops.cc|	RewriteAnnotatedOps|
|Device Annotation	|src/relay/transform/device_annotation.cc	|DeviceAnnotation|

### 1.7 Meta-Pass Management
Relay provides a meta-pass system to manage and sequence passes.

|Meta-Pass |File	| Key Functions/Classes|
|---|---|---|
|Sequential Pass Manager	|src/relay/transform/sequential.cc|	Sequential, PassManager
|Pass Context	|src/relay/transform/pass.cc|	PassContext, WithPassContext

---

## 2 Lowering to Tensor Expression (TE), Scheduing in TE and Lowering into TIR

The lowering process from Relay IR to Tensor Expression (TE) and Tensor IR (TIR) in TVM involves multiple phases.

These include converting Relay IR to TE, applying tensor computation abstractions, performing scheduling, and constructing device-specific low-level TIR.

Here’s a detailed breakdown of the corresponding TVM source code files and functions for these stages:

### 2.1 Converting Relay IR to Tensor Expression (TE)
This phase converts high-level Relay IR into the computation abstractions provided by TE.

|Process |File	| Key Functions/Classes|
|---|---|---|
|Relay to TE Lowering|	src/relay/backend/te_compiler.cc|	LowerToTE, CreateSchedule, ScheduleGetter|
|Operator Strategy|	src/relay/op/strategy/generic.cc|	GenericFunc, OpStrategy|
|Relay to TE Bridge|	src/relay/backend/te_compiler_cache.cc|	TECompiler, LowerTE|
|Shape Function Lowering|	src/relay/backend/te_compiler.cc|	LowerShapeFunc|

Explanation:
- The Relay IR graph is analyzed, and for each operator, TVM retrieves a corresponding TE function using OpStrategy.
- TE functions define high-level operations like matrix multiplication, element-wise addition, etc.

### 2.2 Abstraction of Computation in Tensor Expression (TE)
TE provides a declarative way to express computation. This includes operations like tiling, unrolling, and vectorizing.
|Process |File	| Key Functions/Classes|
|---|---|---|
|Tensor Expression Build|	src/te/operation/create_primfunc.cc|	CreatePrimFunc, ComputeBody, ScheduleOps|
|Compute Definition|	src/te/operation/compute_op.cc|	ComputeOpNode, ComputeOp|
|Tensor Compute Intrinsics|	src/te/operation/tensorize.cc|	Tensorize, CreateIntrinBody|

Explanation:
- High-level computations are abstracted into a declarative format using ComputeOp.
- Intrinsic support for tensorization is added for specialized hardware operations.

### 2.3 Scheduling in Tensor Expression
Scheduling is where TVM optimizes how computations are performed on the target device.
|Process |File	| Key Functions/Classes|
|---|---|---|
|Tile, Unroll, Vectorize	|src/te/schedule/schedule_dataflow_rewrite.cc	|ScheduleDataFlowRewrite, Tile, Unroll, Vectorize|
|Thread and Block Mapping	|src/te/schedule/schedule_lang.cc	|bind, split, reorder, fuse|
|AutoScheduler Interface	|src/auto_scheduler/compute_dag.cc|	ComputeDAG, ApplySteps|
|Lowering Schedule to TIR	|src/te/schedule/graph.cc|	ScheduleGraph, LowerSchedule|

Explanation:
- This phase defines how computations should be split into smaller tiles, mapped to hardware threads and blocks, and optimized using unrolling and vectorization.
- Tensor schedules are converted into lower-level forms through ScheduleGraph.

### 2.4 Constructing Low-Level Tensor IR (TIR)
TIR represents a low-level, device-specific IR used to generate target-specific code.

|Process |File	| Key Functions/Classes|
|---|---|---|
|TIR Construction	|src/tir/stmt_functor.cc|	StmtFunctor, VisitStmt, MakeStmt
|Lowering to TIR	|src/tir/transforms/lower_tir.cc|	LowerTIR, TransformTIR
|Memory Planning	|src/tir/transforms/storage_rewrite.cc|	StorageRewrite, PlanMemory
|Device-Specific TIR	|src/target/codegen.cc|	Build, BuildIRModule

Explanation:
- TE schedules are converted into TIR, which provides explicit control over memory accesses and device-specific optimizations.
- StorageRewrite optimizes memory allocation and reuse.

### 2.5 Device-Specific Optimizations
Device-specific optimizations tailor the generated code for the target platform (e.g., CUDA).

|Transformation |File	| Key Functions/Classes|
|---|---|---|
|Thread/Block Mapping	|src/tir/transforms/thread_storage_sync.cc|	ThreadStorageSync|
|Loop Partitioning	|src/tir/transforms/loop_partition.cc|	LoopPartition|
|Device Codegen	|src/target/source/codegen_cuda.cc|	CodeGenCUDA, PrintKernel|

High-Level Summary of the Workflow
- Relay to TE:\
Converts high-level operations into Tensor Expression (TE) definitions using strategies (src/relay/backend/te_compiler.cc).
- Computation Abstraction:
Defines computations in TE with ComputeOp (src/te/operation/compute_op.cc).
- Scheduling:\
Applies optimizations like tiling, unrolling, and mapping computations to threads/blocks (src/te/schedule/schedule_lang.cc).
- Lowering to TIR:\
Translates the schedule into TIR, which explicitly handles device memory and control flow (src/tir/transforms/lower_tir.cc).
- Device-Specific Codegen:\
Emits target-specific code (e.g., CUDA) via CodeGenCUDA (src/target/source/codegen_cuda.cc).

---
## 3. Code Generation

### 3.1 GPU Code Generation
This phase translates Tensor IR (TIR) into GPU-compatible low-level code, generating CUDA kernels and API calls.

|Process|File	| Key Functions/Classes|
|---|---|---|
|TIR to CUDA Kernel	|src/target/source/codegen_cuda.cc	|CodeGenCUDA, GenerateKernel, PrintStmt|
|CodeGen Base Class	|src/target/source/codegen_c.cc	|CodeGenC, PrintExpr|
|Shared Memory Handling	|src/target/source/codegen_cuda.cc	|PrintStorageScope, PrintStorageSync|
|Thread/Block Synchronization	|src/tir/transforms/thread_storage_sync.cc|	ThreadStorageSync|

**Explanation:**
CodeGenCUDA translates TIR to CUDA kernels, emitting device-side code and managing constructs like thread/block mappings, shared memory, and synchronization.
Synchronization points are inserted using PrintStorageSync.


### 3.2. Kernel Construction
Kernel construction involves creating CUDA device kernels and host-side launcher code to invoke them.

|Process|File	| Key Functions/Classes|
|---|---|---|
|Kernel Emission	|src/target/source/codegen_cuda.cc	|PrintFuncBody, EmitFunction|
|Kernel Launch Code	|src/runtime/cuda/cuda_module.cc	|CUDAWrappedFunc, LaunchKernel|
|Kernel Metadata Management|	src/runtime/module.cc|	PackImports, ExportModule|

Explanation:
The EmitFunction generates kernel function declarations and definitions for execution on the GPU.
Host-side kernel launchers are defined in cuda_module.cc.

### 3.3. cuBLAS/CUTLASS Integration
When using cuBLAS or CUTLASS for tensor computations (e.g., GEMM), TVM generates calls to these libraries instead of writing explicit CUDA kernels.

|Process|File	| Key Functions/Classes|
|---|---|---|
|cuBLAS Integration	|src/runtime/contrib/cublas/cublas.cc|	CUBLASCall, InitCUBLASHandle, GemmOp|
|CUTLASS Integration	|src/contrib/cutlass/gen_cutlass_gemm.cc|GenerateCutlassGemm, EmitCutlassCode|
|External Code Generation	|src/relay/backend/contrib/cublas_codegen.cc|	CUBLASFunction, CodegenCUBLAS|

Explanation:
cublas.cc provides wrappers for cuBLAS API calls like cublasSgemm, with TVM handling data layout transformations as needed.
CUTLASS integration uses template-based code generation in gen_cutlass_gemm.cc, emitting optimized kernels for matrix operations.

### 3.4. Target-Specific Optimizations
Target-specific optimizations fine-tune the generated CUDA code based on the GPU architecture and memory hierarchy.

|Process|File	| Key Functions/Classes|
|---|---|---|
|Thread/Block Mapping	|src/tir/transforms/thread_storage_sync.cc|	ThreadStorageSync, OptimizeThreads|
|Loop Partitioning	|src/tir/transforms/loop_partition.cc|	LoopPartition|
|Memory Planning	|src/tir/transforms/storage_rewrite.cc|	StorageRewrite, PlanMemory|
|Warp-Level Optimization	|src/tir/transforms/vectorize_loop.cc|	VectorizeLoop, Vectorizer|

Explanation:
Thread and block mapping ensures optimal utilization of GPU threads and memory.
Loop partitioning and vectorization optimize data access patterns for warp-level efficiency.
StorageRewrite minimizes memory usage by analyzing reuse patterns and adjusting allocation.

### 3.5. Memory Management
Efficient memory management involves optimizing shared/global memory usage and enabling memory reuse.

|Process|File	| Key Functions/Classes|
|---|---|---|
|Shared Memory Usage	|src/target/source/codegen_cuda.cc	|PrintStorageScope, EmitSharedMemory|
|Memory Allocation	|src/tir/transforms/storage_rewrite.cc	|PlanMemory, ReuseMemory|
|Memory Alignment	|src/target/source/codegen_cuda.cc	|PrintStorageAlloc|

Explanation:
Shared memory scopes are explicitly emitted during CUDA codegen (EmitSharedMemory).
PlanMemory optimizes allocation to minimize fragmentation and overhead.

### 3.6. Overall Codegen Workflow
Key Stages and Their Files

- TIR Lowering:\
File: src/tir/transforms/lower_tir.cc\
Function: LowerTIR, TransformTIR

- CUDA Kernel Emission:\
File: src/target/source/codegen_cuda.cc\
Function: EmitFunction, GenerateKernel

- cuBLAS Integration:\
File: src/runtime/contrib/cublas/cublas.cc\
Function: CUBLASCall, InitCUBLASHandle

- CUTLASS Integration:\
File: src/contrib/cutlass/gen_cutlass_gemm.cc\
Function: GenerateCutlassGemm, EmitCutlassCode

- Target-Specific Optimizations:\
File: src/tir/transforms/thread_storage_sync.cc\
Function: ThreadStorageSync, OptimizeThreads
