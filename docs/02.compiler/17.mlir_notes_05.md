---
title: MLIR Essential Concepts
date: 2025-03-04
permalink: /pages/000017/
---

# MLIR Essential Concepts

> This is generated by ChatGPT.

## MLIR Dialects for GPU Code Generation

**Torch Dialect (Torch-MLIR)**: PyTorch operations are first expressed in the Torch dialect (provided by Torch-MLIR).

This dialect represents PyTorch ops and semantics within MLIR.

From there, a series of lowering passes gradually converts Torch dialect ops into lower-level MLIR dialects like linalg (for linear algebra on tensors), arith (basic arithmetic), math, tensor, etc.

For example, Torch-MLIR provides passes to convert high-level PyTorch ops into linalg or tosa ops​, which are easier to optimize and eventually lower to code. 

The goal is to end up with a combination of dialects that MLIR’s backends can consume (e.g., linalg + scf loops, or mhlo, depending on the chosen path​
).

Once the model is represented in these lower dialects, we can target GPU-specific dialects for code generation.

**GPU Dialect**: MLIR’s GPU dialect provides a mid-level abstraction for GPU kernels and parallel execution, independent of any specific GPU vendor.

It introduces operations for launching kernels (gpu.launch), thread/block IDs, shared memory, etc., similar to CUDA’s programming model​.

Once the model is represented in these lower dialects, we can target GPU-specific dialects for code generation.


The GPU dialect abstracts away the driver or backend details and lets us express parallel loops and memory in a CUDA/OpenCL-like fashion.

For instance, one can map loop nests to GPU grid dimensions (blocks and threads) using passes like `--gpu-map-parallel-loops​`.

The result is GPU dialect IR containing constructs such as `gpu.launch_func` and `gpu.block_id/gpu.thread_id`.​

*NVGPU and NVVM Dialects*: To target NVIDIA GPUs specifically, MLIR provides the NVGPU and NVVM dialects.

The NVVM dialect corresponds closely to NVIDIA’s PTX ISA (parallel thread execution instructions).

NVVM ops are essentially MLIR’s representation of PTX instructions and intrinsics.

For example, operations like `nvvm.mma.sync` might represent warp-level matrix multiply-accumulate (WMMA) PTX instructions.

The NVGPU dialect sits between the generic GPU dialect and NVVM, offering higher-level NVIDIA-specific operations to simplify programming tensor cores and other advanced features.

NVGPU ops like `nvgpu.warpgroup.mma` or `nvgpu.mma.sync` encapsulate complex PTX sequences (for tensor core operations) behind more readable ops.

This separation means a compiler developer can use NVGPU’s higher-level ops, and then rely on a conversion pass to translate them into the exact NVVM (PTX) instructions.

In summary, the GPU dialect is vendor-agnostic, while NVGPU/NVVM dialects handle NVIDIA-specific lowering (with NVVM being a direct PTX mirror and NVGPU providing some abstraction on top).

## Lowering from PyTorch IR to LLVM IR and CUDA PTX
Pipeline Overview:
Compiling a PyTorch model to GPU involves several stages of lowering in MLIR, eventually producing LLVM IR and PTX for execution. A typical pipeline might look like:
- PyTorch to MLIR (Torch Dialect)
  - Use Torch-MLIR to import the PyTorch model into an MLIR module in the torch dialect.
  - This captures PyTorch ops and data structures in MLIR.
- High-Level Optimizations
  - Apply passes to convert from torch dialect to MLIR’s standard ML dialects. For example, convert to linalg on tensors for computations, and use tensor/memref for memory.
  - Many PyTorch ops (like convolutions, gemm, activations) can be lowered to combinations of linalg operations, loops, etc.
  - At this stage, device-agnostic optimizations (like fusion, loop tiling, etc.) can be done on the MLIR.
- Introduce GPU Dialect
  - Transform the MLIR to use the GPU dialect for portions meant to run on the GPU.
  - This often involves outlining kernel functions and marking them as `gpu.func` within a `gpu.module`.
  - For example, MLIR provides a pass (gpu-kernel-outlining) that takes computations (loops) and outlines them into separate GPU kernels.
  - We also map loop iterations to GPU threads/blocks (e.g., one loop dimension to blockIdx.x, another to threadIdx.x, etc.) using mapping passes​
  - After this, the IR contains constructs like `gpu.launch_func` (on the host side to launch kernels) and `gpu.func` (device code).
- Lower to NVVM (PTX Dialect):
  - Next, we convert the GPU dialect device code to the NVVM dialect. MLIR has a conversion pass (convert-gpu-to-nvvm) that turns GPU dialect ops into NVVM ops.​
  - This is where GPU operations become PTX-equivalent operations.
  - We also attach target information (such as the CUDA SM version or PTX version) to guide code generation.
  - For instance, an MLIR pipeline might include an NVVM target attach step like `nvvm-attach-target{chip=sm_90}` to specify we’re targeting NVIDIA SM90 (Hopper architecture) with a certain PTX version​
  - The NVVM dialect ops now directly correspond to PTX instructions that will run on the GPU.
- Lower to LLVM IR (NVPTX):
  - Once we have NVVM ops, MLIR can lower these to the LLVM dialect and utilize LLVM’s NVPTX backend.
  - A pass like `gpu-to-llvm` or a translation step converts the MLIR NVVM module into LLVM IR​
  - Essentially, MLIR hands off to LLVM with calls or intrinsics that represent PTX instructions. The LLVM NVPTX backend then generates PTX assembly from that LLVM IR. In MLIR’s tooling, one can use `mlir-translate --mlir-to-llvmir` to get the final LLVM IR and then use LLVM to emit PTX or even a cubin​
- PTX and Cubin Generation
  - Finally, the NVPTX backend (part of LLVM) compiles the LLVM IR to PTX code.
  - MLIR’s gpu-module-to-binary pass can invoke the PTX assembler (ptxas) to produce a cubin or embed the PTX as a binary blob​
  - The result is that we have either PTX text or a compiled binary for the GPU kernel, which the host code can load and execute.
  - The MLIR GPU compilation documentation shows an example where after conversion to NVVM and LLVM, the gpu.module is serialized to a binary (this is the device code)​
  - The host-side MLIR (or LLVM IR) will contain the necessary runtime calls to launch the kernel (for example, if using CUDA driver APIs or a GPU runtime wrapper)​.

Throughout this pipeline, MLIR plays the central role of progressively lowering the representation:

**PyTorch IR → high-level MLIR → GPU dialect (with parallelism) → NVVM dialect (PTX) → LLVM IR → machine code.**

Each stage leverages MLIR’s modular transformation passes, and the final code generation leverages LLVM. The end product is CUDA PTX or binaries that can run on the GPU.

## WMMA and Tensor Core Optimizations in MLIR
NVIDIA’s tensor cores (starting from Volta’s WMMA API up to Ampere/Hopper’s more advanced WMMA and WMMA/WMMA instructions) are critical for accelerating matrix operations.

MLIR has introduced special support to generate code that uses these tensor cores:

**WMMA Ops in MLIR**: MLIR’s GPU dialect added WMMA (Warp Matrix Multiply-Accumulate) operations to represent matrix-multiplication fragments processed on tensor cores.

For example, there have been MLIR dialect ops to represent operations like mma.sync (matrix multiply-accumulate on tensor cores).

These started in the GPU dialect and later also appeared in the NVVM dialect for complete lowering.

By modeling these as high-level ops, the compiler can reason about matrix tiles and schedule their computation.

**NVVM Dialect Tensor Core Ops**: The NVVM dialect (targeting PTX) implemented newer PTX instructions corresponding to tensor core operations (e.g., **WMMA instructions, Hopper’s wgmma.mma_async, etc.).

This means MLIR can emit NVVM ops like `nvvm.mma.sync` or other PTX intrinsics which map to actual hardware instructions for tensor cores.

For instance, Ampere and Hopper generation tensor-core ops have been added as NVVM ops.

These NVVM ops ensure that when lowering to LLVM IR, we generate the correct PTX or call the right LLVM intrinsics for tensor core usage.

**NVGPU High-Level Abstractions**: To simplify using these powerful but complex instructions, MLIR’s NVGPU dialect provides more human-readable ops.

One example is `nvgpu.warpgroup.mma`, which performs a warp-level matrix multiplication (e.g., a 128×128×64 fragment) with FP16 inputs and FP32 accumulation.

This single op hides the complexity of coordinating 128 threads, loading matrix tiles, and issuing multiple low-level instructions.

The compiler can lower this one NVGPU op into the appropriate sequence of NVVM ops (like multiple mma.sync or the newer wgmma PTX instructions).

This separation of concerns allows MLIR to optimize at a high level (treating the tensor-core op as a single unit) and then expand it into hardware-specific code late in the pipeline.

**Conversion Patterns for WMMA**: MLIR includes transformation patterns specifically to handle WMMA ops.

For instance, the Transform dialect defines a conversion pattern to lower GPU dialect WMMA ops to NVVM ops​.

This ensures any gpu.wmma ops you use (if any exist in the GPU dialect) will reliably turn into correct PTX instructions.

Essentially, MLIR’s compiler passes know how to recognize a high-level “matrix multiply on tensor core” operation and substitute it with the corresponding PTX intrinsic sequence.

**Optimizations like Loop Tiling**: In addition to direct WMMA ops, MLIR can perform typical loop optimizations (tiling, unrolling, etc.) tailored for tensor cores.

For example, tiling a matrix multiplication into 16x16 tiles that fit the WMMA fragment size, emitting loads and stores to shared memory, and then using the WMMA ops for the math.

Such patterns were demonstrated in research using MLIR to achieve near-peak performance​.

The MLIR-based codegen can overlap memory operations with computation, use asynchronous copies, and synchronize warps appropriately – all using MLIR’s structured ops (like nvgpu.device_async_copy, barriers, etc.) and then lowering to PTX​.

The end result is that MLIR can automate the generation of highly optimized GPU kernels that utilize tensor cores, which traditionally required expert-crafted CUDA code.

## Tools and Frameworks

Several tools and frameworks facilitate compiling PyTorch models via MLIR to GPU:

**Torch-MLIR**: As described, Torch-MLIR is the primary frontend for PyTorch. 

It provides Python APIs (like `torch_mlir.compile(...)`) to convert a PyTorch nn.Module into MLIR in a chosen dialect (e.g., Torch dialect or directly to Linalg-on-tensors).

This is often the first step in an MLIR-based workflow for PyTorch.

The Torch-MLIR repository also includes example scripts (e.g., lowering a ResNet) that demonstrate obtaining MLIR from PyTorch and then running optimization passes​.

Torch-MLIR itself focuses on the front-end conversion and basic lowerings; for full GPU codegen, it can be used in conjunction with other MLIR tools or frameworks (since the MLIR core provides the GPU/LLVM lowerings).

**MLIR Core Tools (mlir-opt, mlir-translate)**: MLIR comes with command-line tools like `mlir-opt` for applying compiler passes and `mlir-translate` for converting MLIR to other forms (like LLVM IR or assembly).

Developers often take the MLIR module produced by Torch-MLIR and run a sequence of passes to generate GPU code. For example, one could run:

```bash
mlir-opt model.mlir -pass-pipeline="builtin.module( 
    torch-lowerings,..., 
    linalg-bufferize, convert-linalg-to-loops, 
    gpu-map-parallel-loops, gpu-kernel-outlining, 
    convert-gpu-to-nvvm, gpu-to-llvm, gpu-module-to-binary 
  )" -o out.mlir
```

This hypothetical pipeline would lower Torch ops to linalg, convert to loops, map loops to GPU, outline kernels, lower to NVVM, then to LLVM and embed the binary​.

Finally, `mlir-translate out.mlir --mlir-to-llvmir > out.ll` yields LLVM IR with embedded PTX, which can be run or further compiled​.

(In practice, each framework may have its own tuned pipeline, but MLIR’s modular passes make it possible to script these workflows.)

**IREE**: IREE is an MLIR-based end-to-end compiler that can target multiple backends (CPU, Vulkan, CUDA).

It can consume models from TensorFlow, TFLite, and via Torch-MLIR for PyTorch. Using Torch-MLIR to import a PyTorch model, one can hand the MLIR to IREE, which then applies its pipeline to generate GPU code.

IREE has its own GPU support (for CUDA it can emit PTX, and for Vulkan it emits SPIR-V).

For CUDA specifically, IREE leverages MLIR’s NVVM and LLVM pathways similar to what we described​.

While IREE abstracts a lot, it is a practical example of a framework where PyTorch models (via MLIR) can be compiled to a CUDA driver executable.

Tools like IREE’s compile command can take in an MLIR (from Torch-MLIR) and output a module that contains PTX for execution on NVIDIA GPUs.

**Triton (OpenAI)**: Triton is a domain-specific language for writing GPU kernels, and it has recently been leveraging MLIR in its compiler stack.

While Triton is not an IR for full models but rather for custom kernels, it’s worth noting because PyTorch 2.x’s TorchInductor uses Triton under the hood for GPU kernel generation.

The interesting intersection is that Triton’s compiler is moving to use MLIR as well. In fact, there are efforts to align Triton’s PTX generation with MLIR’s NVVM dialect (to avoid duplicating effort).

So, while not a direct compiler of PyTorch models via MLIR, Triton and MLIR share technology for GPU codegen.

The LLVM MLIR community has discussed unifying PTX generation approaches – for example, using MLIR’s NVVM dialect instead of Triton’s custom PTX emission.

## Example Workflow
To illustrate how this all comes together, consider a simple example of compiling a PyTorch model with a single matrix multiplication for GPU:
1. PyTorch Model Definition: You have a simple model:
```python
import torch
class MyModel(torch.nn.Module):
    def forward(self, x, y):
        return x @ y
model = MyModel().eval()
```

Assume x and y are 2-D tensors (matrix multiply).

2. Lower to MLIR (Torch-MLIR): Use Torch-MLIR’s Python API to lower this model:
```python
import torch_mlir
module = torch_mlir.compile(model, 
            (torch.randn(128, 64), torch.randn(64, 128)), 
            output_type=torch_mlir.OutputType.LINALG_ON_TENSORS)
mlir_text = str(module)
```
Here, `output_type=LINALG_ON_TENSORS` tells Torch-MLIR to produce MLIR where the `PyTorch aten::matmul` has been lowered to a `linalg.matmul op on tensors`. 

The resulting MLIR (viewable via `mlir_text`) will have the high-level structure of the computation in MLIR form (no GPU specifics yet).

3. Transform to GPU Dialect: Next, we apply MLIR passes (via Python API or command-line) to target the GPU:
```python
import mlir
mlir.apply_passes(module, "builtin.module( 
    linalg-bufferize, convert-linalg-to-loops, 
    gpu-map-parallel-loops, gpu-kernel-outlining 
  )")
```
Convert the `linalg op` to loops (linalg.matmul → loops and vector ops, for example).

Map those loops to a GPU configuration. For instance, tile the loops and map the outer loops to blockIdx and inner to threadIdx. MLIR’s gpu.
`map_parallel_loops` pass can do this automatically​.


```python
import mlir
mlir.apply_passes(module, "builtin.module( 
    linalg-bufferize, convert-linalg-to-loops, 
    gpu-map-parallel-loops, gpu-kernel-outlining 
  )")
```

This introduces gpu.thread_id and gpu.block_id ops into the loop body (as seen in the NVGPU example code)​.

Outline the loop body as a GPU kernel (gpu.kernel_outlining pass), which produces a gpu.module with a device function (gpu.func) containing the math, and a host-side gpu.launch_func that launches it​.


4. Lower to NVVM and LLVM: After outlining, run the conversion to NVVM dialect:
The `convert-gpu-to-nvvm` pass turns things like shared memory allocations, thread ops, etc., into NVVM equivalents​.

For example, gpu.thread_id might become an nvvm.read.ptx.sreg.tid.x intrinsic under the hood. If our matmul can use WMMA, this is where high-level ops could be converted to `nvvm.mma.sync` calls (assuming we used a higher-level op that maps to WMMA).
Then use `gpu-to-llvm` to finalize conversion to the LLVM dialect​.

This might produce an LLVM function for the host with calls to a kernel launcher, and an LLVM function representing the device code (in NVPTX IR form).

Finally, either use gpu-module-to-binary to generate the actual PTX binary blob inside the MLIR module​, or translate to LLVM IR and run LLVM’s llc with the NVPTX target. For example: llc -march=nvptx64 -mcpu=sm_80 -filetype=obj out.ll -o out.ptx.

5. Run on GPU: The end result is that we have a CUDA PTX (or cubin) for the matrix multiply kernel, and the host code can launch it.

If using MLIR’s runner utilities, mlir-cpu-runner and mlir-runner-utils can execute the module (with a GPU binary embedded) by invoking the CUDA driver under the hood.

Alternatively, one could embed the PTX into a small C++ harness that loads it via the CUDA API and runs the kernel.

This workflow demonstrates how MLIR serves as the end-to-end compiler: from PyTorch model to optimized GPU code.

At each stage, different MLIR dialects and passes come into play (Torch → Linalg → GPU → NVVM → LLVM), and specific optimizations like tiling for tensor cores (WMMA) are applied by using the proper ops and passes. The combination of Torch-MLIR + MLIR’s GPU compilation pipeline allows targeting CUDA PTX, including advanced instructions for tensor cores, all within a unified compiler framework.

## References and Further Reading
- **Torch-MLIR project (PyTorch to MLIR):** Official GitHub repository and docs. Introduces the Torch dialect and how PyTorch ops are represented in MLIR.
- **MLIR GPU Dialect:** Documentation of the GPU dialect and compilation pipeline​ MLIR.LLVM.ORG MLIR.LLVM.ORG, showing how GPU modules are lowered to NVVM and then to binary/LLVM IR.
- **NVGPU and NVVM Dialects:** MLIR docs and discussions on NVIDIA GPU support, explaining how high-level ops map to PTX and how MLIR handles new tensor core instructions.
- **WMMA/Tensor Core Codegen:** MLIR community slides and papers, e.g. “High Performance GPU Code Generation for Matrix-Matrix Multiplication using MLIR”​
ARXIV.ORG (reports near-CuBLAS performance via MLIR), and MLIR’s transform patterns for WMMA​ MLIR.LLVM.ORG.
- **Example Code:** The MLIR code example for using tensor cores (CUDA WMMA) in MLIR’s GitHub tests​ GITHUB.COM – demonstrates how warp-level matrix mult and async copies are orchestrated in MLIR (Python API usage).
- **TOOLS:** IREE’s documentation on PyTorch and CUDA backend​ IREE.DEV , and the Torch-MLIR docs (e.g., development.md with a ResNet example)​ GITHUB.COM for practical guidance on using these tools together.