---
title: VLLM Notes
date: 2025-12-11 12:32:49
permalink: /pages/dc7072/
---

1. vLLM Office Hours - Distributed Inference with vLLM - January 23, 2025

---

## [1] vLLM Office Hours - Distributed Inference with vLLM - January 23, 2025

[link](https://www.youtube.com/watch?v=LH2QZehVJoc)

## Summary of Video: "vLLM Office Hours - Distributed Inference with vLLM"

The video detailed the need for distributed inference to serve large LLMs (like 600B+ parameters) that exceed single-GPU capacity.

### Distributed Techniques:

| Technique | Goal | Mechanism | Trade-offs |
| :--- | :--- | :--- | :--- |
| **Tensor Parallelism (TP)** | Fit model on a single node; improve latency and throughput. | Shards weights **horizontally** across multiple GPUs (SPMD paradigm). Uses AllGather/AllReduce operations. | High communication overhead, mitigated by high-speed interconnects (e.g., NVLink). |
| **Pipeline Parallelism (PP)** | Fit model across multiple nodes. | Shards the model **by layer** across multiple GPUs/nodes. Uses Send/Receive operations. | Low communication overhead, but doesn't inherently improve latency and can suffer from GPU idle time ("bubbles"). |
| **Combination** | PP can be used *across nodes* for low communication, and TP *within a node* for speed and memory efficiency. |

### Key Optimization: Chunk Prefill

Chunk Prefill is critical, especially for PP, and involves breaking large prefill operations into smaller chunks and mixing them with decode batches.

* **Impact of Chunk Size:** A small, carefully chosen size is essential for a smooth, bubble-free execution pipeline and maximum throughput.
* **Memory Management:** It limits the memory spike from long inputs, allowing the system to safely allocate more **KV Cache** space, which boosts concurrency.

## 3. vLLM Architecture Components (Scheduler, Executor, Worker)

These components coordinate to manage and execute inference requests in a distributed vLLM environment:

* **Scheduler:** Manages the flow of requests, schedules batches for processing, and receives the final results.
* **Executor:** The primary distributed manager. It manages the Workers, handles hardware backends (Ray, multi-processing), and issues the coordinated distributed inference commands.
* **Worker:** The execution unit associated with a specific accelerator (e.g., GPU). It performs the actual forward pass and inference computation.

## 4. Definition of `Chunk Prefill Size`

The **Chunk Prefill Size** parameter dictates the maximum number of tokens from a large input prompt that vLLM will process in a single step (or "chunk").

It serves to:
1.  **Enforce Memory Safety:** Prevents large, arbitrary inputs from causing a sudden, massive spike in activation memory, thus preventing Out-of-Memory (OOM) errors.
2.  **Amortize Cost:** Smoothes the workload by breaking the long prefill into smaller, manageable pieces that can be interspersed with decode operations.
3.  **Boost Concurrency:** By guaranteeing a maximum memory footprint, the system can allocate more space for the KV Cache, leading to higher overall throughput.

## 5. Tensor Parallelism vs. Pipeline Parallelism Comparison

The fundamental difference lies in **how** the model is split:

| Feature | Tensor Parallelism (TP) | Pipeline Parallelism (PP) / Layer Parallelism |
| :--- | :--- | :--- |
| **Model Split** | **Intra-Layer Parallelism:** Splits computation/weights *within* individual layers. | **Inter-Layer Parallelism:** Splits the model **by layer** or group of layers (stages). |
| **Communication** | **Frequent:** Communication (All-Reduce/All-Gather) happens *after* nearly every linear layer. | **Infrequent:** Communication (Send/Receive of activations) happens only at the **boundaries** between stages. |
| **Best Used For** | **Scaling within a Node** (high-speed interconnects like NVLink). | **Scaling across Nodes** (slower interconnects). |
| **Goal** | Allow a single large layer/tensor to **fit** and **speed up** computation. | Allow the entire model to **fit** by distributing its total memory footprint. |

### 6. Column and Row Parallelism in LLMs

Tensor Parallelism is implemented using two specialized patterns within the Transformer layers, especially in the Multi-Layer Perceptron (MLP) block:

1.  **Column Parallelism (`ColumnParallelLinear`):**
    * **Mechanism:** Splits the weight matrix **by columns**.
    * **Flow:** The input is multiplied by the sharded weight matrix. The output is already sharded across GPUs.
    * **Communication:** **No communication** is required immediately after the multiplication (e.g., after the Up-Projection layer).
2.  **Row Parallelism (`RowParallelLinear`):**
    * **Mechanism:** Splits the weight matrix **by rows**.
    * **Flow:** The input (which is sharded from the previous Column Parallel step) is multiplied by the sharded weight matrix.
    * **Communication:** Requires a final **All-Reduce** operation to sum the partial results from all GPUs and generate the correct output (e.g., after the Down-Projection layer).

LLMs combine these: The **Up-Projection** uses **Column Parallelism** (no communication), and the subsequent **Down-Projection** uses **Row Parallelism** (single All-Reduce).

This trick is used through all layers.

This minimizes the required synchronization steps.

<img width="1421" height="349" alt="image" src="https://github.com/user-attachments/assets/b245f392-85a1-499f-90d0-ac11da9efc72" />


<img width="1401" height="336" alt="image" src="https://github.com/user-attachments/assets/481808db-01fd-4bb8-98dd-e4d1c500c150" />


<img width="1438" height="427" alt="image" src="https://github.com/user-attachments/assets/0c6ddc84-6d1c-4b17-b1f3-f7dd5c403953" />

Major takeaway:
1. first column parallelism, then row parallelism
2. prefer tensor parallelism, gpu is parallelized but with heavy communation, this is prefered in intranodes, communicate will happen in each layer
   As to pipeline parallelism, there might be bubble accross gpus. When gpu0 is done with layer0-3, it passes the intermediate result into gpu1 for layer4-7. Only result from layer3 needs to be transfered.\
   less communication, but might introduce buffer. Used in multi node case.
3. chunk size is essential. we can amortize the prefilling phase into decoding phase. So even case with long prefill, it still works well.
