---
title: LLM Compression
date: 2025-04-2 12:32:49
permalink: /pages/dc7062/
---

1. [1] BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models
2. [1075] Adafactor Adaptive Learning Rates with Sublinear Memory Cost

---

## 1. [1] BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models

**It change the finetuing into block coordinate descent (BCD)-type optimization** which I dont understand.
 
For instance, to finetune an LLM with M billion parameters, Adam [23] necessitates roughly 18M GB of GPU memory for successful training, and
this estimate does not even account for the storage of activations used in the backpropagation (BP) process.

Despite the success of PEFT methods, finetuning within a substantially lower-dimensional subspace may potentially **limit downstream performance**.

![image](https://github.com/user-attachments/assets/8def8b42-d493-4358-80bc-515746a2cc17)


We first analyze the memory cost of Adam with mixed precision training.

One needs to store the FP16 model parameters for the BP process, which costs 2M memory.

For a more precise update, the optimizer also maintains a master copy of a FP32 model, which costs 4M memory.

Then, it comes to store the **gradient (converted to FP32), momentum, and second moment** in FP32 precision, costing **4M + 4M + 4M = 12M** memory.

In total, Adam needs roughly **18M memory**. 

In terms of BAdam, it needs to store the up-to-date model parameters (see Figure 1) in FP16 precision, which costs 2M memory. Importantly, since BAdam only updates the active block at one time, we
can store the model parameters, gradient, momentum, and second moment only for the active block θπi in FP32 precision, where the FP32 model parameters and gradient of the active block can be
obtained by transforming their FP16 versions to the FP32 versions.

Let us consider the simple case where the partitioned D blocks are equal-sized. Then, BAdam only needs in total 

![image](https://github.com/user-attachments/assets/f0ee0fbd-cf93-4b2b-a1b0-f7df945d5d16)

---

## 2. [1075] Adafactor Adaptive Learning Rates with Sublinear Memory Cost

This paper introduces **Adafactor** , an optimization algorithm designed to provide **adaptive learning rates with significantly reduced memory overhead**  compared to traditional methods like Adam.

The core innovation is replacing full per-parameter second-moment estimators (used to scale gradients) with **factored approximations**  based on the **row and column sums**  of squared gradients.

**Similar to LORA, using only the per-row and percolumn sums of these moving averages, and estimating the per-parameter second moments based on these sums.**

This change reduces memory usage from O(nm) to O(n + m) for matrix-shaped parameters.

Key contributions and details include:

- **Factored Second Moment Estimation** :
 
  - Adafactor leverages a rank-1 approximation using the generalized Kullback-Leibler divergence to estimate the second moment of gradients.
  - Instead of storing full-size accumulators, it maintains exponential moving averages of row and column sums, achieving memory savings while preserving empirical performance.
  - A closed-form solution for the rank-1 approximation ensures computational efficiency and compatibility with exponential smoothing.
 
- **Removing Momentum** :
  - **Momentum (first moment) is omitted (β₁ = 0)** to further reduce memory cost. This change initially causes instability, especially without learning rate warmup.
- **Stabilizing Updates** :
  - **Update Clipping** : Caps the RMS of unscaled updates to avoid large, destabilizing parameter jumps due to outdated second-moment estimators.
  - **Increasing β₂ Schedule** : Proposes a decay schedule like β̂₂ₜ = 1 − t^(-c), which adapts over time and avoids the need for bias correction.
  - These methods independently and jointly stabilize training in the absence of momentum and warmup.
- **Relative Step Sizes** :
  - Instead of fixed absolute learning rates, Adafactor uses **parameter-relative step sizes** , scaling updates based on the parameter norm, making it more resilient to differing parameter magnitudes (e.g., in embeddings).
- **Experiments** :
  - Conducted on Transformer models for WMT’14 En→De machine translation.
  - Adafactor with factored moments, no momentum, update clipping, increasing decay rate, and relative step sizes performs comparably to Adam while using less memory.
  - Results show robustness to poor parameter initialization and scaling, unlike Adam.
- **Practical Use** :
  - Adafactor enables training larger models on memory-constrained hardware.
  - Implementation is available in the Tensor2Tensor library.

**Three-Sentence Summary:**

The paper proposes **Adafactor** , a memory-efficient optimizer that approximates second-moment estimators using factored row and column sums, drastically reducing auxiliary memory usage from O(nm) to O(n + m).

To address instability caused by omitting momentum and slow-decaying estimators, the authors introduce **update clipping**  and an **increasing decay schedule** , both of which stabilize training.

Combined with **relative step sizes** , Adafactor achieves comparable performance to Adam on large-scale Transformer tasks while enabling significantly larger models on limited hardware.
