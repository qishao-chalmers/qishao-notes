---
title: Reasoning in LLM
date: 2025-01-27 23:32:49
permalink: /pages/dc7049/
---

1. [331] Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting
2. [9831] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
3. [641] Towards Reasoning in Large Language Models: A Survey
4. [2 2025] Self-Training Elicits Concise Reasoning in Large Language Models

---
## [331] Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting

- Key Findings:
  - Unfaithful Explanations: CoT explanations can be plausible but systematically unfaithful, failing to reflect the true reasoning process.
  - Biasing Features: Models are heavily influenced by biasing features (e.g., reordering multiple-choice options), which are not mentioned in explanations.
  - Accuracy Drop: Biasing models toward incorrect answers leads to a 36% drop in accuracy on BIG-Bench Hard tasks.
  - Social Bias: Models justify stereotype-aligned answers without acknowledging the influence of social biases.
  - Counterfactual Simulatability: Models rarely acknowledge biasing features, making explanations systematically unfaithful.

They instruct llm with bias：

![image](https://github.com/user-attachments/assets/9c02c120-64d7-488c-b1af-bebeb28e8582)

![image](https://github.com/user-attachments/assets/c11dfc4e-35af-4ce2-b88f-fd51f8980805)

---

## [9831] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models

Key findings:
- Chain-of-thought prompting significantly improves the performance of large language models on a variety of reasoning tasks.
- Chain-of-thought prompting is an emergent property of model scale, meaning that it only provides significant performance gains when used with very large language models (around 100 billion parameters).
- The improvements from chain-of-thought prompting are robust across different language models, datasets, and annotators

Usage cases:
- Arithmetic reasoning: CoT prompting can help language models solve math word problems that require multiple steps, such as the GSM8K benchmark.    
- Commonsense reasoning: CoT prompting can also improve the performance of language models on tasks that require commonsense reasoning, such as the StrategyQA dataset, which requires models to infer a multi-hop strategy to answer questions.    
- Symbolic reasoning: CoT prompting has also been shown to be effective for symbolic reasoning tasks, such as last letter concatenation, which requires the model to concatenate the last letters of words in a name.

---

## [641] Towards Reasoning in Large Language Models: A Survey

![image](https://github.com/user-attachments/assets/a54b39f5-4293-4d16-ad88-ce289ed787a9)

Large language models (LLMs) have made impressive strides in natural language processing, but their ability to reason remains a hot topic. This blog post delves into the fascinating world of reasoning in LLMs, exploring the techniques, evaluations, and key findings that are shaping this field.

### What is Reasoning?
Reasoning is the process of using evidence, logic, and past experiences to form conclusions or make decisions. It's a fundamental aspect of human intelligence, allowing us to solve problems, think critically, and understand the world around us.   There are different types of reasoning, including:   

- Deductive reasoning: Drawing a conclusion based on the truth of the premises (e.g., if all mammals have kidneys and all whales are mammals, then all whales have kidneys).    
- Inductive reasoning: Drawing a conclusion based on observations or evidence (e.g., if every winged creature we've seen is a bird, then a new winged creature is likely a bird).    
- Abductive reasoning: Drawing a conclusion based on the best explanation for a set of observations (e.g., if the car won't start and there's a puddle under it, then the car probably has a leak).    

### Techniques for Enhancing Reasoning in LLMs
Researchers are constantly developing new techniques to improve or elicit reasoning in LLMs. Some of the most promising methods include:

- Fully supervised fine-tuning: This involves fine-tuning a pre-trained LLM on a dataset containing explicit reasoning examples. For instance, a model could be trained to generate rationales explaining its predictions.    
- Prompting and in-context learning: This approach involves prompting LLMs with a question and a few examples of how to solve similar questions. Chain-of-thought prompting is a popular technique where the examples include intermediate reasoning steps, guiding the LLM to generate its own reasoning process.
  - Prompting & In-Context Learning: in CoT prompting, ⟨input, output⟩ demonstrations are replaced with ⟨input, chain of thought, output⟩ tripples
    - manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation
  -  Rationale Engieering： creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by LLMs.
    -  Rationale refinement
      -  complexity-based prompting to create rationales with more reasoning steps. Their experiments show that the performance of LLMs improves with the increased rationale complexity
      -  algorithmic prompting, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations
    - Rationale exploration: decoding strategy, sampling a divese set of rationale, instead of the greedy one
    - Rationale verification

![image](https://github.com/user-attachments/assets/3cda04b5-8ce4-4149-910e-920c0113efa0)
    
- Hybrid methods: These methods combine techniques like pre-training or fine-tuning LLMs on datasets that include reasoning, along with prompting techniques to elicit reasoning.
  - LLMs trained on datasets containing scientific and mathematical data can achieve better performance on reasoning tasks like quantitative reasoning problems when using CoT prompting.
  - bootstrapping & self improving: using LLMs to self-improve their reasoning abilities through a process known  as bootstrapping.
      - Specifically, with CoT prompting, the model first generates initial rationales. And then, the model is finetuned on rationales that lead to correct answers. This process can be repeated, with each iteration resulting in an improved model that can generate better training data.

### Evaluating Reasoning in LLMs
Evaluating the reasoning abilities of LLMs is crucial. Researchers use various methods and benchmarks to assess their performance, including:

- End task performance: This involves measuring the accuracy of LLMs on **tasks requiring reasoning, such as arithmetic, commonsense, and symbolic reasoning benchmarks.**    
- Analysis of reasoning: This approach focuses on directly assessing the reasoning steps taken by LLMs, rather than just the final answer. This can involve analyzing the quality of the generated rationales or using formal metrics to evaluate the reasoning process.

### Key Findings and Implications
Research in reasoning in LLMs has yielded some interesting findings:

- Emergent ability: Reasoning seems to be an emergent ability of LLMs, **becoming more pronounced as the models get larger (around 100 billion parameters or more).**    
- Chain-of-thought prompting: This technique has been shown to significantly improve the performance of LLMs on various reasoning tasks.    
- Complex reasoning challenges: Despite progress, LLMs still struggle with complex reasoning tasks, suggesting that current benchmarks might be too simple.

### Open Questions and Future Directions
The field of reasoning in LLMs is still evolving, with many open questions and exciting avenues for future research:

- True reasoning or mimicry?: Are LLMs truly capable of reasoning, or are they simply learning to mimic human reasoning through pattern recognition?    
- Improving reasoning capabilities: How can we further enhance the reasoning capabilities of LLMs? This could involve developing new training methods, model architectures, or prompting techniques.

By addressing these questions and continuing to explore the intricacies of reasoning in LLMs, we can unlock their full potential and pave the way for more intelligent and reliable language-based AI systems.

---

## Self-Training Elicits Concise Reasoning in Large Language Models
### 1. Introduction & Background 
- **Chain-of-Thought (CoT)**  reasoning significantly boosts LLM performance by breaking complex tasks into intermediate reasoning steps. However, CoT inherently generates redundant tokens that increase inference costs unnecessarily.
 
- The authors hypothesize that current LLMs inherently possess the latent ability to reason concisely, as evidenced by the presence of shorter correct reasoning paths within their output distributions. Their goal is to explicitly unlock this capability through targeted fine-tuning.

### 2. Preliminary Investigation 
The authors conducted preliminary studies to support their hypothesis:
#### Concise Reasoning Definition: 
- Defined as reasoning correctly with fewer output tokens compared to the model’s default (average correct) output length.
#### Analysis of Reasoning Length Distribution: 
- A kernel density estimation showed significant potential for concise reasoning across various model families (e.g., Llama, DeepSeekMath, Qwen, Gemma).
 - Models frequently generated solutions that were shorter than their default output, suggesting inherent latent potential for conciseness.
#### Limitations of Zero-Shot Prompting: 
- Popular zero-shot prompts aimed at concise reasoning ("Be Concise," "Fixed Budget," "Estimated Budget," and handcrafted prompts) significantly reduced token count but often compromised accuracy and exhibited inconsistent effects, particularly on math-specialized models.
### 3. Proposed Methods 
To reliably elicit concise reasoning without sacrificing accuracy, the authors proposed several fine-tuning methods based on self-training:
#### 3.1. Naive Best-of-N (BoN) Sampling: 
- Generates multiple reasoning paths (N paths per question) and selects the shortest correct reasoning path for fine-tuning.
- Effective but sample inefficient as length reduction benefits saturate quickly.
#### 3.2. Few-Shot Conditioning for Efficient Reduction: 
- Combines few-shot prompting and BoN sampling to enhance concise reasoning.
- Few-shot exemplars are derived from:
  - Human annotations (FS-Human)
  - Proprietary LLMs like GPT-4o (FS-GPT4o)
  - Self-generated concise examples from the target model itself (FS-Self)
- Few-shot conditioning significantly improves sample efficiency in eliciting concise reasoning.
#### 3.3. Sample Augmentation: 
- To improve accuracy retention, they augment the few-shot conditioned data with additional samples generated from naive BoN.
- Ensures coverage of both easy and difficult questions, preserving accuracy while maintaining conciseness.
### 4. Experimental Setup 
- Evaluated across **two mathematical reasoning datasets** : GSM8K and MATH.
- Models tested include general-purpose (e.g., Llama, Gemma, Qwen2.5) and math-specialized models (e.g., DeepSeekMath, Qwen2.5-Math).
- Metrics: **Accuracy and output length**  were primary evaluation metrics. Length is measured in tokens.
### 5. Key Findings & Contributions 
#### 5.1. Main Results 
- **Naive BoN**  reduced reasoning length modestly (~12%) without major accuracy loss.
- **Few-shot conditioned methods**  (especially FS-GPT4o and FS-Human) yielded significant length reductions (~30% average across datasets) while maintaining accuracy.
- The combined method (**FS-GPT4o-BoN** ) achieved the greatest reduction in length (average 30%) with minimal accuracy drop.
- Self-training methods consistently outperformed zero-shot prompting and external fine-tuning baselines in terms of balancing conciseness with accuracy.
#### 5.2. In-depth Analysis 
- Models adaptively adjusted the length of reasoning paths based on question complexity—shorter paths for easier questions and longer paths for harder ones.
- Scaling analysis (1B, 3B, and 8B models) showed length reduction consistently improved with larger models.
- Fine-tuning effectively transferred concise reasoning learned from training to test outputs.
### 6. Discussion & Broader Impact 
- The default reasoning behavior of current LLMs inherently contains redundancy, likely due to training data and optimization methods not promoting conciseness.
- The authors suggest that lightweight fine-tuning approaches are sufficient for eliciting concise reasoning, aligning with the "Superficial Alignment Hypothesis" (minimal training can significantly shift model behavior).
- Emphasized the potential for integrating concise reasoning supervision into standard training pipelines for more efficient LLM inference.
### 7. Limitations and Future Directions 
- Further exploration of advanced training schemes (reinforcement learning, iterative refinement).
- Investigating more sophisticated few-shot exemplar selection and prompting strategies.
- Extending the scalability study beyond 8B parameter models.
- Generalizing concise reasoning methods to broader, non-mathematical tasks.
### 8. Contributions & Novelty (Summary): 
- Identified latent concise reasoning capabilities within standard LLMs.
- Demonstrated ineffectiveness and inconsistency of popular zero-shot concise reasoning methods.
- Proposed a novel, efficient, and effective fine-tuning approach (few-shot conditioned best-of-N sampling with augmentation) that consistently improves conciseness without significant accuracy degradation.
- Provided extensive experimental validation across multiple models and datasets.
### 9. Practical Implications: 
- The authors highlight practical benefits: reduced inference costs, lower latency, and more efficient model deployment in scenarios where reasoning-based inference costs are critical.
### 10. Conclusion 
The paper convincingly demonstrates that standard large language models can be fine-tuned to significantly enhance concise reasoning using self-generated concise examples.

This approach is broadly applicable and beneficial across different LLM architectures and model sizes, offering substantial improvements in inference efficiency without compromising reasoning accuracy.


Overall, this paper contributes significantly to understanding and improving reasoning efficiency in LLMs, providing valuable methodologies for practitioners aiming to optimize inference performance in real-world scenarios.
