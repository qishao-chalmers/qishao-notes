---
title: LLM Mixed Precision & Quantization & Outlier
date: 2025-01-27 23:32:49
permalink: /pages/dc7050/
---

1. [UnRead 2] A Comprehensive Study on Quantization Techniques for Large Language Models :+1：
2. [UnRead 11] A Comprehensive Evaluation of Quantization Strategies for Large Language Models :+1: :+1:
3. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization
4. [43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models
5. [26] FP8-LM: Training FP8 Large Language Models :+1: :+1: :+1: from Microsoft <br> *This is discussed that first order in adam could be FP8, but second order in adam should be FP16. and the weight should be reserved a copy of FP32 full-precision or FP16 with tensor scaling..*
6. [139] FP8 Formats for Deep Learning
7. [41] With Shared Microexponents, A Little Shifting Goes a Long Way :+1: from Meta, Microsoft :+1:
8. [34] Stable and low-precision training for large-scale vision-language models :+1: <br>
   *mentioned in Deepseek paper mixed precision training section. Not read yet.*
9. [Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization
10. [47] Microscaling Data Formats for Deep Learning
11. [Unread 2 Y2024] To FP8 and Back Again: Quantifying the Effects of Reducing Precision on LLM Training Stability
12. [145] Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model
13. [Unread 27]Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference
14. [45] PB-LLM: Partially Binarized Large Language Models
15. [55] QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models
16. [33] SpinQuant: LLM quantization with learned rotations
17. [35] FP8 versus INT8 for efficient deep learning inference
18. [28] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models
19. [434] Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation
20. [73] FP8 Quantization: The Power of the Exponent
21. [UnRead 121] Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers
22. [381] I-BERT: Integer-only BERT Quantization
23. [637] Training Deep Neural Networks with 8-bit Floating Point Numbers
24. [904] LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale
25. [UnRead 2568] DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients :+1:
26. [239] Model Accuracy and Runtime Tradeoff in Distributed Deep Learning: A Systematic Study :+1:
27. [Y2024]Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs

**Outlier**

1.[106] Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling

---
## 1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization

### Problems
The problems encountered when attempting to quantize transformer models like BERT, as highlighted in the paper:

- High Dynamic Range of Activations
  - The authors observed that the activation tensors in transformer models, particularly in the residual connections, exhibit a very high dynamic range.    
  - This means that the values within these tensors vary significantly in magnitude.
  - Quantization, especially when using low-bit fixed-point formats (like 8-bit integer), struggles to accurately represent such a wide range of values.    
  - Trying to capture both very small and very large values within the limited range of a low-bit format inevitably leads to significant quantization errors.
    
- Presence of Structured Outliers
  - The authors identified the presence of structured outliers within the activation tensors, particularly in the residual connections after the feed-forward network (FFN).    
  - These outliers are not random; **they appear to be correlated with specific input tokens and embedding dimensions**.    
  - Further analysis revealed that **these outliers influence the attention mechanism in the subsequent layers, encouraging the model to attend to specific tokens like the separator token ([SEP])**.

> In BERT-like models, an intermediate hidden activation tensor x has a shape (B, T, d), where B is the batch size, T is the sequence length, and d is the number of embedding dimensions (d = 768 for BERT-base, Devlin et al. 2019).
> In the following figure, you could tell the x-axis is in dimension of 768.

![image](https://github.com/user-attachments/assets/2d6e801d-8e6a-4ebd-8237-a67bb5b7ca95)

  - While this attention behavior might be beneficial for the model's performance, the outliers that cause it also create challenges for quantization.
    
- Sensitivity to Quantization Noise
  - Different parts of the transformer model exhibit varying sensitivities to the noise introduced by quantization.    
  - Some components, like the residual connections and certain attention heads, are particularly sensitive, and quantizing them aggressively can lead to a significant drop in accuracy.    
  - This sensitivity necessitates careful consideration of which parts of the model to quantize and at what bit-width.

![image](https://github.com/user-attachments/assets/520b9b0c-c112-4c49-92a1-03842f41b92d)

### Solutions

solutions proposed in the paper:

- Mixed-precision PTQ
  - The authors observed that different parts of the BERT model have varying sensitivities to quantization noise.    
  - To address this, they proposed using a higher **bit-width (16-bit) for the more sensitive activation tensors**, **particularly the residual sum after the feed-forward network (FFN)**.    
  - This higher bit-width allows for more accurate representation of both the FFN's input and output, minimizing potential errors.    
  - Additionally, they explored **using low-bit (2-4) quantization for weights and token embeddings**, which can significantly reduce model size without much accuracy loss.    

- Per-embedding-group PTQ
  - The authors identified that outliers in the activation tensors **primarily reside in a few specific embedding dimensions**.  
  - To address this, they proposed a novel per-embedding-group (PEG) quantization scheme, where distinct quantization parameters are used for different embedding dimensions or groups of dimensions.    
  - This method involves **splitting the activation tensor into groups along the embedding dimension and applying separate quantization parameters to each group**.    
  - To optimize this process, they introduced a range-based permutation step to ensure that all outliers are grouped together.    
  - This approach effectively handles outliers without significantly increasing computational overhead.

- Quantization-aware training (QAT)
  - The authors also explored QAT, where the model is trained with simulated quantization operations.    
  - This allows the model to adapt to the quantization noise, leading to improved performance compared to post-training quantization.    
  - During QAT, they used **learnable ranges for both weights and activations**, further enhancing the model's adaptability to quantization.

---

## 2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models

### **Key Takeaways in Three Sentences**  
1. The study demonstrates that **low-bit floating-point formats, particularly FP8, provide superior quantization accuracy for LLMs compared to INT8** , with comparable hardware efficiency at 8-bit precision.
2. The **Mixture of Formats Quantization (MoFQ) approach optimally selects between INT and FP per layer** , improving accuracy without increasing computational overhead.
3. MoFQ achieves **state-of-the-art results in both W4-only and W8A8 quantization** , outperforming existing methods like GPTQ, AWQ, LLM.int8(), and SmoothQuant while maintaining **efficient inference speed** .

### **Abstract**

The study finds that optimal quantization formats vary across layers in LLMs, leading to the **Mixture of Formats Quantization (MoFQ)**  approach, which selects the best format per layer.\
MoFQ achieves superior or comparable performance to current quantization methods for weight-only (W-only) and weight-activation (WA) quantization without additional hardware overhead.

### **Introduction** 

Quantization minimizes LLMs' size and inference costs, with prior work focusing on low-bit integer formats.\
However, as LLMs grow, integer quantization becomes less effective, requiring optimizations or alternatives. Low-bit floating-point formats have emerged as viable alternatives, with FP8 already supported in NVIDIA’s H100 GPUs.

The study:
1. Compares INT and FP formats in terms of hardware efficiency and quantization error. 
2. Proposes **Mixture of Formats Quantization (MoFQ)** , selecting the best format per layer.
3. Implements an inference system for W-only quantization, maintaining performance parity with INT-based systems.

### **Background and Related Works**

**Integer vs. Floating-Point Formats**  
- **Integer (INT)** : Uniformly distributed values.
- **Floating-Point (FP)** : Non-uniform distribution, allowing higher precision for small values but reduced precision for large values.
- **Hardware efficiency** : FP operations typically cost more than INT, but at 8-bit, FP8 and INT8 MAC (Multiply-Accumulate) units have nearly identical area costs.
 
**Post-Training Quantization (PTQ) for LLMs**

Two main PTQ strategies:
1. **Weight-Only (W-only) Quantization:**  Applies to weights only, e.g., W4A16.
2. **Weight-Activation (WA) Quantization:**  Quantizes both weights and activations, e.g., W8A8.

State-of-the-art (SOTA) methods:
 
- **LLM.int8()** : Uses mixed precision (INT8+FP16). 
- **SmoothQuant** : Redistributes quantization difficulty from activations to weights.
- **GPTQ & AWQ** : Use second-order information and pre-scaling techniques to improve quantization.


### **Comparative Analysis of INT and FP Formats**

**A. Hardware Cost of INT vs. FP MAC Units**
- **At 8-bit precision, FP8 and INT8 MACs require nearly the same hardware area** , aligning with H100 GPU capabilities.

**B. Quantization Error Comparison**  
1. **4-bit Weight-Only (W4) Quantization**  (LLaMA-65B model): 
  - 🔥 Some layers perform better with INT4, while others favor FP4, indicating **layer-dependent format preference** .

![image](https://github.com/user-attachments/assets/97be028e-fdb3-4883-a92e-b784d1ff1f87)

2. **8-bit Weight-Activation (W8A8) Quantization** : 
  - 🔥**Weights** : INT8 generally has lower quantization error. 
  - 🔥**Activations** : FP8 shows **better robustness**  for dynamic activation tensors.
  - Best choice: INT8 for weights, FP8 for activations—but hardware constraints necessitate using the same format per layer.

![image](https://github.com/user-attachments/assets/7af41adf-5821-4c0b-8599-1385c1982c87)

### **Exploiting INT and FP Complementarity**

**A. Improved Low-Bit FP4 Format** 
- IEEE floating-point format reserves exponent values for NaN and Inf. 
- **Reallocating NaN & Inf to normalized numbers improves FP4 precision**  by 35%.

**B. Mixture of Formats Quantization (MoFQ)**

- Selects the best quantization format (INT or FP) **per layer**  based on quantization error.
- Works for both **W-only and WA quantization** .
- **Algorithm** : Iterates through layers, computes quantization error for INT and FP, and selects the lower-error format.

**C. Low-Bit W-Only Inference System**  
- **INT4 and FP4 require conversion to FP16 before computation**  due to FP16 activations.
- **W8A8 quantization** : FP16 activations are converted to FP8 or INT8 based on next-layer format selection.
- **No additional hardware overhead for FP-based or MoFQ-based inference**  compared to INT-based quantization.

![image](https://github.com/user-attachments/assets/7d5e0e92-deda-4e40-8775-b428fff2ca0c)


### **Conclusion**  
- **Comparative study** : INT and FP formats have complementary strengths. 
- **Key finding** : **FP8 and INT8 MAC units have similar hardware costs at low-bit quantization** . 
- **MoFQ method** : 
  - Selects the best quantization format **per layer** . 
  - **Achieves state-of-the-art accuracy**  in W4-only and W8A8 quantization. 
  - **No additional inference latency or hardware overhead** .
 
---

## 5. [26] FP8-LM: Training FP8 Large Language Models

### **Abstract**

The paper explores **FP8 low-bit data formats**  for training large language models (LLMs), significantly reducing **memory usage and computation costs**  while maintaining accuracy.

The authors introduce an **FP8 automatic mixed-precision training framework**  with three levels of FP8 utilization, improving mixed-precision and distributed parallel training.

**Key results**  show that training the **GPT-175B model on an H100 GPU platform**  using FP8: 
- **Reduces memory usage by 39%**
- **Speeds up training by 75% compared to BF16 (Megatron-LM)**
- **Outperforms Nvidia Transformer Engine by 37%**

The **FP8 training methodology is generalizable**  to fine-tuning, instruction tuning, and reinforcement learning with human feedback (RLHF). The framework is **open-sourced**  at [aka.ms/MS.AMP](https://github.com/Azure/MS-AMP) .

### **1. Introduction**

LLMs have demonstrated exceptional performance in various domains but are extremely expensive to train.\
The cost of training models like **GPT-3 (175B) or PaLM (540B)**  is enormous, requiring **thousands of GPUs or TPUs** .\
Low-precision training is a **promising solution**  as it: 
- **Increases speed**
- **Reduces memory usage**
- **Minimizes communication overhead**

Most existing frameworks, such as **Megatron-LM, MetaSeq, and Colossal-AI** , use **FP32, FP16, or BF16 mixed-precision training** , but **FP8 offers significant efficiency gains** : 

- **2× speed-up**
- **50%-75% memory and communication savings**

#### **Challenges of FP8 Training**  
1. **Data underflow/overflow issues**  due to FP8’s limited dynamic range.
2. **Numerical instabilities and divergence**  during training.
#### **Proposed FP8 Mixed-Precision Framework**  
- Introduces **three levels of FP8 utilization**  (gradients, optimizer states, and distributed learning).
- Uses **precision decoupling**  and **automatic scaling**  to mitigate numerical instability.
- Achieves **29%-39% memory savings**  and **63%-65% communication cost reductions** .

> The resulting FP8 mixed-precision networks are more efficient than their pure FP16 counterparts, but a network that is in full INT8 is expected to be significantly more efficient yet.

### **2. FP8 LLM Training**

#### **2.1 FP8 Gradient and All-Reduce Communication**  
- Traditional mixed-precision training uses **FP16/FP32 for gradients** , leading to high communication costs.
- Applying **FP8 directly to gradients**  results in **loss of accuracy**  due to underflow/overflow.
- The paper proposes an **automatic scaling technique**  to adapt scaling factors dynamically, preventing numerical instability.

#### **2.2 FP8 Optimizer**  
- The **Adam optimizer**  typically consumes **16 bytes per parameter**  due to high-precision storage of gradients and optimizer states.
- The proposed **FP8 optimizer**  stores: 
  - FP8 first-order moment
  - FP16 master weights (with tensor scaling)
  - FP16 second-order moment
- This reduces **memory consumption from 16 bytes to 6 bytes per parameter**  (2.6× savings).

> My main takeaway is that direction of gradient matters, instead of magnitude.

![image](https://github.com/user-attachments/assets/aa8bdd9a-c5ea-4910-bfdf-836eca32f9b5)


1) FP8 master weight induces performance degradation (see the #2a vs. #3 lines in Fig. 8), while FP16 can maintain accuracy as FP32 (see #2a vs. #0 and #1) but requiring using tensor scaling.
   It reveals that the master weight is precision-sensitive.
   This can be attributed to **the master weight’s** role in updating weights, which tend to exhibit small magnitudes, necessitating high precision to maintain accuracy.
2) The training loss of **BF16** master weight is **slightly higher** than that of FP16 with a scaling factor because BF16 has fewer mantissa bits, resulting in lower precision (see #2a vs. #2b).
3) ❗ The second-order gradient moment is more precision-sensitive than the first-order one, because the ❗square calculation is easy to cause underflow and leads to accuracy degradation.
   Utilizing FP8 for the second-order gradient moment can lead to divergent training loss (see the #4 dot in Fig. 8).

Please Notice that FP8 #4 is diverged, not shown in the figure.

![image](https://github.com/user-attachments/assets/127e6274-629c-4f40-81f7-57cbb1ce9d98)


#### **2.3 FP8 Distributed Parallel Training**  
- **Tensor Parallelism** : Uses **FP8 for weight and activation tensors** , reducing compute and communication overhead.
- **Sequence Parallelism** : Converts activation tensors to **FP8 before communication** , reducing costs.
- **ZeRO (Zero Redundancy Optimizer) Support** : Distributes **full tensors**  across devices while preserving **FP8 scaling factors** .

### **3. Experimentation**
#### **3.1 Experimental Setup**  
- **Training Dataset** : Collected from **CommonCrawl, The Pile, C4, OpenWebText, Wikipedia, RedPajama** , and other curated sources.
- **Model Configuration** : Uses a **decoder-only Transformer**  architecture (like GPT-3), with **RoPE embeddings and Flash Attention** .

#### **3.2 Main Results**
##### **Model Performance**  
- **Loss curves of FP8 models match BF16 models** , confirming **accuracy preservation**
- **Zero-shot evaluations**  on **Lambada, HellaSwag, BoolQ, PIQA, COPA**  show **comparable performance between FP8 and BF16** .
- **Fine-tuning (SFT & RLHF)** : FP8 achieves: 
  - 27% faster fine-tuning
  - 32% reduction in model weight memory
  - 62% optimizer state memory savings

**System Performance**  
- **Memory reduction** : FP8 achieves **28%-39% lower memory usage**  than BF16.
- **Training speed improvement** : 
  - 75% faster training for GPT-175B
  - 37% faster than Nvidia Transformer Engine
- **Communication efficiency** : 
  - 63%-65% reduction in weight gradient communication
  - 34% lower activation-related communication costs

#### **3.3 Ablation Study**  
- **Gradient Scaling** : **Automatic scaling**  reduces **underflow/overflow errors** , improving training stability.
- **Optimizer Precision** : 
  - FP16 master weights outperform FP8 master weights in accuracy preservation.
  - FP8 first-order gradient moment is viable, but FP8 second-order moment leads to divergence.
- **Parallelism Optimization** : 
  - **FP8 sequence and tensor parallelism**  reduce communication costs by **34%** .
  - **FP8 ZeRO**  maintains a balanced GPU memory load while saving memory.

### **4. Related Work**  
- **Mixed-Precision Training** : Prior work focused on **FP16/BF16** , but **FP8 remains underexplored** .
- **Low-Precision LLM Training** : 
  - **OPT, Bloom, Gopher, Chinchilla**  used **BF16**  for better numerical stability.
  - FP8 support was limited before Nvidia Hopper GPUs.
  - This work provides the **first systematic FP8 training framework**  for **pre-training and fine-tuning LLMs** .


### **5. Conclusion**  
- Introduces a **new FP8 mixed-precision training framework**  with **automatic scaling**  and **precision decoupling** . 
- Achieves **significant reductions in memory, compute, and communication costs** . 
- **Maintains model accuracy**  across **GPT models from 125M to 175B parameters** .
- Demonstrates **versatility**  in pre-training, instruction tuning, and RLHF.
- **Future work**  includes scaling to even larger models, training multi-modal models, and deploying FP8 LLMs on edge devices.

### *Key Summary in 3 Sentences**

This paper introduces an **FP8 mixed-precision training framework**  that reduces memory consumption by **39%** , speeds up training by **75%** , and **outperforms Nvidia Transformer Engine by 37%**  while maintaining LLM accuracy.\
The framework uses **automatic scaling and precision decoupling**  to stabilize training, supports **FP8 optimizers and distributed training** , and generalizes to **fine-tuning and reinforcement learning with human feedback (RLHF)** .\
These findings establish **FP8 as the next-generation precision format for training LLMs** , significantly lowering costs while preserving model performance.

---

## 6. [139] FP8 Formats for Deep Learning
### **1. Introduction**
Deep learning models require increasing computational resources, necessitating lower-precision formats for efficiency.

FP8 is a **natural evolution**  from FP16 and BF16, reducing **compute and memory costs**  while maintaining **accuracy comparable to FP16** .

### Key contributions:
- **Two FP8 formats:**  
  - **E4M3** : 4-bit exponent, 3-bit mantissa (for weights and activations).
  - **E5M2** : 5-bit exponent, 2-bit mantissa (for gradients).
 
- **Training and inference in FP8**  match FP16/BF16 accuracy across CNNs, RNNs, and Transformers.
- **Post-training quantization (PTQ)**  using FP8 **outperforms int8**  while preserving model accuracy.


### **2. Aspects of FP8 Usage in Deep Learning**  
- **FP8 computations**  will be performed in **higher precision (FP16/FP32)** , with final results cast back to FP8.
- **Scaling factors**  are applied to **optimize FP8 precision** , similar to **loss-scaling in FP16 mixed precision** .
- **Handling of special values (NaNs, Infs) is modified in E4M3**  to increase dynamic range.

### **3. FP8 Binary Interchange Format**

![image](https://github.com/user-attachments/assets/dc054ba5-6892-4c63-888e-ef24c9789d8a)

FP8 includes **two encodings** : 
- **E4M3** : 
  - **Used for weights and activations** .
  - **No representation for infinities**  (max value: **448** ).
  - **Single NaN representation to extend range** .
 
- **E5M2** : 
  - **Used for gradients** . 
  - **Standard IEEE-like format** , supporting **NaNs and infinities** . 
  - Larger range (up to **57,344** ).

![image](https://github.com/user-attachments/assets/ac2dbf07-fd59-4902-a55b-0bb4b0385408)
  
#### **3.1 Special Value Representations**  
- **E4M3 removes infinities**  and limits NaNs to a **single pattern** , extending its **dynamic range** . 
- **E5M2 follows IEEE-754** , allowing **straightforward conversion from FP16** .

#### **3.2 Exponent Bias**  
- **E4M3 bias = 7, E5M2 bias = 15**  (matching IEEE-style representation). 
- Some models require **per-tensor scaling**  rather than a fixed exponent bias (Figure 2).

### **4. Empirical Results**
#### **4.1 Training**  
- FP8 training achieves **accuracy comparable to FP16/BF16**  across CNNs, RNNs, and Transformers.
- **Image Classification** : 
  - FP8 accuracy is **within statistical variation**  of FP16 for most CNNs (ResNet, MobileNet, VGG, etc.).
- **Language Translation** : 
  - FP8 BLEU scores **match FP16**  for Transformer and GNMT models.
- **NLP Models (Table 4, Figure 1)** : 
  - GPT models (126M to 175B parameters) trained in FP8 **match FP16 in perplexity** .
#### **4.2 Inference**  
- **FP8 post-training quantization (PTQ) outperforms int8** , retaining **full precision accuracy**  for:
  - BERT (F1 score on SQuAD).
  - GPT-3 (perplexity on Wikitext103).
- **FP8-trained models require no additional quantization steps** , simplifying deployment.

#### **4.3 Per-Tensor Scaling**  
- **Fixed exponent bias fails**  when additional tensors (e.g., residuals) are stored in FP8.
- **Per-tensor scaling maintains accuracy** , making FP8 viable for **expanded use beyond GEMMs** .


### **5. Conclusions**  
- **FP8 formats (E4M3, E5M2) efficiently reduce training and inference costs while maintaining accuracy** .
- **FP8 training is on par with FP16/BF16** , without hyperparameter changes.
- **FP8 simplifies inference**  by eliminating the need for quantization-aware training (QAT) required for int8.
- **Future work** : Expanding FP8 usage to **more tensor types and operations**  beyond matrix multiplications.


### **Key Takeaways in Three Sentences**

FP8 formats (E4M3 for weights/activations, E5M2 for gradients) **significantly reduce computation and memory overhead**  while maintaining **accuracy equivalent to FP16/BF16**  across CNNs, RNNs, and Transformer models.

**Post-training quantization (PTQ) with FP8 outperforms int8** , allowing for **simpler and more effective deployment**  of trained models. The study **validates FP8 training up to 175B parameters** , proving its scalability for large-scale deep learning applications.

---

## [Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization
### **Abstract**
The study introduces **ParetoQ** , a unified framework for evaluating extremely low-bit quantization (1-bit to 4-bit) in large language models (LLMs).\
It identifies a **learning transition**  between 2-bit and 3-bit models, with 3-bit and higher retaining pre-trained distributions, while 2-bit and below undergo drastic representation changes.\
By optimizing training strategies and quantization functions, **ParetoQ achieves state-of-the-art (SOTA) performance**  across multiple bit-widths.\
Notably, a **ternary (1.58-bit) 600M model surpasses a previous 3B ternary model** , using only one-fifth of the parameters.\
The study finds that **2-bit quantization offers a superior balance between accuracy, model size, and hardware efficiency**  compared to 4-bit and binary quantization.

### **1. Introduction**
As models scale, lower-precision computation is gaining traction due to **memory savings and computational efficiency** .\
Prior studies suggest conflicting optimal quantization levels (1.58-bit, 2-bit, 4-bit, etc.), but **no unified framework existed**  to systematically compare their effectiveness.

### **Key Questions:** 
- What is the optimal trade-off between bit-width and model size?
- How does quantization impact **scaling laws**  in low-bit settings?
- What training strategies and quantization functions yield **Pareto-optimal results** ?

### **ParetoQ Approach:**  
- Incorporates **five key dimensions** : model size (**N** ), token count (**D** ), quantization precision (**P** ), training strategy (**S** ), and quantization function (**F** ). 
- Identifies **bit-specific training schemes and quantization functions** . 
- Establishes that **binary quantization significantly degrades accuracy** , while **ternary (1.58-bit), 2-bit, and 3-bit quantization match or exceed 4-bit performance** .

### **2. A Better QAT Scheduling Strategy for Extreme Low-Bit LLMs**
#### **2.1 Training Budget Allocation**  
- **Post-Training Quantization (PTQ)**  is easier to implement but **performs poorly**  below 4-bit.
- **Quantization-Aware Training (QAT)**  integrates quantization during training, **improving low-bit performance** .
- **Optimal budget split:**  **90% full-precision training, 10% QAT fine-tuning** .
- **Finding-1:**  **QAT fine-tuning outperforms both PTQ and QAT from scratch** , achieving the best trade-off between accuracy and efficiency.

#### **2.2 Fine-tuning Characteristics**  
- Fine-tuning **improves accuracy across all bit-widths** , including binary and ternary models.
- **Lower-bit models (≤2-bit) require more fine-tuning (30B tokens), while 3-bit and 4-bit saturate at 10B tokens** .
- **Finding-2:**  **Bit-width transition effect:**  
  - **3-bit & 4-bit recover near full precision with fine-tuning** . 
  - **1-bit to 2-bit undergo substantial weight transformations** , requiring more tokens.
  - QAT serves as "compensation" for 3-bit+ but "reconstruction" for ≤2-bit models.

### **3. A Hitchhiker’s Guide to Quantization Method Choices**
#### **3.1 Trade-offs in Low-bit Quantization**  
1. **Range Clipping** : Lower-bit quantization suffers from outlier effects, requiring range clipping or **learnable scales** .
2. **Quantization Grids** : 
  - Binary & Ternary require balanced levels.
  - 2-bit prefers symmetric distribution*.
  - 3-bit and 4-bit benefit from including "0" in quantization levels.
#### **3.2 Introducing ParetoQ**
- **Combines the best quantization functions per bit-width** : 
  - **1-bit** : Elastic Binarization.
  - **1.58-bit, 2-bit** : **Stretched Elastic Quant (SEQ)** .
  - **3-bit, 4-bit** : **Learned Step Size Quantization (LSQ)** . 
- **Finding-3:**  No single best function for all bit-widths.
  **Learnable range settings outperform fixed statistical methods** , especially for **sub-4-bit quantization** .


### **4. Pareto-Optimality of Extremely Low-Bit LLMs**
#### **4.1 Accuracy-Compression Trade-off**  
- Ternary (1.58-bit), 2-bit, and 3-bit outperform 4-bit in accuracy-size efficiency.
- 2-bit and ternary quantization sit on the Pareto frontier.
#### **4.2 Hardware Constraints**  
- Ternary (1.58-bit) appears efficient but is difficult to implement due to indexing overhead. 
- 2-bit is more hardware-friendly**  due to **simpler storage and arithmetic operations.
#### **4.3 Accuracy-Speed Trade-off**  
- 2-bit achieves higher speed at the same accuracy as 4-bit.
- 2-bit kernels are significantly faster than 4-bit kernels in large matrix multiplications.

### **6. Related Work**  
- **Early quantization research**  focused on **8-bit and 4-bit LLMs** .
- Recent **sub-4-bit research**  explored **ternary, 2-bit, and 1-bit models** , but lacked a **unified comparison framework** .
- **ParetoQ is the first study to systematically compare sub-4-bit quantization schemes** .

**7. Conclusions**  
- ParetoQ unifies training and quantization schemes across five bit-widths.
- Ternary (1.58-bit), 2-bit, and 3-bit quantization outperform 4-bit in the accuracy-size trade-off.
- 2-bit is the most practical choice due to its hardware efficiency. 
- First framework that ensures fair comparisons across different quantization methods.

**Key Takeaways (3 Sentences)**  
1. ParetoQ introduces a unified framework for extreme low-bit quantization, identifying 2-bit as the most efficient trade-off between accuracy, memory, and computational speed.
2. A clear transition exists between 2-bit and 3-bit models, where lower-bit quantization requires substantial fine-tuning to compensate for drastic weight transformations.
3. With its optimized quantization functions and training schemes, ParetoQ achieves state-of-the-art performance across all bit-widths, surpassing previous specialized methods.

---
## [47] Microscaling Data Formats for Deep Learning
![image](https://github.com/user-attachments/assets/e1533c6c-54ca-47f8-946a-2d6fe7d08aef)


### **Introduction**
The rapid advancement of deep learning models has led to increased computational and storage costs.\
One approach to mitigating these costs is reducing bit-width precision in data formats, moving beyond traditional **FP32**  to lower-bit formats such as **FP16, BFloat16, FP8, and INT8**.\
However, per-tensor scaling in low-bit-width formats struggles with dynamic range limitations.\
**Microscaling (MX) data formats**  introduce **per-block scaling factors**  to enhance efficiency, maintain accuracy, and ease deployment in AI hardware.

### **Microscaling (MX) Data Formats**
MX formats encode numerical values in **fixed-size blocks** , where each block consists of: 
- A **shared scaling factor (X)**
- Multiple **narrow bit-width elements (Pi)**

This approach **extends the dynamic range**  beyond what per-tensor scaling allows, making sub-8-bit computations feasible.\
MX formats are **hardware-efficient**  while minimizing accuracy loss and **ensuring seamless adoption in existing AI frameworks**.

### **Concrete MX Formats**
MX formats are categorized based on **block size, scale format, and element bit-width**.

| Format | Block Size | Scale Data Format | Scale Bits | Element Format | Element Bit-width | 
| --- | --- | --- | --- | --- | --- | 
| MXFP8 | 32 | E8M0 | 8 | FP8 (E4M3/E5M2) | 8 | 
| MXFP6 | 32 | E8M0 | 8 | FP6 (E2M3/E3M2) | 6 | 
| MXFP4 | 32 | E8M0 | 8 | FP4 (E2M1) | 4 | 
| MXINT8 | 32 | E8M0 | 8 | INT8 | 8 | 

### **Scalar Float to MX Format Conversion**
To convert floating-point data to an MX format, the **shared scaling factor (X)**  is computed based on the largest absolute value in a block.\
Each element is then **normalized using X and quantized**  to the desired format. The conversion follows a **quantization algorithm**  that: 
1. **Determines the scaling exponent**  from the maximum value in the block.
2. **Computes X as a power of two** .
3. **Quantizes elements (Pi) based on X** .

### **Compute Flow and Training Pipeline**
For deep learning workloads, **dot-product operations (e.g., matrix multiplication, convolutions) are performed in MX formats** , while **non-dot operations (e.g., activations, normalization, residual add) remain in higher-precision formats like FP32 or BFloat16** .\
Training involves keeping a **master FP32 copy of weights**  while performing compute-intensive operations in MX formats.\
**Quantization-aware fine-tuning**  is often required for best accuracy, particularly for lower-bit formats like MXFP6 and MXFP4.

**Experimental Results**
**Inference** MX data formats were tested across **language models, vision transformers, speech recognition, and recommendation models**.

**Direct-Cast Inference (No Fine-Tuning)**  
- **MXINT8 performs nearly identically to FP32**  across all tasks, making it a **drop-in replacement** . 
- **MXFP8 and MXFP6 maintain good accuracy** , but MXFP6 requires fine-tuning for best results. 
- **MXFP4 suffers from significant accuracy loss** , especially in complex models.

**Post-Training Quantization (PTQ) with Error Diffusion**  
- Error diffusion (similar to **GPFQ-based post-training quantization** ) helps recover accuracy. 
- **MXFP6 achieves results close to FP32**  after error diffusion. 
- **MXFP4 remains significantly worse than FP32, limiting its practical use in inference** .

**Finetuned Inference**  
- **MXFP6 achieves FP32-level accuracy after fine-tuning** .
- **MXFP4 improves slightly but still lags behind** .
- MXINT8 continues to serve as the most effective **low-friction alternative to FP32** .

**Generative Model Inference (GPT-3, LLaMA)**  
- **MXINT8 closely matches FP32 performance**  on GPT-3 and LLaMA.
- **MXFP6 and MXFP8 perform well in most tasks** , but some degradation is observed in complex benchmarks.
- **MXFP4 shows noticeable loss** , especially in **zero-shot settings** .

**Training with MX Formats**
For the **first time, MX formats enable sub-8-bit training of large-scale transformers**  with minimal accuracy loss.

**Training with MXFP6**  
- **MXFP6 (E3M2) trains models with no accuracy drop compared to FP32** .
- This represents the **first demonstration of 6-bit training for large transformer models**  without modifications to the training recipe.
- **Hyperparameters remain unchanged from FP32 training**, making MXFP6 a practical choice.

**Training with MXFP4 + MXFP6**  
- **MXFP4 weights combined with MXFP6 activations/gradients**  yield slightly worse performance but remain viable for training.
- **Loss curves show only a minor increase in training loss** , proving feasibility.

### **Conclusion**
Microscaling (MX) data formats introduce **per-block scaling**  to **reduce bit-width**  while maintaining **high accuracy, hardware efficiency, and seamless integration** .

### Key findings: 
1. MXINT8 is an effective drop-in replacement for FP32 inference.
2. MXFP6 enables sub-8-bit inference and training with minimal accuracy loss.
3. MXFP4 combined with MXFP6 remains viable for training but suffers in inference. 
4. First-ever demonstration of training large generative models using 6-bit weights, activations, and gradients** .
MX formats offer a compelling path toward **lower precision deep learning**  without sacrificing model quality.


### **Three-Sentence Summary**
Microscaling (MX) data formats introduce **per-block scaling factors** , improving the efficiency and accuracy of sub-8-bit deep learning computations.

**MXINT8 serves as a near-lossless drop-in replacement for FP32 inference** , while **MXFP6 enables large-scale deep learning models to be trained with sub-8-bit precision without altering training recipes** . 

The results demonstrate that **MX formats significantly reduce computational and storage costs while maintaining model performance** , making them a strong alternative to traditional floating-point formats.

---

## 14. [33] SpinQuant: LLM quantization with learned rotations

### Abstract & Introduction
SpinQuant is a novel method for post-training quantization (PTQ) of Large Language Models (LLMs) that uses learned rotation matrices to enhance quantization accuracy while maintaining full-precision outputs. 

**Traditional PTQ struggles with outliers, which widen quantization ranges and reduce effective bit usage. Instead of relying on random rotations, SpinQuant learns rotation matrices that optimize quantization performance.**

The method significantly reduces accuracy gaps in 4-bit quantization across weights, activations, and KV-cache, outperforming previous quantization methods like LLM-QAT, SmoothQuant, and QuaRot.

Specifically, SpinQuant reduces the zero-shot reasoning accuracy gap on LLaMA-2 7B from 12.1 to 1.6 points, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points.

### Motivation & Outlier Reduction
Quantization reduces memory and computation costs but suffers from outliers, which stretch the value distribution.

Existing methods like mixed-precision quantization and weight-activation trade-offs attempt to mitigate this but remain suboptimal.

**SpinQuant rotates activation and weight matrices to remove outliers, making them more Gaussian-like, thus improving quantization efficiency.**

The paper demonstrates that some random rotations lead to better results than others, with a 13-point accuracy difference in zero-shot reasoning tasks.

### Methodology

![image](https://github.com/user-attachments/assets/5906ef52-5f44-4783-a131-2e5ea9eb26c6)

The method optimizes rotation matrices using Cayley SGD, which maintains orthonormality while minimizing the quantized network loss.

This process enhances quantization robustness, significantly outperforming random rotations.

![image](https://github.com/user-attachments/assets/421f725d-9051-49e6-b640-4db5b51ded7f)


### Experiments & Results
Experiments were conducted on LLaMA-2 (7B/13B/70B), LLaMA-3 (1B/3B/8B), and Mistral-7B, evaluated across zero-shot reasoning tasks and perplexity on WikiText2.

### Key findings:

SpinQuantₙₒₕₐₑd effectively closes the quantization gap in W4A8KV8 settings, making it comparable to QuIP# and OmniQuant.

SpinQuantₕₐₑd achieves 64.0 average accuracy in W4A4KV4, reducing the accuracy gap to full precision to 2.9 points (LLM-QAT had a 22-point gap).

Against QuaRot, SpinQuant improves accuracy by up to 28.6 points in extreme quantization settings.

Speed analysis shows that SpinQuantₕₐₑd incurs only an 8% latency overhead, making it a practical approach.

### Conclusions
SpinQuant is the first PTQ method to optimize learned rotation matrices, offering:

- Significant improvements over existing quantization techniques by reducing outliers.
- State-of-the-art performance in 4-bit quantization settings.
- Compatibility with advanced weight quantization methods like GPTQ.

Its learned rotations make quantized LLMs more robust, reducing the performance gap to full-precision models, making low-bit LLM inference practical.

### Three-Sentence Summary

SpinQuant introduces learned rotation matrices to mitigate outliers in weight and activation distributions, improving LLM quantization efficiency.

Using Cayley SGD optimization, it significantly reduces the accuracy gap in 4-bit quantization, outperforming SmoothQuant, LLM-QAT, and QuaRot.

Experiments on LLaMA-2, LLaMA-3, and Mistral-7B show state-of-the-art quantization performance, with SpinQuant narrowing the accuracy gap to full precision by up to 45.1% relative to QuaRot.

---

## 17. [35] FP8 versus INT8 for efficient deep learning inference
This paperexplores the efficiency and accuracy of using FP8 (8-bit floating point) and INT8 (8-bit integer) formats for deep learning inference, particularly on edge devices.

The key points and findings of the paper are summarized as follows:

### 1. Introduction and Motivation
The paper discusses the growing interest in using FP8 for neural network training, especially with Nvidia's introduction of FP8 in their Hopper architecture GPUs.

While FP8 is being considered for training, the paper focuses on its implications for efficient inference on edge devices, where INT8 is commonly used due to its efficiency.

The authors question whether training networks in FP8 and deploying them in the same format could bypass the need for quantization, which is currently required when converting FP32/FP16 models to INT8.

### 2. Hardware Considerations
The paper argues that FP8 is less efficient than INT8 in terms of hardware area and energy consumption. FP8 requires 50% more area and energy compared to INT8, making it less suitable for efficient inference.

Floating-point operations are inherently more complex and costly in hardware compared to integer operations, especially when considering the need for floating-point accumulators.

The authors highlight that FP8 implementations often involve mixed precision (e.g., FP16 for activations), which can lead to inefficiencies, particularly in networks with large activation tensors.

### 3. Accuracy Comparison
The paper provides a theoretical and empirical comparison of FP8 and INT8 formats in terms of network accuracy.

The key difference between FP8 and INT8 lies in their ability to handle outliers. FP8, with its exponent bits, can better represent outliers, while INT8 is more efficient for well-behaved, Gaussian-like distributions.

> the only significant difference between the two formats is in their ability to capture outliers.

❗ In post-training quantization (PTQ), INT8 generally performs better for networks without significant outliers, while FP8-E4 (4 exponent bits) is better for networks with outliers, such as **ransformers**.

❗ In quantization-aware training (QAT), INT8 often outperforms FP8, as training can reduce the impact of outliers, making INT8 more accurate and efficient.

### 4. Transformer Networks
Transformer networks, particularly BERT, **exhibit significant outliers in certain layers, making FP8-E4 more accurate in PTQ settings.**

However, the paper argues that these outlier issues can be mitigated with techniques like mixed precision (W8A16) or quantization-aware training, allowing INT8 to achieve similar accuracy without the hardware inefficiencies of FP8.

**FP8-E4 vs FP8-E5**

❗ It’s all about the outliers. If a distribution has very significant outliers, the FP8-E4/FP8-E5 format is more accurate.

For well-behaved networks without many outliers, the INT8 format is significantly more accurate in the PTQ setting than FP8-E4 and FP8-E5.

We also see that FP8-E5 is never the best format for inference; even for the transformer layers with significant outliers, the FP8-E4 format is better.

For some computer vision networks, INT8 is better; for some networks with significant outliers, the FP8-E4/FP8-E5 formats are better.

Purely taking these results into account, the FP8-E4 format looks comparatively worse than FP8-E2 and FP8-E3.

Combining these findings with the hardware implementation costs, the FP8-E4 format itself looks like it is a
worse choice than its lower-exponent bit brethren, which are both cheaper hardware-wise and more accurate. 

If anything, the **FP8-E3 format** stands out positively in this accuracy analysis compared to
other FP formats.

**However, Deepseek FP8 use E4M4 for both forward and backwards computation.**

**QAT versus PTQ**

![image](https://github.com/user-attachments/assets/3bc7ef7b-959d-4869-ac98-855451453709)

One surprising trend is that the INT8 results improve more than their PTQ baseline than their FP8 counterparts.

There is a good reason for this; again, it’s about the outliers.

When performing QAT, outliers are clipped and do not receive a gradient.

The clipped weights/activations then tend towards the clipping threshold due to regularization.

But most importantly, with the outliers clipped, the network learns weights that still perform well despite the outliers being removed.

when training the quantization parameters with a method like LSQ (Esser et al. (2020)), the network can learn to make the ranges smaller so as to find a better trade-off between the clipping and quantization errors.

The smaller the range, the more accurate your quantized representation will be.

This is especially the case for INT8, where the sensitivity to the quantization ranges is much larger than the floating-point formats with more exponent bits that are naturally more resistant to outliers.

This way, the INT8 format benefits significantly more from QAT than the FP formats.


**QAT**

Numerical representation chosen for training (e.g., FP8-E4) does not dictate how the distributions of weights and activations form.

Instead, these distributions are primarily shaped by the overall training process, including factors like regularization, optimizer settings, and initialization.

**QAT Transformer**

There are significant outliers in the summation going into the layer-norm in some of the fully connected modules,  especially in the final layers of the network.

These outliers force the attention mechanism in the next layer to pay attention to some meaningless tokens – like **sentence separator tokens, periods, or commas** – that occur in the text, causing that specific token to not update significantly.

Simply clipping these outlier reduces accuracy significantly.

Only a few layers are the best in FP8-E4. The other layers find a lower MSE error with the FP8-E2 and FP8-E3 formats.

In the most naive PTQ setting, the FP8-E4 format performs better than INT8. 

**Comparison to other work**

> **The problems with the quantization of gradients put forth in the paper are well-known and have been
addressed in many works in the past. (Sun et al. (2019); Gupta et al. (2015)). Many works have shown
the necessity of stochastic rounding and proper range setting for the backward pass that alleviate
these issues and make INT8 for gradients work just as well (Sun et al. (2019).**
> What is this?

Transformer models can be executed in INT8 as well, both with PTQ and QAT.

Finally, the only comparison with the INT8 format comes in the form of comparing transformer-based language models in the PTQ setting.

However, as argued, these problems are easily fixable for transformer networks, making them able to execute entirely in INT8 or in mixed precision with a small number of activations in INT16.

### 5. Comparison to Other Work
The authors compare their findings with other works, such as those from Nvidia, Arm, and Intel, and Graphcore, which also explore FP8 for training.

They find that their results are consistent with these works but provide a more comprehensive comparison between FP8 and INT8.

The paper highlights that other works often omit critical comparisons, such as the hardware efficiency of FP8 versus INT8, and the impact of mixed precision on inference performance.

Several works have shown that even in the INT8 PTQ setting you can get back your original accuracy with any of several possible tricks

### 6. FP8 to INT8 Conversion
The paper explores the feasibility of converting FP8-trained networks to INT8.

For networks without significant outliers, the conversion is straightforward and can even improve accuracy.

**For networks with outliers, such as transformers, the conversion to INT8 may degrade accuracy, but this can be mitigated with quantization-aware training.**

### 7. INT Quantization Paradigm
The authors advocate for the use of INT8 and INT4 formats for efficient inference, as they offer better hardware efficiency and accuracy for most networks.

They present a quantization paradigm where INT16 is used for high accuracy, INT8 for most networks, and INT4 for further efficiency, especially in weight-bound networks like large language models.

### 8. Conclusion
The paper concludes that FP8 is not a suitable replacement for INT8 in efficient deep learning inference.

While FP8 can handle outliers better in certain cases, the hardware inefficiencies and the availability of techniques to mitigate outlier issues in INT8 make it a less attractive option.

The authors recommend using INT8 and INT4 formats for efficient on-device inference, as they provide the best trade-off between accuracy and efficiency.

### Key Takeaways:
FP8 is less efficient than INT8 in terms of hardware area and energy consumption.

INT8 is more accurate for most networks, especially after quantization-aware training, which can reduce the impact of outliers.

**FP8 is only beneficial in specific cases, such as transformer networks with significant outliers, but these issues can be addressed with INT8 using mixed precision or QAT.**

INT4 and INT8 are recommended for efficient inference, offering a better balance of accuracy and hardware efficiency compared to FP8.

Overall, the paper provides a comprehensive analysis of the trade-offs between FP8 and INT8, concluding that INT8 remains the superior choice for efficient deep learning inference on edge devices.


---

## 16. [28] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models
This paper, explores the use of low-bit quantization techniques to reduce the size and computational cost of Large Language Models (LLMs) during inference.

The authors investigate the effectiveness of both integer (INT) and floating-point (FP) formats for quantization and propose a novel method called Mixture of Formats Quantization (MoFQ) that selects the optimal format (INT or FP) for each layer of the model.

The key contributions and findings of the paper are summarized below:

### 1. Introduction and Motivation
Large Language Models (LLMs) are widely used in multimedia applications, but their deployment is challenging due to their large size and high computational cost.

Low-bit quantization (e.g., INT8, INT4, FP8, FP4) is a common technique to reduce model size and inference cost.

Traditionally, integer formats have been used, but as LLMs grow larger, integer quantization faces challenges in maintaining accuracy.

Floating-point formats (e.g., FP8, FP4) are emerging as alternatives, supported by advanced hardware like NVIDIA's H100 GPU.

FP formats offer a larger dynamic range and better precision for small values but may have higher hardware costs.

The paper aims to compare INT and FP formats for LLM quantization and propose a method to leverage the strengths of both formats.

### 2. Comparative Analysis of INT and FP Formats
Hardware Efficiency: The authors compare the hardware cost of INT and FP multiply-accumulate (MAC) units across different bit-widths.

They find that at 8-bit, the hardware cost of FP8 and INT8 MAC units is **almost identical**, making FP8 a viable alternative.

Quantization Error: The authors analyze the quantization error of INT and FP formats on weight and activation tensors from the LLaMA-65B model. They find that:

For 4-bit weight quantization, some layers prefer INT4, while others prefer FP4.

For 8-bit weight and activation quantization, INT8 is better for weights, while FP8 is better for activations due to its robustness to dynamic ranges.

### 3. Mixture of Formats Quantization (MoFQ)
Based on the observation that different layers prefer different formats, the authors propose MoFQ, a method that selects the optimal format (INT or FP) for each layer based on quantization error.

MoFQ is applicable to both weight-only (W-only) and weight-activation (WA) quantization scenarios:

For W-only quantization, MoFQ selects the best format for weight tensors.

For WA quantization, the same format is used for both weights and activations in each layer, as current hardware does not support mixed INT8 and FP8 operations.

The authors also improve the FP4 format by reallocating NaN and Inf representations to increase precision, reducing quantization errors by about 35%.

### 4. Implementation and Experiments
The authors implement an FP/MoFQ-based inference system for W-only quantization, which maintains consistent inference speed compared to INT-based systems.

Experiments are conducted on LLaMA and OPT models using datasets like LAMBADA, PIQA, HellaSwag, and WikiText-2.

Results:

- For 8-bit WA quantization, MoFQ8 (a mixture of FP8 and INT8) achieves accuracy close to FP16 models, outperforming both INT8 and FP8 quantization.
- For 4-bit W-only quantization, MoFQ4 (a mixture of FP4 and INT4) improves quantization accuracy, with most layers preferring FP4.
- MoFQ-based quantization is faster than traditional methods like GPTQ and AWQ, with no additional hardware overhead.

### 5. Key Contributions
The paper provides a comparative analysis of INT and FP formats for LLM quantization, offering insights into their hardware efficiency and quantization error.

It proposes MoFQ, a **layer-wise format selection method that leverages the complementary advantages of INT and FP formats**, achieving state-of-the-art results in both 4-bit W-only and 8-bit WA quantization.

The authors implement an FP/MoFQ-based inference system that maintains consistent inference speed with INT-based systems, demonstrating the practicality of their approach.

### 6. Conclusion
The paper demonstrates that floating-point formats can significantly improve LLM quantization, especially when combined with integer formats through the MoFQ method.

The proposed method achieves better or comparable results to existing quantization techniques, with no additional hardware overhead, making it a promising approach for efficient LLM deployment.

In summary, this paper introduces a novel quantization method that combines the strengths of integer and floating-point formats, offering a practical solution for reducing the computational cost of large language models while maintaining high accuracy.

---

## 17. [434] Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation
This paper explores integer quantization techniques for deep learning inference, which help reduce model size and improve computational efficiency by leveraging high-throughput integer math pipelines.

The authors provide a mathematical foundation for different quantization choices and evaluate their empirical performance across multiple deep learning models spanning vision, speech, and language domains.

The study focuses on 8-bit integer quantization, demonstrating that accuracy loss can be minimized to within 1% of the floating-point baseline, even for challenging models like MobileNets and BERT-large.

### Key Highlights:
Benefits of Integer Quantization
- Integer operations offer up to 16× speedup over FP32 on NVIDIA Turing GPUs.
- Smaller word sizes reduce memory bandwidth pressure and improve cache utilization.
### Quantization Methods
- Affine Quantization: Uses a scale factor and zero-point but adds computational overhead.
- Scale Quantization: More efficient as it avoids additional computations by using only a scale factor.
### Post Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)
- PTQ quantizes weights and activations after training but may lead to accuracy degradation for some models.
- QAT fine-tunes the network with quantization effects included, yielding better results, especially for MobileNets and Transformers.

Per-channel quantization for weights and per-tensor quantization for activations is recommended for accuracy and performance.

### Calibration Methods

Max, entropy, and percentile-based (99.9%-99.999%) calibrations are tested.

No single calibration is best for all networks; entropy and 99.99% percentile provide optimal accuracy for most models.

### Optimizations to Reduce Accuracy Loss

- Partial Quantization: Leaves sensitive layers in floating point, improving accuracy while retaining performance benefits.
- Learning Quantization Parameters: Fine-tuning the quantization ranges (PACT method) slightly improves accuracy but is not always necessary for int8 quantization.

**ELU Clipping: For BERT, modifying the GELU activation range significantly enhances post-training quantization results.**

### Recommended Workflow

- Start with PTQ using max, entropy, and percentile calibrations.
- If accuracy loss is high, use Partial Quantization to leave sensitive layers unquantized.
- If further accuracy recovery is needed, perform QAT with pre-determined best calibration.

### Final 3-Sentence Summary:
This paper presents a quantization workflow for deep learning inference, balancing speed and accuracy by leveraging int8 quantization with scale quantization for weights and activations.

It demonstrates that a combination of post-training quantization, partial quantization, and quantization-aware training ensures that accuracy remains within 1% of floating-point models, even for challenging architectures like MobileNets and BERT.

The proposed methods significantly improve inference efficiency, making integer quantization a practical approach for deploying neural networks on hardware accelerators.

---

## FP8 Quantization: The Power of the Exponent
This paper investigates FP8 quantization for neural network inference, comparing it with traditional INT8 quantization.

The authors analyze different FP8 format configurations, focusing on the trade-off between exponent and mantissa bits.

The study includes theoretical analysis, post-training quantization (PTQ), and quantization-aware training (QAT) across various deep learning models.

**Key findings indicate that FP8 outperforms INT8 in post-training quantization, particularly for networks with activation outliers, such as Transformers.**

However, in quantization-aware training, the accuracy difference between INT8 and FP8 diminishes as the network learns to adapt to the quantization scheme.

### Key Takeaways:
- FP8 vs. INT8: FP8 offers an additional degree of freedom through exponent bits, which helps manage outliers better than INT8, improving post-training quantization accuracy.
- Optimal Format Selection: **The choice of FP8 format (e.g., 5M2E vs. 4M3E) depends on the severity of outliers in a network,** with higher exponent bits benefiting models with significant activation outliers.
- QAT Reduces Differences: Quantization-aware training helps networks adapt to the quantization format, making INT8 and FP8 perform similarly in trained models.

### Three-Sentence Summary
This paper explores FP8 quantization and its advantages over INT8, showing that FP8 provides better accuracy in post-training quantization due to its ability to handle activation outliers more effectively.

Through analytical and empirical studies, the authors determine that the optimal FP8 configuration depends on the balance between exponent and mantissa bits, with higher exponent bits benefiting networks with larger outliers.

However, in quantization-aware training, the differences between FP8 and INT8 diminish as the network learns to optimize within the quantization scheme.

---

## 19. [121] Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers
### **1. Introduction** 
Deep neural networks (DNNs) have achieved remarkable success in fields such as image processing, object detection, and natural language processing.

However, training these models requires extensive floating-point (FP) operations, leading to high memory, compute, and energy costs.

DNN quantization has been explored as a solution to this problem, primarily focusing on inference quantization (e.g., BWN, XNOR-Net).

Recent advancements extend quantization to training, but existing methods still leave parts of the computation in high-precision floating-point (e.g., FP8, FP16) or do not quantize Batch Normalization (BN).

The major challenges in achieving full quantization include:
 
- **Incomplete quantization** : Some parts of the model remain in floating-point, limiting memory and compute savings.
- **Unquantized Batch Normalization (BN)** : BN is critical for training stability but is often left in floating-point.
- **Lack of a unified low-bit training framework** : No existing method successfully trains large-scale models with only low-bit integer operations.

This work introduces **WAGEUBN** , a unified **INT8 training framework**  that quantizes all major operations, including: 
- **Weights (W)**
- **Activations (A)**
- **Gradients (G)**
- **Errors (E)**
- **Updates (U)**
- **Batch Normalization (BN)**
- **Momentum optimizer**

### **2. Related Work**

#### **2.1. Inference Quantization** 
Inference quantization aims to reduce the memory and compute cost of DNN inference by converting FP operations to bit-wise integer operations. Some key works include:
 
- **BWN (Binary Weight Networks)** : Quantizes only weights to {-1,1}.
- **XNOR-Net** : Quantizes both weights and activations to binary values.
- **ADMM-based Quantization** : Compresses models via alternating direction method of multipliers.
- **FP8/INT16-based Methods** : Reduce bit-width to maintain accuracy.

However, inference quantization only focuses on the forward pass and does not address the backward pass needed for training.

#### **2.2. Training Quantization** 
Training quantization extends quantization to the backward pass (gradients and updates). Key approaches include:
 
- **DoReFa-Net** : Uses low-bit activations, weights, and gradients but retains FP elements.
- **MP (Mixed Precision)** : Uses FP16 for training but is not purely integer-based.
- **FP8 Training** : Reduces training precision to FP8 but retains FP operations in BN.
- **QBP2** : Uses 8-bit INT for weights, activations, and errors, but gradients remain FP.
- **WAGE** : The most complete prior work, quantizing W, A, G, E, and U, but it lacks BN layers, making it unsuitable for large-scale DNNs.

![image](https://github.com/user-attachments/assets/2d177f82-6d45-424e-9801-3bf3b85c3ca8)


### **3. WAGEUBN Framework**
#### **3.1. Key Contributions**  
- Fully quantizes **all**  training data paths (W, A, G, E, U, BN, and Momentum).
- Introduces three custom quantization functions for different training components.
- Quantizes Batch Normalization (BN) for the first time.
- Applies INT8 quantization to large-scale networks like ResNet on ImageNet.

#### **3.2. Straight-Through Estimator (STE)**

Quantization introduces a **non-differentiability problem** , making gradient updates challenging.

**STE**  is used to approximate gradients during backpropagation:
$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial x_q}$$

This method allows training to proceed despite non-differentiability.
#### **3.3. Quantization Functions** 
The framework introduces three quantization functions tailored for different data types:
 
1. **Direct Quantization** : Used for **weights, activations, and BN parameters**.
$$Q(x, k) = \frac{\text{round}(x \cdot 2^{k-1})}{2^{k-1}}$$

This function approximates floating-point values to the nearest discrete integer.
 
2. **Constant Quantization** : Used for **gradients**  to ensure a fixed update bit-width.
$$CQ(x) = \frac{Sd(x)}{2^{k-1}}$$

This function scales data dynamically to avoid excessive precision loss.
 
3. **Shift Quantization** : Used for **errors** , which are typically small-magnitude values.
$$SQ(x, k) = R(x) \cdot \text{clip}(Q(\text{Norm}(x), k), -1, 1)$$

This function ensures errors maintain a meaningful range.
#### **3.4. Quantized Training Steps** 
The framework quantizes the entire training process:
- **Forward Pass:** 
  - Quantizes inputs, applies INT8 convolutions, quantizes BN, and applies INT8 activation.
- **Backward Pass:** 
  - Uses INT8 gradients and error propagation.
  - Applies INT8 momentum optimization.
  - Uses fixed-point updates for weight adjustments.
- **Momentum Quantization:** 
  - Conventional optimizers like Adam/Momentum use floating-point accumulations.
  - WAGEUBN constrains them to fixed-point INT8.
  
### **4. Results**
#### **4.1. Accuracy Evaluation**
The framework was tested on **ResNet18/34/50 with ImageNet**.
Two versions were evaluated: 
1. **Full 8-bit INT** : All computations use 8-bit integers.
2. **16-bit E2 Variant** : Uses 16-bit error gradients to improve convergence.
| Model | Vanilla FP32 | WAGEUBN (16-bit E2) | WAGEUBN (Full 8-bit) | 
| --- | --- | --- | --- | 
| ResNet18 | 68.70% | 67.40% | 64.79% | 
| ResNet34 | 71.99% | 68.50% | 67.63% | 
| ResNet50 | 74.66% | 69.07% | 67.95% | 
 
- **Accuracy loss is minimal**  (~3-5% top-1 accuracy).
- **16-bit E2 improves accuracy**  over pure 8-bit training.
- **Comparable accuracy to FP8-based methods**.

#### **4.2. Efficiency Gains** 
WAGEUBN significantly reduces hardware overhead compared to FP32:
| Precision | Compute Speedup | Power Reduction | Circuit Area Reduction | 
| --- | --- | --- | --- | 
| INT8 | 3× - 9× | 10× - 30× | 9× - 30× | 
| FP8 | 0.73× | 0.31× | 0.4× | 
| FP16 | 0.58× | 0.4× | 0.4× | 
 
- **Memory is reduced by 4×** .
- **Computation is up to 9× faster** .
- **Power usage is up to 30× lower** .
### **5. Analysis**  
- **Batch Size Sensitivity** : WAGEUBN works best with batch sizes ≥32. Smaller batches lead to higher accuracy loss. 
- **Error Gradient Sensitivity** : The **8-bit Flag QE2 method**  significantly improves accuracy over simple 8-bit quantization.
- **Quantization Impact** : 
  - **BN and Errors are the most sensitive to precision loss** .
  - **Weights and activations are more robust to INT8 constraints** .
### **6. Conclusion** WAGEUBN is the **first complete INT8 quantization framework**  for training large-scale DNNs. It achieves: 
- **End-to-end INT8 training**  (including BN and optimizers).
- **Competitive accuracy with significant hardware efficiency improvements** .
- **Potential for online learning on energy-efficient devices** .
Future work includes specialized **hardware architectures**  to fully exploit WAGEUBN’s benefits.

### **Three-Sentence Summary**

WAGEUBN is a **fully quantized INT8 training framework**  for large-scale deep learning, covering all data paths (W, A, G, E, U, BN, and Momentum).

By introducing novel quantization functions and INT8 batch normalization, it **reduces memory by 4×, accelerates computation by up to 9×, and cuts power usage by 30×** , while achieving **comparable accuracy to FP-based models** 

This work establishes a **scalable and efficient approach for energy-efficient AI hardware and online learning** .

---

## 20. [381] I-BERT: Integer-only BERT Quantization

### Challenges Addressed
Inefficiency of Transformer-Based Models: BERT and RoBERTa achieve high accuracy but have high memory, latency, and power costs, making them difficult to deploy on edge devices and data centers.

Limitations of Previous Quantization Approaches: Prior Transformer quantization methods rely on floating-point arithmetic, preventing efficient execution on integer-only hardware like ARM Cortex-M processors and Turing Tensor Cores.

Difficulty in Handling Non-Linear Functions: Existing integer-only quantization techniques are mainly designed for CNNs with piece-wise linear functions like ReLU. Transformers use complex non-linear functions (GELU, Softmax, LayerNorm), which are hard to process using integer arithmetic without significant accuracy loss.

![image](https://github.com/user-attachments/assets/da7b8969-c3fe-4db7-a0a0-8d1ed1210382)

### Solution - I-BERT Approach:
Integer-Only Approximation for Non-Linear Functions:

- GELU: Approximated using a second-order polynomial (i-GELU), avoiding floating-point computation while maintaining accuracy.
- Softmax: Transformed into a stable integer-friendly form using logarithm and bit-shift operations (i-exp).
- LayerNorm: Computed using an integer-only square root algorithm.

End-to-End Integer Execution:

- MatMul and embeddings are computed using INT8 multiplication and INT32 accumulation.
- Non-linear operations (GELU, Softmax, LayerNorm) are applied directly to INT32 values and re-quantized to INT8.
- The entire inference process remains in integer arithmetic without dequantization.

### Results and Impact:
- Accuracy: I-BERT achieves comparable or slightly better accuracy than FP32 models on the GLUE benchmark, with an improvement of 0.3 (Base) and 0.5 (Large) in average score.
- Efficiency: I-BERT provides 2.4× – 4.0× speedup in inference compared to FP32 on NVIDIA T4 GPUs.
- Deployment Feasibility: Eliminates floating-point dependency, making it ideal for deployment on integer-only hardware like ARM Cortex-M processors and specialized accelerators.

### Key Takeaways in 3 Sentences
I-BERT introduces a novel integer-only quantization method for BERT, eliminating floating-point operations and enabling efficient deployment on integer-only hardware.

By approximating non-linear functions like GELU, Softmax, and LayerNorm with polynomial and integer arithmetic, it maintains high accuracy while significantly improving inference speed.

Evaluation on the GLUE benchmark and hardware tests demonstrate that I-BERT achieves up to 4× speedup while maintaining or slightly improving accuracy over FP32 models.

---

## 21. [904] LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale
The paper introduces LLM.int8(), a quantization method enabling 8-bit matrix multiplication for large transformers without degrading performance.

Traditional 8-bit quantization methods struggle with large-scale models due to systematic outlier features that disrupt quantization precision beyond 6.7B parameters.

To overcome this, LLM.int8() combines vector-wise quantization (which assigns separate normalization constants per inner product) and mixed-precision decomposition, where **outlier dimensions are computed in 16-bit** while 99.9% of values remain in 8-bit.

This approach allows large-scale transformers like OPT-175B and BLOOM to run on a single consumer GPU without accuracy loss.

![image](https://github.com/user-attachments/assets/36c7a517-2cd5-4f69-ad74-31388c354235)

### Key findings include:

- Emergent outliers: Beyond 6.7B parameters, certain feature dimensions dominate transformer attention and predictive performance, requiring higher precision.
- Quantization challenge: Existing methods fail due to these outliers, as they occupy only 0.1% of the data but significantly impact accuracy.

### LLM.int8 solution

By isolating outliers in 16-bit operations while keeping most computations in 8-bit, the method retains full-precision inference while cutting memory usage by half.

The study empirically validates that LLM.int8() maintains 16-bit accuracy across models up to 175B parameters, making LLMs more accessible and practical. The method is open-sourced and integrated with Hugging Face Transformers.

### Three-Sentence Key Takeaways
LLM.int8() enables performance-preserving 8-bit quantization for transformers up to 175B parameters by combining vector-wise quantization and mixed-precision decomposition to handle emergent large-magnitude features.

These systematic outliers, appearing beyond 6.7B parameters, disrupt standard 8-bit quantization but can be isolated in 16-bit precision while keeping over 99.9% of computations in 8-bit, achieving a 2× memory reduction.

This allows massive models like OPT-175B and BLOOM to run efficiently on consumer GPUs, making large-scale LLM inference more accessible.

---

## 22.[637] Training Deep Neural Networks with 8-bit Floating Point Numbers

### 1. Introduction
The paper addresses the challenge of training deep neural networks (DNNs) with reduced precision floating point numbers, specifically using 8-bit floating point (FP8).

While inference has been successfully performed with low precision (as low as 2–4 bits), training has traditionally required at least 16-bit precision due to gradient fidelity concerns.

The paper proposes novel techniques that allow DNN training using FP8 without accuracy loss, promising 2–4× improvements in energy efficiency and throughput.

### 2. Challenges in Low-Precision Training
Three major challenges arise when reducing DNN training precision:
- Loss of accuracy when all operands (weights, activations, errors, gradients) are quantized to 8 bits.
- Reduced accumulation precision (moving from FP32 to FP16) significantly impacts convergence.
- Weight updates in 16-bit may degrade accuracy unless managed properly.

![image](https://github.com/user-attachments/assets/90a7d951-8c63-46f1-8159-1bc21964bb6c)


### 3. Proposed Solutions
The paper introduces several key innovations:

- Custom FP8 Format: A new FP8 format (1-bit sign, 5-bit exponent, 2-bit mantissa) that effectively represents DNN parameters.
- **Chunk-Based Accumulation**: Breaking matrix multiplications into small chunks before accumulation to prevent truncation errors.

![image](https://github.com/user-attachments/assets/ff65ed3e-1fce-4d12-b10c-263b4ebd29b1)

- Floating Point Stochastic Rounding: A rounding method that retains small numerical details to prevent loss of information.
- Mixed-Precision Computations: Using FP8 for most computations while keeping critical accumulations and weight updates in FP16.

### 4. Experimental Results
The proposed FP8 training method was tested on various models, including ResNet18/50, AlexNet, and CIFAR10-CNN. Results show:
- No significant accuracy loss compared to FP32.
- Memory savings: Model sizes were reduced by ~50%.
- Energy-efficient hardware implementation: A prototype chip demonstrated 2–4× efficiency gains.

### 5. Discussion
- **The first and last layers of DNNs require higher precision (FP16) for better stability.**
- Gradient accumulation in FP16 must be carefully handled with chunk-based summation.
- Stochastic rounding outperforms nearest rounding in weight updates.

### 6. Conclusion
The paper successfully demonstrates DNN training with FP8 while maintaining accuracy.

The combination of chunk-based accumulation, stochastic rounding, and mixed-precision strategies opens the door for more efficient hardware training platforms.

### Key Takeaways in 3 Sentences
The paper proposes training deep neural networks using 8-bit floating point numbers by introducing a custom FP8 format, chunk-based accumulation, and stochastic rounding to prevent accuracy loss.

Experiments across multiple models (ResNet, AlexNet) confirm that FP8 training achieves the same accuracy as FP32 while significantly reducing memory and energy costs.

These innovations enable future hardware architectures with 2–4× improved efficiency, paving the way for practical low-precision DNN training.

--- 

## 25. [Y2024]Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs
### Overview
This paper introduces Integer Scale, a novel post-training quantization method designed to accelerate fine-grained quantization of large language models (LLMs) while maintaining accuracy.

Traditional fine-grained quantization methods suffer from computational inefficiencies, particularly due to frequent type conversions between integer and floating-point representations.

Integer Scale resolves this bottleneck by replacing floating-point scale factors with integer scales and an adaptive amplifier, reducing conversion overhead.

This approach requires no extra calibration or fine-tuning, making it a plug-and-play enhancement for existing quantization techniques such as GPTQ, AWQ, and Omniquant.

### Key Contributions
Inference Bottleneck in Fine-grained Quantization: The paper identifies the inefficiencies of traditional fine-grained quantization methods, where frequent float-to-integer conversions slow down inference despite lower bit-widths.

Integer Scale with Adaptive Scale Amplifier: By converting float scales to integers and introducing an integer amplifier, Integer Scale retains precision while eliminating expensive type conversions.

Performance Gains: The proposed method achieves up to 1.85× speedup compared to traditional fine-grained quantization and 2.31× faster inference than FP16, with minimal accuracy degradation.

Scalability to Complex Models: Integer Scale successfully quantizes complex models like Mixtral-8x7B and LLaMA-3, which were previously challenging for low-bit quantization.

### Experimental Results
Benchmarked on models including LLaMA-2, LLaMA-3, and Mixtral-8x7B, evaluated on datasets like LAMBADA, C4, and WikiText-2.

Shows comparable or superior accuracy to existing methods while significantly improving inference latency.

Demonstrates substantial efficiency gains in W4A8 quantization compared to float scale-based approaches.

### Conclusion
Integer Scale presents a breakthrough in fine-grained LLM quantization, balancing speed and accuracy while being compatible with existing post-training quantization techniques.

By reducing computational overhead, this method enables faster and more efficient LLM inference, making it an ideal choice for real-world deployment.

### Key Takeaways in 3 Sentences
The paper introduces Integer Scale, a novel technique that accelerates fine-grained quantization of LLMs by replacing floating-point scales with integer-based scaling, significantly reducing inference latency.

Experimental results demonstrate up to 2.31× speedup over FP16 while maintaining comparable accuracy, making it a practical enhancement for state-of-the-art quantization techniques like GPTQ and AWQ.

The method is particularly effective for complex models like Mixtral-8x7B and LLaMA-3, offering a scalable solution for efficient low-bit quantization.
