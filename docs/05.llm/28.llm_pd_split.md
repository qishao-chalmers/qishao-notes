---
title: LLM Prefilling & Decoding Split
date: 2025-04-06 12:32:49
permalink: /pages/dc7065/
---

1. [409 2022] ORCA: A Distributed Serving System for Transformer-Based Generative Models
2. [Y2023] Splitwise: Efficient generative llm inference using phase splitting
3. [Y2023] SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills
4. [Y2023] Ring Attention with Blockwise Transformers for Near-Infinite Context
5. [146 2024] Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve
6. [Y2024] DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving
7. [Y2024] Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads
8. [Y2024] Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving
9. [Y2025] HexGen-2: Disaggregated Generated Inference of LLM in Heterogeneous Environment
