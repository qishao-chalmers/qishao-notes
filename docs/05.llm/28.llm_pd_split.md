---
title: LLM Prefilling & Decoding Split
date: 2025-04-06 12:32:49
permalink: /pages/dc7066/
---

1. [409 2022] ORCA: A Distributed Serving System for Transformer-Based Generative Models
2. [Y2023] Splitwise: Efficient generative llm inference using phase splitting
3. [Y2023] SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills
4. [Y2023] Blockwise Parallel Transformer for Large Context Models
5. [Y2023] Ring Attention with Blockwise Transformers for Near-Infinite Context
6. [146 2024] Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve
7. [Y2024] DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving
8. [Y2024] Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads
9. [Y2024] Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving
10. [Y2025] HexGen-2: Disaggregated Generated Inference of LLM in Heterogeneous Environment

---
## 1. [409 2022] ORCA: A Distributed Serving System for Transformer-Based Generative Models

### Overview and Motivation
The paper introduces Orca, a specialized serving system designed for efficient inference of Transformer-based generative models, such as GPT models, which are widely used for various generative tasks like text generation and summarization.

The authors identify significant performance bottlenecks in traditional serving systems (e.g., NVIDIA Triton with FasterTransformer), especially when serving models with an autoregressive inference pattern—generating output tokens one by one.

### Problem Context
Existing inference-serving systems batch requests at the request-level granularity, where each batch processes all requests entirely before starting the next batch. This design leads to two significant problems:

Early-finished requests: A request finishing early cannot immediately return the result until the entire batch completes, causing unnecessary latency.

Late-arriving requests: Newly arrived requests must wait until the current batch completes, causing increased queuing delay and suboptimal resource utilization.

Additionally, batching becomes complicated because different requests in a batch often have different lengths or require a different number of iterations to finish processing.

### Key Contributions of Orca
Orca addresses these issues with two main innovations:

#### 1. Iteration-Level Scheduling

Instead of batching requests, Orca batches iterations.

The inference process is scheduled iteration-by-iteration rather than request-by-request.

Requests are dynamically added or removed from processing between iterations, enabling early-completed requests to exit immediately and late-arrived requests to join rapidly.

#### 2. Selective Batching

Selective batching applies batching to specific Transformer operations, except the Attention operations, due to their requirement for regular-shaped tensors.

Operations such as Linear, Layer Normalization, and GeLU can be efficiently batched regardless of token positions by flattening tensor shapes.

This approach maintains GPU efficiency and minimizes overhead associated with varying tensor shapes due to different token indices across requests.

### Orca's System Architecture
Orca's architecture comprises several key components:

- Scheduler: Manages request pools, dynamically decides which requests are processed per iteration, and handles memory reservation for attention states.

- Execution Engine: Implements Transformer operations, supports selective batching, and uses dedicated Attention Key/Value managers to maintain state across iterations.

- Distributed Infrastructure: Employs both intra-layer parallelism (splitting matrix computations within layers across GPUs) and inter-layer parallelism (splitting layers across different GPUs).

- Separate Control/Data Planes: Communication for control messages (via gRPC) and data messages (via NCCL) is distinctly handled, significantly reducing synchronization overhead.

### Implementation
Orca is implemented in C++ and CUDA, incorporating optimized kernels for operations such as LayerNorm and Attention.

It extensively leverages existing libraries such as NCCL for GPU-to-GPU tensor communications and gRPC for control-plane communication.

### Experimental Evaluation
Evaluations conducted using GPT models (13B, 101B, 175B, and 341B parameters) demonstrate the following:

#### Microbenchmarks

When processing homogeneous batches, Orca performs similarly or slightly better than FasterTransformer due to optimized data-plane communication.

#### End-to-End Performance

Under realistic workloads with heterogeneous requests:

Orca achieves significantly higher throughput (up to 36.9×) at comparable latencies compared to FasterTransformer.

Orca maintains stable and predictable latency, even under varying workload intensities.

Performance improvements are particularly pronounced when using pipeline parallelism (inter-layer parallelism), where Orca demonstrates dramatically superior efficiency.

### Comparison with Related Work
Compared to FasterTransformer and other inference engines (TurboTransformers, LightSeq), Orca is uniquely designed to integrate the scheduler and execution engine, specifically optimizing inference for generative models through iteration-level scheduling.

While general-purpose serving systems (Triton, Clipper) separate scheduling and execution, Orca tightly couples these two layers for improved performance.

### Significance and Impact
Orca significantly advances the state-of-the-art in serving Transformer-based generative models by addressing critical bottlenecks in traditional batching and scheduling strategies.

Its iteration-level scheduling and selective batching effectively manage latency-throughput trade-offs, making Orca particularly suitable for interactive applications demanding both low latency and high throughput.

### Three-sentence Concise Summary
Orca introduces iteration-level scheduling and selective batching to efficiently serve Transformer-based generative models, dynamically adding and removing requests between inference iterations to minimize latency and maximize GPU utilization.

The system effectively applies parallelism and optimized communication to handle extremely large models distributed across multiple GPUs.

Experiments demonstrate Orca significantly surpasses existing methods in throughput and latency performance, providing up to a 36.9× improvement over traditional inference-serving systems.

---
## 2. [Y2023] Splitwise: Efficient generative llm inference using phase splitting

---
## 3. [Y2023] SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills

SARATHI is a system designed to improve large language model (LLM) inference by addressing two main inefficiencies:
- the memory-bound nature of the decode phase (which generates tokens one at a time)
- pipeline bubbles that arise when using pipeline parallelism.

<img src="https://github.com/user-attachments/assets/2c14b243-d22d-41e7-bbed-3efe7c440edb" style="width:600px;height:auto;">

It focuses on two key optimizations:
- “chunked-prefills,” which splits the prefill (prompt-processing) step into smaller, compute-saturating chunks
- “decode-maximal batching,” which combines one chunk of prefill tokens with a large batch of decode tokens so the model weights fetched for the prefill can be reused by the decode operations.

By **piggybacking decodes on a concurrent prefill chunk, SARATHI effectively reduces the cost of generating each new token, dramatically improving decode throughput (up to 10× in some cases)** and reducing end-to-end inference time by as much as 1.33× for LLaMA-13B and 1.25× for LLaMA-33B on specific GPU hardware.

Beyond single-GPU scenarios, SARATHI’s technique of forming uniform, compute-saturating micro-batches (via chunked-prefills) also mitigates pipeline imbalances (or “bubbles”) that occur in multi-GPU pipeline-parallel deployments.

In GPT-3 simulations spanning 64 A100 GPUs, it cuts median pipeline bubbles by a factor of more than six, accelerating overall end-to-end throughput by nearly 1.9× compared to conventional iteration-level scheduling.

In contrast, older iteration-level approaches such as Orca can occasionally overlap prefills and decodes incidentally, but cannot systematically exploit partial prefill chunks, leading to less impressive speedups and more frequent pipeline idle times when requests differ in sequence length or timing.

Under the hood, chunked-prefills must carefully set attention masks each time a subsequent prefill chunk processes additional tokens, and attention-based lookups thus refetch earlier tokens’ key-value (KV) pairs multiple times.

Although this can slightly reduce prefill efficiency—due to extra reads of cached values and reduced arithmetic intensity from shorter chunks—SARATHI’s gains on decode efficiency usually outweigh the slower prefill.

The system therefore chooses chunk sizes that strike a balance between piggybacking enough decodes to justify extra chunk overhead and preserving as much prefill efficiency as possible.

In large-scale deployments, when paired with pipeline parallelism, SARATHI’s chunk-based micro-batches ensure more uniform execution times across pipeline stages.

This eliminates common stalls that happen when one micro-batch finishes early while another continues with a lengthy prefill or decode.

Consequently, multi-GPU inference can exploit larger batch sizes without suffering big pipeline idle times—achieving both high throughput and better utilization of limited GPU memory.

Overall, SARATHI demonstrates that by systematically interleaving smaller, GPU-saturating prefill chunks with memory-bound decodes, one can recast a large fraction of decode-side linear operations into efficient batched matrix-matrix multiplies.

This approach cuts per-token generation costs by up to an order of magnitude, can handle varied prompt and decode lengths with minimal overhead, and can be combined with a variety of scheduling frameworks to serve requests with different needs.

### Three-sentence summary
SARATHI addresses two major inefficiencies in LLM inference—slow decodes and pipeline bubbles—by splitting the prompt processing into smaller, compute-saturating chunks and combining them with decodes that “piggyback” on the same batch.

This design transforms the decode phase into a far more efficient operation and equalizes micro-batch execution times, thereby eliminating pipeline stalls.

As a result, SARATHI achieves substantial throughput gains, slashing decode costs by as much as 10× and improving end-to-end LLM serving by up to nearly 2× in large-scale deployments.

---

## 4. [Y2023] Blockwise Parallel Transformer for Large Context Models

### **Quantized Analysis from the Paper**

#### Vanilla Transformer
Attention: For Q, K, V , saving their input x needs 2bsh bytes, where b is batch size, s is sequence length, and h is hidden dimension.

For QKT matmul, saving activations Q and K needs 4bsh bytes.

For softmax(QKT), saving input QKT needs 2bs^2a bytes, where a is the number of attention heads.

For mask and dropout, saving mask needs bs^2a bytes.

For score × V , saving score needs 2bs^2a bytes, and saving V needs 2bsh bytes.

For output projection and dropout, saving the input needs 2bsh bytes, and saving dropout mask needs bsh bytes.

The maximum attention activation size of attention is O(s^2) with checkpointing.

**FFN**:

For the first linear layer, saving input needs 2bsh bytes.

For activation, saving input needs 8bsh bytes.

For the second linear layer, saving input needs 8bsh bytes.

For dropout, saving the mask needs bsh bytes.

With checkpointing, the maximum activation size of FFN is 8bsh bytes.

Consequently, for a large context length, the memory cost of activation in vanilla Transformer is O(s^2).

#### BPT
**Attention**: Since BPT does not materialize full attention and instead computes it blockwise, it needs to store intermediate blockwise activations in the key-value loop, which has a maximum activation size of 4bch with checkpointing.

Additionally, it needs to store q output activations for the query loop, which requires 2bsh bytes.

Since s ≫ c, the maximum activation size is 2bsh.

**FFN**: When iterating the FFN over blocks, BPT needs to save the following activations: For the first linear layer, saving input needs 2bch bytes.

For activation, saving input needs 8bch bytes.

For the second linear layer, saving input needs 8bch bytes.

For dropout, saving the mask needs bch bytes.

In total, 19bch bytes are needed.

Additionally, storing the output of the for loop requires 2bsh bytes. 

Therefore, the maximum FFN activation size is 2bsh.

Consequently, each BPT layer’s memory cost of activation is 2bsh.

### **1. Usual Transformer bottleneck** 

In a standard Transformer layer, you have two major components:
 
- The self-attention mechanism, which looks at how each token (word, position, etc.) “attends” to all the others.
 
- A feedforward network that further processes the output of self-attention for each token.

When sequences are very long, it’s hard to compute both of these pieces without running out of memory. Traditional memory-saving techniques primarily reduce the memory usage of *self-attention* — but the large feedforward network can still be a huge bottleneck.

### **2. Blockwise self-attention** 

The authors point out that you can handle self-attention in a “blockwise” or “chunked” manner.

Instead of computing attention for the entire sequence at once, you split the sequence into blocks of manageable size. For each block of queries (the “current” positions), you loop over all the blocks of keys and values (the positions you’re attending to).

Doing so avoids creating the huge intermediate attention matrices that normally blow up memory usage (those are the s×s matrices, where s is the sequence length). Instead, you build partial results for each chunk and combine them at the end. This alone cuts down the memory footprint for self-attention significantly.

<img src="https://github.com/user-attachments/assets/c7e48494-8048-45e6-b671-51b7f70bf668" style="width:600px;height:auto;">


### **3. Merging feedforward in a blockwise loop** 

Here’s the new insight: since you’re already processing the sequence in blocks for self-attention, you don’t have to wait until *all* self-attention computations are done on the *entire* sequence before you apply the feedforward network. Instead, for each chunk of queries:
 
- Compute that chunk’s self-attention (by looping over all the key/value blocks).
 
- Immediately run that chunk’s feedforward network and apply the residual connection.

Only then do you move on to the next chunk of queries.

In other words, you interleave the feedforward step inside the same chunkwise loop used by self-attention.

That means you never have to hold the entire (potentially huge) feedforward network’s intermediate outputs for all tokens in memory at once. You only keep the feedforward outputs for the current chunk.

### **4. Why does this help with memory?** 
 
- **Self-attention**  is handled chunk by chunk, so you’re not storing the entire attention matrix for all positions.
 
- **Feedforward**  is also computed chunk by chunk, so the intermediate activations for the huge feedforward layers only exist in memory for a single block at a time (instead of for the entire input sequence).

Because of that, you greatly reduce the total “peak” memory usage. This is especially important for very large models or extremely long sequences.

### **5. Putting it all together** 
 
- **Outer loop**  over your query blocks. (Pick a small chunk of the sequence to treat as queries.)
- **Inner loop** : for that chunk of queries, loop over the key/value blocks to build attention outputs block by block.
- Once you have the attention outputs *for that chunk*, run them through the feedforward network right away (plus the usual residual connections).
- Move on to the next chunk of queries.

By the end, you’ve computed one layer of Transformer operations for the entire sequence, but you’ve never had to store the massive intermediate tensors that appear if you do attention and feedforward all at once on a super-long sequence.

### **Key takeaway** 

The big trick is fusing the feedforward computation directly into the same “blockwise” loop used for attention.

That way, you avoid the huge memory spike from storing feedforward activations across the entire sequence. This allows training or inference over far longer sequences than usual—hence the term “Blockwise Parallel Transformer.”

I hope this clarifies the idea! If any part is still confusing, just let me know and I can elaborate further.

---

## 5.[Y2023] Ring Attention with Blockwise Transformers for Near-Infinite Context

<img src="https://github.com/user-attachments/assets/6972ebdf-ebae-4cc6-871d-5fb41482bbe0" style="width:600px;height:auto;">

Same author from blockwise transformer.

**Top (a)** We use the same model architecture as the original Transformer but reorganize the compute.

In the diagram, we explain this by showing that in a ring of hosts, each host holds one query block, and key-value blocks traverse through a ring of hosts for attention and feedforward computations in a block-by-block fashion.

As we compute attention, each host sends key-value blocks to the next host while receives key-value blocks from the preceding host.

The communication is overlapped with the computation of blockwise attention and feedforward.

**Bottom (b)** We compute the original Transformer block-by-block.

Each host is responsible for one iteration of the query’s outer loop, while the key-value blocks rotate among the hosts.

As visualized, a device starts with the first query block on the left; then we iterate over the key-value blocks sequence positioned horizontally.

The query block, combined with the key-value blocks, are used to compute self-attention (yellow box), whose output is pass to feedforward network (cyan box).
