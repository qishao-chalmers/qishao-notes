---
title: LLM Posttraining/Finetuning
date: 2025-02-05 23:32:49
permalink: /pages/dc7058/
---

1. [1171 Google] Towards a Human-like Open-Domain Chatbot
2. [10596 OpenAI] Training language models to follow instructions with human feedback
3. [Y2025] o3-mini vs DeepSeek-R1: Which One is Safer?
---

## 1.[1171] Towards a Human-like Open-Domain Chatbot
### **Key Insights from "Towards a Human-like Open-Domain Chatbot"**
This paper presents **Meena** , a **2.6B parameter open-domain chatbot**  developed by Google Research, trained end-to-end on **40B words**  mined from public-domain social media conversations.

The key contributions include the introduction of **a novel evaluation metric (SSA)**  and demonstrating that **perplexity correlates with chatbot quality** . Below are the major takeaways:

### **1. Main Contributions**
#### **1.1. Sensibleness and Specificity Average (SSA)**  
- **SSA measures conversational quality**  using two fundamental aspects: 
  1. **Sensibleness**  – whether a response makes sense in the context. 
  2. **Specificity**  – whether the response is meaningful rather than generic (e.g., avoiding vague replies like "I don't know").
- **Findings:**  SSA strongly correlates with **perplexity** , meaning that training models to minimize perplexity improves conversation quality.
#### **1.2. Correlation Between Perplexity and SSA**  
- **Perplexity (how well the model predicts the next token) is a strong predictor of chatbot quality.**
- **R² = 0.93**  correlation between perplexity and SSA in multi-turn evaluations.
- Suggests that **improving perplexity will lead to near-human conversation quality**  (human SSA = 86%, Meena = 79%).
#### **1.3. Large-Scale End-to-End Training**  
- **Uses an Evolved Transformer architecture**  (a variant of the Transformer optimized via Neural Architecture Search).
- **2.6B parameters** , trained with **Adafactor optimizer on a TPU-v3 Pod** .
- Unlike **retrieval-based**  or **hand-crafted**  chatbots (like Mitsuku, Cleverbot), **Meena is purely generative and trained end-to-end** .

### **2. Key Findings**
#### **2.1. Meena Outperforms Other Chatbots**  
- **SSA Scores (higher is better):**  
  - **Human:**  **86%** 
  - **Meena:**  **79%**  (72% base model, 79% after improvements)
  - **Mitsuku:**  **56%**
  - **DialoGPT:**  **48%**
  - **Cleverbot:**  **44%**
  - **XiaoIce:**  **31%**
- **Meena is closer to human-level conversations than any previous chatbot.**
#### **2.2. Model Scaling & Data Quality Matter More Than Architecture**  
- **Training data plays a major role in performance.** 
- A **large dataset with diverse conversations**  improves chatbot quality significantly.
- End-to-end models can outperform complex rule-based systems when trained at scale.
#### **2.3. Static vs. Interactive Evaluation**  
- **Static Evaluation:**  Measured SSA over a fixed dataset of **1,477 multi-turn conversations** . 
- **Interactive Evaluation:**  **Humans freely chatted**  with the chatbot, with responses evaluated by **crowd workers** . 
- **Findings:**  **Both evaluations showed strong perplexity-SSA correlation.**

### **3. Major Improvements Over Prior Work**
#### **3.1. Better Decoding Strategy: Sample-and-Rank**

*Sample and rank might be the most important contribution in this paper.* :+1:

- **Standard Beam Search often produces generic, repetitive responses.**
- **Meena uses Sample-and-Rank:**  
  1. Generate **N=20 diverse responses**  using **temperature-controlled sampling** .
 
  2. **Rank them based on likelihood**  and **select the most contextually appropriate one** .
- **This significantly increases specificity.**
#### **3.2. Filtering Out Repetitions**  
- **Common failure mode:**  Chatbots repeat responses across turns.
- **Fix:**  Apply a **cross-turn repetition filter** , which improved SSA by **5%** .
#### **3.3. Tuning Perplexity Further**  
- Further reducing perplexity **could push SSA beyond 86%** , reaching human-like conversational quality.

### **4. Surprising & Unintuitive Findings**
#### **4.1. Higher Perplexity = Worse Conversations**  
- Unlike BLEU or ROUGE (which are **poor correlates of chatbot quality** ), **perplexity is a strong predictor of human-like conversations** . 
- The best-performing model had a **perplexity of 10.2** .
#### **4.2. Human Sensibleness vs. Specificity**  
- **Humans are nearly always sensible (~97%)**  but often fail on specificity (~75%). 
- **Meena's specificity (70%) is closer to humans than previous chatbots.**
#### **4.3. Some Rule-Based Chatbots (e.g., Mitsuku, Cleverbot) Still Compete**  
- While **Meena**  outperforms them, **some rule-based chatbots still generate sensible responses in specific scenarios** . 
- **Mitsuku scored 56% SSA** , better than many neural models.

### **5. Future Research & Limitations**
#### **5.1. Remaining Challenges**  
- **Does not handle long-term memory well.** 
- **Still produces some inconsistent or repetitive responses.** 
- **Fails on deep reasoning, knowledge discussion, humor, and empathy.** 
- **Cannot fact-check or validate external knowledge.** 
- **SSA does not measure other aspects of human conversation (e.g., empathy, humor, reasoning).**
#### **5.2. What’s Next?**  
- **Scaling the model further**  could improve performance beyond **86% SSA** . 
- **Integrating retrieval mechanisms**  (e.g., knowledge-grounded chatbots).
- **Expanding evaluation metrics**  beyond SSA (e.g., humor, empathy, coherence).

### **6. Summary & Takeaways**
- ✅ **Introduces Meena** , a **2.6B parameter chatbot** , trained on **40B words** .
- ✅ **SSA metric (Sensibleness + Specificity) shows strong correlation with Perplexity.** 
- ✅ **Meena outperforms previous chatbots, reaching 79% SSA (human-level: 86%).** 
- ✅ **Uses "Sample-and-Rank" decoding, outperforming Beam Search.** 
- ✅ **Higher perplexity = lower conversational quality (counterintuitive finding).** 
- ✅ **Future improvements could bring it to human-like levels.** This paper is a **landmark step towards human-like chatbots** , demonstrating that **scaling data and reducing perplexity significantly improve conversation quality** .

---

## 3. [Y2025] o3-mini vs DeepSeek-R1: Which One is Safer?
DeepSeek-R1 is not safer compared with o3-mini.
