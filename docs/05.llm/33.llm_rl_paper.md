---
title: LLM RL Paper
date: 2025-12-10 12:32:49
permalink: /pages/dc7071/
---
0. [backgroud] background of RL
1. [qwen] Stabilizing RL with LLMs: Formulation and Practices

---
# ðŸ¤– [0] Summary of Reinforcement Learning Concepts

This document summarizes the core definitions and differences for key concepts in policy gradient methods, as discussed in the context of the paper, "Stabilizing Reinforcement Learning with LLMs".


## 1. On-Policy vs. Off-Policy Training

These terms define the relationship between the policy generating the data and the policy being optimized.

| Feature | On-Policy Training | Off-Policy Training |
| :--- | :--- | :--- |
| **Definition** | The policy sampling the responses ($\mu_{\theta_{old}}$) is the same as the policy being optimized ($\pi_{\theta}$). | The policy sampling the responses ($\mu_{\theta_{old}}$) is *different* from the policy being optimized ($\pi_{\theta}$). |
| **Policy Staleness** | Minimized, as the policies are theoretically identical (omitting training-inference discrepancy). | Increased, typically to accelerate training by reusing data for multiple gradient updates. |
| **Sample Efficiency** | Lower. Requires generating a new batch of data for every (or very few) update steps. | Higher. Allows for reusing old data for multiple gradient steps. |
| **LLM Stabilization** | Requires **IS Correction** to address the training-inference discrepancy. | Requires **Clipping** and **Routing Replay (R2/R3)** to safely mitigate the instability caused by policy staleness. |
| **Example Algorithms** | REINFORCE, traditional GRPO (when data is used once). | Modern PPO, GRPO/MiniRL when using large batch sizes split into mini-batches. |


## 2. REINFORCE vs. PPO

These are two of the most significant algorithms in the policy gradient family.

### A. REINFORCE (The Foundation)

* **Policy Type:** On-Policy.
* **Mechanism:** It is the foundational token-level objective used to optimize the policy directly by moving the probability of actions up based on the final **Monte Carlo** reward (return, $G_t$).
* **Drawback:** Suffers from **high variance** in its gradient estimates, leading to unstable and slow training. The paper's **MiniRL** baseline is a modified REINFORCE objective.

### B. PPO (Proximal Policy Optimization)

* **Policy Type:** On-Policy (Designed for safe data reuse).
* **Mechanism:** PPO is an **Actor-Critic** method. It replaces the noisy Monte Carlo return ($G_t$) with a low-variance **Advantage Estimate** ($A_t$) calculated using a value network (Critic).
* **Key Stability Feature:** It uses a **clipped surrogate objective** to explicitly limit how far the new policy can deviate from the old policy in a single step. This ensures stability and allows for safe data reuse.


### C. Comparison

| Feature | REINFORCE | PPO (Proximal Policy Optimization) |
| :--- | :--- | :--- |
| **Stability** | Low stability (High variance in $G_t$). | High stability (Low variance in $A_t$ and uses clipping). |
| **Value Function (Critic)** | No Critic (Pure Policy Gradient). | Yes, uses a Critic to calculate the Advantage ($A_t$). |
| **Data Usage** | Sample Inefficient (Data used once). | Sample Efficient (Data reused safely due to clipping). |


## 3. GRPO (Group Policy Optimization)

* **Framework:** GRPO operates within a **policy optimization framework**.
* **Classification:** It is a policy gradient algorithm often used in an on-policy setup.
* **Paper's Finding of 1[qwen] paper** The paper argues that GRPO's common objective is prone to failure because it includes **length normalization** (which introduces bias) and omits the crucial **training-inference IS correction**.

---

# 1 [qwen] Stabilizing RL with LLMs: Formulation and Practices

This paper proposes a theoretical formulation and provides empirical practices for stabilizing Reinforcement Learning (RL) with Large Language Models (LLMs). The central challenge addressed is the mismatch between optimizing a **sequence-level reward** (score for the complete response) and using a **token-level objective** (updates based on individual tokens).

### 1. Core Theoretical Insight

The paper posits that the token-level objective ($\mathcal{J}^{token}$) can be used as a **first-order approximation** of the true sequence-level objective ($\mathcal{J}^{seq}$). This approximation is valid only when two specific discrepancies are minimized:

1.  **Training-Inference Discrepancy:** The numerical difference between the training engine ($\pi_{\theta_{old}}$) and the inference engine ($\mu_{\theta_{old}}$).
2.  **Policy Staleness:** The difference between the data-sampling policy ($\mu_{\theta_{old}}$) and the policy being optimized ($\pi_{\theta}$).

### 2. Key Stabilization Techniques (Including Your Questions)

The paper explains how crucial stabilization techniques work by minimizing these discrepancies:

| Technique | Discrepancy Addressed | Role in MoE Models |
| :--- | :--- | :--- |
| **Importance Sampling (IS) Correction** | Training-Inference Discrepancy. | The factor $\frac{\pi_{\theta_{old}}}{\mu_{\theta_{old}}}$ corrects for numerical differences between the training and inference engines, despite having the same parameters. **Omitting this causes rapid training collapse**. |
| **Clipping** | Policy Staleness. | Prevents aggressive policy updates, thereby restraining policy staleness. |
| **Routing Replay (R2/R3)** | Training-Inference Discrepancy (R3) & Policy Staleness (R2/R3). | **Essential for MoE models**. It stabilizes training by fixing the routed experts during policy optimization. |

### 3. Empirical Findings (On-Policy vs. Off-Policy)

The experiments use **MiniRL**, a baseline based on the REINFORCE objective but with IS correction and clipping.

* **On-Policy Training** (Low Staleness): The basic policy gradient algorithm with **IS correction (MiniRL)** is the most stable and performs best.
* **Off-Policy Training** (High Staleness): Introducing off-policy updates (data reuse for multiple steps) requires **both clipping and Routing Replay (R2/R3)** to achieve stability and prevent collapse. The degree of off-policiness determines whether R2 or R3 performs better.

### 4. Algorithm Context (GRPO)

* **GRPO (Group Policy Optimization)** is a policy gradient algorithm with a token-level optimization framework.
* The paper argues that GRPO's common formulationâ€”which includes **length normalization** and **omits the training-inference IS correction**â€”invalidates the first-order approximation, leading to a biased objective and suboptimal performance.

### 5. Final Conclusion

Stable training plays a decisive role in successfully scaling RL. Once training is stabilized with the proper techniques, the model consistently converges to comparable final performance, regardless of the initial cold-start data.
