---
title: Thinking of LLM Prefilling & Decoding Split
date: 2025-04-06 12:32:49
permalink: /pages/dc7067/
---


## SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills
For example, on an A6000 GPU, for the LLaMA-13B model, a prefill with a sequence length of 512 tokens saturates GPU compute even at a batch size of just one.

The decode phase results in very low GPU utilization at low batch sizes.

For example, our experiments reveal that, at small batch sizes, the decode cost per token can be as high as ∼ 200 times the prefill cost per token.

![image](https://github.com/user-attachments/assets/36abc78b-ee57-46d1-9fb1-f35a51494583)


Figure 3 shows the per-token cost of each of the six transformer operations (§2.1) for prefill and decode at various batch sizes for a fixed sequence length (prefill+decode) of 1024.

First, we observe that prefill has almost constant pertoken cost across various batch sizes, indicating that prefill saturates the GPU even at batch size of 1.

Second, we see that decode behaves very differently from prefill as the pertoken cost reduces significantly when the batch size increases.

Third, we see that the decode cost per-token is **200×, 100×, and 16.7× that of prefill at batch size of 1, 2 and 18**, respectively. Thus, it is clear that optimizing decodes is critical for
efficient LLM inference.

We observe that the throughput of the prefill phase saturates at about 180 tokens/millisecond when B × L ≥ 512: e.g., a
single prefill request can achieve peak throughput at L ≥ 512.

In contrast, the decode throughput increases linearly with small batch sizes.

To further understand the saturation point of decode phase, we profile a single layer as opposed to the 40 layers of the full model.

This enables us to fit 40× larger batches on the GPU due to the reduced memory footprint of model weights and KV caches.

We find that decode saturates at a much larger batch (e.g., 256 with 1024 sequence length).

Such large batches are infeasible to run with the full model.

![image](https://github.com/user-attachments/assets/74f86857-3bad-4d18-88af-e13d686b1137)

> Please note that in (b) batch size with 256, even arithmetic intensity is large and become compute-intensive. However, scaling up the batch size to such high values is infeasible due to the KV-cache footprint of each request. For instance, we can fit a maximum batch size of 18 requests at a sequence length of 1K for the LLaMA-13B model on an A6000 GPU. Therefore, in the range of batch sizes that are practical today, the decode phase remains memory-bound.

## DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving

![image](https://github.com/user-attachments/assets/860b33d2-d07f-4cc7-9a9c-09c5dbacb889)

The maximum achievable goodput on a single A100 GPU, which is constrained by the more stringent one of TTFT and TPOT requirements, is about 1.6 requests per second (rps).

The performance contrasts sharply when each phase is served independently on a separate GPU, shown by the orange and green curves, which achieve per-GPU goodput of 5.6 rps for the prefill phase and 10 rps for decoding.

Ideally, by allocating 2 GPUs for prefill and 1 GPU for decoding, we can effectively serve the model with an overall goodput
of 10 rps, or equally 3.3 rps per GPU, which is 2.1x higher than existing systems.

Ideally, by allocating 2 GPUs for prefill and 1 GPU for decoding, we can effectively serve the model with an overall goodput
of 10 rps, or equally 3.3 rps per GPU, which is 2.1x higher than existing systems.

First, colocation leads to strong prefill-decoding interference.

**A prefill step often takes much longer than a decoding step.**

When batched together, decoding steps in the batch are delayed by the prefill steps, significantly elongating their TPOT.

![image](https://github.com/user-attachments/assets/c0dabf55-49ae-48de-81a3-015756710bbd)

As Figure 2 shows, adding a single prefill job to a batch of decoding requests significantly slows down both processes, leading to a marked increase in TTFT and TPOT.

Specifically, the decoding tasks in the batch must wait for lengthier prefill jobs to complete, thus extending TPOT; the slowdown intensifies with a longer prefill, shown in Figure 2(b).

Adding decoding jobs to prefill also increases the time to complete the prefill task, particularly when the GPU is already at capacity (Figure 2 blue curves).


### Analysis for Prefill Instance

#### Batching Strategy
Assuming a given arrival rate, our goal is to fulfill the service’s latency requirement on TTFT using the least resources. 

![image](https://github.com/user-attachments/assets/9988fe53-8108-47e3-855b-74eeab999c14)

For a 13B parameter LLM, processing a single sequence of 512 tokens can fully engage an A100 GPU; larger models require
shorter sequences to reach GPU saturation.

Once the GPU becomes compute-bound, adding more requests to the batch no longer improves GPU efficiency.

Hence, for prefill instances, it is necessary to profile the specific LLM and GPUs in advance to identify a critical input length threshold, denoted as Lm, beyond which the prefill phase becomes compute-bound.

**In practice, user prompts typically average over hundreds of tokens [7]. Batch sizes for the prefill instance are generally kept small.**

#### Parallelism plan

![image](https://github.com/user-attachments/assets/bfbb2743-f67e-4d77-bc91-0272dce038d8)

A more stringent SLO will make intra-op parallelism more advantageous, due to its ability to support higher request rates while adhering to SLOs.

The value of K depends on factors such as the input length, model architecture, communication bandwidth, and placement.

### Analysis for Decoding Instance

It receives the intermediate states (KV caches) and the first token from the prefill instance and generates subsequent tokens one at a time.

For decoding instances, our optimization goal is to satisfy the application’s TPOT requirement using minimal computing resources.

Post-disaggregation, the batch size for decoding may be constrained by GPU memory capacity, as it is necessary to maintain the KV caches for all active requests.

> Intra-operator parallelism partitions computationally intensive operators, such as matrix multiplications, across multiple GPUs, accelerating computation but causing substantial communication.
> Inter-operator parallelism organizes LLM layers into stages, each running on a GPU to form pipelines. It moderately increases execution time due to inter-stage communication, but linearly scales the system’s rate capacity with each added GPU.

![image](https://github.com/user-attachments/assets/a1da67f1-d51d-47c3-99b0-49bdd09d1d4d)

Intra-op parallelism reduces latency with diminishing returns, caused by communication and reduced utilization
after partitioning.

Inter-op parallelism can almost linearly scale the throughput.

Hence, when the TPOT SLO is stringent, intra-op parallelism is essential to reduce TPOT to meet latency goals. Beyond this, inter-op parallelism is preferable to enhance throughput linearly.

### Practical Problems

- Variable prefill length
- Communication overhead

The KV cache size of a single 512-token request on OPT-66B is approximately 1.13GB.

Assuming an average arrival rate of 10 requests per second, we need to transfer 1.13 × 10 = 11.3 GB data – or equivalently 90Gbps bandwidth to render the overhead invisible.

The size of the KV caches increases with average input length and arrival rate.

While many modern GPU clusters for LLMs are equipped with Infiniband (e.g., 800 Gbps), in cases where cross-node bandwidth is limited, disaggregation relies on the commonly available intra-node NVLINK, where the peak bandwidth between A100 GPUs is 600 GB/s, again **rendering the transmission overhead negligible**.

