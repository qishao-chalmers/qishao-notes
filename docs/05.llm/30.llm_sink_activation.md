---
title: From Attention Sink to Massive Activation
date: 2025-04-30 12:32:49
permalink: /pages/dc7068/
---

1. [596 Y2023] Efficient Steaming Language Models with Attention Sinks
2. [Y2025] When Attention Sink Emerges in Language Models
3. [192 Y2024] Massive Activations in Large Language Models


## 1. [596 Y2023] Efficient Steaming Language Models with Attention Sinks

Attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention.

In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a “sink” even if they are not semantically important.

<img src="https://github.com/user-attachments/assets/e78d12d3-80c5-4771-bb09-4a39440d089c" style="width:600px;height:auto;">


The results show the insufficiency of introducing merely one or two initial tokens, whereas a threshold of four initial tokens appears enough, with subsequent additions contributing marginal effects.

The nature of the SoftMax function prevents all attended tokens from having zero values.

This requires aggregating some information from other tokens across all heads in all layers, even if the current embedding has sufficient self-contained information for its prediction.

Consequently, the model tends to dump unnecessary attention values to specific tokens.

This result justifies our choice of introducing 4 initial tokens as attention sinks in StreamingLLM.

We’ve noted that LLMs are typically trained to utilize multiple initial tokens as attention sinks rather than just one.

<img src="https://github.com/user-attachments/assets/fb405010-15cb-4e44-aac4-95a91b98df55" style="width:600px;height:auto;">

The KV cache in StreamingLLM can be conceptually divided into two parts, as illustrated in Figure 4: (1) Attention sinks (four initial tokens) stabilize the attention computation; 2) Rolling KV Cache retains the most recent tokens, crucial for language modeling.

<img src="https://github.com/user-attachments/assets/ff71fbb0-f72d-42e1-8dd7-b6cf41f7d3df" style="width:600px;height:auto;">

They also test pretraining with a single sink token.

The vanilla model requires the addition of multiple tokens as attention sinks to maintain stable streaming perplexity.

In contrast, the model trained with a sink token achieves satisfactory streaming performance **using just the sink token**.

<img src="https://github.com/user-attachments/assets/96760ee8-9989-483f-8a2e-af2f2894eed2" style="width:600px;height:auto;">

---

## 2. [Y2025] When Attention Sink Emerges in Language Models

**Background**

Xiao et al. (2023b) revealed that LLMs allocate significant attention scores to specific token positions, e.g. the first token (not necessary to be a BOS token), resulting in “vertical” attention patterns. 

**Contribution**
- Attention sink emerges after LMs are trained effectively on sufficient training data.
  It appears less obvious in LMs trained with small learning rates.
  While weight decay encourages the emergence of attention sink. 
- The sink position is highly related to the loss function and data distribution and can be shifted to other positions rather than the first token.
- Attention sink acts more like key biases, storing extra attention and meanwhile not contributing to the value computation.
  This phenomenon (at least partially) stems from tokens’inner dependence on attention scores due to the softmax normalization.
  After relaxing such dependence by replacing softmax attention with other attention operations, e.g., sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters.

### What is Attention Sink?

An attention sink is when tokens disproportionately attend to the first token (often the BOS token or first word), even when it holds little semantic importance.

This creates a vertical attention pattern, frequently leveraged for efficiency in:
- Streaming inference
- KV cache optimization
- Quantization-aware training

### Main Contributions
#### Empirical Demonstration of Universality
Attention sink appears across LMs (GPT2-XL, LLaMA2/3, Mistral) — even with:
- Random token sequences
- Small-scale models
This suggests the phenomenon is model-agnostic and tied to the training process, not the data or size.

#### Mechanistic Understanding
First token’s key vector acts like a bias: due to angle alignment with other queries, not due to large vector norms.

The cosine similarity, not norm product, drives attention sink — meaning high attention persists even with small key norms.

#### Emergence During Pretraining
Attention sink becomes prominent after sufficient optimization.

It's less obvious with:
- Fewer training steps
- Smaller learning rates
- Less training data

#### Influence of Data Distribution
Sink position can shift:
- From token 1 to 2 if token 1 is randomized
- To fixed tokens if injected intentionally
- **repeated tokens in input suppress attention sink in relative positional encoding models (e.g., LLaMA)**.

#### Loss Function & Optimization Effects
- Weight decay encourages attention sink, but excessive decay suppresses it by harming learning.
- In prefix LM tasks, sink moves from token 1 to the prefix span.
- Shifted window attention (e.g., in Mistral) localizes the sink to absolute positions — not relative ones.

#### Attention Sink = Key Biases

<img src="https://github.com/user-attachments/assets/1faf375f-a01c-4b7d-af36-8d41e2f5b1fa" style="width:600px;height:auto;">

We further show that due to the different manifold of $\mathbf{k}_1^{l,h}$, the angles between $\mathbf{k}_1^{l,h}$ and $\mathbf{q}_t^{l,h}$ play an important role. Considering
$\mathbf{q}_t^{l,h} {\mathbf{k}_j^{l,h}}^\top = \|\mathbf{q}_t^{l,h}\| \cdot \|\mathbf{k}_j^{l,h}\| \cdot \cos(\mathbf{q}_t^{l,h}, \mathbf{k}_j^{l,h}),$

we visualize the cosine similarity between keys and values, and the product of $\ell_2$-norm between keys and values in Figure 2(*Bottom*)** .

> **Although**  $$\|\mathbf{q}_t^{l,h}\| \cdot \|\mathbf{k}_1^{l,h}\|$$ **is comparatively small,** 

$$\cos(\mathbf{q}_t^{l,h}, \mathbf{k}_1^{l,h})$$ **is significantly large, leading to attention sink.** 

This explains why attention sink exists despite the small $$\ell_2$$-norm of keys of the first token.

To conclude, the first token leverages its keys to act as biases, thus minimizing the angles between $$\mathbf{k}_1^{l,h}$$ and $$\mathbf{q}_t^{l,h}$$, and **exhibiting attention sink** .

- Attention sink does not contribute to value computation, acting more like a **key-space artifact**.
- Simply **adding key biases (even without value biases) shifts the sink away from real tokens, proving it’s an optimization artifact**.

#### Attention Sink Under Different Inputs
- input domains have negligible effects on our attention sink metric Sinkϵ1
- (i) randomly sample T tokens from the tokenizer vocabulary V to construct a sequence
- (ii) randomly sample 1 token from the tokenizer V and repeat it T times.
As present in Table 1(Left), attention sink still exists when the inputs are **random tokens instead of natural language**.

However, with repeated tokens, attention sink in Mistral (Jiang et al., 2023) and LLaMA models disappears.
**we prove that for LMs with NoPE/relative PE/ALiBI/Rotary, if the first T tokens are the same, their corresponding hidden states are the same. They all have massive activations, thus dispersing the attention sink.**

#### Effects of Optimization on Attention Sink
- Attention sink emerges after LMs are trained effectively.
- Attention sink appears less obvious in LMs trained with small learning rates.

#### Effects of Data Distribution pData on Attention Sink

1. Attention sink emerges after LMs are trained on sufficient training data.
2. Attention sink could be shifted to other positions rather than the first token if modifying pdata.

#### Effects of Loss Function on Attention Sink

<img src="https://github.com/user-attachments/assets/c2dbbdd3-68f5-4b76-8e5f-e3cc788fd0ae" style="width:600px;height:auto;">

1. Weight decay encourages the emergence of attention sink.
2. With prefix language modeling, attention sink appears among the prefix tokens rather than the first token
only.
3. With shifted window attention, attention sink appears on the “absolute”, not the “relative” first token. Smaller window size prevents the emergence of attention sink.

#### Effects of Model Architecture on Attention Sink
- we note that all these LMs, even the one without explicit PE (NoPE), have attention sink.
##### Attention Bias
considered a learnable sink token in each chunk before the input tokens during LM pre-training.\
As this token is fixed in the first token, this could be considered as implicitly introducing biases k,v,q in attention.\
as long as there are key biases k\*<sup>l,h</sup> attention sink disappears on the first token but on the biases.

**So they prove that v\*<sup>l,h</sup> is not important, could just be zero.**

<img src="https://github.com/user-attachments/assets/3f9afb16-8169-4262-a8a8-665574b1d971" style="width:600px;height:auto;">

1. Positional embedding, FFN design, LN location, and multi-head design do not affect the emergence of attention sink.
2. Attention sink acts more like key biases, storing extra attention and meanwhile not contributing to the value computation.
3. When relaxing tokens’ inner dependence on attention scores, attention sink does not emerge in LMs.
   We note that the LMs with no attention sink typically relax tokens’ inner dependence on attention scores.\
   Their attention scores during pre-training could be negative or not add up to one.\
   This indicated that attention sink (at least partially) stems from such inner dependence.\
   Besides the attention metric computed by proxy attention scores, we also observe that the above LMs also have no massive activations.

<img src="https://github.com/user-attachments/assets/4b0d5fe4-940b-45b9-a9bd-6f151dd163b5" style="width:600px;height:auto;">

#### Role of Attention Normalization
- **Softmax normalization creates inter-token dependence, reinforcing attention sink**
- **Replacing softmax with sigmoid or non-normalized attention (e.g., ELU+1) eliminates attention sink — even in 1B-parameter models**

---

## 3. [192 Y2024] Massive Activations in Large Language Models

- Certain activations exhibit huge magnitudes, e.g., more than 4 orders of magnitude larger than the median.
  - These activations are also extremely rare, often numbering fewer than 10 among tens of millions of total activations.
- Regarding the depth dimension of LLMs, the appearance of massive activations is mostly abrupt: they emerge suddenly after a single layer of computation, and diminish at the last few layers.
  - Further, we find massive activations occur in a small number of feature dimensions that are input agnostic.
  - Many of these activations are found within the starting word token and delimiter tokens.
  - Additionally, we show that massive activations are not the same as outlier features (Dettmers et al., 2022), a previously known phenomenon in LLMs.
- Massive activations act as fixed but crucial bias terms in LLMs.
  - Certain internal states of the models that are independent from the inputs, analogous to the bias term b in a linear layer y = W x + b.
  - First, we show that massive activations play a critical role in LLMs’ capabilities. For instance, in LLaMA2-7B, setting merely four massive activations (out of millions of activations) to zero would result in catastrophic collapse in model performance.
  - Further, **setting them to their mean values does not hurt the model, suggesting their role is equivalent to simple constant biases**.
  - Our analysis reveals that after the initial layers, LLMs repurpose the tokens linked with massive activations to store these important biases.
- Massive activations are closely connected with self-attention.
  - In particular, we show massive activations cause attention to be attracted to the tokens associated with them.
  - Our findings extend the observations from “attention sinks” (Xiao et al., 2023b)—we demonstrate that LLMs allocate excessive attention to more than just the first token, and provide an in-depth analysis on how such attention concentration patterns arise.
  - Our analysis suggests that LLMs try to learn implicit bias components in self-attention via massive activations, during their pretraining phase.
  - We thus experiment with augmenting self-attention with additional key and value embeddings that are explicitly designed as biases.
  - Remarkably, we demonstrate that training with them eliminates the need for LLMs to learn massive activations.

![image](https://github.com/user-attachments/assets/96b97b21-81a5-41fb-af6d-dd382b7beaa1)

