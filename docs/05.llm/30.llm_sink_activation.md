---
title: From Attention Sink to Massive Activation
date: 2025-04-30 12:32:49
permalink: /pages/dc7068/
---

1. [596] Efficient Steaming Language Models with Attention Sinks


## 1. [596 2023] Efficient Steaming Language Models with Attention Sinks

Attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention.

In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a “sink” even if they are not semantically important.

<img src="https://github.com/user-attachments/assets/e78d12d3-80c5-4771-bb09-4a39440d089c" style="width:600px;height:auto;">


The results show the insufficiency of introducing merely one or two initial tokens, whereas a threshold of four initial tokens appears enough, with subsequent additions contributing marginal effects.

The nature of the SoftMax function prevents all attended tokens from having zero values.

This requires aggregating some information from other tokens across all heads in all layers, even if the current embedding has sufficient self-contained information for its prediction.

Consequently, the model tends to dump unnecessary attention values to specific tokens.

This result justifies our choice of introducing 4 initial tokens as attention sinks in StreamingLLM.

We’ve noted that LLMs are typically trained to utilize multiple initial tokens as attention sinks rather than just one.

<img src="https://github.com/user-attachments/assets/fb405010-15cb-4e44-aac4-95a91b98df55" style="width:600px;height:auto;">

The KV cache in StreamingLLM can be conceptually divided into two parts, as illustrated in Figure 4: (1) Attention sinks (four initial tokens) stabilize the attention computation; 2) Rolling KV Cache retains the most recent tokens, crucial for language modeling.

<img src="https://github.com/user-attachments/assets/ff71fbb0-f72d-42e1-8dd7-b6cf41f7d3df" style="width:600px;height:auto;">

They also test pretraining with a single sink token.

The vanilla model requires the addition of multiple tokens as attention sinks to maintain stable streaming perplexity.

In contrast, the model trained with a sink token achieves satisfactory streaming performance **using just the sink token**.

<img src="https://github.com/user-attachments/assets/96760ee8-9989-483f-8a2e-af2f2894eed2" style="width:600px;height:auto;">

