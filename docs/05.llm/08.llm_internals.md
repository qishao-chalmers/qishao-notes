---
title: LLM Paper List
date: 2024-12-15 23:32:49
permalink: /pages/dc7042/
---

Too many paper on llms...

**Survey**
1. [C60 2024] Understanding LLMs: A Comprehensive Overview from Training to Inference 
2. [C20 2024] Mobile Edge Intelligence for Large Language Models: A Contemporary Survey
3. [P 58] A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms
4. [P 30] A Comprehensive Evaluation of Quantization Strategies for Large Language Models
5. [P 8]  A Comprehensive Study on Quantization Techniques for Large Language Models
6. [C24] Model Compression and Efficient Inference for Large Language Models: A Survey
7. [C26 2024] Towards Better Chain-of-Thought Prompting Strategies: A Survey
8. [C24 2024] A Survey of Reasoning with Foundation Models
9. [C75 2025] Resource-efficient Algorithms and Systems of Foundation Models: A Survey :+1:
10. [C4 2024] LLM for Mobile: An Initial Roadmap
11. [C1 2024] Achieving Peak Performance for Large Language Models: A Systematic Review
12. [C1 2024] A Survey: Collaborative Hardware and Software Design in the Era of Large Language Models
13. [C577 2014] Mixture of experts: a literature survey
14. [C187 2024] Large Language Model based Multi-Agents: A Survey of Progress and Challenges
15. [C80 2024] Understanding the planning of LLM agents: A survey
16. [C895 2023] A survey on large language model based autonomous agents\
17. [C60 2024] Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects
18. [C59 2024] Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline

**KV Cache**
1. [C60 2024 USENIX] InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management  :+1:
2. [C1 2024]LayerKV: Optimizing Large Language Model Serving with Layer-wise KV Cache Management
3. [C1 2024 wei] Unifying KV Cache Compression for Large Language Models with LeanKV

**Quantization**
1. [C8 Y2024] An Empirical Study of LLaMA3 Quantization: From LLMs to MLLMs

**Cross-layer Attention**
1. [C25 Y2024] **Reducing Transformer Key-Value Cache Size** with Cross-Layer Attention
2. [C4 2024] Cross-layer Attention Sharing for Large Language Models


**Attention**
1. [C1709 2020] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
2. [C573 2018] Efficient Attention: Attention With Linear Complexities
3. [C93 2024] Gated Linear Attention Transformers with Hardware-Efficient Training
4. [C24 2024] Tensor Attention Training: Provably Efficient Learning of Higher-order Transformers
5. [2024] When Attention Sink Emerges in Language Models: An Empirical View :+1:
6. [C40 2024] Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon

**Is All You Need**
1. [2025] Element-wise Attention Is All You Need
2. [2025] Tensor Product Attention Is All You Need

**Feedforward Layers**
1. [2024] Building on Efficient Foundations: Effectively Training LLMs with Structured Feedforward Layers
2. [C3 2024] FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference

**Attatch Memory**
1. [C2 2024] Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU
2. [C63 2023] LLM in a flash: Efficient Large Language Model Inference with Limited Memory
 
**Novel LLM**

1. [C2 2024] Larimar: Large Language Models with Episodic Memory Control

**Batch**
1. [C59 2023] Batch Prompting: Efficient Inference with Large Language Model APIs

**Pruning**
1. [C25 2024] ZipLM: Inference-Aware Structured Pruning of Language Models

**Speculative decoding**
1. [C25 2024] Efficient Inference for Large Language Model-based Generative Recommendation
2. [C39 2024] Enhancing Inference Efficiency and Accuracy in Large Language Models through Next-Phrase Prediction  :+1:
3. [C5 2023] Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy
4. [C64 2024] Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding   :+1:
5. [C4 2024] Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference
6. [C46 2023] SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference  :+1:

**Interesting**
1. [C60 2024] SnapKV: LLM Knows What You Are Looking for before Generation  :+1:
2. [C6 2024] Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training
3. [C165 2019] Efficient Training of BERT by Progressively Stacking
4. [C9 ISCA 2024] LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference
5. [C6 2024] Efficient Large Foundation Models Design: A Perspective From Model and System Co-Design
6. [C83 2023] Compressing Context to Enhance Inference Efficiency of Large Language Models  :+1:
7. [C13 2024 OSDI] ServerlessLLM: Low-Latency Serverless Inference for Large Language Models
8. [C12 2024] From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models
9. [C418 2022] Transformers Learn In-Context by Gradient Descent :+1:
10. [C51 2024] Massive Activations in Large Language Models :+1:
11. [C2007 2019] Generating Long Sequences with Sparse Transformers :+1:
12. [C1890 2019] What does BERT look at? An Analysis of BERTâ€™s Attention
13. [C1798 2019] BERT Rediscovers the Classical NLP Pipeline
14. [C402 2019] Analyzing the Structure of Attention in a Transformer Language Model
15. [C347 2018] Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures
16. [C112 2021] MoEfication: Transformer Feed-forward Layers are Mixtures of Experts
17. [C1325 2019] Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Prune
18. [C101] An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation
19. [C38517 2020] Language Models are Few-Shot Learners :+1:
20. [C24 2024] What can a Single Attention Layer Learn? A Study Through the Random Features Lens
21. [C9 2023] Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps
22. [C606 2021] Transformer Feed-Forward Layers Are Key-Value Memories :+1:
23. [Blog] The Feedforward Demystified: A Core Operation of Transformers
24. [C211 2024] ConvBERT: Improving BERT with Span-based Dynamic Convolution  :+1:
25. [C15 2020] Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models
26. [C147 2023] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
27. [C24 2024] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention
28. [C0 2024] Densing Law of LLMs
29. [C9 2024] Demystifying the Compression of Mixture-of-Experts Through a Unified Framework
30. [C7 2024] Configurable Foundation Models: Building LLMs from a Modular Perspective
31. [C2 2024] Task Scheduling for Efficient Inference of Large Language Models on Single Moderate GPU Systems
32. [C7 2024] Towards Sustainable Large Language Model Serving :+1:
33. [C42 2024] LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding
34. [C10395 0221] GPT Understands,Too
35. [C36 2023] Simplifying Transformer Blocks
36. [C2 2024] When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1
37. [C188 2023] LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion

**Why Infer?**
1. [C78 2024] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU  :+1:
2. [C25 2024] Powerinfer-2: fast large language model inference on a smartphone
3. [C6 2024] InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference
4. [C0 2024] CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation
5. [C3 2024] NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference
6. [C2 2024] MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs
7. [C2 2025] SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference
8. [C1 2024] BlendServe: Optimizing Offline Inference for Auto-regressive Large Models with Resource-aware Batching
9. [C5 2024] Efficient LLM Inference with Activation Checkpointing and Hybrid Caching
10. [C12 2024]Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache
11. [C2-24] HiRE: High Recall Approximate Top-k Estimation for Efficient LLM Inference*
12. [C1 2024] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference

**Chain of Thoughts**
1. [C1954 2024] Tree of Thoughts: Deliberate Problem Solving with Large Language Models :+1:
2. [C795 2022] Automatic Chain of Thought Prompting in Large Language Models
3. [C109 2022] Iteratively Prompt Pre-trained Language Models for Chain of Thought
4. [C2 2024] Reducing Costs - The Path of Optimization for ChainofThought Reasoning via Sparse Attention Mechanism
5. [C17 2024] Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future
6. [C63 2024] Chain of Thought Empowers Transformers to Solve Inherently Serial Problems

**General Efficient**
1. [C44 2023] Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity
2. [C4 2024] Efficient Training and Inference: Techniques for Large Language Models Using Llama
3. [C226 2023] H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models
4. [C17 2024] EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism

**Big tech**
1. [NVIDIA 1868] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism
2. [2579] Scaling Laws for Neural Language Models
3. [C510 2021] Baidu ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
4. [C6 2024] Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent
5. [C1671] Qwen Technical Report 
6. [C1212] Constitutional AI: Harmlessness from AI Feedback
7. [C1082] Scaling Language Models: Methods, Analysis & Insights from Training Gopher
8. [C11289] Llama 2: Open Foundation and Fine-Tuned Chat Models

 **Hyperparameter**
 1. [C2070 2020] Designing Network Design Spaces
 2. [C25389 2019] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
 3. [C1422 2018] A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay

 **New Design**
 1. [1719 2022] Mamba: Linear-Time Sequence Modeling with Selective State Spaces
 2. [29 2024] Demystify Mamba in Vision: A Linear Attention Perspective\
    This paper explains that **Mamba and linear attention Transformer can be formulated within a unified framework**.

 **Old Gold Time of Transformer**
 1. [C4622 2019] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
 2. [C10646 XLNet 2020] XLNet: Generalized Autoregressive Pretraining for Language Understanding

 **AI Agent**
 1. [C147 2023] MemoryBank: Enhancing Large Language Models with Long-Term Memory
 2. [C22 2023] Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges
 3. [C2051 2022] ReAct: Synergizing Reasoning and Acting in Language Models
 4. [C220 2023] OpenAGI: When LLM Meets Domain Experts


---

## Attention
### 1. Transformers are RNNs:Fast Autoregressive Transformers with Linear Attention :+1: :+1:
### 2. Efficient Attention: Attention With Linear Complexities

Also another paper:
Gaussianâ€‘Linearized Transformer with Tranquilized Timeâ€‘Series Decomposition Methods for Fault Diagnosis and Forecasting of Methane Gas Sensor Arrays.

These two paper are almost the same.
Basic idea:
classic calculation of attention:

<img src="https://github.com/user-attachments/assets/70208eed-9009-4677-a017-57a14773dfb8" width="40%">

Instead of calculating QK, it calculates KV.

<img src="https://github.com/user-attachments/assets/9a6ec80b-8119-47fc-8198-851803dcff6d" width="70%">

<img src="https://github.com/user-attachments/assets/f81039ea-43fd-43c5-a5d7-6b9787459a1b" width="70%">

Figure from: Gaussianâ€‘Linearized Transformer

Class calculation complexity:

<img src="https://github.com/user-attachments/assets/5902c7ec-ac3d-4bbd-92bb-fa95c82208f5" width="50%">

Provided the model dimension is << sequence length, 

compute complexity in linear attention:

<img src="https://github.com/user-attachments/assets/134b7825-1f53-4811-b101-25b6ce251316" width="50%">

[Figure Source](https://towardsdatascience.com/linear-attention-is-all-you-need-5fa9c845c1b5)

<img src="https://github.com/user-attachments/assets/e245f311-52fa-47ec-b183-2fd986a1001e" width="50%">

Source:Demystify Mamba in Vision: A Linear Attention Perspective

<img src="https://github.com/user-attachments/assets/a2b0c73e-4d90-4e4f-954c-ccf83f8fe2ac" width="50%">

[Figure Source](https://sandeshgh.com/post/transformers_rnn/)

Provided the model dimension is << sequence length, linear complexity is much lower compute complexity compared with classic attention.

Please notice that in  the KV result. The intermediate result is dmodel * dmodel, instead of N * N.
N is context length and dmodel is model dimension.

---

## Is All You Need
### Tensor Product Attention Is All You Need
Idea: factorizes Q, K, and V activations using contextual tensor-decompositions

<img src="https://github.com/user-attachments/assets/be3c6aaf-d33f-42d7-8f15-4b57a8fa9b15" width="70%">

<img src="https://github.com/user-attachments/assets/0c345baa-5481-43f8-9f07-9d88dd9f7644" width="70%">


Instead of caching Q,K,V, it could only cache A,B of Q and A,B of K.
This matrix factorization might be similar to LORA, using multiplication of two matrices A and B to represent Q.

By caching A,B, it reduces large memory required by KV cache.

<img src="https://github.com/user-attachments/assets/b64ad8ac-bbba-409d-919d-b438c1859b5a" width="70%">

## Interesting
### 21. [C21 2022] MoEfication: Transformer Feed-forward Layers are Mixtures of Experts
![image](https://github.com/user-attachments/assets/a809c124-b2f3-4b94-8db4-61731a793873)\
*Source from the paper*

### 28. [C10395 0221] GPT Understands,Too
Straightforward Idea.\
Changing a single word in the prompt might result in substantial performance drop.\
P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts.\
Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE.\
P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.

![image](https://github.com/user-attachments/assets/0ad0e6ae-afc8-4c8f-9905-a1650f28a751)

## Old Gold Time of Transformer
### 1. tansformer-xl
#### Segment-Level Recurrence Mechanism
- Extends context length by reusing hidden states (memory) across segments.
- Instead of processing fixed-length context windows independently, Transformer-XL introduces a **memory module**:
  - Memory at segment `t` is represented as:
    
  - This allows the model to attend to representations from previous segments.
- Enables modeling of long-term dependencies without recomputation of overlapping segments.
#### Relative Positional Encoding (RPE):
- Introduces position encodings relative to each query-key pair, improving generalization across sequence lengths.
- The positional bias is calculated as:

![image](https://github.com/user-attachments/assets/8d74f888-ef26-40d5-b12f-52300f54b251)

#### Segment-Level Attention:
- The attention mechanism incorporates both the current segment and memory from previous segments:
![image](https://github.com/user-attachments/assets/c11b0860-6b85-4c9f-b16c-7426c97b3605)

## Response Length Prediction
### 18. [C59 2024] Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline
They finetune the model to get accurate length prediction.

The benefit is that they could group the request into batches, so requests could be grouped with similar length.

![image](https://github.com/user-attachments/assets/6795f63c-4a37-499d-8037-6a2f5977232f)



