---
title: Memory Optimizations in LLM
date: 2025-01-26 23:32:49
permalink: /pages/dc7048/
---

1. [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources
2. [C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors
3. [C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection
4. [C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training
5. [C2 2024] ProTrain: Efficient LLM Training via Adaptive Memory Management

---

## **Memory Optimizations**
- **Activation Checkpointing**  
  Recomputation during backward pass.
- **Quantization-Aware Training (QAT)**  
  Train with INT8/FP8 precision.
- **Dynamic Memory Allocation**  
  Buffer reuse to avoid fragmentation.
- **Low-Rank Gradient Projection (GaLore)**  
  **NEW** Compress gradients via low-rank approximations during training.

---

## 1. [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources

- Use SGD instead of Adam for fine-tuning weights.
- Update layer by layer in backward pass. Traditional Adam will backward probgation all layers and then update weigths.
- SGD also avoid state memory of ADAM.

![image](https://github.com/user-attachments/assets/c1e7d018-84e2-49f7-bd74-cdf4edaa0343)

![image](https://github.com/user-attachments/assets/a98a808f-1cb1-4432-b0d0-7c51e9f4ee48)

---

## 2. [C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors

This paper discovers that LORA can be approximated by a random projection.

LORA restricts overall weights update matrices to be low-rank.

FLORA use *random projection matrix*, which allows high-rank update gradients.

> Our intuition arises from investigating LoRA and observing that a LoRA update is dominated by a random projection, which compresses the gradient into a
lower-dimensional space.
> Our FLORA resamples the random projection and is able to mitigate the low-rank limitation of LoRA. Further, our approach only stores the compressed gradient
accumulation and momentum, thus saving the memory usage of optimization states to the sublinear level.

Gradident Accumulation:
- Gradient accumulation stores the sum of gradients over multiple batches to simulate a larger effective batch size (helpful when memory limits prevent using large batches).
- Normally, this requires a memory buffer equal to the model size to store the full gradient matrix.

Momentum
- Momentum smooths gradient updates by keeping an exponentially weighted moving average (EMA) of past gradients.
- Maintaining momentum for large models requires significant memory since an additional buffer equal to the model size is needed.

FLORA Compression:
- compress gradients accumulation: Applying a random projection matrix A to reduce the dimensionality of the gradients.
- compress momentum: Using random projection to compress the momentum term M.

---

## 3. [C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection
![image](https://github.com/user-attachments/assets/3ddb7188-8d90-4232-8be1-cb570a74bc56)

> Galore: gradient Low-Rank Projection (GaLore), a training strategy that allows fullparameter learning but is more memory-efficient than common low-rank adaptation  methods such as LoRA.
> Key idea is to leverage the slowchanging low-rank structure of the gradient G(m×n) of the weight matrix W, rather than trying to approximate the weight matrix itself as low rank.
> while the weight matrices are not necessarily low-rank, the gradient indeed becomes low-rank during the training for certain gradient forms and associated network
architectures.

---

## 4. [C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training
<img src="https://github.com/user-attachments/assets/37a40cf7-5a3b-4c55-b847-1fb1e9c732a5" style="width:600px;height:auto;">

> By storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters.
> CompAct saves low-rank compressed activations during the forward pass, instead of the full activation tensors.
> The resulting gradients are low-rank as well, also reducing the size of optimizer states.
> As CompAct decompresses the gradients back to full size only for the update step, it compresses a large part of the compute graph, which in turn translates to major memory savings.

CompAct is a logical next step from previous work, moving from **low-rank parameters**, through **compressed low-rank gradients** , to **compressed activations**.

> compared to GaLore, our approach may be viewed as a simple change in the order of operations, applying the compression one step before GaLore does, to the **activations** rather than to the **gradients**.

![image](https://github.com/user-attachments/assets/c0e05d1b-b19b-4bb0-92df-4842010b6502)

---

## 5. [C2 2024] ProTrain: Efficient LLM Training via Adaptive Memory Management

### Background

**Model State**: Zero Redundancy Optimizer (ZeRO) (37; 51) distributes them across multiple GPUs, leveraging aggregated memory capacity to accommodate large models in data parallelism.

**activations, gradient checkpointing** reduces memory consumption by discarding certain activations during the forward pass and recomputing them during the backward pass.

### Contribution
- To reduce memory consumption, ProTrain adaptively decides whether to use offloading or gradient checkpointing, determines the amount of model states and activations to offload and the number of transformer blocks to apply gradient checkpointing, all without user inputs.
- For computation, ProTrain keeps forward/backward computation on the GPU for efficiency, while dynamically determining the portion of parameter updates to be performed on the CPU and GPU.
  Additionally, ProTrain performs CPU parameter updates concurrently with backward computation on the GPU to hide the overhead of CPU updates.
- ProTrain overlaps IO communication with computation by proactively prefetching future parameters during forward/backward computation, parallelizing gradient offloading with backward computation, and swapping activations only when
the overhead can be hidden by computation.


- ProTrain proposes a Chunk-Based Model State Management system that organizes model states into uniformly sized chunks
- ProTrain also proposes **Block-Wise Activation Management** to handle activations at the **transformer block** level, performing swapping or gradient checkpointing as needed for each block.
- To hide the swapping overhead, ProTrain applies interleaved swapping and checkpointing, where each block of swapping is typically followed by multiple blocks of checkpointing.
  This ensures that ProTrain’s swapping reduces memory usage without compromising performance.

**Discussion about zero**

ZeRO operates in three stages
- ZeRO-1 partitions optimizer states across GPUs
- ZeRO-2 extends this by also distributing gradients
- ZeRO-3 further divides the parameters, which are required to be gathered before forward/backward computation.

*The most interesting contribution of this work to me, it is the interleaved gradient checkpointing and swapping.*


Since the training follows the specific sequence:
- the last layer: no optimization
- the second last layer: gradient checkpointing
- the third last layer: swapping

**In the backward pass, blocks without optimization are processed first, consuming activations and freeing memory for subsequent checkpointing and swapping.**

<img src="https://github.com/user-attachments/assets/b0daba0b-50b0-4d9d-b94b-1697910afedb" style="width:600px;height:auto;">
