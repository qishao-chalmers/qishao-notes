---
title: Memory Optimizations in LLM
date: 2025-01-26 23:32:49
permalink: /pages/dc7048/
---

## **Memory Optimizations**
- **Activation Checkpointing**  
  Recomputation during backward pass.
- **Quantization-Aware Training (QAT)**  
  Train with INT8/FP8 precision.
- **Dynamic Memory Allocation**  
  Buffer reuse to avoid fragmentation.
- **Low-Rank Gradient Projection (GaLore)**  
  **NEW** Compress gradients via low-rank approximations during training.

---

### [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources

- Use SGD instead of Adam for fine-tuning weights.
- Update layer by layer in backward pass. Traditional Adam will backward probgation all layers and then update weigths.
- SGD also avoid state memory of ADAM.

![image](https://github.com/user-attachments/assets/c1e7d018-84e2-49f7-bd74-cdf4edaa0343)

![image](https://github.com/user-attachments/assets/a98a808f-1cb1-4432-b0d0-7c51e9f4ee48)

---

### [C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors

This paper discovers that LORA can be approximated by a random projection.

LORA restricts overall weights update matrices to be low-rank.

FLORA use *random projection matrix*, which allows high-rank update gradients.

> Our intuition arises from investigating LoRA and observing that a LoRA update is dominated by a random projection, which compresses the gradient into a
lower-dimensional space.
> Our FLORA resamples the random projection and is able to mitigate the low-rank limitation of LoRA. Further, our approach only stores the compressed gradient
accumulation and momentum, thus saving the memory usage of optimization states to the sublinear level.

Gradident Accumulation:
- Gradient accumulation stores the sum of gradients over multiple batches to simulate a larger effective batch size (helpful when memory limits prevent using large batches).
- Normally, this requires a memory buffer equal to the model size to store the full gradient matrix.

Momentum
- Momentum smooths gradient updates by keeping an exponentially weighted moving average (EMA) of past gradients.
- Maintaining momentum for large models requires significant memory since an additional buffer equal to the model size is needed.

FLORA Compression:
- compress gradients accumulation: Applying a random projection matrix A to reduce the dimensionality of the gradients.
- compress momentum: Using random projection to compress the momentum term M.

---

### [C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection
![image](https://github.com/user-attachments/assets/3ddb7188-8d90-4232-8be1-cb570a74bc56)

> Galore: gradient Low-Rank Projection (GaLore), a training strategy that allows fullparameter learning but is more memory-efficient than common low-rank adaptation  methods such as LoRA.
> Key idea is to leverage the slowchanging low-rank structure of the gradient G(mÃ—n) of the weight matrix W, rather than trying to approximate the weight matrix itself as low rank.
> while the weight matrices are not necessarily low-rank, the gradient indeed becomes low-rank during the training for certain gradient forms and associated network
architectures.

---

### [C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training

![image](https://github.com/user-attachments/assets/37a40cf7-5a3b-4c55-b847-1fb1e9c732a5)


> By storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters.
> CompAct saves low-rank compressed activations during the forward pass, instead of the full activation tensors.
> The resulting gradients are low-rank as well, also reducing the size of optimizer states.
> As CompAct decompresses the gradients back to full size only for the update step, it compresses a large part of the compute graph, which in turn translates to major memory savings.

CompAct is a logical next step from previous work, moving from **low-rank parameters**, through **compressed low-rank gradients** , to **compressed activations**.

> compared to GaLore, our approach may be viewed as a simple change in the order of operations, applying the compression one step before GaLore does, to the **activations** rather than to the **gradients**.

![image](https://github.com/user-attachments/assets/c0e05d1b-b19b-4bb0-92df-4842010b6502)

