---
title: Memory Optimizations in LLM
date: 2025-01-26 23:32:49
permalink: /pages/dc7048/
---

1. [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources
2. [C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors
3. [C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection
4. [C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training
5. [C2 2024] ProTrain: Efficient LLM Training via Adaptive Memory Management
6. [C6 2024] Memo: Fine-grained Tensor Management For Ultra-long Context LLM Training
7. [2025] Efficient LLM Inference with Activation Checkpointing and Hybrid Caching
8. [6] Elixir: Train a Large Language Model on a Small GPU Cluster
9. [7] LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs <br> This is a very good paper that estimate memory usage based on different parallism. :+1:
10. [12] vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention

---

## **Memory Optimizations**
- **Activation Checkpointing**  
  Recomputation during backward pass.
- **Quantization-Aware Training (QAT)**  
  Train with INT8/FP8 precision.
- **Dynamic Memory Allocation**  
  Buffer reuse to avoid fragmentation.
- **Low-Rank Gradient Projection (GaLore)**  
  **NEW** Compress gradients via low-rank approximations during training.

---

## 1. [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources

- Use SGD instead of Adam for fine-tuning weights.
- Update layer by layer in backward pass. Traditional Adam will backward probgation all layers and then update weigths.
- SGD also avoid state memory of ADAM.

![image](https://github.com/user-attachments/assets/c1e7d018-84e2-49f7-bd74-cdf4edaa0343)

![image](https://github.com/user-attachments/assets/a98a808f-1cb1-4432-b0d0-7c51e9f4ee48)

---

## 2. [C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors

This paper discovers that LORA can be approximated by a random projection.

LORA restricts overall weights update matrices to be low-rank.

FLORA use *random projection matrix*, which allows high-rank update gradients.

> Our intuition arises from investigating LoRA and observing that a LoRA update is dominated by a random projection, which compresses the gradient into a
lower-dimensional space.
> Our FLORA resamples the random projection and is able to mitigate the low-rank limitation of LoRA. Further, our approach only stores the compressed gradient
accumulation and momentum, thus saving the memory usage of optimization states to the sublinear level.

Gradident Accumulation:
- Gradient accumulation stores the sum of gradients over multiple batches to simulate a larger effective batch size (helpful when memory limits prevent using large batches).
- Normally, this requires a memory buffer equal to the model size to store the full gradient matrix.

Momentum
- Momentum smooths gradient updates by keeping an exponentially weighted moving average (EMA) of past gradients.
- Maintaining momentum for large models requires significant memory since an additional buffer equal to the model size is needed.

FLORA Compression:
- compress gradients accumulation: Applying a random projection matrix A to reduce the dimensionality of the gradients.
- compress momentum: Using random projection to compress the momentum term M.

---

## 3. [C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection
![image](https://github.com/user-attachments/assets/3ddb7188-8d90-4232-8be1-cb570a74bc56)

> Galore: gradient Low-Rank Projection (GaLore), a training strategy that allows fullparameter learning but is more memory-efficient than common low-rank adaptation  methods such as LoRA.
> Key idea is to leverage the slowchanging low-rank structure of the gradient G(m√ón) of the weight matrix W, rather than trying to approximate the weight matrix itself as low rank.
> while the weight matrices are not necessarily low-rank, the gradient indeed becomes low-rank during the training for certain gradient forms and associated network
architectures.

---

## 4. [C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training
<img src="https://github.com/user-attachments/assets/37a40cf7-5a3b-4c55-b847-1fb1e9c732a5" style="width:600px;height:auto;">

> By storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters.
> CompAct saves low-rank compressed activations during the forward pass, instead of the full activation tensors.
> The resulting gradients are low-rank as well, also reducing the size of optimizer states.
> As CompAct decompresses the gradients back to full size only for the update step, it compresses a large part of the compute graph, which in turn translates to major memory savings.

CompAct is a logical next step from previous work, moving from **low-rank parameters**, through **compressed low-rank gradients** , to **compressed activations**.

> compared to GaLore, our approach may be viewed as a simple change in the order of operations, applying the compression one step before GaLore does, to the **activations** rather than to the **gradients**.

![image](https://github.com/user-attachments/assets/c0e05d1b-b19b-4bb0-92df-4842010b6502)

---

## 5. [C2 2024] ProTrain: Efficient LLM Training via Adaptive Memory Management :+1:

### Background

**Model State**: Zero Redundancy Optimizer (ZeRO) (37; 51) distributes them across multiple GPUs, leveraging aggregated memory capacity to accommodate large models in data parallelism.

**activations, gradient checkpointing** reduces memory consumption by discarding certain activations during the forward pass and recomputing them during the backward pass.

### Contribution
- To reduce memory consumption, ProTrain adaptively decides whether to use offloading or gradient checkpointing, determines the amount of model states and activations to offload and the number of transformer blocks to apply gradient checkpointing, all without user inputs.
- For computation, ProTrain keeps forward/backward computation on the GPU for efficiency, while dynamically determining the portion of parameter updates to be performed on the CPU and GPU.
  Additionally, ProTrain performs CPU parameter updates concurrently with backward computation on the GPU to hide the overhead of CPU updates.
- ProTrain overlaps IO communication with computation by proactively prefetching future parameters during forward/backward computation, parallelizing gradient offloading with backward computation, and swapping activations only when
the overhead can be hidden by computation.


- ProTrain proposes a Chunk-Based Model State Management system that organizes model states into uniformly sized chunks
- ProTrain also proposes **Block-Wise Activation Management** to handle activations at the **transformer block** level, performing swapping or gradient checkpointing as needed for each block.
- To hide the swapping overhead, ProTrain applies interleaved swapping and checkpointing, where each block of swapping is typically followed by multiple blocks of checkpointing.
  This ensures that ProTrain‚Äôs swapping reduces memory usage without compromising performance.

**Discussion about zero**

ZeRO operates in three stages
- ZeRO-1 partitions optimizer states across GPUs
- ZeRO-2 extends this by also distributing gradients
- ZeRO-3 further divides the parameters, which are required to be gathered before forward/backward computation.

*The most interesting contribution of this work to me, it is the interleaved gradient checkpointing and swapping.*


Since the training follows the specific sequence:
- the last layer: no optimization
- the second last layer: gradient checkpointing
- the third last layer: swapping

**In the backward pass, blocks without optimization are processed first, consuming activations and freeing memory for subsequent checkpointing and swapping.**

<img src="https://github.com/user-attachments/assets/b0daba0b-50b0-4d9d-b94b-1697910afedb" style="width:600px;height:auto;">

---

## 6. [C6 2024] Memo: Fine-grained Tensor Management For Ultra-long Context LLM Training

I feel this paper shares the same idea from [C2 2024] ProTrain: Efficient LLM Training via Adaptive Memory Management.

like the balance between swapping and recomputing gradient, here they argue that for long context llm training, swapping might be more economic compared with gradient checkpointing.

**Feel the trend on the right... Full offload might be more economic when length grows larger.**

<img src="https://github.com/user-attachments/assets/43d4b157-76a2-4022-8620-c393ae0b0a70" style="width:600px;height:auto;">

<img src="https://github.com/user-attachments/assets/2147e9ea-ed8e-4609-a03c-200dbc85842c" style="width:600px;height:auto;">

Contemporary mainstream LLM training frameworks such as Megatron-LM and DeepSpeed prefer activation recomputation to swapping, which is due to the fact that the GPU computing ability has a far more rapid growth than the
connectivity between CPU and GPU memory in the past few years.

However, we find that the situation is a bit different in long context training of LLMs. Denote (ùë†) as the sequence length. The computation complexity of one transformer layer is ùëÇ(ùë†^2), while the activation memory complexity is ùëÇ(ùë†) thanks to FlashAttention.

During GPU computation, we can leverage the idle CPU-GPU bandwidth, offloading activations to CPU memory during the forward
pass, and fetching the activations during the backward pass.

As the sequence length increases, there is greater potential for overlapping computation and communication, given that their time requirements scale **quadratically** and **linearly** with the sequence length.

We introduce a fine-grained activation recomputation and swapping mechanism to manage the skeletal activations. We consider both tensor-level and token-level activation management.

<img src="https://github.com/user-attachments/assets/7491aeab-f900-4d96-866b-aa7d68d94586" style="width:600px;height:auto;">

---

## 7. [2025] Efficient LLM Inference with Activation Checkpointing and Hybrid Caching

The activation cache stores activation checkpoints generated during intermediate inference stages, allowing the fast recomputation of KV cache while model parameters are transferred to GPU from host memory.


Unlike conventional methods that recompute the KV cache from scratch using token IDs, the activation cache allows bypassing projection and FFN operations.

To address the memory and communication challenges of the host memory offloading, this study proposes a novel activation checkpointing scheme based on Activation caching. 

Unlike the token recomputation relying on a full prefill step, activation values of all layers are stored in the memory. Computing the keys and values of a layer from the activation values requires a modest computation capacity.

**Keeping activation checkpoints** instead of keys and values reduce the memory capacity consumption and the communication traffic by half.

> The major arguement: activation memory is less than K&V, K&V could be recomputed.

<img src="https://github.com/user-attachments/assets/f7fd3b57-a94f-4429-82f2-47805c3f535d" style="width:600px;height:auto;">

since completely removing the KV cache incurs excessively repetitive computations across token generations, maintaining only a portion of the context as KV cache and the rest as token IDs is more promising.

As balancing computation and communication is a key challenge for improving throughput, the ratio of KVs to token IDs must be
adjusted carefully.

<img src="https://github.com/user-attachments/assets/74164b54-d369-4d97-91ca-41026c7d563c" style="width:600px;height:auto;">

---
## 8. [6] Elixir: Train a Large Language Model on a Small GPU Cluster

<img src="https://github.com/user-attachments/assets/7e8a9e6b-c565-4bf9-8584-f6bb0154b79b" style="width:600px;height:auto;">

**We argue that using free GPU memory to store optimizer states or retaining gathered parameters during training can improve training throughput.**

‚Ä¢ We build a pre-runtime profiler designed for large models. It is capable of obtaining the computation graph and the memory usage of the model before training. We bring this powerful tool to support large model profiling.
‚Ä¢ We introduce rCache to control the degree of memory redundancy. Moreover, we build a search engine to find the optimal configuration, maximizing training efficiency automatically.
  Different from previous works, our optimal configuration considers both memory partitioning and memory offloading.

### Background

#### Memory Usage

Memory usage during training primarily consists of five components: parameters, gradients, optimizer states, activations, and buffers.

Optimizer states are the extra memory footprint consumed by the optimizer.

For example, Adam [26] needs to store averaged momentum and variance of gradients.

We refer to parameters, gradients, and optimizer states collectively as **model states**.

Activations are the intermediate temporary variables generated during training.

Typically,  activations are stored for the backward pass to compute gradients.

However, their memory usage may vary depending on the training framework.

In PyTorch, the **temporary gradients of intermediate tensor variables** can also be viewed as **activations**.

Compared to other components, buffers consume a relatively small amount of memory.

We assume that buffers are always stored in the GPU for subsequent analysis.

#### Mixed Precision Training

The SOTA approach to train large models utilizes both the halfprecision floating-point (FP16) format and the single-precision floating-point (FP32) format during training.

**Parameters, gradients, and activations** are stored and computed in FP16 to reduce memory usage and improve efficiency. 

Meanwhile, the accumulation operator in the optimizer update is sensitive to underflow in low-precision formats. The master weight, which is an FP32 copy of the parameters, is used to accumulate gradients in each optimizer update and is rounded to FP16 parameters before the forward pass.

In this case, the memory usage of parameters, gradients, and activations is halved, but the memory usage of optimizer states is increased due to the addition of the master weight.

For example, if we use Adam and the model size is M, training requires 2M bytes for parameters, 2M bytes for gradients, and 12M bytes for optimizer states.

<img src="https://github.com/user-attachments/assets/44c48f9f-1b69-41f9-8e5b-a3a58a034b95" style="width:600px;height:auto;">

<img src="https://github.com/user-attachments/assets/ff98c9bf-fcec-4d6e-94ea-99a4f145c445" style="width:600px;height:auto;">

---

## [7] LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs

Determining the most effective method for achieving rapid fine-tuning while preventing GPU outof-memory issues in a given environment remains unclear.

To address this challenge, we introduce LLMem, a solution that estimates the GPU memory consumption when applying distributed finetuning methods across multiple GPUs and identifies the optimal method.

---
## [12 ASPLOS] vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention

<img 
  src="https://github.com/user-attachments/assets/0689a237-6fcb-4d37-959f-f8aaa402d223" 
  alt="My Image" 
  style="width:300px;height:auto;"
/>

Pageattention needs two level software mapping.

> For example, the vLLM paper acknowledges that the PagedAttention-based implementation was 20‚àí26% slower than the corresponding none-paged FasterTransformer kernel, primarily due to the overhead of looking up Block-Tables and executing extra branches.

> Our analysis reveals that the number of instructions executed in PagedAttention kernels is 7 ‚àí 13% higher than the non-paged kernels. Caching page indices also increases register pressure, causing register spilling.

In vAttention, they use CUDA VMM API to avoid the two level mapping.

<img 
  src="https://github.com/user-attachments/assets/513e1257-1a54-4667-a656-4fe40161d621"
  style="width:300px;height:auto;"
/>
