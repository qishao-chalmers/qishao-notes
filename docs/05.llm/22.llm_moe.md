---
title: LLM MOE Inference
date: 2025-03-23 23:32:49
permalink: /pages/dc7059/
---

> This is deepresearch report generated from ChatGPT.

# Inference-Time Challenges in Mixture-of-Experts Models for LLMs 


## Introduction 

Mixture-of-Experts (MoE) architectures have enabled **trillion-parameter**  scale language models while keeping inference computation per token relatively low.
Unlike dense Transformers where every layer’s feed-forward network (FFN) is applied to every input, MoE models activate only a few “expert” FFN sub-models per input token​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=These%20layers%20use%20a%20gating,MoE%20Transformers%20have%20reduced).
For example, Google’s 1.2 trillion-parameter GLaM model routes each token to 2 out of 64 experts, achieving better accuracy than GPT-3 (175B) while using about **50% fewer FLOPs per token** ​[AR5IV.ORG](https://ar5iv.org/pdf/2112.06905#:~:text=incurring%20substantially%20less%20training%20cost,performance%20across%2029%20NLP%20tasks) ​[AR5IV.ORG](https://ar5iv.org/pdf/2112.06905#:~:text=numbers%20of%20activated%20parameters%20per,We%20adopt%20the%20notation%20of).
Similarly, a Vision MoE (V-MoE) matched the performance of state-of-the-art vision models using **half the inference compute**  of a dense network​[ARXIV.ORG](https://arxiv.org/abs/2106.05974#:~:text=excellent%20scalability%20in%20Natural%20Language,off%20performance%20and) .
These sparse models demonstrate that enormous parameter counts can be leveraged without proportional increases in computation.

However, **inference-time performance**  of MoEs often falls short of these theoretical gains.
As Fedus et al. note, adoption of MoE models has been hindered by added system complexity and communication overhead on current hardware​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=Mixture,communication%20costs%2C%20and%20training%20instabilities).
In practice, a MoE’s **massive parameter size**  and dynamic token routing can introduce new bottlenecks.

Serving a MoE at scale requires careful orchestration:
- experts are typically distributed across devices (e.g. via Google’s GShard sharding framework​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=computation%20but%20more%20memory%20usage,all%20communication%20is%20required) )
- extra networking is needed to route tokens and gather results.

These factors can lead to higher latency, memory usage, and engineering challenges during inference.
In fact, Microsoft found that without special optimizations, a quality-equivalent MoE can be **15× slower**  in inference latency than a dense model despite similar FLOPs​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=dimensions%E2%80%94token%20throughput%2C%20memory%20use%2C%20load,a%20new%20gating%20function%20that) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=each%20expert%E2%80%99s%20computational%20capacity%20to,GPU%20kernels%20for%20even%20greater).

This survey reviews the key **inference-time challenges**  of MoE-based large language models – focusing on cost and performance issues (not training instabilities) – and summarizes notable findings and solutions from both research literature and industry implementations (Google’s Switch Transformer and GLaM, Meta’s NLLB MoE, Microsoft’s DeepSpeed-MoE, Alibaba’s M6, etc.).

We structure the discussion by major challenge areas:
- **expert routing and load balancing**
- **latency and parallelism**
- **memory and bandwidth**
- **hardware compatibility**
- **expert sparsity/imbalance**
- **real-time streaming inference**

## Expert Routing and Load Balancing 

**Routing tokens to experts**  is at the heart of MoE inference.

A learned gating network computes scores for each expert and selects the top-$k$ experts for each token (typically $k=1$ or $k=2$)​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=These%20layers%20use%20a%20gating,MoE%20Transformers%20have%20reduced) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=routes%20tokens%20to%20their%20corresponding,in%20vision%2C%20text%2C%20speech%20and) .

This conditional routing can lead to **load imbalance** : some experts may receive far more tokens than others in a given inference batch.
If one expert is overloaded, it becomes a bottleneck while other experts sit idle.

To mitigate this, MoE **training** regimes include an auxiliary load-balancing loss that nudges the gate toward a uniform token-to-expert distribution​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=training%20and%20inference,per%20batch%20number%20of%20experts%13) ​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=A%20capacity%20factor%20greater%20than,routed%20to%20an%20expert%205).
For instance, the Switch Transformer used a gating capacity factor (often 1.2× the average load) and would drop excess tokens if an expert’s capacity was exceeded​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=training%20and%20inference,capacity%20%3D%12%20tokens%20per%20batch) ​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=%C3%97%20capacity%20factor,routed%20to%20an%20expert%205).

In practice, with well-tuned capacity, only a small fraction of tokens (<1%) are dropped during training/inference in Switch Transformer​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=training%20and%20inference,capacity%20%3D%12%20tokens%20per%20batch) ​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=A%20capacity%20factor%20greater%20than,routed%20to%20an%20expert%205).
This ensures each expert processes roughly similar numbers of tokens, avoiding stragglers that slow down the batch.

Despite such measures, **runtime imbalances**  still occur.

The distribution of tokens at **inference can differ from training averages** – e.g. certain prompts might trigger one expert almost exclusively, violating load assumptions​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=such%20as%20offloading,of) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=are%20trained%20with%20a%20loss,memory%20errors%20and).
Empirical studies have observed that in a language model MoE, a few “hot” experts consistently receive a large share of tokens while many others get very few or even none for a given workload​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=PILE%20dataset%20,evaluate%20expert%20activation%20by%20performing) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=imbalanced%20load%20across%20experts,hotness%20levels%20vary%20across%20domains).
This activation imbalance was seen across domains: in one experiment, some experts were always heavily used for Wikipedia text but idle for code (and vice versa)​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=PILE%20dataset%20,evaluate%20expert%20activation%20by%20performing) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=experts,hotness%20levels%20vary%20across%20domains).
An imbalanced gate not only under-utilizes model capacity but can also cause **latency spikes**  if a single expert (and its host device) must handle a disproportionate amount of work.

**Proposed solutions**  to routing imbalance include **improved gating algorithms** and **coarse-grained routing**.

*Base Layers* (Lewis et al. 2021) formulated expert assignment as an optimal transport problem to ensure each expert gets an equal number of tokens​[PAPERS.NEURIPS.CC](https://papers.neurips.cc/paper_files/paper/2022/file/2f00ecd787b432c1d36f3de9800728eb-Paper-Conference.pdf#:~:text=%5BPDF%5D%20Mixture,Hash%20layers) ​[ARXIV.ORG](https://arxiv.org/pdf/2103.16716#:~:text=arXiv%20arxiv,frequent%20previous%20input%20token%20when), thereby achieving balanced routing without an auxiliary loss.

Alternatively, **hash-based routing**  replaces the learned gate with a deterministic hash function that assigns tokens to experts uniformly at random​[PROCEEDINGS.NEURIPS.CC](https://proceedings.neurips.cc/paper/2021/hash/92bf5e6240737e0326ea59846a83e076-Abstract.html#:~:text=Hash%20Layers%20For%20Large%20Sparse,Transformers%20and%20BASE%20Layers%2C) ​[OPENREVIEW.NET](https://openreview.net/pdf?id=MaYzugDmQV#:~:text=,such%20as%20BASE%20Layers).

This removes gating overhead and guarantees balance, though at a potential cost to quality (since routing is no longer learned).

On the industry side, Google explored **task-level routing**  to avoid token-level variance: Kudugunta et al. (2021) route entire sentences or tasks to fixed experts, effectively partitioning the model by task​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=prohibitively%20large%20and%20practitioners%20often,of%20the%20BLEU) ​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=routing%20%28task,MoE%20%2813B%20parameters%29%20performs).

In multilingual machine translation, this *task-MoE* approach allowed extracting a single-expert subnetwork per language pair for inference.

It preserved all the quality gains of the full MoE while improving **peak throughput by 1.9×**  over token-level MoE, since each inference uses only one expert with no on-the-fly switching​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=routing%20%28task,MoE%20%2813B%20parameters%29%20performs) ​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=outperforms%20the%20best%20performing%20token,6x) . 
The trade-off is reduced flexibility – expert allocation must be decided at the granularity of a sentence or user query – but it guarantees balanced loads and simpler scheduling.
More dynamically, researchers have proposed adjusting the gate *during inference* based on observed loads: one recent system tracks the running load on each expert/GPU and **redistributes some tokens to less-busy experts**  to avoid overloads​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=such%20as%20offloading,of) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=often%20differs%20from%20that%20during,memory%20errors%20and) . While this may deviate from the gate’s first choice, it can prevent worst-case slowdowns and out-of-memory errors.

> In summary, robust MoE inference requires not just a smart gate, but possibly a **capacity-aware router**  that ensures no expert becomes a throughput sink.

Another aspect of routing is the **overhead of the gating function itself**.

The gating computation and token-to-expert mapping add extra steps to each inference.

For large MoEs (e.g. hundreds of experts), computing the softmax scores and selecting top-$k$ experts for every token can be non-trivial.

More significantly, many frameworks historically implemented token routing with inefficient operations (e.g. constructing sparse masks or using scatter/gather on CPUs).

A recent analysis found that the gating operations and associated data shuffling were a **major contributor to MoE inference latency and memory use** ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=dimensions%E2%80%94token%20throughput%2C%20memory%20use%2C%20load,a%20new%20gating%20function%20that) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=components%20of%20the%20model%20architecture,reduces%20latency%20and%20memory%20use).

To address this, researchers introduced **Dynamic Gating**  that more tightly couples the routing decisions with the available capacity.

Instead of always padding each expert’s batch to a fixed size (which wastes computation on “empty” slots), dynamic gating only allocates exactly as many slots as needed per expert, reducing unnecessary computation on placeholder tokens​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=We%20address%20MoE%E2%80%99s%20inefficiency%20with,could%20be%20integrated%20with%20other) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=each%20expert%E2%80%99s%20computational%20capacity%20to,GPU%20kernels%20for%20even%20greater).

This optimization was shown to cut gating overhead significantly, enabling inference with larger batch sizes or fewer devices than before.

In essence, efficient MoE serving demands that the routing mechanism be **lightweight and balanced** , so that the cost of deciding “which expert?” does not outweigh the benefit of skipping computations.

## Latency and Parallelism Bottlenecks 

Serving a MoE model efficiently requires navigating a complex **parallelism hierarchy**.

In a multi-GPU or TPU setup, different experts reside on different devices (this distribution is necessary to fit the huge model in memory, as discussed later).

Thus, each inference step involves a pattern of communication and parallel computation: tokens assigned to various experts must be **sent to the appropriate devices, processed in parallel, and then the results merged**  in the original sequence order.

This all-to-all exchange is a key throughput limiter.

Lepikhin et al.’s GShard paper noted that while spreading experts across $N$ devices cuts compute per device, it incurs an all-to-all communication to route tokens and collect outputs​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=layers%20deploy%20many%20additional%20FFNs%2C,Experts%20Characterization) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=,and%20collecting%20results%20from%20experts).

For small batches or many experts, the communication overhead can dominate.

In fact, early MoE benchmarks attributed the **longer inference latency**  of MoEs primarily to the cost of frequent all-to-all communication between devices​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=communication%20is%20required%20when%20distributing,is%20set%20to%208%20for) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=dimensions%E2%80%94token%20throughput%2C%20memory%20use%2C%20load,a%20new%20gating%20function%20that) .

**Parallelism bottlenecks**  manifest especially when the batch or sequence length is not large enough to fully utilize all experts concurrently.

In an ideal scenario, if we have as many tokens as experts, each expert/device can handle one slice of the batch concurrently – achieving near-linear speedups.

This is the *best-case* view described by Microsoft’s DeepSpeed-MoE team: for a 52B MoE model with 128 experts of 1.3B each, each token only activates a 1.3B subnetwork, so distributing those experts across 128 GPUs means each device handles 1/128 of the model per token (1.3B parameters)​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=From%20the%20best,size%20is%2052%20billion%20parameters) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Image%3A%20Figure%203%3A%20Illustration%20of,expert%29%20of%20the%20model).

In theory, with perfect parallelization and no comm overhead, such a model could run **5× faster**  per token than a dense 6.7B model that offers similar quality​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=device%2C%205x%20smaller%20than%20its,discuss%20in%20the%20next%20section) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Therefore%2C%20in%20theory%2C%20an%20MoE,discuss%20in%20the%20next%20section).

In practice, however, achieving this requires careful coordination.

The *worst-case* is that a group of tokens collectively needs every expert, meaning the system ends up touching the full model parameters within a single inference step​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=For%20example%2C%20when%20inferencing%20with,size%20is%2052%20billion%20parameters) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=From%20the%20worst,short%20latency%20and%20high%20throughput).

In that regime, latency and throughput suffer.

Modern MoE serving systems therefore try to **“steer performance toward the best-case”**  via smart parallelism and scheduling​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Design%20goals%20for%20the%20DS,inference%20system) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=The%20design%20goal%20of%20our,is%20illustrated%20in%20Figure%203) . 

Microsoft’s DeepSpeed-MoE inference design, for example, combines **multiple parallelism strategies** : it uses *expert parallelism* (different GPUs host different experts and process different token groups concurrently) and traditional *tensor or data parallelism* for the non-expert layers​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=DS,coordinated%20optimizations) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=%2A%20Explore%20how%20DS,slicing%2C%20data%2C%20and%20expert%20parallelism).

By grouping tokens that share the same expert path on the same device, they minimize redundant weight access and maximize each GPU’s utilization​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=best,is%20illustrated%20in%20Figure%203) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=DS,coordinated%20optimizations).

This design effectively **minimizes each device’s critical path**, so that no single GPU has to load all experts’ weights.

Experiments showed that with expert parallelism equal to the number of experts (one expert per GPU), the sequential work per device is dramatically smaller than in an equivalent dense model – in their example, each GPU saw 5× fewer parameters per token than a dense baseline​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=device%2C%205x%20smaller%20than%20its,discuss%20in%20the%20next%20section) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Therefore%2C%20in%20theory%2C%20an%20MoE,discuss%20in%20the%20next%20section).

The remaining challenge is communication: naive all-to-all algorithms scale poorly as the number of devices grows, since latency increases linearly with devices in typical networks​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Expert%20parallelism%20requires%20all,point) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=To%20optimize%20this%2C%20we%20develop,layout%20transformations).

To tackle this, DeepSpeed-MoE introduced a **custom communication stack**  with optimizations like *hierarchical all-to-all*.

Instead of one global shuffle, they first perform intra-node exchange of tokens, then a smaller inter-node exchange​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Hierarchical%20All,message%20size%20is%20more%20latency) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Hierarchical%20All,message%20size%20is%20more%20latency) . This reduced communication hops from $O(p)$ to $O(G + p/G)$ (for $p$ total devices and $G$ devices per node)​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Hierarchical%20All,message%20size%20is%20more%20latency) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=process%20with%20a%20data,bound) , improving latency for small batches on large clusters.

They further coordinated communications for expert and data parallel steps to avoid redundant transfers​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=4%3A%20Illustration%20of%20the%20proposed,all%20design) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Parallelism%20Coordinated%20Communication%20Optimization%3A%20Combining,communication%20steps%20will%20be%20inefficient).

With these system optimizations, DeepSpeed reported up to **7.3× lower latency**  and **4.5× faster inference throughput**  for MoEs compared to prior systems​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=reduce%20MoE%20inference%20latency%20by,equivalent%20dense%20models) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=3,fast%20inference%20latencies)  – bringing MoE latency in line with (or better than) dense models of similar quality.

Other industry efforts likewise targeted the all-to-all bottleneck.

Microsoft’s open-source **Tutel**  library focuses on accelerating MoE inference/training on GPU clusters.

Tutel optimized low-level CUDA kernels and NCCL communication, achieving an **8.5× speedup**  for a single MoE layer on 8 GPUs, and about **40% end-to-end latency reduction**  for a 1.1-trillion-parameter MoE model spread over 512 GPUs​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/tutel-an-efficient-mixture-of-experts-implementation-for-large-dnn-model-training/#:~:text=execute%20MoE%20more%20easily%20and,1%20trillion%E2%80%93parameter%20MoE%20language%20model).

This was measured against the baseline MoE implementation in Meta’s Fairseq, highlighting how much overhead could be trimmed with custom all-to-all routines.

Google’s Switch Transformer team similarly noted that existing hardware and libraries were geared toward dense matrix ops, so they simplified MoE routing (using $k=1$ expert) to reduce overhead​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=Sparse%20training%20is%20an%20active,MoE) ​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=Mixture,communication%20costs%2C%20and%20training%20instabilities).

By doing so, they attained **7× faster pre-training**  than a dense T5-XXL with the same compute budget​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=%E2%80%A2%20The%20Switch%20Transformer%20architecture%2C,hold%20even%20with%20limited%20computational) ​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=%E2%80%A2%20Scaling%20properties%20and%20a,hold%20even%20with%20limited%20computational)  – and also found that even with as few as 2 experts (on 2 cores) the MoE provided speedups​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=we%20also%20show%20that%20the,which%20simplifies%20and%20improves%20over) ​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=but%20is%20beneficial%20even%20with,which%20simplifies%20and%20improves%20over).

This implies MoEs can benefit even “small” deployments if the parallelism is carefully exploited.

Despite these advances, a fundamental issue remains: MoE inference efficiency tends to scale well with **throughput (batch size or concurrent requests)**  but not as well with **single-input latency**.

If only one sequence is processed at a time (e.g. an interactive user query), many experts will be idle.

In a Switch-style MoE, each token is sent to only one expert – so out of (say) 64 experts, 63 do nothing for that token.

The next token might hit a different expert, but *sequentially* you’re not utilizing the full model’s parallel potential.

This is one reason MoE inference can have inconsistent latency.

In fact, DeepSpeed explicitly notes that **inference usually runs at small batch sizes** , meaning the time to load model weights from memory often dominates, and **lesser compute** doesn’t automatically translate to faster inference​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Optimizing%20for%20MoE%20inference%20latency,the%20overall%20achievable%20memory%20bandwidth) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=inference%20latency%20of%20an%20MoE,the%20overall%20achievable%20memory%20bandwidth).

Systems mitigate this by packing multiple requests together or running large batches of tokens – but that adds latency or requires enough traffic to batch.

In real-time streaming, one approach is to use **multithreading or coroutines**  to serve many generation streams on the MoE model simultaneously, effectively faking a larger batch and keeping all experts busy.

Another approach is the coarse routing mentioned earlier: if a whole session or task can be pinned to a subset of experts, the model effectively reduces to a smaller dense model for that session, avoiding frequent cross-expert communication. This was shown to **improve throughput 2–3×**  in multilingual MoE inference​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=routing%20%28task,MoE%20%2813B%20parameters%29%20performs) ​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=outperforms%20the%20best%20performing%20token,while%20improving%20the%20peak%20inference) . The trade-off is that it forgoes token-level adaptability in favor of steadier, more predictable usage of experts.

In summary, **latency optimization**  for MoEs revolves around using parallelism to the fullest and minimizing coordination costs.

Techniques like expert parallelism, optimized collectives, and request batching ensure that the sparse activations translate into actual wall-clock speedups.

With these in place, MoE models can serve significantly more tokens per second – even outperforming dense models of comparable quality on throughput​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=reduce%20MoE%20inference%20latency%20by,equivalent%20dense%20models) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=equivalent%20dense%20models%3A).

But without such optimizations, an MoE can be **slower**  than a dense model, as the overhead of many small computations and communications overwhelms the FLOP savings​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=dimensions%E2%80%94token%20throughput%2C%20memory%20use%2C%20load,a%20new%20gating%20function%20that) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=components%20of%20the%20model%20architecture,reduces%20latency%20and%20memory%20use).

Achieving low **tail latency**  for single examples remains challenging, requiring careful scheduling or fallback to distilled models when ultra-low latency is needed.

## Memory Usage and Bandwidth Constraints 

MoE models trade compute for size – they introduce a huge number of parameters (the experts) that must be stored and accessed efficiently at inference.

A standard Transformer might have billions of parameters, whereas MoE models can scale to **trillions of parameters**  by replicating FFN layers into many experts. For example, Google’s Switch-C Transformer reached 1.6 trillion parameters with 2048 experts​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,Experts%20%28Nov) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,Feb%202022) , and Alibaba’s M6-T model reportedly pushed to 10 trillion parameters using MoE techniques​[ALIBABACLOUD.COM](https://www.alibabacloud.com/blog/cloud-future-new-possibilities-green-ubiquitous-trusted-computing_598835#:~:text=released%20Alibaba%20DAMO%20Academy%20ultra,commerce%2C%20clothing) ​[ALIBABACLOUD.COM](https://www.alibabacloud.com/blog/cloud-future-new-possibilities-green-ubiquitous-trusted-computing_598835#:~:text=M6%20finally%20realized%20that%20it,and%20the%20green%20AI%20is).

In inference, even though only a fraction of those weights are used per token, **all experts generally need to be loaded in memory**  so that any token can be routed to the appropriate ones​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,tuning%20is%20promising) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,tuning%20is%20promising).

This creates a **memory footprint**  far larger than dense models.

As the Hugging Face team succinctly put it, MoEs *“require high VRAM as all experts are loaded in memory”*​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,tuning%20is%20promising) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,tuning%20is%20promising) . For instance, an 8×7B MoE (Mistral’s Mixtral model) totals 47B parameters loaded (~8× more than a single expert) to allow any combination of 2 experts to be active​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=match%20at%20L166%20Mixtral%208x7B%2C,just%20two%20experts%20are%20being) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,in%20the%20announcement%20blog%20post).

Riquelme et al. (2021) noted a similar overhead in V-MoE vision models: the sparse model needed additional memory compared to a dense one of similar throughput, due to storing many experts that are each rarely used​[ARXIV.ORG](https://arxiv.org/abs/2106.05974#:~:text=processed%20by%20every%20parameter,on) ​[ARXIV.ORG](https://arxiv.org/abs/2106.05974#:~:text=dense%20networks,on%20ImageNet) .


Beyond just capacity, **memory bandwidth**  becomes a limiting factor.

In modern accelerators, reading weights from GPU memory (or across a network) can bottleneck throughput if the working set is large.

DeepSpeed’s developers point out that in MoE inference, the time to *load* model parameters from memory often outweighs arithmetic costs​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Optimizing%20for%20MoE%20inference%20latency,the%20overall%20achievable%20memory%20bandwidth) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=inference%20latency%20of%20an%20MoE,the%20overall%20achievable%20memory%20bandwidth).

They identify **overall model size and achievable memory bandwidth**  as the main determinants of MoE latency, rather than FLOPs​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Optimizing%20for%20MoE%20inference%20latency,the%20overall%20achievable%20memory%20bandwidth) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=that%20lesser%20compute%20should%20lead,the%20overall%20achievable%20memory%20bandwidth).

In their analysis, a MoE’s “critical path” per token is limited to the few experts selected, but the **aggregate**  parameters needed for a batch could be as large as the entire model​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=From%20the%20best,size%20is%2052%20billion%20parameters) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=parameters) . This means if tokens all go to different experts, the system might still have to touch most of the 52B weights (in their example) within a short time, stressing memory bandwidth. Indeed, Huang et al. (2023) found that a dense model requiring X GB of memory per GPU might require **3×–9× more memory**  when converted to a sparsely-activated MoE of similar accuracy​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=PILE%20dataset%20,evaluate%20expert%20activation%20by%20performing) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=imbalanced%20load%20across%20experts,200%20%5B9%5D.%20Fig) . They measured an 8.6× increase in memory usage for a language model MoE versus a dense model (2.2 GB -> 18.9 GB per GPU), due to both the larger parameter set and extra activation buffers for gating​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=PILE%20dataset%20,evaluate%20expert%20activation%20by%20performing) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=imbalanced%20load%20across%20experts,inputs%20show%20sparse%20expert%20activations) . Such demands can easily exhaust device memory or lead to frequent swapping of weights from host memory, which kills inference performance.
To manage this, **model sharding**  is used extensively. GShard (Lepikhin et al. 2020) demonstrated that distributing experts across devices allows the MoE’s memory load to be split, albeit at the cost of communication​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=computation%20but%20more%20memory%20usage,all%20communication%20is%20required) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=layers%20deploy%20many%20additional%20FFNs%2C,and%20collecting%20results%20from%20experts) . Typically, each GPU holds a subset of the experts’ weights and a full copy of the shared weights (e.g. attention layers, embeddings)​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=computation%20but%20more%20memory%20usage,all%20communication%20is%20required) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=layers%20deploy%20many%20additional%20FFNs%2C,and%20collecting%20results%20from%20experts) . This way, no single GPU needs to store all $E$ experts, reducing per-device memory. For example, if you have 8 GPUs and 64 experts, each GPU might store 8 experts. The trade-off is that when an input token needs an expert not on its current GPU, the token’s data must be sent to the GPU owning that expert (as described in the previous section). Still, for very large MoEs, sharding is the only way to even fit the model in a cluster’s combined memory. Meta’s 1.1T-parameter MoE in Fairseq was spread across 64+ GPUs, and Microsoft noted that **each GPU then only loads ~1/64 of the model weights** , enabling the model to be served at all​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=device%2C%205x%20smaller%20than%20its,discuss%20in%20the%20next%20section) .
Even with sharding, the **total memory footprint**  can be problematic, especially if multiple models or other workloads share the hardware. One recent idea to address this is **expert buffering**  or on-demand loading. Huang et al. implemented a mechanism that keeps only a small number of “hot” experts in GPU memory and leaves the rest in CPU memory, swapping them in when needed​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=Furthermore%2C%20we%20develop%20optimizations%20that,is%20orthogonal%20to%20existing%20memory) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=Expert%20Buffering%20exploits%20high%20temporal,to%20existing%20memory%20management%20techniques) . They observed that MoE workloads often have **temporal locality**  – the same expert tends to be reused for several batches in a row, especially in translation or sequential tasks​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=Machine%20Translation%20%28MT%29,several%20consecutive%20batch%2C%20then%20go) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=to%20French%2C%20Japanese%2C%20and%20Austrian,several%20consecutive%20batch%2C%20then%20go) . Exploiting this, their system could significantly cut GPU memory usage (by 5× or more) with minimal performance loss, by dynamically loading experts that haven’t been used recently from CPU to GPU just-in-time​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=Expert%20Buffering%20exploits%20high%20temporal,mitigates%20severe%20load%20imbalance%20across) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=Expert%20Buffering%20exploits%20high%20temporal,is%20orthogonal%20to%20existing%20memory) . This is somewhat analogous to a cache for expert weights. Of course, it relies on a fast interconnect and careful pre-fetching to not stall the pipeline. Another approach is **model compression or merging** : for serving, one can reduce MoE model size via weight consolidation. Researchers have experimented with merging the weights of multiple experts into a single expert or smaller set (“aggregation of experts”) to shrink the model for inference​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,of%20parameters%20at%20inference%20time) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,of%20parameters%20at%20inference%20time) . This essentially turns a sparse model into a semi-dense one by averaging or otherwise combining experts, attempting to preserve performance while cutting memory needs. Alternatively, **distillation**  can be used: train the MoE for quality, then distill its knowledge into a smaller dense model for deployment​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,of%20parameters%20at%20inference%20time) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,of%20parameters%20at%20inference%20time) . The Switch Transformer authors showed that by distilling a 7B MoE into a 1.6B dense model, they retained ~30–40% of the MoE’s quality gain over a baseline​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,of%20parameters%20at%20inference%20time) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,of%20parameters%20at%20inference%20time) . In other words, a distilled model can outperform a same-size model trained from scratch, thanks to the MoE teacher, and it is much easier to serve (no multi-expert routing). Companies like Google and OpenAI often use distillation or weight pruning on giant research models to get something more tractable for production. That said, distillation sacrifices some peak quality – for example, the task-MoE approach by Kudugunta et al. preserved *100%* of the MoE performance in a slimmed model, whereas distilling their token-level MoE to a dense model kept only 32% of the BLEU improvement​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=routing%20%28task,MoE%20%2813B%20parameters%29%20performs) ​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=outperforms%20the%20best%20performing%20token,all%20the%20gains%20with%20the) . Thus, there is interest in **directly optimizing MoE memory efficiency**  so the full sparse model can be used in production.
One insight from recent work is that a lot of memory waste in MoEs comes from **padding and under-utilized capacity** . Because frameworks often allocate a fixed-size buffer for each expert’s input (equal to the capacity factor times batch size), an expert that only gets a few tokens will still consume memory as if it got the maximum. Huang et al. quantified this waste: in their tests, static gating with a generous capacity led to *12.8×* more allocated slots than actual tokens for LM inference in the worst case​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=Under%20the%20baseline%20MoE%20design%2C,1%20shows%20a%20highly) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=number%20of%20tokens%20actually%20assigned,a%20large%20share%20of%20tokens) . Techniques like the aforementioned dynamic gating and custom kernels aim to eliminate this padding. The MegaBlocks system from Stanford reformulated MoE computations as **block-sparse matrix ops** , allowing irregular batch sizes per expert to be handled without padding​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=Megablocks%20,that%20can%20accommodate%20imbalanced%20assignment) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=drops%20tokens%20and%20maps%20efficiently,that%20can%20accommodate%20imbalanced%20assignment) . By treating the expert weight matrix as blocks and only computing the blocks needed for the non-zero inputs, they could “never drop tokens” and efficiently handle imbalanced loads​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=Megablocks%20,that%20can%20accommodate%20imbalanced%20assignment) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=drops%20tokens%20and%20maps%20efficiently,that%20can%20accommodate%20imbalanced%20assignment) . This improved hardware utilization and cut memory overhead, since an expert with 3 tokens would only produce 3 token’s worth of intermediate activations instead of, say, 32. In fact, MegaBlocks reported **significant speedups and memory savings**  on GPU hardware by this method​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=gate%20that%20picks%20experts%20based,leading%20to%20a%2017x%20speedup) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=Megablocks%20,that%20can%20accommodate%20imbalanced%20assignment) . Such custom kernels and memory optimizations are crucial for MoE inference on GPUs, which otherwise might waste VRAM on empty activations.
In summary, MoE inference is **memory-intensive** : the model’s many experts must be stored and accessed, and naive implementations incur extra memory overhead from padding and duplicated parameters. Solutions like expert sharding, intelligent weight offloading (keeping only active experts on device), and reducing padding waste are key to making MoEs fit in available memory and bandwidth budgets. Furthermore, utilizing the **aggregate memory bandwidth**  of multiple devices is critical – DeepSpeed’s design explicitly achieved speedups by leveraging *distributed* GPU memory to increase effective bandwidth​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=In%20the%20previous%20section%2C%20we,equivalent%20dense%20models) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=In%20the%20previous%20section%2C%20we,equivalent%20dense%20models) . With enough GPUs each handling a slice of the weights, the model can effectively stream parameters from many memory pools in parallel, overcoming the bottleneck of a single memory channel. This approach enabled serving models that would otherwise be “bandwidth-bound.” For instance, Alibaba’s 10-trillion M6 model could only be trained/inferred by splitting it across 512 GPUs, effectively dividing the enormous parameter matrix into manageable chunks​[ALIBABACLOUD.COM](https://www.alibabacloud.com/blog/cloud-future-new-possibilities-green-ubiquitous-trusted-computing_598835#:~:text=M6%20finally%20realized%20that%20it,and%20the%20green%20AI%20is) . The bottom line is that to deploy MoEs at scale, one must treat **memory as a first-class constraint**  – optimize what is stored, where it’s stored, and how fast it can be transferred to compute.

## Hardware Compatibility (TPUs, GPUs, and Systems) 

The challenges above often interplay with the specifics of the hardware platform. Early MoE successes came on Google’s TPU clusters, which provided high-bandwidth interconnects and specialized support for collective operations. The **TPU software stack (XLA)**  also required static shapes for all operations. As a result, Google’s MoE implementations (e.g. GShard, Switch Transformer) had to make the routing computation *statically* allocatable – hence the use of a fixed expert capacity and dropping overflow tokens to maintain a consistent tensor shape​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=deter%02mined%20at%20compilation%20time%2C%20but,expanding%20by%20a%20capacity%20factor) ​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=training%20and%20inference,capacity%20%3D%12%20tokens%20per%20batch) . This static allocation simplified compilation on TPU but introduced the padding inefficiencies discussed. Meanwhile, GPUs offer more flexible programming but traditionally lacked built-in support for dynamic sparse computation. Off-the-shelf deep learning libraries in 2020–2021 were heavily optimized for dense matrix multiplication and **did not cater to dynamic sparsity** ​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=Sparse%20training%20is%20an%20active,MoE) ​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=et%20al,MoE) . Running MoEs on GPU efficiently therefore required considerable engineering. Facebook (Meta) researchers integrated MoE layers into PyTorch/Fairseq in 2021, but they observed suboptimal scaling until improvements like Tutel came along​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/tutel-an-efficient-mixture-of-experts-implementation-for-large-dnn-model-training/#:~:text=execute%20MoE%20more%20easily%20and,64%20NDm%20A100%20v4%20nodes) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/tutel-an-efficient-mixture-of-experts-implementation-for-large-dnn-model-training/#:~:text=an%208,1%20trillion%E2%80%93parameter%20MoE%20language%20model) . The default NCCL all-to-all, for instance, had high latency beyond a single node​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Expert%20parallelism%20requires%20all,challenge%2C%20we%20design%20two%20new) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Expert%20parallelism%20requires%20all,increases%20linearly%20with%20the%20increase) , prompting custom libraries (Microsoft’s MSCCL and SCCL) to implement faster patterns​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=custom%20collective%20communication%20algorithms%20for,accelerators%20supported%20by%20Microsoft%20Azure) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Expert%20parallelism%20requires%20all,and%20achieve%20better%20performance%20than) .
**Hardware topology**  also affects MoE performance. On a TPU pod, each TPU core has direct high-speed links to others in a 2D torus. This makes all-to-all communication relatively efficient up to a certain scale, and Google’s MoE models often scaled to thousands of TPU cores (Switch Transformer used 2048 TPU v3 cores for its largest model​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,Feb) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,Feb%202022) ). On GPU clusters, inter-node communication can be more of a bottleneck (limited by InfiniBand or Ethernet speeds). The DeepSpeed team explicitly addressed this by splitting communication into intra-node (on NVLink/NVSwitch) and inter-node phases​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Hierarchical%20All,message%20size%20is%20more%20latency) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=process%20with%20a%20data,bound) . They found that at small message sizes (typical when splitting a batch of tokens), latency dominates bandwidth, so reducing the number of network hops was vital​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=all,bound) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=all,message%20size%20is%20more%20latency) . This is a reminder that a MoE that is efficient on one hardware setup might not be on another. In fact, Fedus et al. noted that their Switch Transformer was developed with TPUs in mind, but that MoEs “should be similarly trained on GPU clusters” with the right mapping of experts to devices​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=models%20have%20had%20notable%20successes,three%20regimes%20in%20NLP%3A%20pre) ​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=models%20have%20had%20notable%20successes,three%20regimes%20in%20NLP%3A%20pre) . Ensuring *hardware compatibility* often means adapting the parallelism strategy: e.g., using more intra-GPU parallel threads for small expert computations, or merging multiple small matrix ops to better utilize GPU cores (an optimization Tutel performs).
Another compatibility issue is support for **fast dispatching of variable workloads** . Each token in an MoE may go to a different expert, leading to a scatter of work. Traditional GPU execution likes large, uniform batches for efficiency. If each expert only gets a handful of tokens, the GPU kernel launch overhead for each small matmul can hurt performance. Libraries like Tutel addressed this by fusing the computation of all experts on a device into one kernel where possible, to amortize overhead​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/tutel-an-efficient-mixture-of-experts-implementation-for-large-dnn-model-training/#:~:text=execute%20MoE%20more%20easily%20and,64%20NDm%20A100%20v4%20nodes) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/tutel-an-efficient-mixture-of-experts-implementation-for-large-dnn-model-training/#:~:text=match%20at%20L257%20layer%2C%20Tutel,all%20bandwidth%20for) . Similarly, the MegaBlocks approach of using block-sparse operations is about aligning the computation with GPU-friendly primitives​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=Megablocks%20,that%20can%20accommodate%20imbalanced%20assignment) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=drops%20tokens%20and%20maps%20efficiently,that%20can%20accommodate%20imbalanced%20assignment) . As hardware evolves, we may see more native support for sparsity (for example, NVIDIA’s Hopper GPUs introduced a form of dynamic programming sparsity, though not specifically for MoEs). **Custom ASICs**  or future accelerators might even include routing logic on-chip to eliminate the general-purpose overhead of gating.
One more aspect is that MoE models are **complex distributed systems** . Deploying them requires a sophisticated runtime to handle networking, memory management, and load balancing. This blurs the line between model design and system design. Google’s “Pathways” system (2022) was an effort to build a general infrastructure for expert models that route data across multiple model pieces. While details are scarce, the goal was to seamlessly support models that conditionally use different subnetworks – essentially MoEs on a larger scale. Nvidia and Microsoft have also worked on inference managers that can host massive models across clusters (e.g. NVIDIA’s Trinity or Microsoft’s ZeRO-Inference​[MUG.MVAPICH.CSE.OHIO-STATE.EDU](https://mug.mvapich.cse.ohio-state.edu/static/media/mug/presentations/24/2nd-day/7%20-%20DeepSpeed%20and%20Trillion-parameter%20LLMs%20Can%20synergy%20of%20MPI%20and%20NCCL%20improve%20scalability%20and%20efficiency.pdf#:~:text=%5BPDF%5D%20DeepSpeed%20and%20Trillion,GPU%20device) ​[DEEPSPEED.AI](https://www.deepspeed.ai/2022/09/09/zero-inference.html#:~:text=ZeRO,few%20as%20a%20single%20GPU) ). These frameworks must be **hardware-aware** : they might schedule MoE computations to minimize data transfer (e.g. by preferring local experts) or dynamically adjust parallelism depending on available resources.
In practice, **TPUs excel at large all-to-all**  due to their high-bandwidth mesh, whereas GPUs (prior to NVSwitch/H100 advances) needed more tuning. But recent results show GPUs can handle trillion-parameter MoEs efficiently with the right software. DeepSpeed-MoE demonstrated **25 ms inference latency**  for a 1.5 trillion param MoE across hundreds of GPUs​[ARXIV.ORG](https://arxiv.org/pdf/2201.05596#:~:text=arXiv%20arxiv,5x%20faster%20and%209x) ​[PROCEEDINGS.MLR.PRESS](https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf#:~:text=We%20develop%20DeepSpeed,workloads%20on%20hundreds%20of%20GPUs)  – an impressive feat made possible by co-optimizing the model and hardware usage. Meanwhile, on TPUs, Google reported strong scaling of Switch Transformer; they even scaled down to smaller TPU slices to show MoE benefits “with only a few cores”​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=we%20also%20show%20that%20the,which%20simplifies%20and%20improves%20over) ​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=we%20also%20show%20that%20the,which%20simplifies%20and%20improves%20over) , highlighting that the approach wasn’t limited to enormous pods.
A final consideration is **hardware-specific limits**  like memory per core. TPU v3 has 16 GB per chip, which constrained the size of experts that could be hosted per core. GLaM (1.2T) and Switch (1.6T) both navigated this by using many experts with relatively smaller size (e.g. 64 experts each of ~/≍ 10B parameters across many cores). GPU A100s offer 40–80 GB, allowing larger experts per device, but then fewer devices (which can increase communication per token). These trade-offs mean the optimal MoE configuration (number of experts, expert size, distribution) can be different between hardware platforms. **Software libraries**  have begun to abstract this: for example, DeepSpeed’s MoE support can automatically partition experts across GPUs and even offload some layers to CPU if needed​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=Expert%20Buffering%20exploits%20high%20temporal,mitigates%20severe%20load%20imbalance%20across) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=Expert%20Buffering%20exploits%20high%20temporal,is%20orthogonal%20to%20existing%20memory) . The goal is to make MoE deployment “hardware-agnostic,” but in reality, achieving peak performance still requires tuning to the strengths of the hardware (e.g. using tensor parallelism on GPU for big matmuls vs. relying on TPU systolic array utilization, etc.).
In summary, MoE inference pushes the envelope of what our hardware and frameworks were initially designed for. Over the past few years, a synergy of model research and system engineering has emerged: new **routing algorithms**  are designed with hardware limitations in mind (e.g. favor local experts to reduce network traffic​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=FasterMoE%20,leading%20to%20a%2017x%20speedup) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=strategies%2C%20as%20well%20as%20techniques,leading%20to%20a%2017x%20speedup) ), and systems are built to better accommodate dynamic, sparse computation. Ensuring that an MoE model runs efficiently on a given platform often means **bridging the gap**  between model parallelism and the hardware’s communication topology. The advancements from projects like GShard, Tutel, and DeepSpeed-MoE reflect a broad consensus that to serve giant MoEs at low latency, one must co-optimize the model and the hardware execution plan.

## Expert Sparsity and Activation Imbalance 

MoE models are deliberately **sparse**  – at any given layer, only a small fraction of the model’s neurons “activate” for an input token. This sparsity is what gives MoEs their compute advantage, but it also leads to uneven utilization of the model’s capacity. We touched on load imbalance earlier from a performance perspective; here we consider the implications of expert sparsity on model behavior and inference outcomes. In an ideal scenario, each expert in an MoE would specialize in different aspects of language and be utilized regularly when those aspects are present in the input. In practice, MoEs often develop **skewed specializations** : some experts become “generalists” that fire on a wide range of tokens, while others become niche experts that rarely activate. For example, the ST-MoE model analysis found that certain experts specialized in punctuation or rare words​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=Router%20z,which%20can%20be%20quite%20impactful) . If the input doesn’t contain those patterns, those experts stay dormant. Huang et al. (2023) confirmed this phenomenon at scale: in their MoE language model, they observed **multiple hot experts**  that handled a large portion of tokens consistently, whereas many other experts received only a trickle of tokens or none at all​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=PILE%20dataset%20,evaluate%20expert%20activation%20by%20performing) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=imbalanced%20load%20across%20experts,While%20all%20inputs%20show) . Importantly, which experts are hot can vary by domain (e.g. some experts are hot for programming text, others for Wikipedia), but within a given domain or task, the imbalance persists​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=PILE%20dataset%20,evaluate%20expert%20activation%20by%20performing) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=experts,hotness%20levels%20vary%20across%20domains) .
This **activation imbalance**  means a lot of parameters are effectively under-utilized during inference. If 2–3 experts handle, say, 50% of all tokens, then the remaining experts contribute little to the throughput or real-time decision-making. They may hold knowledge that’s rarely needed, which is inefficient from a deployment standpoint – we’re dedicating memory and maintenance to experts that rarely contribute. Moreover, when an expert is rarely used, its weights might not be well-fine-tuned (if the distribution shift from training causes it to activate less, it may perform suboptimally the few times it is chosen). On the flip side, the over-used experts can become points of **contention**  as described, and also single points of failure if they saturate.
Researchers have proposed various strategies to handle this sparsity/imbalance issue. One concept is **expert rejuvenation or pruning** : periodically identify experts that receive almost no traffic and either retrain them on new data or remove/merge them. This is analogous to pruning neurons in a dense model that never fire. There has been less published on pruning experts post-training, but it’s a plausible avenue for model compression – essentially, if an expert is never selected, one could drop it to save memory (with a fallback that the gating network might choose the next best expert instead). Another strategy, as mentioned, is **dynamic load balancing at inference** ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=such%20as%20offloading,of) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=are%20trained%20with%20a%20loss,memory%20errors%20and) . This doesn’t eliminate sparsity but ensures it doesn’t harm performance: if one expert is “hotter” than others in a particular batch, we can spread some of its load to other experts. This approach was shown to improve system robustness significantly​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=often%20differs%20from%20that%20during,LM%29%20and%20machine%20translation) . It effectively reduces activation imbalance *per batch* even if the model’s inherent gating is imbalanced. However, it does change the model’s computations slightly (some tokens go to a second-choice expert), which could affect output quality marginally. The authors reported that this technique avoids OOM errors and keeps latency manageable in pathological cases​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=often%20differs%20from%20that%20during,LM%29%20and%20machine%20translation) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=loads%20,memory%20errors%20and) .
An intriguing finding by Google researchers was that **expert utilization can drop during fine-tuning**  if the load-balancing loss is turned off​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=One%20question%20is%20whether%20to,regularization%20that%20helps%20prevent%20overfitting) . They found that even when up to 11% of tokens were dropped (due to capacity limits) during fine-tuning, the model’s quality wasn’t significantly hurt – suggesting that many experts were not critical for those tokens​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=One%20question%20is%20whether%20to,regularization%20that%20helps%20prevent%20overfitting) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=not%20significantly%20impacted%2C%20even%20when,regularization%20that%20helps%20prevent%20overfitting) . This hints that there is redundancy in expert knowledge: some tokens can afford to skip their top expert and still be processed well by another. That redundancy might be exploited at inference: one could imagine an MoE serving system that, under heavy load, temporarily routes tokens to fewer experts to save compute, relying on the redundancy to maintain output quality. This would be a form of **graceful degradation**  using sparsity.
The **sparsity itself**  (using only a few experts per token) also has implications for output variance. If an MoE uses $k=2$ experts with weighted combination, it is essentially ensembling two expert opinions per token. Many MoEs (like GLaM) do this, which can stabilize outputs at the cost of more compute​[AR5IV.ORG](https://ar5iv.org/pdf/2112.06905#:~:text=match%20at%20L237%20best%20two,of%20one%20in%20the%20classic) ​[AR5IV.ORG](https://ar5iv.org/pdf/2112.06905#:~:text=best%20two%20experts%20for%20each,of%20one%20in%20the%20classic) . Switch Transformer opted for $k=1$ (only one expert per token) to simplify and speed up inference​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=Mixture,communication%20costs%2C%20and%20training%20instabilities) . This increases sparsity but means each token’s representation is determined by a single expert’s weights. That can lead to more variance in token outputs – if an expert has a peculiar behavior, the token’s output will reflect it strongly. In critical applications, one might prefer $k=2$ routing despite the extra cost, for a form of local ensemble averaging. In fact, some recent MoE variants allow adjusting $k$ at inference (trading off quality vs. speed)​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=Capacity%20Factor%20and%20communication%20costs) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=Increasing%20the%20capacity%20factor%20,be%20changed%20to%20reduce%20compute) . For example, one can train with $k=2$ but use only $k=1$ for faster inference, accepting a small quality drop​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=Capacity%20Factor%20and%20communication%20costs) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=Increasing%20the%20capacity%20factor%20,be%20changed%20to%20reduce%20compute) . This is another way to use sparsity as a tunable knob at runtime.
Finally, **knowledge concentration**  in experts has pros and cons for inference. On one hand, having experts specialize (e.g. an expert for legal text, one for code, one for conversation) is useful – it means when such content appears, that expert can handle it well. Indeed, MoEs are partly motivated by the idea of capturing diverse patterns or modalities in different experts. On the other hand, if a specialization is too narrow, the expert might remain idle most of the time. Meta’s team encountered this in their multilingual MoE for No Language Left Behind: some language experts would be mostly unused unless that language was being translated. Their solution was task-based routing as discussed, or training the gating to be language-aware​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=prohibitively%20large%20and%20practitioners%20often,of%20the%20BLEU) . In general, there’s a design choice: **should experts be purely data-driven in what they specialize in, or explicitly structured (by language, by topic, etc.)?**  If structured, one can ensure each expert has a role and gets used when that role is needed (like a module). This can avoid wasted experts but requires knowing the axes of specialization ahead of time. Data-driven specializations may yield unexpected or overlapping expert roles, some of which might not be worth the cost at inference.
In summary, the sparse activation nature of MoEs yields **uneven expert usage** , which raises the question of how to maximize the value of all those parameters. Techniques to balance expert load, both at training (auxiliary losses) and at inference (dynamic token redistribution), have been successful in preventing any expert from becoming a throughput liability​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=training%20and%20inference,per%20batch%20number%20of%20experts%13) ​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=A%20capacity%20factor%20greater%20than,routed%20to%20an%20expert%205) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=such%20as%20offloading,of) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=often%20differs%20from%20that%20during,memory%20errors%20and) . Meanwhile, acknowledging that some experts will inevitably be lightly used, researchers are exploring ways to compress or remove them without losing much model capacity. As MoEs enter production settings, we may see more adaptive inference mechanisms – for instance, a system might monitor expert utilization over time and unload an expert from GPU if it hasn’t been used for a while (similar to expert buffering). The **sparsity that is a blessing for compute is a curse for utilization** , but through careful design, MoEs can ensure that even “cold” experts have their moment to shine when needed, and do not impede the model when they are not.

## Real-Time and Streaming Inference 

One of the ultimate goals for deploying large language models is to serve interactive applications – e.g. chatbots, real-time translation, or personalized assistants – with low latency per user. MoE models pose unique challenges in these scenarios. As discussed, MoEs thrive on throughput: the hardware can be kept busy if there are many tokens to process in parallel. But in a **streaming inference**  setting (generating one token at a time for a user, then immediately using it before generating the next), we often operate at batch size 1. This emphasizes the *per-token overheads* of MoEs. If a model is outputting text token by token, each step involves running the MoE layer routing. The overhead of gating and cross-device communication on every generation step can introduce noticeable latency. For example, if it takes a dense model 50 ms to generate a token and a MoE model 150 ms (due to extra coordination), the user will experience slower response despite the MoE’s theoretical efficiency. In fact, without optimizations, MoE layers can make **auto-regressive generation**  significantly slower per token than a dense model. This was highlighted by Nvidia in the context of small batches: they note that sequential token generation limits throughput because you cannot parallelize the timeline of generation​[DEVELOPER.NVIDIA.COM](https://developer.nvidia.com/blog/low-latency-inference-chapter-1-up-to-1-9x-higher-llama-3-1-performance-with-medusa-on-nvidia-hgx-h200-with-nvlink-switch/#:~:text=...%20developer.nvidia.com%20%20Transformer,per%20generation%20step%20to) . In MoEs, this is exacerbated by the fact that **not all GPU cores are used for each token**  (since only some experts run). Essentially, MoEs trade off some single-stream speed for multi-stream scalability.

There are a few strategies to handle real-time usage of MoEs:

 
- **Concurrent streams** : Run multiple independent sequences through the MoE model in interleaved fashion. This way, while one sequence is waiting for its next token’s computation, another sequence’s token can use other experts. If done cleverly, this can fill the gaps and use all experts efficiently. Many inference servers do this by batching requests that arrive within a short window. For MoEs, you might maintain a rolling batch of tokens from different users. The DeepSpeed-MoE system explicitly allows combining data parallelism (different inputs on different devices) with expert-slicing​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Data%20parallelism%20and%20Tensor,or%20reduction%20in%20compute%20granularity) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=GPU%20memory%20bandwidth%20of%20all,or%20reduction%20in%20compute%20granularity) , which could be leveraged to serve multiple requests at once. The downside is **added latency variance**  – a user’s query might wait a few extra milliseconds to be grouped with another’s. There’s a trade-off between latency and throughput here.
 
- **Quality-vs-latency tuning** : As mentioned, one can reduce the number of experts used at inference to speed up each token, at some cost to output quality​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=Capacity%20Factor%20and%20communication%20costs) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=Increasing%20the%20capacity%20factor%20,be%20changed%20to%20reduce%20compute) . For example, switching from top-2 to top-1 gating at generation time roughly halves the expert compute and communication. If the model was trained with $k=2$, this might degrade quality slightly, but perhaps not dramatically (especially if the gating network often had one dominant expert anyway). This is a way to adapt a MoE for faster interactive use – essentially operating it in a more sparse mode when needed.
 
- **Distillation fallback** : A practical approach used in industry is to maintain a smaller dense model as a fallback for real-time. The MoE might be used offline or for batch tasks where latency is less critical, whereas a distilled version serves the online traffic. Google’s early experiments with Switch Transformer indeed suggested distilling the MoE into a dense model for production serving​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,of%20parameters%20at%20inference%20time) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,of%20parameters%20at%20inference%20time) . They managed to compress about 30–40% of the MoE’s performance gain into the dense model. For streaming applications, that dense model can generate faster and with more predictable timing. The trade-off is losing some of the MoE’s peak accuracy.
 
- **Streaming-friendly routing** : Research is underway on making routing algorithms more cognizant of temporal context. For instance, one could enforce that once a conversation is in progress, the gating tries to reuse the same experts it used for previous tokens (assuming topic/linguistic context persists). This could improve locality and perhaps allow caching of those experts on certain devices (reducing repetitive loading). It’s a bit speculative, but one could imagine a gating that says “for this dialogue turn, stick to expert 5 and 9 unless a very out-of-context token appears.” This starts to resemble the task-level routing, but at a finer session granularity. The benefit would be smoother latency (since subsequent tokens hit the same devices) and possible reuse of intermediate states.
 
- **Real-time load balancing** : In an interactive system, load can fluctuate (many users at peak hours, few at others). MoEs can actually shine in high-concurrency situations by scaling gracefully with load – more incoming tokens naturally utilize more experts, spreading out work. But in low-load conditions, the cluster running a MoE might be underutilized. One solution is to dynamically scale down the number of active devices when load is low (parking some GPUs). However, cold-starting them when load increases could be slow. It’s a general infrastructure problem not unique to MoE, though MoE’s conditional activation makes it tricky to pin which devices can sleep. If certain experts aren’t likely to be used, those could be temporarily paged out to save power, as long as the system can page them in on demand (similar to expert buffering but for power/availability scaling).

The **consistency of inference**  is also a consideration in real-time. Because MoE outputs can be nondeterministic if there's any randomness in gating or ties broken arbitrarily, one might see small variations in outputs between runs. Most MoEs are deterministic given the same model and input, but if load-balancing decisions reroute some tokens differently under pressure, outputs could change run-to-run. For applications like translation or medical text generation, consistent outputs are important. Thus, any adaptive inference tricks have to be carefully evaluated for their effect on the text output, not just latency.
Finally, **streaming MoEs**  for things like continuous speech recognition or video processing face the challenge of *state*. A streaming ASR model might process audio frame by frame; if it were an MoE, each frame routes independently. It might be beneficial to have the same expert handle a contiguous segment of audio to accumulate context. This is a frontier area: combining MoE with recurrent or stateful models for streaming inputs.
In summary, real-time deployment of MoE models involves careful engineering to get the best of both worlds: the MoE’s efficiency during heavy utilization and acceptable latency for single-stream cases. Some notable trade-offs and findings include: (1) MoEs can deliver higher throughput under load, but at the cost of higher per-token latency at low loads​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Optimizing%20for%20MoE%20inference%20latency,the%20overall%20achievable%20memory%20bandwidth) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=inference%20latency%20of%20an%20MoE,the%20overall%20achievable%20memory%20bandwidth) ; (2) batching and concurrency are key to mitigating this – in one experiment, routing by entire tasks improved throughput ~2× while preserving quality, essentially by turning the MoE into multiple smaller models used in parallel​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=routing%20%28task,MoE%20%2813B%20parameters%29%20performs) ​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=outperforms%20the%20best%20performing%20token,6x) ; (3) when latency is paramount, techniques like model compression or gating simplification may be necessary to meet strict SLAs. As the field progresses, we expect to see MoEs become more *elastic* – adapting on the fly to different latency/throughput regimes. The end-game is a system where the MoE’s sparsity can be dialed up or down in real-time: using lots of experts for tough, batch tasks and fewer experts for quick, interactive responses. Achieving that will require advances in both algorithms and serving infrastructure.

## Conclusion 

Mixture-of-Experts models represent a powerful approach to scaling up language models, offering unprecedented parameter counts and the potential for improved quality at lower inference cost. Realizing these benefits in practice, however, demands surmounting several inference-time challenges. We have surveyed the major hurdles: from **expert routing overhead and load imbalance** , to **latency/throughput bottlenecks** , **memory and bandwidth demands** , **hardware integration issues** , **uneven expert utilization** , and **serving in real-time contexts** . Across the literature and industry implementations, a few common themes emerge:
 
- **Load Balancing and Routing** : Effective MoE inference hinges on keeping the workload evenly spread among experts and minimizing routing overhead. Auxiliary training losses and novel gating algorithms (e.g. BASE Layers, hash routing) help distribute tokens, while dynamic routing adjustments at runtime can further smooth out imbalances​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=training%20and%20inference,per%20batch%20number%20of%20experts%13) ​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=such%20as%20offloading,of) . Coarse routing at the level of tasks or sequences is a viable strategy to simplify inference, essentially breaking a giant MoE into manageable sub-models per request​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=different%20granularity%20,of%20the%20BLEU) ​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=outperforms%20the%20best%20performing%20token,6x) .
 
- **System and Parallelism Optimizations** : To achieve low latency, MoE models must leverage parallelism across experts while avoiding excessive communication. Designs like DeepSpeed-MoE and Tutel show that with tailored all-to-all protocols and a mix of parallelism strategies, MoEs can reach **4–8× throughput gains**  over naive implementations​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/tutel-an-efficient-mixture-of-experts-implementation-for-large-dnn-model-training/#:~:text=execute%20MoE%20more%20easily%20and,1%20trillion%E2%80%93parameter%20MoE%20language%20model) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=reduce%20MoE%20inference%20latency%20by,equivalent%20dense%20models) . Hierarchical communication, fused kernels, and scheduling that groups tokens by expert are key techniques​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Hierarchical%20All,message%20size%20is%20more%20latency) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=best,is%20illustrated%20in%20Figure%203) . These optimizations turn the theoretical compute savings of sparsity into real wall-clock improvements.
 
- **Memory Management** : The massive scale of MoEs means memory (not compute) often becomes the primary bottleneck​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=Optimizing%20for%20MoE%20inference%20latency,the%20overall%20achievable%20memory%20bandwidth) . Techniques such as expert sharding (as in GShard​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=layers%20deploy%20many%20additional%20FFNs%2C,and%20collecting%20results%20from%20experts) ), memory-aware gating (to reduce padding​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=We%20address%20MoE%E2%80%99s%20inefficiency%20with,could%20be%20integrated%20with%20other) ), and offloading inactive experts​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=Expert%20Buffering%20exploits%20high%20temporal,mitigates%20severe%20load%20imbalance%20across)  all serve to bring memory usage under control. Research prototypes like **expert buffering**  and **block-sparse execution**  demonstrate that it’s possible to cut memory overhead dramatically without sacrificing much accuracy​[SEAS.UPENN.EDU](https://www.seas.upenn.edu/~leebcc/documents/huang24-neurips.pdf#:~:text=Expert%20Buffering%20exploits%20high%20temporal,mitigates%20severe%20load%20imbalance%20across) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=Megablocks%20,that%20can%20accommodate%20imbalanced%20assignment) . In practice, a combination of hardware memory capacity, distributed memory bandwidth, and software caching is used to accommodate trillion-parameter models.
 
- **Hardware Adaptation** : MoE inference performance is deeply tied to hardware capabilities. Early MoEs leveraged Google’s TPU architecture for scale​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,Feb) , while recent efforts focus on optimizing for GPU clusters with high-speed interconnects​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/tutel-an-efficient-mixture-of-experts-implementation-for-large-dnn-model-training/#:~:text=execute%20MoE%20more%20easily%20and,1%20trillion%E2%80%93parameter%20MoE%20language%20model) . Custom communication libraries (NCCL tweaks, MSCCL) and compiler support for dynamic shapes (as in Meta’s FMI and Megablocks) are bridging the gap. The community has learned that **dense-to-sparse migration**  isn’t plug-and-play – it requires co-designing the model and the system. But with each generation of hardware and software (e.g. NVIDIA’s Hopper GPUs, new networking technologies), MoEs become more feasible to serve at scale.
 
- **Quality vs. Performance Trade-offs** : Many solutions involve a trade-off. Using fewer experts per token, merging experts, or distilling to dense models – all these can ease deployment but may reduce model accuracy​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,of%20parameters%20at%20inference%20time) . The optimal balance depends on the application. Notably, some recent works achieved *full quality retention* with faster inference: e.g. task-MoE kept all gains while boosting speed 2×​[ARXIV.ORG](https://arxiv.org/abs/2110.03742#:~:text=routing%20%28task,MoE%20%2813B%20parameters%29%20performs) , and Mixtral 8×7B matched or beat a dense 70B model with much faster inference​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=,in%20the%20announcement%20blog%20post) ​[HUGGINGFACE.CO](https://huggingface.co/blog/moe#:~:text=match%20at%20L605%20,in%20the%20announcement%20blog%20post) . These results are encouraging, suggesting that clever routing or expert combination can give the best of both worlds.

In conclusion, the inference-time challenges of MoE models are being actively tackled on multiple fronts. Academia and industry have proposed a spectrum of solutions – from algorithmic innovations like balanced routing and adaptive gating, to system-level improvements in kernels and networking, to pragmatic approaches like model compression for serving. The **most notable finding**  across the board is that *sparsity* in large models offers real advantages but requires a sophisticated runtime to harness. When properly managed, MoE models can achieve superior performance-per-compute and even **faster inference than dense models of equivalent quality** ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=match%20at%20L200%20reduce%20MoE,equivalent%20dense%20models) ​[MICROSOFT.COM](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/#:~:text=reduce%20MoE%20inference%20latency%20by,equivalent%20dense%20models) . But without careful design, they can suffer from bottlenecks that erase their theoretical benefits.
The **trade-offs**  often come down to flexibility vs. efficiency: a highly dynamic MoE (routing every token optimally) might maximize quality but be harder to optimize, whereas a more structured or restricted MoE (routing by task or with fixed patterns) might be easier to serve but less adaptive. We see this reflected in designs like Switch (simpler routing, easier scaling) versus GLaM (more complex routing, higher peak quality).
Looking forward, **proposed solutions**  such as more intelligent compilers for MoE (compiling sparse operations into dense blocks), hardware with native support for conditional computation, and hybrid models (sparse layers combined with dense backbones) will further mitigate inference challenges. Companies like Google and Meta continue to refine MoE deployments for multilingual and multitask models, indicating that the approach remains promising if the **serving cost**  can be kept in check. As of 2024, MoEs are not yet the de facto standard for all LLM deployments (many productions use dense models like GPT-4 or LLaMA due to maturity and simplicity), but the gap is closing. With projects like DeepSpeed-MoE demonstrating sub-30ms latency for trillion-param models​[ARXIV.ORG](https://arxiv.org/pdf/2201.05596#:~:text=arXiv%20arxiv,5x%20faster%20and%209x) , it’s clear that MoEs can be made to work in real-time with enough engineering.
In summary, Mixture-of-Experts models introduce unique inference-time challenges, but research has shown these are **surmountable** . Through balanced expert utilization, parallelism-aware system design, memory optimizations, and strategic simplifications, MoEs can live up to their promise of “maximum parameters, minimal compute”​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=to%20dense%20matrix%20multiplications,is%20hindered%20by%20complexity%2C%20communication) ​[JMLR.ORG](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf#:~:text=Mixture,communication%20costs%2C%20and%20training%20instabilities) . The ongoing convergence of model architecture and infrastructure suggests that future large-scale AI systems may routinely use expert mixtures to deliver both **scale and speed**  – dynamically allocating computation only where needed, and thereby achieving new levels of efficiency in large language model inference.
