---
title: LLM Hardware Optimization
date: 2024-01-02 23:32:49
permalink: /pages/dc7036/
---
1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]
2. TurboTransformers: An Efficient GPU Serving System For Transformer Models [82]
3. Improving the Efficiency of Transformers for Resource-Constrained Devices [8]
4. Bag of Tricks for Optimizing Transformer Efficiency [5]
5. Making Transformer inference faster on GPUs[Blog]
6. Energy-efficient Inference Service of Transformer-based Deep Learning Models on GPUs [4]
7. Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs [TACO 2023 Ref 2]
8. hugging face https://huggingface.co/docs/transformers/performance
9. [C17 Y2024 ASPLOS] AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference

---
### 1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]
:thumbsup: :thumbsup: :thumbsup: :thumbsup: 

### 4. Making Transformer inference faster on GPUs[Blog]
https://dev-discuss.pytorch.org/t/making-transformer-inference-faster-on-gpus/190

### 9. [C17 Y2024 ASPLOS] AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference

I dont understand how PIM works, but the discussion about compute-efficiency and batching in LLM inference is solid.

Batching to process multiple input sequences together can be a potential solution to inference throughput, but it is effective only for the FC layer, not the attention layer.

Specifically, batching can improve the utilization of the system’s compute and memory resources for the FC layer by reusing the weight matrices and increasing arithmetic intensity, quantified by the Operation per Byte (Op/B).

However, the Key and Value (KV) matrices of the attention layer are unique per (inference) request.
That is, batching can neither reuse the KV matrices nor improve the throughput of processing the attention layer.

Moreover, the attention layer even limits the maximum batch size and impacts the FC layer throughput due to two
critical constraints: memory capacity and service level objective (SLO).

(1) The memory capacity required to store KV matrices can be prohibitively high.
(2) Even if the memory capacity constraint is resolved, the SLO requirement becomes another limiting factor. As batching does not improve the throughput of the attention layer (§3.2), a larger batch leads to a longer processing time and thus violation of a given SLO constraint.

![image](https://github.com/user-attachments/assets/81d7ee21-3fdd-42f6-bc5d-4697c94051c2)


The Gen (Prefilling) stages typically overwhelm the Sum (Decoding) stage in execution time due to their sequential nature of reading the entire pre-trained weights per generated (output) token.

![image](https://github.com/user-attachments/assets/9aff3457-1a1b-4988-baa0-8933bee358dd)

![image](https://github.com/user-attachments/assets/bbc697c4-efec-4195-b47b-941f5c7f2557)

As batching does not improve the throughput of the attention layer, the execution time for processing a
batch increases with the batch size.

That is, when the SLO is fixed, the maximum batch size is limited due to the attention layer.

Further, the attention layer has a low arithmetic intensity regardless of batch size (see Figure 3).

The primary operation of the attention layer in the Gen stage is the GEMV of the score and context operations, exhibiting a low Op/B (∼1).

Unlike the FC layer, the attention layer still has memoryintensive GEMV operations even after batching because each request uses unique KV matrices,
