---
title: LLM CPU & GPU Workloads
date: 2025-05-13 12:32:49
permalink: /pages/dc7069/
---

1. [Y2024] Understanding Performance Implications of LLM Inference on CPUs

---

## 1. [Y2024] Understanding Performance Implications of LLM Inference on CPUs

**Flops**

![image](https://github.com/user-attachments/assets/43b30c0a-14ac-4239-9cd1-61c65fb0d21f)

**Intel AMX Architecture**

![image](https://github.com/user-attachments/assets/fe5d81c7-97bc-4029-ac73-118328ae5e6a)


**Model Size**

![image](https://github.com/user-attachments/assets/b5dbd3ab-f3e0-4772-a8c8-eb43bc777afa)

**KV Cache Size**

![image](https://github.com/user-attachments/assets/1d7eabab-388f-4cd8-b7aa-9351dc79420e)

**CPU Servers**

![image](https://github.com/user-attachments/assets/3d5b683a-86d2-4fff-9093-8322c3448027)

The memory of CPU is much bigger than GPU.

These normalized results highlight the performance benefits gained from the use of both the matrix multiplication accelerator and high-bandwidth memory on the SPR Max CPU.

The significant reduction in latency and improvement in throughput during the prefill phase is due to AMX support on the SPR Max CPU.

The throughput improvement in the memorybound decode phase is made possible by the higher memory bandwidth provided by HBM.

![image](https://github.com/user-attachments/assets/c75e9230-f6c0-4087-b349-e563d3f4ebb5)

**With larger batch sizes, both models exhibit a decrease in LLC MPKI and an increase in core utilization, indicating a shift towards a more compute-bound execution.**


**Key Finding#1**

With AMX support, larger cores and cache, and HBM integration, the SPR Max CPU significantly reduces latency and increases throughput for BF16 LLM inference compared to the ICL CPU.

**Key Finding#2**

Proper memory and clustering configurations are essential for optimizing performance.

The Flat memory mode with Quadrant clustering offers the best latency and throughput for LLM inference.

**Key Finding#3**

Using 48 SPR cores with HBM maximizes core utilization and minimizes inter-socket communication,
resulting in the best performance across models.


**GPU**

![image](https://github.com/user-attachments/assets/98d80cd2-4d22-448a-bda7-48cd7be9cc7d)

**Key Finding#4**

Overall, GPUs outperform CPUs in LLM inference, but AMX-enabled CPUs can achieve lower latency and higher throughput for larger models requiring offloading.

![image](https://github.com/user-attachments/assets/ac3a7b7d-ebc3-4a9f-8c13-a113f04bd62b)

As the number of input tokens increases, GPU latency and throughput remain stable, while the SPR Max 9468 CPU shows more variability.

This is due to the CPU’s lower compute throughput and memory bandwidth, resulting in less favorable performance scalability.

Interestingly, for larger models such as LLaMA2-70B, the CPU outperforms the GPU in both latency and throughput across all sequence lengths.

This is primarily due to the significant time spent on data loading via the PCIe bus when the batch size is set to 1, as shown in Figure 18.

As the batch size increases to 16, the performance gap between CPUs and GPUs widens, particularly for smaller models.

For larger models such as LLaMA2-70B, we observed that at sequence lengths of 256 or more, the H100 GPU even when using offloading-based LLM inference—achieves lower latency compared to the CPU.

This is because, at these longer sequence lengths, the CPU’s LLM inference throughput continues to decline, resulting in lower performance than the H100.

However, in the case of the A100 GPU, the CPU outperforms the GPU across all sequence lengths.

This demonstrates that lower PCIe bandwidth significantly degrades the performance of offloading-based LLM serving systems.

**Key Finding#5**

For larger batch sizes, GPUs outperform CPUs in small models. Even in larger models that require offloading, CPUs may underperform at longer sequence lengths due to lower compute throughput.

**We also note that new Grace-Hopper Superchip would see lower overheads for offloading from DRAM to the integrated H100 due to its higher NVLink bandwidth (900 GB/s versus PCIe 5.0’s 128 GB/s), albeit at a cost of ∼4x of the SPR CPU
and DDR5 [40].**

![image](https://github.com/user-attachments/assets/54409207-cc8b-48aa-b0c5-67365acf1b4e)



