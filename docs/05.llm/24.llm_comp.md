---
title: LLM Compression
date: 2025-04-1 12:32:49
permalink: /pages/dc7061/
---

1. [35] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models


---
## [35] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models

The paper "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models" addresses the challenge of efficiently deploying large language models (LLMs) by proposing an innovative method for compressing the key-value (KV) cache.

Below is a detailed explanation of the paper, including key insights, contributions, and findings.

### Background and Motivation 
Large Language Models (LLMs) like GPT and LLaMA rely heavily on autoregressive generation, where previously computed tokens' key-value pairs are cached to minimize redundant computation during inference.

However, the KV cache grows linearly with sequence length, becoming a substantial memory burden for long context tasks.<br> The authors identify an overlooked dimension—**cross-layer redundancy** , highlighting high similarities between the KV states of adjacent layers, particularly in middle-to-deep layers of LLMs.

### Key Insights 
- **Cross-Layer Redundancy**<br>KV cache states across adjacent layers share significant redundancy, especially in deeper layers.
- **Unequal Mergeability**<br>Not all KV cache state pairs between adjacent layers are equally suitable for merging—some tokens show distinct semantic differences and thus should not be merged indiscriminately.
- **Reparameterization Approach**<br>Separating state vectors into magnitude and directional components allows effective merging via interpolation in polar coordinates, preserving crucial information and performance.

### Main Contributions 
 
1. **MiniCache Framework** <br>Introduces a novel cross-layer compression strategy that merges KV caches from adjacent layers starting from the middle layers of the LLM.
 
2. **Reparameterization-based Cache Merging**<br>Uses Spherical Linear Interpolation (SLERP) to merge the direction component of the KV cache vectors, preserving the original magnitude for minimal information loss.
 
3. **Token Retention Strategy**<br>Identifies and retains critical, distinct token pairs to prevent semantic degradation during merging, ensuring accuracy with minimal overhead.
 
4. **Orthogonality to Existing Methods**<br>MiniCache complements existing compression methods (e.g., quantization and sparsity), achieving superior compression rates when combined.

### Methodology 
The method involves two main components:
#### Cross-Layer Compression 
- **Identifying Optimal Layers**
  Merging begins at the midpoint of LLM layers, justified by observed higher redundancy at deeper layers.
- **Merge Function**
  KV pairs from adjacent layers are merged via a carefully designed function leveraging SLERP, preserving semantic integrity and directional properties.
#### Cache Merging and Restoration 
- **Reparameterization**
  KV caches are decomposed into directional vectors (normalized) and magnitudes. The directional component undergoes merging using SLERP, ensuring geometrically coherent interpolation.
- **Token Retention**
  Identifies outliers—distinct KV pairs unsuitable for merging—based on angular distance, selectively retaining these tokens to minimize performance loss.
- **Restoration Process**
  Merged caches are restored by scaling merged directions with their original magnitudes and reintegrating the retained distinct tokens.
### Experimental Evaluation 
The authors evaluate MiniCache extensively across several popular LLMs:
- **Models**
  LLaMA-2, LLaMA-3, Mixtral, Phi-3.
- **Datasets**
  Evaluations include GSM8K (math problems), COQA (conversational Q&A), TruthfulQA, and LongBench (long-context tasks).
- **Baselines**
  Comparisons include FP16 baseline (no compression), quantization-based methods (e.g., KIVI, SmoothQuant), and sparsity-driven methods.
### Results and Findings: 
- **Memory Efficiency**
  MiniCache achieves up to 41% memory reduction and a compression ratio up to 5.02× when combined with 4-bit quantization.
- **Throughput Enhancement**
  Improves inference throughput by approximately 5× compared to the FP16 baseline due to reduced memory footprint, enabling larger batch sizes and faster generation.
- **Minimal Performance Drop**
  Compression with MiniCache leads to near-lossless performance, even under aggressive compression settings.
### Ablation Studies: 
- **Interpolation Parameter**
  A critical hyperparameter determining the balance in merging adjacent KV pairs—optimal around `t=0.6`.
- **Retention Threshold**
  Optimal token retention (`γ=0.05`) strikes the best balance between accuracy and memory usage.
### Contributions Summarized 
- The paper proposes a novel depth-wise KV cache compression method, MiniCache.
- It identifies cross-layer KV cache redundancy as a previously unexplored yet crucial dimension.
- It introduces robust merging via SLERP interpolation and a targeted retention strategy for distinct tokens.
- Experimental validation highlights substantial efficiency improvements, minimal accuracy degradation, and strong compatibility with existing methods.
### Limitations and Future Work 
- The SLERP-based merging function is currently limited to pairwise merging.
  Future extensions could explore simultaneous merging across multiple layers.
- Further exploration into adaptive interpolation parameters based on the relative magnitude ratio of vectors is identified as promising.
### Conclusion 
"MiniCache" successfully identifies and exploits an important new dimension—depth-wise redundancy in KV caches of LLMs.<br>
Its combination of reparameterization-based merging and selective token retention provides significant improvements in memory efficiency and inference throughput, establishing a new direction for research and practical optimization in deploying large-scale language models.
