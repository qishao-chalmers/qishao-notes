---
title: LLM Compression
date: 2025-04-1 12:32:49
permalink: /pages/dc7061/
---

1. [35] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models
2. [1 2024]When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models


---
## [35] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models

The paper "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models" addresses the challenge of efficiently deploying large language models (LLMs) by proposing an innovative method for compressing the key-value (KV) cache.

Below is a detailed explanation of the paper, including key insights, contributions, and findings.

Additionally, since MiniCache is orthogonal to existing quantization techniques, it can achieve a compression ratio of up to **5.02×** when combined with the **4-bit quantization** technique.

### Background and Motivation 
Large Language Models (LLMs) like GPT and LLaMA rely heavily on autoregressive generation, where previously computed tokens' key-value pairs are cached to minimize redundant computation during inference.

However, the KV cache grows linearly with sequence length, becoming a substantial memory burden for long context tasks.<br> The authors identify an overlooked dimension—**cross-layer redundancy** , highlighting high similarities between the KV states of adjacent layers, particularly in middle-to-deep layers of LLMs.

![image](https://github.com/user-attachments/assets/84d897bf-0c57-465e-84fe-831d4cdc56e2)


### Key Insights 
- **Cross-Layer Redundancy**<br>KV cache states across adjacent layers share significant redundancy, especially in deeper layers.
- **Unequal Mergeability**<br>Not all KV cache state pairs between adjacent layers are equally suitable for merging—some tokens show distinct semantic differences and thus should not be merged indiscriminately.
- **Reparameterization Approach**<br>Separating state vectors into magnitude and directional components allows effective merging via interpolation in polar coordinates, preserving crucial information and performance.

>**Please notice this consine similarity in tokens index**

![image](https://github.com/user-attachments/assets/8aca3d7b-3970-4d6f-a346-fe6dd9609db4)


### Main Contributions 
 
1. **MiniCache Framework** <br>Introduces a novel cross-layer compression strategy that merges KV caches from adjacent layers starting from the middle layers of the LLM.

![image](https://github.com/user-attachments/assets/282c4d68-fc9c-443b-9ad5-69a231d5e863)

 
2. **Reparameterization-based Cache Merging**<br>Uses Spherical Linear Interpolation (SLERP) to merge the direction component of the KV cache vectors, preserving the original magnitude for minimal information loss.

 
3. **Token Retention Strategy**<br>Identifies and retains critical, distinct token pairs to prevent semantic degradation during merging, ensuring accuracy with minimal overhead.
 
4. **Orthogonality to Existing Methods**<br>MiniCache complements existing compression methods (e.g., quantization and sparsity), achieving superior compression rates when combined.

### Methodology 
The method involves two main components:

#### Cross-Layer Compression 
- **Identifying Optimal Layers**
  Merging begins at the midpoint of LLM layers, justified by observed higher redundancy at deeper layers.
- **Merge Function**
  KV pairs from adjacent layers are merged via a carefully designed function leveraging SLERP, preserving semantic integrity and directional properties.
#### Cache Merging and Restoration 
- **Reparameterization**
  KV caches are decomposed into directional vectors (normalized) and magnitudes. The directional component undergoes merging using SLERP, ensuring geometrically coherent interpolation.
- **Token Retention**
  Identifies outliers—distinct KV pairs unsuitable for merging—based on angular distance, selectively retaining these tokens to minimize performance loss.
- **Restoration Process**
  Merged caches are restored by scaling merged directions with their original magnitudes and reintegrating the retained distinct tokens.
### Experimental Evaluation 
The authors evaluate MiniCache extensively across several popular LLMs:
- **Models**
  LLaMA-2, LLaMA-3, Mixtral, Phi-3.
- **Datasets**
  Evaluations include GSM8K (math problems), COQA (conversational Q&A), TruthfulQA, and LongBench (long-context tasks).
- **Baselines**
  Comparisons include FP16 baseline (no compression), quantization-based methods (e.g., KIVI, SmoothQuant), and sparsity-driven methods.
### Results and Findings: 
- **Memory Efficiency**
  MiniCache achieves up to 41% memory reduction and a compression ratio up to 5.02× when combined with 4-bit quantization.
- **Throughput Enhancement**
  Improves inference throughput by approximately 5× compared to the FP16 baseline due to reduced memory footprint, enabling larger batch sizes and faster generation.
- **Minimal Performance Drop**
  Compression with MiniCache leads to near-lossless performance, even under aggressive compression settings.
### Ablation Studies: 
- **Interpolation Parameter**
  A critical hyperparameter determining the balance in merging adjacent KV pairs—optimal around `t=0.6`.
- **Retention Threshold**
  Optimal token retention (`γ=0.05`) strikes the best balance between accuracy and memory usage.
### Contributions Summarized 
- The paper proposes a novel depth-wise KV cache compression method, MiniCache.
- It identifies cross-layer KV cache redundancy as a previously unexplored yet crucial dimension.
- It introduces robust merging via SLERP interpolation and a targeted retention strategy for distinct tokens.
- Experimental validation highlights substantial efficiency improvements, minimal accuracy degradation, and strong compatibility with existing methods.
### Limitations and Future Work 
- The SLERP-based merging function is currently limited to pairwise merging.
  Future extensions could explore simultaneous merging across multiple layers.
- Further exploration into adaptive interpolation parameters based on the relative magnitude ratio of vectors is identified as promising.
### Conclusion 
"MiniCache" successfully identifies and exploits an important new dimension—depth-wise redundancy in KV caches of LLMs.<br>
Its combination of reparameterization-based merging and selective token retention provides significant improvements in memory efficiency and inference throughput, establishing a new direction for research and practical optimization in deploying large-scale language models.

---
## 2. [1 2024]When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models

### Insights
Simply applying a scaling technique to model weights makes the quantized weight distribution more uneven, which can improve compressibility but reduce accuracy by approximately 60%.

To address this issue, we propose a compression-aware quantization and pruning approach that expands important values and reduces unimportant weight.

![image](https://github.com/user-attachments/assets/383e24d5-432a-4f76-a251-799b4482cf85)

![image](https://github.com/user-attachments/assets/b9801e28-ffd4-40bb-9e0d-1c667b09cdb6)

![image](https://github.com/user-attachments/assets/743aee20-cc78-4ab7-ab23-b5936e031509)


