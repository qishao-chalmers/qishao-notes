---
title: LLM RL Framework
date: 2024-12-08 12:32:49
permalink: /pages/dc7070/
---

# TRL Repo

[repo](https://github.com/huggingface/trl)

## 1. Repository Overview

**TRL (Transformer Reinforcement Learning)**
- Library for post-training foundation models
- Key trainers: `SFTTrainer`, `GRPOTrainer`, `DPOTrainer`, `RLOOTrainer`, `PPOTrainer`, `RewardTrainer`
- **GRPO**: Group Relative Policy Optimization (memory-efficient RL variant)

---

## 2. Reward Functions

### Types of Rewards

- **Token-wise rewards**: Per-token signals (e.g., MiniLLM teacher-student log prob differences)
- **Final result rewards**: Single scalar per completion (common in custom reward functions)

### Built-in Reward Functions

#### `accuracy_reward`
- Checks if completion matches ground truth (math verification)
- Returns: `1.0` (correct), `0.0` (incorrect), `None` (unparseable)

#### `think_format_reward`
- Checks if reasoning is enclosed in `<think>...</think>` tags
- Returns: `1.0` (correct format), `0.0` (wrong format)

#### `get_soft_overlong_punishment`
- Penalizes overly long completions
- Formula:
  - `0.0` if length ‚â§ L_max - L_cache
  - Linear penalty if between
  - `-1.0` if > L_max

### Multiple Reward Functions

- `rewards_per_func` shape: `(num_completions, num_reward_functions)`
- Each column = one reward function
- Combined via weighted sum:
  ```python
  rewards = (rewards_per_func * reward_weights).nansum(dim=1)
  ```

---

## 3. GRPO Mechanism

### Core Algorithm Steps

1. **Generation**: Generate `G` completions per prompt (default `G=8`)
2. **Reward Computation**: Compute rewards for each completion
3. **Group-Relative Advantage**:
   ```python
   advantages = (rewards - mean_grouped_rewards) / std_grouped_rewards
   ```
   - Normalized within each group (prompt)
4. **Loss Computation**: Clipped surrogate objective with KL regularization
5. **Backpropagation**: Gradients flow back to update model

### Loss Types

| Loss Type | Normalization Method | Notes |
|-----------|---------------------|-------|
| `"grpo"` | Per completion, then average | Has length bias |
| `"dapo"` | Global token count | **Recommended** (no length bias) |
| `"dr_grpo"` | `batch_size √ó max_completion_length` | Fixed constant |
| `"bnpo"` | Local batch token count | Varies with batch size |
| `"cispo"` | Clips importance sampling weights | Different clipping strategy |
| `"sapo"` | Soft adaptive policy optimization | Temperature-controlled |

### Verification Against Theory

‚úÖ **Implementation matches theory**:
- Group-relative advantages ‚úì
- Clipped surrogate loss ‚úì
- KL regularization ‚úì

---

## 4. Loss Computation Details

### Per-Completion vs Final Loss

- **Per-token losses**: Computed for each token in each completion ‚Üí shape `(B, T)`
- **Per-completion losses**: Aggregated per completion ‚Üí shape `(B,)`
- **Final scalar loss**: Averaged across completions ‚Üí shape `()` (scalar)

### Loss Aggregation (for `"grpo"`)

```python
per_completion_loss = (per_token_loss * mask).sum(-1) / mask.sum(-1)  # (B,)
final_loss = per_completion_loss.mean()  # scalar
```

### Each Completion Contributes Differently

- ‚úÖ **Good completions** (positive advantage) ‚Üí negative loss contribution (encouraged)
- ‚ùå **Bad completions** (negative advantage) ‚Üí positive loss contribution (discouraged)
- ‚ö™ **Average completions** (zero advantage) ‚Üí neutral contribution

---

## 5. Forward Pass & Hidden States

### Hidden States

- **Shape at each layer**: `(B, L, H)` where:
  - `B` = total completions
  - `L` = sequence length
  - `H` = hidden dimension
- **Each completion has independent hidden states** (NOT averaged)
- **Processed in parallel** as a batch

### Forward Pass Flow

```
input_ids (B, L)
    ‚Üì
Model
    ‚Üì
Hidden States Layer 1 (B, L, H)
    ‚Üì
Hidden States Layer 2 (B, L, H)
    ‚Üì
... (all layers)
    ‚Üì
Logits (B, L, vocab_size)
    ‚Üì
Per-token logprobs (B, T)
    ‚Üì
Per-token losses (B, T)
    ‚Üì
Per-completion losses (B,)
    ‚Üì
Final scalar loss ()
```

---

## 6. Backpropagation & Gradients

### Why Final Scalar Loss is Needed

- ‚ùå **Cannot call `.backward()`** on tensor with shape `(B,)`
- ‚úÖ **PyTorch requires scalar** for `.backward()`
- ‚úÖ **Final scalar loss** enables backpropagation

### Gradient Flow Through `.mean()`

When `final_loss = mean([loss_0, loss_1, ..., loss_B-1])` and `final_loss.backward()`:

- Each completion receives `1/B` of the gradient
- Gradients flow **independently** through each completion's computational graph
- Model parameters **accumulate** gradients from all completions

### Mathematical Flow

```
Final Loss (scalar): loss = mean([loss_0, loss_1, loss_2])
                              ‚Üì backward() with grad=1.0
                              ‚Üì
Mean Operation:       grad_loss_i = 1.0 / B
                              ‚Üì
Per-Completion Losses:
  loss_0: receives grad = 1/B ‚Üí flows to completion_0's tokens
  loss_1: receives grad = 1/B ‚Üí flows to completion_1's tokens
  loss_2: receives grad = 1/B ‚Üí flows to completion_2's tokens
                              ‚Üì
Model Parameters:
  Gradients accumulate from all completions
```

---

## 7. Pure RL Trainers (No Teacher Model)

### Trainers WITHOUT Teacher Models

- **`GRPOTrainer`** (when `beta=0.0`): No reference model, pure RL
- **`RLOOTrainer`**: REINFORCE Leave-One-Out
- **`PPOTrainer`**: Proximal Policy Optimization

### Trainers WITH Teacher Models

- **`MiniLLMTrainer`**: Uses teacher model for knowledge distillation
  - Rewards = `teacher_logprobs - student_logprobs` (token-wise)

---

## 8. Key Implementation Details

### Token-wise vs Sequence-wise

- Advantages can be `(B,)` (sequence-level) or `(B, T)` (token-level)
- Loss supports both via conditional unsqueeze
- Importance sampling can be token-level or sequence-level

### Distributed Training

- Rewards gathered across processes before group normalization
- Ensures correct group-relative advantage computation

### Memory Efficiency

- GRPO is memory-efficient compared to PPO
- Supports gradient checkpointing
- Optional reference model (`beta=0.0` means no ref model)

---

## 9. Key Takeaways

### Core Concepts

1. ‚úÖ **GRPO uses group-relative advantages** (normalized within each prompt group)
2. ‚úÖ **Multiple completions per prompt** enable comparative learning
3. ‚úÖ **Each completion contributes differently** based on its advantage
4. ‚úÖ **Loss computed per-token**, aggregated per-completion, then averaged to scalar
5. ‚úÖ **Hidden states remain independent** per completion (NOT averaged)
6. ‚úÖ **Gradients flow equally** (`1/B` each) through `.mean()` operation

### Practical Insights

- üí° Use `"dapo"` loss type to avoid length bias
- üí° Multiple reward functions can be combined with weights
- üí° Length penalties are optional (not automatic)
- üí° `beta=0.0` enables pure RL without reference model
- üí° Token-level advantages enable fine-grained learning signals

### Technical Understanding

- üîß PyTorch requires scalar loss for backpropagation
- üîß `.mean()` distributes gradients equally (`1/B` to each element)
- üîß Batch processing enables parallel computation
- üîß Gradients accumulate at shared model parameters

---

**End of Summary** ‚Äî This covers the main topics discussed today about TRL, GRPO, reward functions, loss computation, and backpropagation mechanics.

---

## Q1: Does TRL support both step-based (token-wise) and final result rewards?

**Initial Answer**: Not directly supported - reward functions typically return a single scalar per completion.

**Clarification**: 
- ‚úÖ **Token-wise rewards ARE supported** in `PPOTrainer` and `GRPOTrainer`
- ‚úÖ While custom `reward_funcs` typically return sequence-level scalars, these are used to compute **advantages** which can be applied token-wise
- ‚úÖ In `MiniLLMTrainer`, rewards are computed directly token-wise as `teacher_logprobs - student_logprobs`
- ‚úÖ `GRPOTrainer`'s `_compute_loss` explicitly handles advantages of shape `(B, T)` (token-wise) or `(B,)` (sequence-wise)

**Key Insight**: The distinction is between **reward functions** (usually sequence-level) and **advantages** (can be token-level or sequence-level).

---

## Q2: How are token-wise rewards and losses computed when model outputs and ground truth don't match?

**Question**: How does the system handle cases where:
- Answers have extra meaningless tokens but are correct?
- Answers are totally wrong?
- How are rewards/losses calculated token-wise?

**Answer**:
- **Padding and masking**: Variable-length sequences are handled with `completion_mask` to mask out padding tokens
- **Token order matters**: `torch.gather` is used to extract log probabilities for the **actual generated tokens** at each position
- **Same token, different positions**: The same token appearing at different positions receives different probabilities and rewards due to varying contexts
- **Per-token loss**: Each token position has its own loss based on:
  - The token's log probability
  - The completion's advantage (group-relative)
  - Importance sampling ratio (if using off-policy updates)

**Key Code**:
```python
# Extract log probs for generated tokens
log_probs = torch.gather(log_probs, dim=-1, index=completion_ids.unsqueeze(-1))
# Apply completion mask to ignore padding
per_token_loss = per_token_loss * completion_mask
```

---

## Q3: Is there a teacher model in MiniLLM?

**Answer**: 
- ‚úÖ **Yes**, `MiniLLMTrainer` explicitly uses a teacher model
- Teacher model provides target log probabilities for knowledge distillation
- Rewards are computed as: `teacher_logprobs - student_logprobs` (token-wise)
- This is different from a **reference model** used in PPO/GRPO for KL regularization

**Distinction**:
- **Teacher model** (MiniLLM): Used for knowledge distillation, provides target probabilities
- **Reference model** (PPO/GRPO): Used for KL regularization, prevents policy from deviating too much

---

## Q4: Are there training methods that operate purely on RL without a teacher model?

**Answer**: 
- ‚úÖ **Yes**, several trainers operate purely on RL:
  - `GRPOTrainer` (when `beta=0.0`): No reference model, pure RL
  - `RLOOTrainer`: REINFORCE Leave-One-Out
  - `PPOTrainer`: Proximal Policy Optimization
- These use reward functions (not teacher models) to guide learning

---

## Q5: Why does `rewards_per_func` have two columns?

**Question**: In `_calculate_rewards`, why does `rewards_per_func` have shape `(num_completions, num_reward_functions)`?

**Answer**:
- **Number of columns = number of reward functions**
- Each column represents rewards from one reward function
- Allows combining multiple reward functions with different weights
- Example: Column 0 = `accuracy_reward`, Column 1 = `format_reward`
- Combined via: `rewards = (rewards_per_func * reward_weights).nansum(dim=1)`

**Visual Example**:
```
rewards_per_func = [
    [1.0, 0.8],   # Completion 0: [accuracy_reward, format_reward]
    [1.0, 0.7],   # Completion 1: [accuracy_reward, format_reward]
    [0.0, -0.2],  # Completion 2: [accuracy_reward, format_reward]
]
```

---

## Q6: What do `accuracy_reward` and `format_reward` mean?

**Answer**:

### `accuracy_reward`
- Checks if completion matches ground truth solution
- Uses math verification for LaTeX expressions
- Returns: `1.0` (correct), `0.0` (incorrect), `None` (unparseable - skipped)

### `think_format_reward` (format reward)
- Checks if reasoning is enclosed in `<think>...</think>` tags
- Returns: `1.0` (correct format), `0.0` (wrong format)
- Ensures structured output format

**Usage**: Often combined with weights, e.g., 70% accuracy + 30% format

---

## Q7: Does GRPO put a penalty on the length of answers?

**Question**: Is there automatic length penalty?

**Answer**:
- ‚ùå **No automatic length penalty**
- ‚úÖ **Optional length penalty** available via `get_soft_overlong_punishment`
- Must be explicitly added to `reward_funcs` list
- Formula:
  - `0.0` if length ‚â§ L_max - L_cache
  - Linear penalty if between
  - `-1.0` if > L_max
- Default in scripts: `max_completion_len=1280`, `soft_punish_cache=256`

**Key Point**: Length penalties are **optional** and must be explicitly configured.

---

## Q8: Does GRPO implementation match the theoretical mechanism?

**Question**: Does the code implementation match the GRPO paper's algorithm?

**Answer**: 
- ‚úÖ **Yes, verified match**:
  - Group-relative advantage computation ‚úì
  - Clipped surrogate objective ‚úì
  - KL divergence regularization ‚úì
  - Token-level loss aggregation ‚úì

**Verification Points**:
1. Advantages computed as: `(rewards - mean_grouped_rewards) / std_grouped_rewards`
2. Loss uses clipped importance sampling: `min(œÅA, clip(œÅ, 1-Œµ, 1+Œµ)A)`
3. KL regularization applied when `beta != 0.0`
4. Multiple loss normalization strategies supported

---

## Q9: Is loss calculated for each completion?

**Question**: Does GRPO calculate loss for each completion separately?

**Answer**:
- ‚úÖ **Yes**, loss is computed per-token for each completion
- **Per-token losses**: Shape `(B, T)` - one loss per token per completion
- **Per-completion losses**: Aggregated per completion ‚Üí shape `(B,)`
- **Final scalar loss**: Averaged across completions ‚Üí shape `()` (scalar)

**Loss Flow**:
```
Per-token losses (B, T) 
    ‚Üí Per-completion losses (B,) 
    ‚Üí Final scalar loss ()
```

**Key Point**: Each completion contributes differently based on its advantage (good/bad/average).

---

## Q10: What are the hidden states during forward/backward pass? Are they averaged?

**Question**: During backpropagation, what are the hidden states at each layer? Are they averaged across completions?

**Answer**:
- ‚ùå **Hidden states are NOT averaged** across completions
- ‚úÖ **Each completion has independent hidden states**
- **Shape at each layer**: `(B, L, H)` where:
  - `B` = total completions (batch_size √ó num_generations)
  - `L` = sequence length
  - `H` = hidden dimension
- **Processed in parallel** as a batch
- **Gradients accumulate** at shared model parameters from all completions

**Key Insight**: Hidden states remain independent per completion; only gradients accumulate at parameters.

---

## Q11: Do different completions share the same scalar loss?

**Question**: Since there's a final scalar loss, do all completions share the same loss value?

**Answer**:
- ‚ùå **No**, each completion has its own loss contribution
- ‚úÖ **Final scalar loss** is the **average** of all per-completion losses
- Each completion contributes differently:
  - **Good completions** (positive advantage) ‚Üí negative loss contribution (encouraged)
  - **Bad completions** (negative advantage) ‚Üí positive loss contribution (discouraged)
  - **Average completions** (zero advantage) ‚Üí neutral contribution

**Example**:
```
Completion 0 (Correct):  reward=1.0, advantage=+0.5  ‚Üí  per_completion_loss = -0.3
Completion 1 (Wrong):   reward=0.0, advantage=-0.5  ‚Üí  per_completion_loss = +0.2
Completion 2 (Partial): reward=0.5, advantage=0.0   ‚Üí  per_completion_loss = 0.0

Final scalar loss = mean([-0.3, +0.2, 0.0]) = -0.033
```

**Key Point**: The final scalar loss is an aggregation mechanism; each completion contributes differently based on quality.

---

## Q12: Why do we need final scalar loss for backpropagation?

**Question**: If we compute per-completion losses, why do we need to aggregate them into a final scalar loss?

**Answer**:
- ‚ùå **PyTorch cannot call `.backward()`** on tensor with shape `(B,)`
- ‚úÖ **PyTorch requires scalar** for `.backward()`
- ‚úÖ **Final scalar loss** enables backpropagation
- The `.mean()` operation distributes gradients equally (`1/B` to each completion)

**Mathematical Flow**:
```
final_loss = mean([loss_0, loss_1, ..., loss_B-1])
final_loss.backward()

# Each completion receives: grad_loss_i = 1.0 / B
# Gradients flow independently through each completion's graph
# Model parameters accumulate gradients from all completions
```

**Key Insight**: The final scalar loss is a **technical requirement** for PyTorch's autograd, not a conceptual necessity.

---

## Q13: Do completions receive mean gradient during backpropagation?

**Question**: When gradients flow back through `.mean()`, does each completion receive the mean gradient?

**Answer**:
- ‚úÖ **Yes**, each completion receives `1/B` of the gradient (where B = number of completions)
- This comes from the `.mean()` operation's gradient: `‚àÇmean/‚àÇx[i] = 1/B`
- Gradients flow **independently** through each completion's computational graph
- Model parameters **accumulate** gradients from all completions

**Mathematical Flow**:
```
Final Loss: loss = mean([loss_0, loss_1, loss_2])
    ‚Üì backward() with grad=1.0
    ‚Üì
Mean Operation: grad_loss_i = 1.0 / B = 1/3
    ‚Üì
Each completion receives equal gradient share (1/B)
    ‚Üì
Gradients flow independently through each completion
    ‚Üì
Model parameters accumulate gradients from all completions
```

**Key Point**: Equal gradient distribution (`1/B` each) ensures all completions contribute equally to parameter updates.

---

## Summary of Key Clarifications

1. **Token-wise rewards**: Supported via advantages, not directly in reward functions
2. **Loss computation**: Per-token ‚Üí per-completion ‚Üí scalar (each step serves a purpose)
3. **Hidden states**: Independent per completion, NOT averaged
4. **Gradients**: Distributed equally (`1/B`) through `.mean()` operation
5. **Final scalar loss**: Technical requirement for PyTorch, not conceptual necessity
6. **Multiple reward functions**: Each gets its own column in `rewards_per_func`
7. **Length penalties**: Optional, must be explicitly added
8. **Teacher vs Reference**: Different concepts (knowledge distillation vs KL regularization)

# GRPO Mathematical Demo: Rewards ‚Üí Advantages ‚Üí Loss

This document provides a concrete mathematical demonstration of how rewards (sequence-level/sparse) flow through advantages (sequence-level, broadcast) to token-level losses in GRPO.

## Executive Summary

**GRPO uses two relative measures:**

1. **Advantages are group-relative**: `A_i = r_i - mean(r_group)` 
   - Advantages are computed relative to the average reward of completions for the same prompt
   - This enables comparative learning within each prompt group

2. **Importance sampling ratio is policy-relative**: `œÅ = œÄ_Œ∏_current / œÄ_Œ∏_gen`
   - Measures how much the policy changed since generation
   - `old_per_token_logps`: Computed right after generation with `Œ∏_gen`
   - `per_token_logps`: Computed during training with `Œ∏_current` (may have been updated)
   - Corrects for off-policy learning when reusing old completions

**Loss computation:**
- Loss = Advantage √ó Importance Sampling Ratio (with clipping)
- **No ground truth comparison** - purely reward-based learning
- Rewards are sparse (one per completion), broadcast to all tokens
- Each token's loss depends on completion quality (via advantage) and policy change (via importance ratio)

---

## Setup

**Assumptions:**
- 1 prompt
- 3 completions per prompt (`num_generations = 3`)
- Completion lengths: 5, 4, 6 tokens respectively
- Batch size `B = 3` (total completions)

---

## Step 1: Rewards (Sequence-Level, Sparse)

### Reward Computation

Rewards are computed **once per completion** (at the end, after seeing the full completion):

```python
# Reward function evaluates each completion
rewards = [1.0, 0.0, 0.5]  # Shape: (3,)
#           ‚Üë    ‚Üë    ‚Üë
#        comp0 comp1 comp2
```

**Mathematical representation:**

$$
\mathbf{r} = \begin{bmatrix} r_0 \\ r_1 \\ r_2 \end{bmatrix} = \begin{bmatrix} 1.0 \\ 0.0 \\ 0.5 \end{bmatrix}
$$

**Shape:** `(B,) = (3,)` - one scalar per completion

**Key point:** Rewards are **sparse** - computed only once per completion, not per token.

---

## Step 2: Group-Relative Advantages (Sequence-Level)

### Advantage Computation

Advantages are computed from rewards, normalized within the group:

```python
# Group mean
mean_grouped_rewards = rewards.mean()  # = (1.0 + 0.0 + 0.5) / 3 = 0.5

# Group std (for scaling)
std_grouped_rewards = rewards.std()  # ‚âà 0.408

# Advantages (sequence-level)
advantages = (rewards - mean_grouped_rewards) / std_grouped_rewards
# advantages = [(1.0 - 0.5) / 0.408, (0.0 - 0.5) / 0.408, (0.5 - 0.5) / 0.408]
# advantages ‚âà [1.225, -1.225, 0.0]
```

**Mathematical representation:**

$$
\bar{r} = \frac{1}{B} \sum_{i=0}^{B-1} r_i = \frac{1.0 + 0.0 + 0.5}{3} = 0.5
$$

$$
\sigma_r = \sqrt{\frac{1}{B-1} \sum_{i=0}^{B-1} (r_i - \bar{r})^2} \approx 0.408
$$

$$
\mathbf{A} = \begin{bmatrix} A_0 \\ A_1 \\ A_2 \end{bmatrix} = \begin{bmatrix} \frac{r_0 - \bar{r}}{\sigma_r} \\ \frac{r_1 - \bar{r}}{\sigma_r} \\ \frac{r_2 - \bar{r}}{\sigma_r} \end{bmatrix} \approx \begin{bmatrix} 1.225 \\ -1.225 \\ 0.0 \end{bmatrix}
$$

**Shape:** `(B,) = (3,)` - one scalar per completion

**Key point:** Advantages are **sequence-level** - one value per completion.

---

## Step 3: Prepare Advantages for Token-Level Broadcasting

### Unsqueeze Operation

Advantages are unsqueezed to enable broadcasting to tokens:

```python
# Original advantages
advantages = [1.225, -1.225, 0.0]  # Shape: (3,)

# After unsqueeze
advantages = advantages.unsqueeze(1)  # Shape: (3, 1)
# advantages = [[1.225], [-1.225], [0.0]]
```

**Mathematical representation:**

$$
\mathbf{A} = \begin{bmatrix} A_0 \\ A_1 \\ A_2 \end{bmatrix} \rightarrow \mathbf{A}_{broadcast} = \begin{bmatrix} A_0 \\ A_1 \\ A_2 \end{bmatrix} = \begin{bmatrix} 1.225 \\ -1.225 \\ 0.0 \end{bmatrix}
$$

**Shape:** `(B, 1) = (3, 1)` - ready for broadcasting

---

## Step 4: Per-Token Log Probabilities

### Token-Level Log Probs

For each completion, we compute log probabilities for each token:

```python
# Completion 0: 5 tokens
per_token_logps_0 = [log_p(token_0), log_p(token_1), log_p(token_2), log_p(token_3), log_p(token_4)]
# Example values:
per_token_logps_0 = [-2.1, -1.8, -2.3, -1.9, -2.0]  # Shape: (5,)

# Completion 1: 4 tokens
per_token_logps_1 = [-2.2, -1.7, -2.4, -2.1]  # Shape: (4,)

# Completion 2: 6 tokens
per_token_logps_2 = [-2.0, -1.9, -2.2, -1.8, -2.3, -2.1]  # Shape: (6,)
```

**After padding to max length (6 tokens):**

```python
per_token_logps = [
    [-2.1, -1.8, -2.3, -1.9, -2.0, -100],  # comp0, padded
    [-2.2, -1.7, -2.4, -2.1, -100, -100],  # comp1, padded
    [-2.0, -1.9, -2.2, -1.8, -2.3, -2.1]   # comp2
]  # Shape: (3, 6)
```

**Mathematical representation:**

$$
\mathbf{L} = \begin{bmatrix}
L_{0,0} & L_{0,1} & L_{0,2} & L_{0,3} & L_{0,4} & -\infty \\
L_{1,0} & L_{1,1} & L_{1,2} & L_{1,3} & -\infty & -\infty \\
L_{2,0} & L_{2,1} & L_{2,2} & L_{2,3} & L_{2,4} & L_{2,5}
\end{bmatrix}
$$

**Shape:** `(B, T_max) = (3, 6)` where `T_max = 6` is the maximum sequence length

---

## Step 5: Importance Sampling Ratios (Token-Level)

### Policy Ratio Computation

For each token, compute the importance sampling ratio:

```python
# Old log probs (from generation time)
old_per_token_logps = per_token_logps.detach()  # Shape: (3, 6)

# Current log probs (from current forward pass)
current_per_token_logps = per_token_logps  # Shape: (3, 6)

# Log ratio
log_ratio = current_per_token_logps - old_per_token_logps  # Shape: (3, 6)

# Example (assuming small changes):
log_ratio = [
    [0.1, 0.05, -0.1, 0.08, 0.02, 0],  # comp0
    [-0.05, 0.1, -0.08, 0.05, 0, 0],   # comp1
    [0.08, -0.05, 0.1, 0.05, -0.08, 0.1]  # comp2
]

# Importance sampling ratio
coef_1 = exp(log_ratio)  # Shape: (3, 6)
```

**Mathematical representation:**

$$
\rho_{i,t} = \exp\left(\log \pi_\theta(o_{i,t} \mid q, o_{i,1:t-1}) - \log \pi_{\theta_{\mathrm{old}}}(o_{i,t} \mid q, o_{i,1:t-1})\right)
$$

where $o_{i,1:t-1}$ denotes tokens from position 1 to $t-1$ in completion $i$, and $\pi_{\theta_{\mathrm{old}}}$ denotes the old policy (with parameters $\theta_{\mathrm{old}}$).

$$
\boldsymbol{\rho} = \begin{bmatrix}
\rho_{0,0} & \rho_{0,1} & \rho_{0,2} & \rho_{0,3} & \rho_{0,4} & 1 \\
\rho_{1,0} & \rho_{1,1} & \rho_{1,2} & \rho_{1,3} & 1 & 1 \\
\rho_{2,0} & \rho_{2,1} & \rho_{2,2} & \rho_{2,3} & \rho_{2,4} & \rho_{2,5}
\end{bmatrix}
$$

**Shape:** `(B, T_max) = (3, 6)` - token-level

---

## Step 6: Token-Level Loss Computation

### Broadcasting Advantages to Tokens

Advantages are broadcast to match token dimensions:

```python
# Advantages (sequence-level, broadcast-ready)
advantages = [[1.225], [-1.225], [0.0]]  # Shape: (3, 1)

# Broadcast to token-level
advantages_broadcast = advantages.expand(-1, 6)  # Shape: (3, 6)
# advantages_broadcast = [
#     [1.225, 1.225, 1.225, 1.225, 1.225, 1.225],  # comp0: all tokens get same advantage
#     [-1.225, -1.225, -1.225, -1.225, -1.225, -1.225],  # comp1: all tokens get same advantage
#     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  # comp2: all tokens get same advantage
# ]
```

**Mathematical representation:**

$$
\mathbf{A}_{broadcast} = \begin{bmatrix}
A_0 & A_0 & A_0 & A_0 & A_0 & A_0 \\
A_1 & A_1 & A_1 & A_1 & A_1 & A_1 \\
A_2 & A_2 & A_2 & A_2 & A_2 & A_2
\end{bmatrix} = \begin{bmatrix}
1.225 & 1.225 & 1.225 & 1.225 & 1.225 & 1.225 \\
-1.225 & -1.225 & -1.225 & -1.225 & -1.225 & -1.225 \\
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0
\end{bmatrix}
$$

**Key insight:** Each token in a completion receives the **same advantage value** (the completion's advantage).

### Loss Computation

**‚ö†Ô∏è Critical Understanding: This is NOT a traditional supervised loss!**

The loss is **purely reward-based** - there is NO ground truth comparison:

```python
# Clipped importance sampling ratio
coef_2 = clamp(coef_1, 1 - epsilon, 1 + epsilon)  # epsilon = 0.2
# coef_2 = clamp(coef_1, 0.8, 1.2)

# Per-token loss components
# Loss = Advantage √ó Importance Sampling Ratio
# There is NO comparison to ground truth tokens!
per_token_loss1 = coef_1 * advantages_broadcast  # Shape: (3, 6)
per_token_loss2 = coef_2 * advantages_broadcast  # Shape: (3, 6)

# Per-token loss (clipped surrogate)
# This tells the model: "increase/decrease token probabilities based on completion quality"
per_token_loss = -min(per_token_loss1, per_token_loss2)  # Shape: (3, 6)
```

**What this means**:
- **No ground truth**: Unlike supervised learning, we don't compare each token to a target token
- **Reward signal only**: The loss is computed as: `Loss = Advantage √ó Importance_Ratio`
- **Broadcast reward**: The completion's reward (via advantage) is broadcast to ALL tokens in that completion
- **Token-level learning**: Each token's probability is adjusted based on:
  - The completion's overall reward (via advantage)
  - How much the policy changed for that token (importance sampling ratio)
- **Good completion** (advantage = +1.225): All tokens get positive signal ‚Üí increase probabilities
- **Bad completion** (advantage = -1.225): All tokens get negative signal ‚Üí decrease probabilities

**Mathematical representation:**

For each token `(i, t)`:

$$
l_{i,t} = -\min\left( \rho_{i,t} \cdot A_i, \operatorname{clip}(\rho_{i,t}, 1-\epsilon, 1+\epsilon) \cdot A_i \right)
$$

$$
\mathbf{L}_{loss} = \begin{bmatrix}
l_{0,0} & l_{0,1} & l_{0,2} & l_{0,3} & l_{0,4} & l_{0,5} \\
l_{1,0} & l_{1,1} & l_{1,2} & l_{1,3} & l_{1,4} & l_{1,5} \\
l_{2,0} & l_{2,1} & l_{2,2} & l_{2,3} & l_{2,4} & l_{2,5}
\end{bmatrix}
$$

**Shape:** `(B, T_max) = (3, 6)` - **token-level loss**

**Key point:** Loss is computed **per token**, but each token in a completion uses the **same advantage** (the completion's advantage).

---

## Step 7: Loss Aggregation

### Mask Out Padding

```python
# Completion mask (1 for real tokens, 0 for padding)
completion_mask = [
    [1, 1, 1, 1, 1, 0],  # comp0: 5 real tokens
    [1, 1, 1, 1, 0, 0],  # comp1: 4 real tokens
    [1, 1, 1, 1, 1, 1]   # comp2: 6 real tokens
]  # Shape: (3, 6)

# Masked loss
masked_loss = per_token_loss * completion_mask  # Shape: (3, 6)
```

### Aggregate to Per-Completion Loss

```python
# Per-completion loss (sum tokens, normalize by length)
per_completion_loss = masked_loss.sum(-1) / completion_mask.sum(-1)  # Shape: (3,)
# per_completion_loss = [
#     (l_0,0 + l_0,1 + l_0,2 + l_0,3 + l_0,4) / 5,  # comp0
#     (l_1,0 + l_1,1 + l_1,2 + l_1,3) / 4,           # comp1
#     (l_2,0 + l_2,1 + l_2,2 + l_2,3 + l_2,4 + l_2,5) / 6  # comp2
# ]
```

**Mathematical representation:**

$$
\bar{l}_i = \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} l_{i,t}
$$

$$
\bar{\mathbf{L}} = \begin{bmatrix} \bar{l}_0 \\ \bar{l}_1 \\ \bar{l}_2 \end{bmatrix}
$$

**Shape:** `(B,) = (3,)` - per-completion loss

### Final Scalar Loss

```python
# Final scalar loss (average across completions)
final_loss = per_completion_loss.mean()  # Shape: () - scalar
# final_loss = (l_0 + l_1 + l_2) / 3
```

**Mathematical representation:**

$$
\mathcal{L} = \frac{1}{B} \sum_{i=0}^{B-1} \bar{l}_i = \frac{1}{B} \sum_{i=0}^{B-1} \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} l_{i,t}
$$

**Shape:** `()` - scalar

---

## Complete Flow Summary

```
Rewards (sequence-level, sparse):
  r = [1.0, 0.0, 0.5]  (B,)

    ‚Üì Group-relative normalization

Advantages (sequence-level):
  A = [1.225, -1.225, 0.0]  (B,)

    ‚Üì Unsqueeze for broadcasting

Advantages (broadcast-ready):
  A = [[1.225], [-1.225], [0.0]]  (B, 1)

    ‚Üì Broadcast to tokens

Advantages (token-level, broadcast):
  A = [[1.225, 1.225, 1.225, 1.225, 1.225, 1.225],
       [-1.225, -1.225, -1.225, -1.225, -1.225, -1.225],
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]  (B, T)

    ‚Üì Multiply with importance ratios

Per-token loss:
  L_loss = [[l_0,0, l_0,1, l_0,2, l_0,3, l_0,4, 0],
            [l_1,0, l_1,1, l_1,2, l_1,3, 0, 0],
            [l_2,0, l_2,1, l_2,2, l_2,3, l_2,4, l_2,5]]  (B, T)

    ‚Üì Aggregate per completion

Per-completion loss:
  L_bar = [l_0, l_1, l_2]  (B,)

    ‚Üì Average

Final scalar loss:
  L = mean([l_0, l_1, l_2])  ()
```

---

## Key Insights

1. **Rewards are sparse**: One scalar per completion, computed at the end (sequence-level)
2. **Advantages are group-relative**: Advantages are computed relative to the **group average** (mean reward for all completions of the same prompt)
   - Formula: `A_i = r_i - mean(r_group)`
   - This enables comparative learning: "Is this completion better than others for the same prompt?"
3. **Advantages are broadcast**: The same advantage value is applied to all tokens in a completion
4. **Loss is reward-based, not ground-truth-based**: The loss does NOT compare tokens to ground truth. Instead:
   - Loss = Advantage √ó Importance Sampling Ratio (with clipping)
   - Each token's loss depends on:
     - The completion's overall reward (via advantage, which is group-relative)
     - The token's importance sampling ratio (how much the policy changed)
   - Good completions (positive advantage) ‚Üí negative loss ‚Üí increase token probabilities
   - Bad completions (negative advantage) ‚Üí positive loss ‚Üí decrease token probabilities
5. **All tokens in a completion share the same advantage**: This is the key insight - the advantage reflects the overall quality of the completion relative to its group, not individual tokens
6. **No direct token-level supervision**: Unlike supervised learning, there's no per-token ground truth comparison. The model learns purely from the reward signal.
7. **Importance sampling ratio is relative to old log probs**: 
   - `old_per_token_logps`: Computed right after generation with `Œ∏_gen` (generation-time weights)
   - `per_token_logps`: Computed during training with `Œ∏_current` (current weights, may have been updated)
   - Ratio: `œÅ = œÄ_Œ∏_current / œÄ_Œ∏_gen` - measures how much the policy changed since generation

---

## Why This Design?

- **Rewards are naturally sequence-level**: Most reward functions evaluate the entire completion (e.g., accuracy, format)
- **Group-relative advantages normalize across prompts**: Different prompts may have different difficulty/reward scales. By comparing within groups, we normalize this bias
- **Token-level loss enables fine-grained learning**: Each token's probability is adjusted based on the completion's overall quality (relative to its group)
- **Broadcasting is efficient**: One advantage computation per completion, applied to all tokens
- **Reward-based learning**: Unlike supervised learning that compares each token to ground truth, RL-based training uses the reward signal to guide learning:
  - If a completion gets a high reward relative to its group ‚Üí all its tokens get positive signal ‚Üí increase their probabilities
  - If a completion gets a low reward relative to its group ‚Üí all its tokens get negative signal ‚Üí decrease their probabilities
  - The importance sampling ratio ensures we only update tokens proportionally to how much the policy changed since generation

## Timing of Log Probabilities

### When are they computed?

**`old_per_token_logps`**:
- **When**: Right after generation (in `_generate_and_score_completions`)
- **Model weights**: `Œ∏_gen` (same as generation-time weights)
- **Purpose**: Capture what the generation-time policy thought about these completions
- **Stored**: Saved with completions for reuse across multiple training steps

**`per_token_logps`**:
- **When**: During training (in `_compute_loss`)
- **Model weights**: `Œ∏_current` (may have been updated multiple times since generation)
- **Purpose**: Capture what the current policy thinks about the same completions
- **Computed**: Fresh forward pass with current weights

### Timeline Example

```
Step 0 (Generation):
  Model weights: Œ∏‚ÇÄ
  ‚Üí Generate completions with Œ∏‚ÇÄ
  ‚Üí Compute old_per_token_logps with Œ∏‚ÇÄ
  ‚Üí Store: [completions, old_per_token_logps] (both from Œ∏‚ÇÄ)

Step 1 (Training):
  Model weights: Œ∏‚ÇÅ (updated)
  ‚Üí Retrieve stored completions (from Œ∏‚ÇÄ)
  ‚Üí Compute per_token_logps with Œ∏‚ÇÅ
  ‚Üí Importance ratio: œÄ_Œ∏‚ÇÅ / œÄ_Œ∏‚ÇÄ
  ‚Üí Update model: Œ∏‚ÇÅ ‚Üí Œ∏‚ÇÇ

Step 2 (Training):
  Model weights: Œ∏‚ÇÇ (updated again)
  ‚Üí Retrieve same stored completions (still from Œ∏‚ÇÄ)
  ‚Üí Compute per_token_logps with Œ∏‚ÇÇ
  ‚Üí Importance ratio: œÄ_Œ∏‚ÇÇ / œÄ_Œ∏‚ÇÄ  ‚Üê Still comparing to Œ∏‚ÇÄ!
  ‚Üí Update model: Œ∏‚ÇÇ ‚Üí Œ∏‚ÇÉ

Step 3 (Training):
  Model weights: Œ∏‚ÇÉ
  ‚Üí Retrieve same stored completions (still from Œ∏‚ÇÄ)
  ‚Üí Compute per_token_logps with Œ∏‚ÇÉ
  ‚Üí Importance ratio: œÄ_Œ∏‚ÇÉ / œÄ_Œ∏‚ÇÄ

Step 4 (NEW Generation):
  Model weights: Œ∏‚ÇÑ
  ‚Üí Generate NEW completions with Œ∏‚ÇÑ
  ‚Üí Compute old_per_token_logps with Œ∏‚ÇÑ
  ‚Üí Store: [new_completions, old_per_token_logps] (both from Œ∏‚ÇÑ)
  ‚Üí Cycle repeats...
```

**Key insight**: `old_per_token_logps` is computed with generation-time weights (`Œ∏_gen`), stored, and reused. `per_token_logps` is computed with current weights (`Œ∏_current`) during training. The importance sampling ratio corrects for the policy drift between generation and training.

---

## Complete Summary

### Two Relative Measures in GRPO

1. **Group-Relative Advantages**:
   - Formula: `A_i = r_i - mean(r_group)`
   - Group = all completions for the same prompt
   - Advantages normalize rewards within each prompt group
   - Enables comparative learning: "Is this completion better than others for the same prompt?"

2. **Policy-Relative Importance Sampling Ratio**:
   - Formula: `œÅ = œÄ_Œ∏_current / œÄ_Œ∏_gen`
   - Measures how much the policy changed since generation
   - `old_per_token_logps`: Computed right after generation with `Œ∏_gen`
   - `per_token_logps`: Computed during training with `Œ∏_current` (updated weights)
   - Corrects for off-policy learning when reusing old completions

### Loss Computation Flow

```
Rewards (sparse, sequence-level)
    ‚Üì
Group-relative normalization
    ‚Üì
Advantages (sequence-level, relative to group mean)
    ‚Üì
Broadcast to tokens
    ‚Üì
Multiply with Importance Sampling Ratio (policy-relative)
    ‚Üì
Token-level loss (reward-based, NOT ground-truth-based)
    ‚Üì
Aggregate per completion
    ‚Üì
Final scalar loss
```

### Key Takeaways

- ‚úÖ **Rewards**: Sparse, one scalar per completion
- ‚úÖ **Advantages**: Group-relative (normalized within prompt groups)
- ‚úÖ **Importance Ratio**: Policy-relative (measures policy change)
- ‚úÖ **Loss**: Reward-based, no ground truth comparison
- ‚úÖ **Timing**: `old_logprobs` from generation-time weights, `current_logprobs` from training-time weights

---

