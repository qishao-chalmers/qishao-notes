---
title: GPU Workload Scheduling
date: 2024-01-08
permalink: /pages/45896/
---

1. [1 2024] Uncovering Real GPU NoC Chara cteristics:Implications on Interconnect Architecture

---

The paper, "Uncovering Real GPU NoC Characteristics: Implications on Interconnect Architecture", thoroughly analyzes the network-on-chip (NoC) architectures of modern GPUs from NVIDIA (V100, A100, H100), examining critical aspects such as latency and bandwidth, and discusses key architectural implications.

Here is a profound explanation of the key observations and findings from this significant work:

### Key Observations and Findings:
#### 1. Non-uniform Latency:

Core-L2 slice latency varies significantly based on their physical placement within GPUs.

For instance, the V100 GPU shows a latency variation up to 71 cycles (about 33%), depending on the proximity of the SM (Streaming Multiprocessor) to the specific L2 slice accessed.

Physical location heavily influences latency, thus emphasizing the importance of core and memory slice placement strategies in GPU architecture.

#### 2. Impact of Physical Placement:

Detailed analysis using Pearson correlation identifies that SMs within the same GPU cluster (GPC) tend to have highly correlated latency patterns.

This characteristic allows reverse-engineering of physical SM placement using latency patterns, presenting opportunities for optimization and potential security vulnerabilities (such as side-channel attacks).

#### 3. Hierarchy-Induced Non-uniformity (A100, H100 GPUs):

Recent GPUs (A100, H100) introduce multi-partition architectures, significantly amplifying latency non-uniformities due to their increased complexity and larger physical size.

H100 introduces an intermediate hierarchy known as Compute Processing Cluster (CPC), causing further latency differentiation within a GPU cluster (GPC). This hierarchical change is notable in how cores within the same GPC can exhibit varied latency characteristics depending on their CPC placement.

#### 4. Latency Crossing GPU Partitions:

Latency significantly increases when accesses cross GPU partitions, especially evident in A100, where latency to a remote partition nearly doubles from approximately 212 cycles to around 400 cycles.

H100 partially mitigates this issue through a localized caching policy, thus providing more uniform latency compared to A100 when accessing different partitions.

#### 5. On-chip Bandwidth Uniformity and Non-uniformity:

Interestingly, while latency shows clear non-uniformity, bandwidth remains relatively uniform across different cores and L2 slices.

Bandwidth uniformity ensures reliable performance irrespective of SM or L2 slice placement, crucial for balanced parallel execution.

#### 6. Increasing On-chip Bandwidth vs. Memory Bandwidth:

The total aggregate on-chip L2 bandwidth far exceeds off-chip memory bandwidth (by 2.4× to 3.5×).

Such high internal bandwidth is necessary to prevent performance bottlenecks, emphasizing the critical role of NoC architecture design.

#### 7. Hierarchical Bandwidth Speedup:

The study identifies hierarchical speedup strategies (TPC, GPC levels) implemented by GPUs to ensure sufficient bandwidth.

Recent GPUs provide substantial speedup improvements compared to earlier generations, although non-uniformities emerge, especially evident when crossing partitions.

#### 8. Load-balancing is Crucial:

Load balancing across cores (SMs) is crucial due to asymmetric speedups at various NoC hierarchy levels.

Conversely, load balancing across L2 slices is less critical, highlighting the need to focus on SM-level workload distribution for optimal performance.

#### 9. Traffic Distribution and Load-Balancing:

Real workloads, analyzed through Rodinia benchmarks, exhibit balanced on-chip traffic, implying that GPU address hashing techniques effectively distribute memory accesses evenly across memory channels and thus NoC traffic.


### Ideas

#### NoC-Aware CTA Scheduler
**Insight used.** Performance collapses when many CTAs run on SMs inside one GPC and hammer a single memory partition because GPC-level speed-up is limited and asymmetric 

**Idea.** Extend Accel-Sim’s thread-block scheduler so that, at launch time, it spreads CTAs across GPCs/partitions to equalise NoC ingress bandwidth.

**Evaluation.** Compare latency-sensitive kernels (e.g., pointer-chasing, Graph workloads) under: (i) stock round-robin scheduling and (ii) your NoC-aware scheme. Measure SM-stall cycles and achieved L2 slice bandwidth.

#### Partition-Aware Memory Allocator for Multi-Tenant GPUs
**Insight used.** Crossing the partition boundary in Ampere doubles L2 hit latency (~212 → ~400 cy) and throttles single-SM bandwidth (39 → 26 GB s⁻¹) .

**Idea.** In a multi-application setting, modify the simulated CUDA malloc to prefer pages whose L2 slice lives on the same partition as the requesting SM set.

**Evaluation.** Run two co-located DL inference kernels. Track tail latency and fairness with vs. without partition-aware placement.

#### SM-Locality-Driven Cache Compression
**Insight used.** SM-to-L2 latency is lowest when the slice is physically near the SM; but bandwidth uniformity means compressed lines mainly help latency, not BW.

**Idea.** Implement a simple line-level compressor in the L1. When compression ratio ≥ R, cache the line in the nearest L2 slice (distance calculated from the paper’s measured latency heat-map).

**Evaluation.** Compare average memory stall cycles of SPEC/GPU workloads with: (i) baseline uncompressed, (ii) random L2 placement of compressed data, (iii) locality-aware placement.

#### Dynamic Prefetch Throttling for Irregular Accesses
**Insight used.** Only four SMs can saturate a single L2 slice; beyond that, latency not bandwidth is the bottleneck.

**Idea.** Add a prefetch controller that monitors per-slice queue depth; when > X, it throttles additional irregular prefetches from “far” SMs but allows those from “near” SMs.

**Evaluation.** Use irregular pointer-chase micro-benchmarks; plot IPC vs. prefetch accuracy/coverage.

#### Hierarchical Load-Balancing Policies
**Insight used.** Some GPC-speed-up is “in space” (extra links) rather than “in time”; distributing SMs across more MPs yields 2.18× speed-up over packing them into one MP.

**Idea.** Build a run-time policy that (a) groups CTAs by data locality, then (b) assigns these groups to MPs in a way that maximises the number of distinct MP destinations seen by each GPC.

**Evaluation.** Stress-test with graph analytics and stencil mixes; report overall throughput and worst-case SM idle time.

####  Model-Driven Interconnect Speed-up Exploration
**Insight used.** The paper quantifies TPC, GPC_local, GPC_global and (for H100) CPC speed-ups for reads vs. writes.

**Idea.** Parameterise these speed-ups in Accel-Sim and sweep them to find the knee point where extra NoC ports stop improving perf/area.

**Evaluation.** Produce Pareto curves (perf vs. simulated crossbar area/energy).

####  Compression-Aware Reply Coalescing
**Insight used.** Reply traffic (128-byte cache lines) is the dominant consumer of NoC BW; input speed-up exists but is still limited for writes.

**Idea.** Compress replies on-the-fly (e.g., frequent-value) and coalesce multiple compressed lines into a single flit when returning to an SM group.

**Evaluation.** Instrument Accel-Sim’s interconnect queue; measure flit count reduction and end-to-end energy per memory op.

