---
title: GPU Workload Scheduling
date: 2024-01-08
permalink: /pages/45896/
---

1. [1 2024] Uncovering Real GPU NoC Chara cteristics:Implications on Interconnect Architecture
2. [53 2015] Asymmetric NoC Architectures for GPU Systems

---
## 1. [1 2024] Uncovering Real GPU NoC Chara cteristics:Implications on Interconnect Architecture

The paper, "Uncovering Real GPU NoC Characteristics: Implications on Interconnect Architecture", thoroughly analyzes the network-on-chip (NoC) architectures of modern GPUs from NVIDIA (V100, A100, H100), examining critical aspects such as latency and bandwidth, and discusses key architectural implications.

Here is a profound explanation of the key observations and findings from this significant work:

### Key Observations and Findings:
#### 1. Non-uniform Latency:

Core-L2 slice latency varies significantly based on their physical placement within GPUs.

For instance, the V100 GPU shows a latency variation up to 71 cycles (about 33%), depending on the proximity of the SM (Streaming Multiprocessor) to the specific L2 slice accessed.

Physical location heavily influences latency, thus emphasizing the importance of core and memory slice placement strategies in GPU architecture.

**Latency**: Access latency from Streaming Multiprocessors (SMs) to L2 cache slices is highly non-uniform, varying by up to ~70% (e.g., 175 to 248 cycles on V100).
This variation is primarily determined by the physical location of the SM within its Graphics Processing Cluster (GPC) and the L2 slice within its Memory Partition (MP). SMs and L2 slices that are physically closer have significantly lower latency.

**Bandwidth**: In contrast, the bandwidth provided from cores (SMs or GPCs) to different L2 slices is remarkably uniform. The average bandwidth per SM-to-L2 slice link is ~34 GB/s, and aggregate bandwidth scales predictably with the number of sources.

#### 2. Impact of Physical Placement:

Detailed analysis using Pearson correlation identifies that SMs within the same GPU cluster (GPC) tend to have highly correlated latency patterns.

This characteristic allows reverse-engineering of physical SM placement using latency patterns, presenting opportunities for optimization and potential security vulnerabilities (such as side-channel attacks).

**Impact of GPU Partitions (A100/H100)**: Modern large GPUs are split into multiple "partitions" (left/right).
**Cross-Partition Latency**: Communication across these partitions introduces significant additional latency (~400 cycles vs. ~212 cycles for intra-partition on A100).
**New Hierarchy (H100)**: The H100 exhibits an additional, previously undocumented hierarchy between the TPC and GPC levels, termed Compute Processing Clusters (CPCs). Latency varies based on CPC distance.
**H100 Optimization**: H100 employs optimizations where L2 caches data locally for SMs directly connected to its partition, leading to more uniform L2 hit latency compared to A100, despite the partition structure.
**Cross-Partition Bandwidth**: Bandwidth to L2 slices in the "far" partition is significantly lower than to those in the "near" partition (e.g., ~26 GB/s vs. ~39.5 GB/s on A100). This non-uniformity diminishes as more SMs saturate the link.

#### 3. Hierarchy-Induced Non-uniformity (A100, H100 GPUs):

Recent GPUs (A100, H100) introduce multi-partition architectures, significantly amplifying latency non-uniformities due to their increased complexity and larger physical size.

H100 introduces an intermediate hierarchy known as Compute Processing Cluster (CPC), causing further latency differentiation within a GPU cluster (GPC). This hierarchical change is notable in how cores within the same GPC can exhibit varied latency characteristics depending on their CPC placement.

#### 4. Latency Crossing GPU Partitions:

Latency significantly increases when accesses cross GPU partitions, especially evident in A100, where latency to a remote partition nearly doubles from approximately 212 cycles to around 400 cycles.

H100 partially mitigates this issue through a localized caching policy, thus providing more uniform latency compared to A100 when accessing different partitions.

#### 5. On-chip Bandwidth Uniformity and Non-uniformity:

Interestingly, while latency shows clear non-uniformity, bandwidth remains relatively uniform across different cores and L2 slices.

Bandwidth uniformity ensures reliable performance irrespective of SM or L2 slice placement, crucial for balanced parallel execution.

#### 6. Increasing On-chip Bandwidth vs. Memory Bandwidth:

The total aggregate on-chip L2 bandwidth far exceeds off-chip memory bandwidth (by 2.4× to 3.5×).

Such high internal bandwidth is necessary to prevent performance bottlenecks, emphasizing the critical role of NoC architecture design.

#### 7. Hierarchical Bandwidth Speedup:

The study identifies hierarchical speedup strategies (TPC, GPC levels) implemented by GPUs to ensure sufficient bandwidth.

Recent GPUs provide substantial speedup improvements compared to earlier generations, although non-uniformities emerge, especially evident when crossing partitions.

#### 8. Load-balancing is Crucial:

Load balancing across cores (SMs) is crucial due to asymmetric speedups at various NoC hierarchy levels.

Conversely, load balancing across L2 slices is less critical, highlighting the need to focus on SM-level workload distribution for optimal performance.

The real bottleneck arises when **you concentrate all your kernel's threads onto SMs within only one or a few GPCs, even if you're accessing many L2 slices**.

<img width="524" height="305" alt="image" src="https://github.com/user-attachments/assets/b1683351-a1dc-424e-a117-1125fbd940f1" />


**The Bottleneck**

The total bandwidth available from a single GPC to an MP is limited by its own speedup factor.

If too many SMs (e.g., 28 SMs as in Figure 15(b)) try to send data to a single MP through only one GPC, they hit this speedup limit.

Performance Degradation: As shown in Figure 15(b), concentrating 28 SMs into just 2 GPCs caused a massive ~62% performance degradation compared to spreading them across 6 GPCs.

The limiting factor was not the number of L2 slices being accessed, but the fact that the traffic was funneled through too few GPCs, overwhelming their maximum output bandwidth despite having ample L2 slice capacity.

#### 9. Traffic Distribution and Load-Balancing:

Real workloads, analyzed through Rodinia benchmarks, exhibit balanced on-chip traffic, implying that GPU address hashing techniques effectively distribute memory accesses evenly across memory channels and thus NoC traffic.


### Ideas

#### NoC-Aware CTA Scheduler
**Insight used.** Performance collapses when many CTAs run on SMs inside one GPC and hammer a single memory partition because GPC-level speed-up is limited and asymmetric 

**Idea.** Extend Accel-Sim’s thread-block scheduler so that, at launch time, it spreads CTAs across GPCs/partitions to equalise NoC ingress bandwidth.

**Evaluation.** Compare latency-sensitive kernels (e.g., pointer-chasing, Graph workloads) under: (i) stock round-robin scheduling and (ii) your NoC-aware scheme. Measure SM-stall cycles and achieved L2 slice bandwidth.

#### Partition-Aware Memory Allocator for Multi-Tenant GPUs
**Insight used.** Crossing the partition boundary in Ampere doubles L2 hit latency (~212 → ~400 cy) and throttles single-SM bandwidth (39 → 26 GB s⁻¹) .

**Idea.** In a multi-application setting, modify the simulated CUDA malloc to prefer pages whose L2 slice lives on the same partition as the requesting SM set.

**Evaluation.** Run two co-located DL inference kernels. Track tail latency and fairness with vs. without partition-aware placement.

#### SM-Locality-Driven Cache Compression
**Insight used.** SM-to-L2 latency is lowest when the slice is physically near the SM; but bandwidth uniformity means compressed lines mainly help latency, not BW.

**Idea.** Implement a simple line-level compressor in the L1. When compression ratio ≥ R, cache the line in the nearest L2 slice (distance calculated from the paper’s measured latency heat-map).

**Evaluation.** Compare average memory stall cycles of SPEC/GPU workloads with: (i) baseline uncompressed, (ii) random L2 placement of compressed data, (iii) locality-aware placement.

#### Dynamic Prefetch Throttling for Irregular Accesses
**Insight used.** Only four SMs can saturate a single L2 slice; beyond that, latency not bandwidth is the bottleneck.

**Idea.** Add a prefetch controller that monitors per-slice queue depth; when > X, it throttles additional irregular prefetches from “far” SMs but allows those from “near” SMs.

**Evaluation.** Use irregular pointer-chase micro-benchmarks; plot IPC vs. prefetch accuracy/coverage.

#### Hierarchical Load-Balancing Policies
**Insight used.** Some GPC-speed-up is “in space” (extra links) rather than “in time”; distributing SMs across more MPs yields 2.18× speed-up over packing them into one MP.

**Idea.** Build a run-time policy that (a) groups CTAs by data locality, then (b) assigns these groups to MPs in a way that maximises the number of distinct MP destinations seen by each GPC.

**Evaluation.** Stress-test with graph analytics and stencil mixes; report overall throughput and worst-case SM idle time.

####  Model-Driven Interconnect Speed-up Exploration
**Insight used.** The paper quantifies TPC, GPC_local, GPC_global and (for H100) CPC speed-ups for reads vs. writes.

**Idea.** Parameterise these speed-ups in Accel-Sim and sweep them to find the knee point where extra NoC ports stop improving perf/area.

**Evaluation.** Produce Pareto curves (perf vs. simulated crossbar area/energy).

####  Compression-Aware Reply Coalescing
**Insight used.** Reply traffic (128-byte cache lines) is the dominant consumer of NoC BW; input speed-up exists but is still limited for writes.

**Idea.** Compress replies on-the-fly (e.g., frequent-value) and coalesce multiple compressed lines into a single flit when returning to an SM group.

**Evaluation.** Instrument Accel-Sim’s interconnect queue; measure flit count reduction and end-to-end energy per memory op.


---

## 2. [53 2015] Asymmetric NoC Architectures for GPU Systems

### 1. Asymmetric Channel Widths: Matching Hardware to Real Traffic

Observation: The paper found that traffic between L1 caches and the L2 cache is highly asymmetric.

**L2-to-L1 traffic (data reads) is much heavier and more critical for performance than L1-to-L2 traffic (writes, invalidations).**

Design Change: Instead of using wide channels in both directions (symmetric), the "asym" designs use:

*A narrower channel for L1-to-L2 communication (e.g., 16x8 vs 22x8 bytes).*

*A wider channel for the critical L2-to-L1 path.*

Impact on EDP:
- Massive Power Savings: Narrower channels mean fewer wires, smaller drivers, and less dynamic power. This directly reduces total power consumption by 37% for MeshX2-asym and 69% for CmeshX2-asym compared to their symmetric baselines.
- Minimal Performance Cost: Since the L1-to-L2 path is less critical, narrowing it causes only a small performance drop (e.g., 92-93% of baseline mesh, 97% of baseline Cmesh). This small loss is offset by other benefits.
🔋 Why this reduces EDP: EDP = Energy * (Delay)^2. By cutting energy (power) drastically while only slightly increasing delay (reducing performance), the overall EDP plummets. 

### 2. Elimination of Unused Links & Reduced Router Radix

**The authors made a second crucial observation: direct communication between L1 caches is rare in typical GPU workloads.**

- Design Change: They completely removed the links that would allow direct L1-to-L1 communication. All inter-L1 data must now go through the L2 cache.
Impact on EDP:
- Reduced Static Power: Fewer physical wires mean lower static (leakage) power.
- Simpler Routers: Removing these links significantly reduces the number of input/output ports (radix) on the routers. For example, router radix was reduced from 10x10 to 8x4. Simpler routers consume less area and power.
- Less Congestion: Having two separate networks (one for each direction) reduces contention and arbitration latency within the network itself.
🔧 Why this reduces EDP: This optimization further slashes power consumption without hurting performance, contributing heavily to the overall energy savings. 

### 3. Exploiting Application-Level Asymmetry for Maximum Efficiency
The final piece is understanding why the above changes are so effective.

Performance is Bottlenecked by Critical Path:

**The start of execution for a wavefront of threads depends on when its data arrives from the L2 cache.**

Therefore, L2-to-L1 latency is the critical path.

- Non-Critical Paths Can Be Slowed Down: Delays in sending writebacks or control messages (L1-to-L2) have a much smaller impact on overall application throughput.
- Optimization Strategy: The asymmetric design allows the system to:
- Prioritize the Critical Path: Keep the L2-to-L1 path fast and high-bandwidth.
- Downgrade the Non-Critical Path: Make the L1-to-L2 path narrower and slower, saving immense power.

This creates an optimal trade-off: you get nearly all the performance of a fully provisioned symmetric network at a fraction of the energy cost.


