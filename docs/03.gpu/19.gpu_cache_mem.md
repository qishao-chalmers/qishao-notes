---
title: GPU Cache & Memory Hirerarchy
date: 2024-08-25
permalink: /pages/45876/
---

1. [248] Dissecting GPU Memory Hierarchy through Microbenchmarking
2. [75] Benchmarking the Memory Hierarchy of Modern GPUs
3. [18] Benchmarking the GPU memory at the warp level
4. [90] Dissecting the NVidia Turing T4 GPU via Microbenchmarking
5. [38] Exploring Modern GPU Memory System Design Challenges through Accurate Modeling :+1: :+1: :+1:
6. [9] OSM: Off-Chip Shared Memory for GPUs
7. [10] Demystifying GPU UVM Cost with Deep Runtime and Workload Analysis  :+1: :+1: :+1:
8. [New2025] Evaluating Performance of Modern GPU with Partitioned Last-Level Caches
9. [21 2023] Morpheus: Extending the Last Level Cache Capacity in GPU Systems Using Idle GPU Core Resources

---

Tags: #dissecting gpu

## 1. Dissecting GPU Memory Hierarchy through Microbenchmarking
A paper in 2015, profile memory in Fermi, Kepler and Maxwell

![image](https://github.com/user-attachments/assets/683d67af-3feb-4d35-9ecf-dfeafb814c37)

### Parameter
![image](https://github.com/user-attachments/assets/08215b14-4856-4d3a-8c6a-b5050f905f02)

![image](https://github.com/user-attachments/assets/5daed100-0155-4fed-9358-e26681294b2a)

![image](https://github.com/user-attachments/assets/60213279-226b-4a30-aa05-36271e9ac0ff)

### L1 Data Cache
On the Fermi and Kepler devices, the L1 data cache and shared memory are physically implemented together.\
On the Maxwell devices, the L1 data cache is unified with the texture cache.

The 16 KB L1 cache has 128 cache lines mapped onto four cache ways.\
For each cache way, 32 cache sets are divided into 8 major sets. Each major set contains 16 cache lines.

The data mapping is also unconventional.\
The 12-13th bits in the memory address define the cache way, the 9-11th bits define the major set, and the 0-6th bits define the memory offset inside the cache line.
![image](https://github.com/user-attachments/assets/f997bf94-4b5b-4948-882c-7f72dd7bd506)

One distinctive feature of the Fermi L1 cache is that its replacement policy is not LRU, as pointed out by Meltzer et.al.
Among the four cache ways, cache way 2 is three times more likely to be replaced than the other three cache ways.

**Another paper[4]** We found that when the L1 data cache saturates, Turing randomly evicts 4 consecutive cache lines (128 B).\
We observed that once a block of cache lines are evicted, the second scan will cause more cache lines from the same set to be evicted.

![image](https://github.com/user-attachments/assets/ae6a8abd-7d57-4e0c-98ea-12264a37ae75)


### L2 Data Cache
- The replacement policy of the L2 cache is not LRU
- **The L2 cache line size is 32 bytes** by observing the memory access pattern of overflowing the cache and visiting array element one by one.
- The data mapping is sophisticated and not conventional bits-defined
-  a hardware-level pre-fetching mechanism from the DRAM to the L2 data cache on all three platforms.\
   **The pre-fetching size is about 2/3 of the L2 cache size and the prefetching is sequential. This is deduced from that if we load an array smaller than 2/3 of the L2 data cache size, there is no cold cache miss patterns.**\
   :raising_hand:(Maybe they can cover the gap just by prefetching sequential line.)

### Global Memory
global memory access involves accessing the DRAM, L1 and L2 data caches, TLBs and page tables.

### Global Memory Throughput
The theoretical bandwidth is calculated as fmem * bus width * DDR factor.
![image](https://github.com/user-attachments/assets/dbb8cdc6-e0cd-4bc6-aec8-f9450ea6d0bf)

![image](https://github.com/user-attachments/assets/707f8b05-88e6-40b2-b3e4-426f984d4405)

the throughput of a larger ILP saturates faster.

The GTX780 has the highest throughput as it benefits from the highest bus width,\
but its convergence speed is the slowest, i.e., it requires the most memory requests to hide the pipeline latency.

**This could be part of the reason that NVIDIA reduced the bus width back to 256 bits in Maxwell devices.**

### Global Memory Latency

**The global memory access latency is the whole time accessing a data located in DRAM/L2 or L1 cache, including the latency of page table look-ups.**

- very large s1 = 32 MB to construct the TLB/page table miss and cache miss (P5&P6)
- set s2 = 1 MB to construct the L1 TLB hit but cache miss (P4)
- After a total of 65 data accesses, 65 data lines are loaded into the cache.\
  We then visit the cached data lines with s1 again for several times, to construct cache hit but TLB miss (P2&P3).
- set s3 = 1 element and repeatedly load the data in a cache line so that every memory access is a cache hit (P1).

![image](https://github.com/user-attachments/assets/e5c397e8-f4b4-46ba-b7ee-41c34fa08b33)

![image](https://github.com/user-attachments/assets/1f616c8e-a758-4151-b1eb-61f15c810246)

![image](https://github.com/user-attachments/assets/822fc563-d0dc-4708-afd2-89549adb7ec4)

- The Maxwell and Kepler devices have a unique memory access pattern (P6) for page table context switching. \
  When a kernel is launched, only memory page entries of 512 MB are activated. \
  If the thread visits an inactivate page entry, the hardware needs a rather long time to switch between page tables.\
  This phenomena is also reported in [22] as page table “miss”.
- The Maxwell L1 data cache addressing does not go through the TLBs or page tables.\
  On the GTX980, there is no TLB miss pattern (i.e., P2 and P3) when the L1 data cache is hit.\
  Once the L1 cache is missed, the access latency increases from tens of cycles to hundreds or even thousands of cycles.
  **My comments: But if we look at GTX560Ti in P2, the latency is different with P1. So does this means that in Fermi, the memory request has to go through TLB first, and then access L1 DataCache? This might be the reason that the latency is longer. But this will degrade the performance....**
- The TLBs are off-chip. we infer that the physical memory locations of the L1 TLB and L2 data cache are close. \
  The physical memory locations of the L1 TLB and L2 TLB are also close, which means that the L1/L2 TLB and L2 data cache are shared off-chip by all SMs.
- The GTX780 generally has the shortest global memory latencies, almost half that of the Fermi, with an access pattern of P2-P5.\
  The page table context switching of the GTX980 is also much more expensive than that of the GTX780.
  
To summarize, the Maxwell device has *long global memory access latencies* for cold cache misses and page table context switching.\
Except for these rare access patterns, its access latency cycles are close to those of the Kepler device. \
because the GTX980 has higher fmem than the GTX780, it actually offers the shortest global memory access time (P2-P4).

![image](https://github.com/user-attachments/assets/8d12e01f-1a6e-49e7-894c-28de28c9f864)

### Shared Memory
In CUDA programming, different CTAs assigned to the same SM have to share the same physical memory space.\
On the Fermi and Kepler platforms, the shared memory is physically integrated with the L1 cache.\
On the Maxwell platform, it occupies a separate memory space.
**Note that the shared memory and L1 cache are separated since Maxwell architecture.**

*Programmers* move the data into and out of shared memory from global memory before and after arithmetic execution,\
to avoid the frequent occurrence of long global memory access latencies.


**We report a dramatic improvement in performance for the Maxwell device.**

### Shared Memory Throughput

the shared memory is organized as 32 memory banks [15].\
The bank width of the **Fermi and Maxwell devices is 4 bytes**, while that of the Kepler device is 8 bytes.
The theoretical peak throughput of each SM (WSM) is calculated as fcore ∗ Wbank ∗ 32.

![image](https://github.com/user-attachments/assets/3bf6da77-b196-4e13-b2fe-af410ee750a4)

**The achieved throughput per SM is calculated as 2 * fcore * sizeof(int) * (number of active threads per SM) * ILP / (total latency of each SM).**
Usually a large value of ILP results in less active warps per SM.\
The peak throughput W0SM denotes the respective maximum throughput of the abovecombinations.\
Two key factors that affect the throughput are the number of active warps per SM and the ILP level.

The GTX980 reaches its peak throughput when the CTA size = 256, CTAs per SM = 2 and ILP = 8, i.e., 16 active warps per SM. The peak throughput is 137.41 GB/s, about *83.9%* of the theoretical bandwidth.
The Maxwell device shows the best use of its shared memory bandwidth, and the Kepler device shows the worst.

GTX980 exhibits similar behavior as GTX780: high ILP is required to achieve high throughput for high SM occupancy.

According to Little’s Law, we roughly have: number of active warps * ILP = latency cycles * throughput.

**GTX780 sucks in ILP = 1, since its limited 64 warps at most to be scheduled concurrently.**\
We consider this to be the main reason the achieved throughput of the GTX780 is poor compared with its designed value.

### Shared Memory Latency

**The shared memory latencies on Fermi, Kepler and Maxwell devices are 50, 47 and 28 cycles, respectively.**

**Fermi and Maxwell devices have the same number of potential bank conflicts because they have the same architecture.**

The shared memory space is divided into 32 banks.\
Successive words are allocated to successive banks.\
If two threads in the same warp access memory spaces in the same bank, a 2-way bank conflict occurs.

![image](https://github.com/user-attachments/assets/c5ef66d3-c05e-46b7-84d6-ace224aafeab)

![image](https://github.com/user-attachments/assets/c8e4560b-68aa-4188-9621-05a9f90fca32)

![image](https://github.com/user-attachments/assets/f8e02655-9d5d-4d80-9bfb-fb6e5aefde8f)

For the Fermi and Kepler devices, where there is a 32-way bank conflict, it takes much longer to access shared memory than regular global memory (TLB hit, cache miss). \
Surprisingly, the effect of a bank conflict on shared memory access latency on the Maxwell device is mild.\
Even the longest shared memory access latency is still at the same level as L1 data cache latency.


In summary, although the shared memory has very short access latency, it can be rather long if there are many ways of bank conflicts.\
This is most obvious on the Fermi hardware.\
The Kepler device tries to solve it by doubling the bank width of shared memory.\
Compared with the Fermi, the Kepler’s 4-byte mode shared memory halves the chance of bank conflict, and the 8-byte mode reduces it further.

However, we also find that the Kepler’s shared memory is inefficient in terms of throughput.\
The Maxwell device has the best shared memory performance.\
With the same architecture as the Fermi device, the Maxwell hardware shows a 2x size, 2x memory access speedup and achieves the highest throughput.\
Most importantly, the Maxwell device’s shared memory has been optimized to avoid the long latency caused by bank conflicts.


### Conclusion
The memory capacity is significantly enhanced in both Kepler and Maxwell as compared with Fermi.\
The Kepler device is performance-oriented and incorporates several aggressive elements in its design, such as increasing the bus width of DRAM and doubling the bank width of shared memory.\
These designs have some side-effects.\
The theoretical bandwidths of both global memory and shared memory are difficult to saturate, and hardware resources are imbalanced with a low utilization rate.\
The Maxwell device has a more efficient and conservative design.\
It has a reduced bus width and bank width, and the on-chip cache architectures are adjusted, including doubling the shared memory size and the read-only data cache size.\
Furthermore, it sharply decreases the shared memory latency caused under bank conflicts.

---

## 4. Dissecting the NVidia Turing T4 GPU via Microbenchmarking

### Result
![image](https://github.com/user-attachments/assets/eb21b04f-6ce8-44ef-8307-d26c35fa8a86)

![image](https://github.com/user-attachments/assets/bae4d3b8-b2df-463b-a8f8-c095fbb53c9d)

### Shared Memory Latency 
![image](https://github.com/user-attachments/assets/ae7a4300-20fb-4be0-9404-e1c39a223d7d)

### Bandwidth
![image](https://github.com/user-attachments/assets/74dc0901-e5a8-4084-b6a1-d8d71175926f)

---
## 4. Benchmarking the GPU memory at the warp level

In this work, we investigate the data accessing capability of a warp of threads: broadcasting and parallel accessing.\
- Broadcasting occurs when multiple threads access the same data element, i.e., multiple threads request a single data element (MTSD).
- We refer the case of multiple threads accessing multiple distinct data elements (MTMD) as parallel accessing.

### Local Memory
- For the simple memory access patterns, we should allocate a sufficient small array to guarantee that it is located in registers.
- For the complex memory access patterns, we should simplify codes to exploit registers. For example, we merge a three-level loop into an one-level loop so that a larger temporal vector can be allocated in registers.
- 
### Shared Memory
- Bank conflicts must be avoided by the ways of e.g., data padding.
- Shared memory supports both broadcasting and parallel accessing.
- Neither consecutively accessing nor aligned accessing is a must.
- The latency decreases when the number of threads increase, and thus we should use a sufficiently large thread block.
- Replacing global memory with shared memory, because the latency of shared memory is smaller than that of global memory.
- Using shared memory bares an overhead (i.e., buffer allocation and data movement) and reusing data in it is a must for improved performance.
## Constant Memory
But constant memory does not support parallel accessing.\
That is, constant memory can only be accessed serially when requesting different data elements.\
On the one hand, constant memory is used to store a small amount of read-only data, which is not sensitive to bandwidth.\
So parallel accessing is not a must for constant memory.


- Constant memory supports the accessing capability of broadcasting.
- Constant memory does not support parallel accessing, and satisfies parallel memory requests in a serial manner.

### Global Memory
- Global memory supports both broadcasting and parallel accessing.
- The data types of 4 or 8 bytes can obtain the near upper-bounded bandwidth of global memory, while the data types cannot.\
  So the char data should be coalesced into the char4 type for improved bandwidth.
- Global memory accesses should be consecutive, but aligned accessing is not necessary for global memory.
- When memory accessing is non-consecutive, the latency changes with the number of threads, but not with the number of blocks.
  So we should configure the thread dimensionality.
---
## 5. Exploring Modern GPU Memory System Design Challenges through Accurate Modeling
 :+1: :+1: :+1:
### Memory Coalescer
the eviction granularity of the cache is 128B, indicating that the L1 cache has 128B lines with 32B sectors.\
Furthermore, the coalescer operates across eight threads, i.e. the coalescer tries to coalesce each group of eight threads separately to generate sectored accesses.

![image](https://github.com/user-attachments/assets/c3d84400-b121-4152-931c-c40074848909)

When the stride=32, the memory access is converged, and all the threads within the same warp will access the same cache line,\
however we receive four read accesses at L1 cache.

**8 Thread register 32bit == 32Byte.**

### L2 Cache 

L2 cache applies something similar to **write-validate** not **fetch on write**.\ :scream:
However, all the reads received by L2 caches from the coalescer are 32-byte sectored accesses.\
Thus, the read access granularity (32 bytes) is different from the write access granularity (one byte).\
To handle this, the L2 cache applies a different write allocation policy, which we named lazy fetch-on-read, that is a compromise between write-validate and fetch-on-write.

When a sector read request is received to a modified sector, it first checks if the sector write-mask is complete, i.e. all the bytes have been written to and the line is
fully readable.\
If so, it reads the sector, otherwise, similar to fetch-on-write, it generates a read request for this sector and merges it with the modified bytes.


## Streaming Throughput-oriented L1 Cache

![image](https://github.com/user-attachments/assets/8edd3919-c6ff-40df-a207-a0853fcfa161)

The L1 cache in Volta is what NVIDIA is calling a streaming cache [33].\
It is streaming because the documentation states that it allows **unlimited cache misses** to be in flight regardless the number of cache lines per cache set [10].

independent of the number of L1 configured size, the number of MSHRs available are the same, even if more of the on-chip SRAM storage is devoted to shared memory.

We believe that unified cache is a plain SRAM where sectored data blocks are shared between the L1D and the CUDA shared memory.\
It can be configured adaptively by the driver as we discussed earlier.\
We assume that the L1D’s TAG and MSHR merging functionality are combined together in a separate table structure (TAG-MSHR table).\
Since, the filling policy is now ON FILL, we can have more TAG entries and outstanding requests than the assigned L1D cache lines.

If it is a hit to a reserved sector (i.e. the status is pending), it sets its corresponding warp bit in the merging mask (64 bits for 64 warps).\
When the pending request comes back, it allocates a cache line/sector in the data block and sets the allocated block index in the table.\
Then, the merged warps access the sector, on a cycle-by-cyle basis.
---
## 6. OSM: Off-Chip Shared Memory for GPUs

![image](https://github.com/user-attachments/assets/0fadce93-433a-4889-b130-e2c330d3b334)

L1-D cache and shared memory use the same 32-bank memory structure (4 KB capacity per bank) as shown in Fig. 4;\
however, they have some differences.\
We can **access 32-bit shared memory arrays via a thread-index directly**, while for accessing L1-D cache, we should read 128B (four 32B sectors) of the cache block.\
In addition, L1 cache requires an extra hardware for managing tags and implementing LRU replacement policy.

---

## 7. Demystifying GPU UVM Cost with Deep Runtime and Workload Analysis
### Main Idea in Short
Profile the GPU UVM with different benchmarks.

For memory that fits in GPU memory, just prefetch.

For memory oversubscription, prefetch & eviction has room to improve.

Prefetch and eviction are maintained by hardware. 

- accuracy of prefetch
- accuracy of eviction


The LRU algorithm is operated on *page fault* pages.

Even frequently accessed page might be evicted.

### Motivation
1. cumulative data access latency without prefetching generally increases one or more orders of magnitude with UVM in comparison to explicit
direct management by programmers
2. when all data fits in GPU global memory, prefetching reduces the cost significantly.\
but the overall time can still be several times higher than the baseline
3. once the GPU global memory is oversubscribed, **data access latency dramatically increases by another order of magnitude depending on access pattern**
4. prefetching can aggravate the performance issues after oversubscription. 
![image](https://github.com/user-attachments/assets/1f0b11d8-0f1d-4b05-889c-55cf9f01fd6b)

### Cost of Demand Paging
**Far Fault** \
Paged migration moves data between devices in response to a page fault, maps the page into the faulter’s physical space, and unmaps from the previous location.

*Remote Mapping* maps the requested data into the requester’s page tables without actually migrating it and accesses it using DMA or a related
mechanism.\
*Read-only duplication* duplicates data at two or more physical devices and maps them locally to each device under the constraint that the data cannot be mutated.

UVM on-demand paging is implemented using GPU hardware and CPU software working in tandem.\
To integrate with the host OS, the UVM driver is provided as a kernel module for the host OS to extend the virtual memory space and map it
to GPU global memory utilizing the host memory layout.


![image](https://github.com/user-attachments/assets/1a38d53a-c3fa-4afe-bc5d-3f4f1506191a)

UVM uses a four-level hierarchy for memory address space: address spaces, virtual address ranges, virtual address blocks, and pages.

In general, a virtual address space is associated with an application.\
Each address space is composed of “ranges”, each corresponding to an arbitrarily sized memory allocation i.e. cudaMallocManaged() or related allocator.\
A range is broken up into 2MB sequential virtual address blocks, VABlocks.\
VABlocks are page-aligned and are composed of OS pages.

#### Flow
operations of the UVM Driver in three groups: pre/post-processing, fault servicing, and fault replay policy.

- *pre-processing* \
the driver stores page fault information read from the GPU fault buffer and sorts them locally.\
Faults are fetched until all the fault pointer queue is empty, the current batch of faults is full, or fault that is not ready is encountered, depending on policy.\
The default batch size is 256 faults. Per batch, the driver groups page faults based on VABlocks and service the faults.

- *Fault Servicing*
Fault servicing involves memory allocation, updating page tables, data transfer, and possibly issuing one or more fault replays or other operations, subject ot the fault replay policy.

![image](https://github.com/user-attachments/assets/d924b57e-8f52-4f63-b0b6-d234e1d6aefb)

#### Cost Overview

Pre/post processing is shown to be negligible in cost, but functionally important for the fault servicing and replay implementation.


Pre-processing first gathers faults from the device, performs basic bookkeeping and logical checks, and sorts them into the appropriate VABlock bins.\
NVIDIA documentation indicates that the driver uses a circular device-side queue to store a fault pointer when a fault occurs [15].\
The host can read these pointers, which subsequently point to locations in the global GPU fault buffer that contain the full fault information.\
The driver will generally read at least a full batch from the queue during every pass and cache the faults on the host to avoid having to make multiple remote updates to the queue.\
Faults may not be immediately available in the GPU fault buffer due to the asynchronicity.\
Thus the driver may need to poll the buffer until the appropriate “ready” field is marked true or may be able to begin processing on previously fetched faults.\
Sorting cost for batches is roughly constant due to the nature of sorting and the relatively small size of batches.

![image](https://github.com/user-attachments/assets/f38bb8e3-12de-42eb-a10e-16a37a8620f6)

#### Sevice Cost Breakdown

Fault servicing is a multi-step process that includes:
- allocating physical space
- zeroing out GPU pages
- migrating data from the source to the destination
- mapping pages and permissions, and a number of other tasks.

##### Physical Memory Allocation

Physical memory allocation accounts for a large but variable quantity of service cost.\
The UVM driver uses a physical memory allocator to track physical allocations on the GPU.\
Allocation is performed by **calling into the main NVIDIA driver**.

The allocator over-allocates memory to cache it, knowing that the cost of each call is quite high.\
**This over-allocation and caching causes the allocation cost to remain relatively constant and negligible at large sizes**. \
This cost is actually contained within the greater “Migrate Pages” category, but is separated here as it is responsible for the “constant” dominating transfer cost within UVM at small sizes.

##### Page Migration
Page migration involves:
- permission checking and updates
- memory allocation and zeroing of newly-allocated memory
- copying data from the source location to staging locations
- eventually issuing GPU instructions to copy data from the staging location to the final destination.
Once data is staged on the destination device, page duplication would be broken and unmapped from source locations.\
The UVM driver initiates the memory copy command, and notifies the GPU to actually perform the data copy using DMA.

##### Mapping Data
Mapping data includes updating the local and remote page tables and issuing appropriate memory barriers to ensure consistency on the GPU.\
While updating the GPU page tables is part of the cost here, the importance of this step is in  <u>bookkeeping and ensuring data consistency and integrity</u>.

**Important Insights**
- First, the number of VABlocks in a batch has a great impact on service time.
- Second, the batch size affects the cost and the optimal size depends on application access patterns and data requirement.
*The appropriate tuning of batch size may differ on a per-application basis, and would be an interesting area of future study.*

##### Replay Policy Cost
Replayable faults do not block the faulting GPU compute unit, which can continue running non-faulting warps until a replay command is received [19].

The replay notification indicates that the original memory access should be tried again.\
Note that a single fault may need to be replayed multiple times due to hardware fault capacity limitations or software policy.

- Block Policy
- Batch Policy
- Batch Flush Policy
- Once Policy

![image](https://github.com/user-attachments/assets/dfa35487-d81c-4194-ac49-54708f20a969)

### Prefetching Challenges and Page-level GPU Access Patterns

#### Prefetching Design Constraints and the Tree-Based Algorithm
UVM faces the fundamental prefetching challenge of a finite lead time, the requirement to effectively hide latency.

- **Lead Time** \
the lead time is more *stringent* than hardware prefetching due to *the greater cost for executing in software*, larger data size, and different interconnect characteristics.

UVM prefetcher should try to prefetch a large volume of data where possible to better utilize the H-D interconnect bandwidth, reduce the overall number of faults, and amortize the prefetching cost.

- **UVM page prefetcher only has coarse-grain, partial information of page access**

Particularly, the only information specific is the address that originated the fault.\
The driver lacks sufficient information for correlating faults with their generating GPU core/thread.\
Poor prediction from limited information may **result in fetching a large amount of unwanted data**, wasting H-D bandwidth.\
Consequently, UVM prefetching is left with the best effort option to support common applications.

- **Add transfer overhead that could be wasted if prediction is poor**

**Two Stage Prefetching**

- First stage: promote 4KB to 64KB
- Second stage: density prefetcher
![image](https://github.com/user-attachments/assets/2b71255f-ae30-4796-a05f-88d00762a05c)


#### Complex GPU Access Patterns
![image](https://github.com/user-attachments/assets/d3cf9eeb-8f8f-4b82-829e-f437da3b6ef4)
The driver does not see non-faulting data, and therefore the driver is oblivious to the full access pattern.

This algorithm largely drops the timing aspect of prefetching.

Instead, it utilizes the information already being tracked about page location to make confidencebased predictions about which data will be used.

If a specific VABlock is saturated with faults over any length of time, the algorithm will confidently predict that the rest of that VABlock
will also be used.

**fault coverage**

In summary, for undersubscribed workloads the density prefetcher can range from effective to highly effective in terms of fault coverage, but can be difficult to code against.

#### Effectiveness of Tree-Based Algorithm
Density prefetching ignores the precise ordering of page faults, which is critical for handling faults from many GPU cores simultaneously.

#### Game-Changer：Oversubscription
##### The Cost of Eviction
###### Eviction in UVM
The eviction mechanism is triggered whenever the driver attempts to allocate memory for a VABlock that does not have memory reserved on the GPU already, e.g. the first page fault.\
Evictions are performed at the VABlock level, mirroring allocation. When evicted, any modified pages are copied back to the host, and the  physical memory allocation for the VABlock is released.

The UVM driver uses least-recently-used eviction.\
The LRU list is updated when a fault is handled from a VABlock.

![image](https://github.com/user-attachments/assets/d41a9270-053d-49fa-93c4-574434d16795)

###### Direct and Indirect Costs
**Direct Costs**
- First, the eviction itself has the same components as a device-to-host fault for a VABlock not present on the host.\
  The changed data needs to be migrated, involving data transfer, memory barrier, and page mapping/unmapping.
- Second, due to the locking scheme in the driver, eviction causes the VABlock faulting path to start over, as the faulting block lock must be
dropped while the evicted block lock is held.

**Indirect Costs**
- Random Access Pattern: increased quantity of evict/map operations for small data sizes.
- Eviction mechanism can evict data that is still being used.\
Because the LRU function is only aware of page-faults, it is possible that the “hottest” regions of data may also be the most likely to be
evicted.\
The data would quickly be migrated to the GPU and then never again updated in the LRU list.

Prefetching can fetch data that will not be used prior to eviction.

Delays due to frequent eviction cause larger backlogs of faults in the fault buffer, requiring increased time to flush the fault buffer before replays and accumulating the time spent handling the fault replay policy.

###### Total Eviction Costs
**It is important to understand that the impact of eviction is at its greatest when data access is irregular.**

The overhead of eviction mixed with the sheer number of additional faults and evictions from the poor access pattern account for the order-of-magnitude performance loss.

The overall cost depends on the measure of oversubscription as well as the access pattern itself..

The worst effect is noticeable when applications cross the threshold where local data no longer fits in-core, and data is evicted prior to being used.

### Discussion and Conclusions

#### Challenges in Effective Prefetching and Eviction
##### Prefetching

Density prefetching has limitations when eviction is involved, because there is no guarantee that any prefetched data will actually be used. \
While a key advantage of density prefetching is its ignorance of precise fault order, it loses a lot of information about spatial locality.

The primary cost of prefetching is in additional data migration.


##### Eviction

Eviction is a very difficult problem because it has its own algorithmic component, as well as a dependency on the memory allocation functionality.\

Algorithmically, the implementation is still dependent on **page fault information**, which is insufficient.\
The granularity of evictions also impacts its performance, which is locked in UVM at the VABlock level.

**This has two key implications:**
- Data that is accessed on the GPU but does not cause a page fault because the page is present will not upgrade its location in the LRU list.
- VABlocks that are fully resident on the GPU will never be upgraded in the LRU list until they are evicted and re-faulted.

Addressing allocation granularity, 2MB blocks may be too coarse for allocations and evictions for irregular applications.\
While 4KB granularity is very small, irregular applications may not even have locality at the 4KB granularity.

#### Potential Paths for Better Prefetching and Eviction

##### Increased fault origin information
The GPU presently provides quite a bit of information along with a GPU fault, primarily for tracing higher-level information about the origin
of a fault.

Another level of information that offers SM ID, logical thread ID, or related information sufficient to pinpoint a specific area of execution.

##### Flexible memory allocation granularity
irregular applications may benefit from a tuneable parameter allowing different sized memory allocations.

**Overhead**
more complex μTLB implementations and highly flexible driver implementation.

##### GPU memory access-aware eviction

NVIDIA has included support for multiple-granularity access counters for GPU-level memory access on GPUs since the Volta architecture [27].

This idea is explored and simulated in Ganguly et al. [4], but has not been explored on a real system.

##### Adpative prefetching
The existing mechanism has demonstrated that it is quite capable depending on the circumstance.\
Using existing information, the driver could adapt some simple heuristics to adaptively tune prefetching.

## 8. [New2025] Evaluating Performance of Modern GPU with Partitioned Last-Level Caches

<img width="629" height="343" alt="image" src="https://github.com/user-attachments/assets/c610c487-759f-45f9-b287-ab1c6cdd7a20" />

From A100, L2 cache is larger than previous generations to speed up repeated data access.

Different GPU partitions:

<img width="628" height="268" alt="image" src="https://github.com/user-attachments/assets/f87f39b1-9587-4f30-bd6a-4f1a47d0d41b" />

Memory access latency:

- local-LLC: 200 cycles
- remote-LLC: 400 cycles
- L2 Miss Penality 240 cycles
<img width="563" height="503" alt="image" src="https://github.com/user-attachments/assets/e3a88a1f-864a-4cfb-a1dc-6160cfec2a38" />

---

## 9. [21 2023] Morpheus: Extending the Last Level Cache Capacity in GPU Systems Using Idle GPU Core Resources

This paper extends the idle gpu memory resources, like register, shared memory, l1 cache for memory-bound workloads.

Core in*cache mode* will run extended LLC kernel and give its memory to core in compute mode.

<img width="758" height="346" alt="image" src="https://github.com/user-attachments/assets/73b9187c-2187-4644-83f3-76fd0e20b25f" />


Memory-bound workloads
<img width="1542" height="599" alt="image" src="https://github.com/user-attachments/assets/9de89fbf-6da2-43d2-ba63-23efe93f7f1b" />

baseline system l2 cache size 5MB.

Performance improvement of 2x and 4x the cache size.

<img width="745" height="390" alt="image" src="https://github.com/user-attachments/assets/b9f1dda3-804b-40b4-af76-6fb32ee2632f" />

Distribution of cache latency:

Interconnection network: 76ns
Cache Bank Access: 8ns.

GDDR6x: 447ns.
<img width="746" height="282" alt="image" src="https://github.com/user-attachments/assets/2808b6a1-bf8c-404b-8fab-609565020e74" />










































