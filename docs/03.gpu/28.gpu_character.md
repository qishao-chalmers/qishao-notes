---
title: GPU Workload Characteristics
date: 2024-10-30
permalink: /pages/45891/
---

1. [354] Benchmarking TPU, GPU, and CPU Platforms for Deep Learning
2. [59 Year:2019] Characterizing Deep Learning Training Workloads on Alibaba-PAI
3. [12] Effective Elastic Scaling of Deep Learning Workloads
4. [30 Year:2024] LLM Inference Unveiled: Survey and Roofline Model Insights
5. [1] Performance Modeling and Workload Analysis of Distributed Large Language Model Training and Inference
6. [Blog] LLM Inference Series: 5. Dissecting model performance
7. [47 Year:2024] Understanding LLMs: A Comprehensive Overview from Training to Inference
8. [37 Year:2022] Reveal training performance mystery between TensorFlow and PyTorch in the single GPU environment :+1:  :+1:  :+1:  :+1:  :+1:
9. [46 Year:2019] Performance Characterization of DNN Training using TensorFlow and PyTorch on Modern Clusters
10. [163 Year:2020] Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference
11. [204 Year:2016] Fathom: Reference Workloads for Modern Deep Learning Methods
12. [1: Year: 2018] µ-cuDNN Accelerating Deep Learning Frameworks with Micro-Batching
13. [89 Year: 2019] Restructuring Batch Normalization to Accelerate CNN Training
14. [29 Year: 2018] Characterizing the Microarchitectural Implications of a Convolutional Neural Network (CNN) Execution on GPUs
15. [13 Year: 2022] cuConv: CUDA implementation of convolution for CNN inference
16. [10 Year: 2018] Performance Analysis of Different Convolution Algorithms in GPU Environment
17. [0 Year:2024] GPU Performance Optimization via Intergroup Cache Cooperation
18. [2024] Accelerating ML Workloads using GPU Tensor Cores: The Good, the Bad, and the Ugly :+1：*love the name*

---

### 1. Benchmarking TPU, GPU, and CPU Platforms for Deep Learning

Paper from Harvard

![image](https://github.com/user-attachments/assets/110ddbc0-1ddf-40fa-b360-9e3f589494c6)

---
### 2 [59 Year:2019] Characterizing Deep Learning Training Workloads on Alibaba-PAI
This paper compares early ml, like single node, single master multi worker, nvlinkd is just introduced to Alibaba at that time.

![image](https://github.com/user-attachments/assets/8e9eb798-a1bf-48da-aa17-d30c7e6973fc)

![image](https://github.com/user-attachments/assets/ec5cdae5-2150-48aa-9626-37869115bff7)

---
### 10. Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference

Sparsity across channels:

![image](https://github.com/user-attachments/assets/d0ab98ae-adcd-438a-97fe-db8c31becb3f)

Sparsity across layers:
![image](https://github.com/user-attachments/assets/68c5159a-f536-4714-aadf-0772b6c73dde)


***CSCC: Convolution Split Compression Calculation Algorithm for Deep Neural Network***
Sparsity across layers:
![image](https://github.com/user-attachments/assets/0496984d-6cad-4727-8ee7-f63023a1656c)

---
### 11.Fathom: Reference Workloads for Modern Deep Learning Methods

![image](https://github.com/user-attachments/assets/04833dea-d804-416e-914f-d59409ab0f5e)

![image](https://github.com/user-attachments/assets/4394bb14-fd24-46de-a604-f34b692fd139)

![image](https://github.com/user-attachments/assets/39728745-788b-4ac1-b777-937f25b9083d)

### 12. µ-cuDNN Accelerating Deep Learning Frameworks with Micro-Batching

Memory requirements of different matrix multiplication algorithm and their execution time.

![image](https://github.com/user-attachments/assets/6d533da4-a5a3-46e4-b6ad-61dfa0c21d71)

### 13. Restructuring Batch Normalization to Accelerate CNN Training

Batch normalization is essential in Densenet.
![image](https://github.com/user-attachments/assets/679ff5f7-0a9d-424e-b1e7-2a9ac9d9842c)

The non-CONV layers of DenseNet-121 are mostly bottlenecked by the peak mainmemory bandwidth of the system we use (230.4GB/s),\
whereas the CONV layers underutilize the available bandwidth (only up to 120GB/s).

non-Conv layers have less data locality and computation intensity, which makes loop blocking techniques less effective,\
leading to higher demand in memory bandwidth.

Memory accesses in ReLU and BN layers mostly come from reading and writing ifmaps and ofmaps.\
Since BN layers have strict data dependency, cross-layer data reuse is prohibited.

- In the forward pass of a BN layer, all pixels of ifmaps that belong to a mini-batch should be retrieved to get per-channel mean and variance values prior to normalizing individual pixels.
- In backpropagation, calculating the partial derivatives on γ (scaling factors) and β (shift factors) accompanies sweeping the partial derivatives on ofmaps; this should precede computing the partial derivatives on ifmaps.

Because these dependencies make data reuse distance far in BN, it is difficult to apply the data reuse techniques that were previously proposed for CONV
layers to these non-CONV layers.

They split batch normalization into two parts:

![image](https://github.com/user-attachments/assets/27a1b975-a383-4ecb-9d39-545ed4069388)

**what is convential kernel fusion of conv+bn about? is that fusion conv+bn totally?**


### 14. Characterizing the Microarchitectural Implications of a Convolutional Neural Network (CNN) Execution on GPUs

:+1: :+1: :+1: :+1: :+1:

Quantization analysis of CNN on GPUs

![image](https://github.com/user-attachments/assets/e91ea58d-414d-4d28-8209-5397b7b7dfd3)

![image](https://github.com/user-attachments/assets/2526f66e-008f-44b9-aa37-21796af5c1b2)

#### Results on K40
##### Conv
- stall exec dependency: the intrinsic program characteristics of this layer
- stall not selected : the warp is not selected to run since the scheduler selects competing warps

bounded by computing

##### relu
- stall_memory_throttle
- stall_memory_dependency
Memory bound

#### layer normalization, pooling and softmax
compute and memory bound\
when it changes to GTX1080, it is only memory-bound. GTX1080 better compute performance/

#### fully connect layer:
fc6_w is somewhat special in that it is partially bounded by memory and *partially bounded by instruction fetch*.

#### Memory Access Behaviour

![image](https://github.com/user-attachments/assets/160a0fe3-6b9b-4663-8fb1-6b242c410924)


#### Memory Access Behaviour

- layers of the linear data transformation make good use of the texture cache.
This can be explained since the computations in both the convolution and fully-connected layers exhibit a high degree of spatial locality.\
The texture cache is designed in such a way as to take advantage of spatial locality.
- Even the activation layer, which has **no temporal locality**, also makes use of the texture cache to exploit **spatial locality**.
- the linear data transformation layers rely heavily on shared memory and the texture cache.
As indicated in the cache hit rate figures, both the convolution and fully-connected layers possess high temporal and spatial locality, given that data accessed within a region is repeatedly accessed.\
As a result, there are a large number of memory transactions issued to these two memory levels, especially read requests.
- for other layers, the utilization of shared memory and texture cache is very limited.
Even for the pooling and LRN layers, the data reuse rate is very low.\
For the other layers, including pooling, LRN, activation, and Softmax, the number of memory transactions does not vary significantly across the memory hierarchy
- shared memory usually takes 38 cycles to read, while the texture cache takes 436-443 cycles.

![image](https://github.com/user-attachments/assets/caf5d7b9-27a6-48ac-ac92-ad84cc2eed40)


#### Potential Optimization
- convolution is compute power hungary instead of DRAM bandwidth.
Increasing the DRAM bandwidth on the GTX1080 will not benefit the CNN throughput very much.
Instead, if we increase the bandwidth of the texture cache, we should see much better performance.

- L1 cache is essentially unused in most of the layers.

we can enable **L1 cache bypassing[27]** for selected layers to avoid unnecessary data requests to the L1 cache.
When we re-run our application with the L1 cache disabled for both reads and writes, we observe a speedup in some layers for both forward and backward propagation.

- apply kernel fusion[28] for the linear data transformation layers and the non-linear activation.
![image](https://github.com/user-attachments/assets/3552f82e-7519-435b-9404-e544d92c7ecc)


### 15. cuConv: CUDA implementation of convolution for CNN inference

They design a new implementaion of convolution.

They discucss about GEMM, Winograd and FFT and design a new two-stage convolution scheme, two-stage convolution.

### 16. Performance Analysis of Different Convolution Algorithms in GPU Environment

They compare the memory size and performance of FFT, gemm, Winograd.
![image](https://github.com/user-attachments/assets/2ad332b7-d0dc-449d-a663-a85d3b920acf)

### 17. GPU Performance Optimization via Intergroup Cache Cooperation
This paper designed cache for gpu and also analyze different level of ache behaviour on different benchmarks.\

L1 hit ratio

![image](https://github.com/user-attachments/assets/47ea33de-8ddb-41b9-9e97-c930ac1ac87d)


Duplicate data

![image](https://github.com/user-attachments/assets/6cd548eb-4994-4a83-8239-d07898d696b7)





