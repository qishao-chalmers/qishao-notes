---
title: GPU Cache Management
date: 2025-06-28
permalink: /pages/45897/
---

1. [22] Adaptive Memory-Side Last-Level GPU Caching
2. [83 2015] Locality-Driven Dynamic GPU Cache Bypassing

---

## 1. [22] Adaptive Memory-Side Last-Level GPU Caching

- Private LLCs, which replicate shared data across multiple slices, provide higher bandwidth but suffer from higher miss rates.
- Shared LLCs avoid redundancy, reducing miss rates, but suffer bandwidth contention under high sharing.


![image](https://github.com/user-attachments/assets/fe7ab2e6-f43a-4881-b154-0782bb84bafc)


In the shared LLC organization, an LLC slice is shared by all SMs.

The LLC slice for a given cache line is determined by a few address bits.

Collectively, all LLC slices associated with a given memory controller cache the entire memory address space served by the memory controller.

In the private LLC organization, an LLC slice is private to a cluster of SMs.

An LLC slice caches the entire memory partition served by the respective memory controller for only a single cluster of SMs.

The LLC slice for a cache line is thus determined by the cluster ID.


![image](https://github.com/user-attachments/assets/86193f2f-2ad3-45c3-a78d-4b4c79db370d)


### Dynamic Reconfiguration Rules

- Switch to private if:
  - Miss rate remains comparable (within 2%)
  - Bandwidth gain outweighs miss rate penalty
- Revert to shared:
  - At new kernel or time epoch

How to profile/

Set Dueling

---
## 2. [83 2015] Locality-Driven Dynamic GPU Cache Bypassing

### Categories of Applications
The paper classifies GPU applications into three categories based on how they benefit (or suffer) from the L1 D-cache:

#### 1. Cache-Unfriendly (CNF)
Definition: Applications that perform better when L1 D-cache is bypassed.

Cause:
- Low data reuse.
- Long reuse distances.

Leads to cache pollution and resource contention.

Impact:
- Memory pipeline stalls.
- Unnecessary eviction of useful lines.

Examples:
- NW, SD2, LUD, HS, PTF, BH, SSSP

Performance gain: Up to 36% IPC improvement by bypassing L1.

#### 2. Cache-Insensitive (CI)
Definition: Applications for which enabling/disabling L1 D-cache has little to no effect.

Cause:
- Heavy use of shared memory.
- Minimal or no global memory accesses.

Low memory intensity or high control divergence.

Impact:
- Cache behavior does not affect IPC.

Examples:
- CFD, MYC, FFT, GS, PF, LFK

#### 3. Cache-Friendly (CF)
Definition: Applications that benefit from L1 D-cache.

Cause:
- High data reuse.
- Short reuse distances.

Impact:
- Disabling L1 severely degrades performance.

Examples:
- MM, HT, SD1, BT, BP

Performance loss: Up to 77% IPC drop when bypassed.
- 🔁 Reuse Behavior Analysis
- 🔢 Reuse Count

What it shows: Number of times a memory address is reused.

Observation:
- CNF apps have few high-reuse accesses.
- Example: >60% of accesses in NW, LUD are reused fewer than 3 times.

#### 📏 Reuse Distance (Figure 4)
Definition: The number of unique memory accesses between two accesses to the same address.

Example: Pattern A–B–C–A → reuse distance = 2.

Observation:
- CNF apps have long reuse distances: often 512–2048.

**These accesses cannot fit in L1 (e.g., 128B lines × 512 = 64KB).**

🧠 L1 vs. L2 Cache Bottlenecks
Experimental Setup (Figure 2)
- The authors increase associativity and capacity of both L1 and L2 caches.

Findings:

![image](https://github.com/user-attachments/assets/4dc6d668-e824-43ee-b0fd-ca229beb6ced)

Cache Level	Observation for CNF Apps
- L2	Performance is insensitive to L2 size and associativity. L2 is not a bottleneck.
- L1	Performance improves with larger/more associative L1. But needs impractically large L1 (e.g., 128-way, 16MB) to be effective. Still insufficient for some apps.

Conclusion: **The L1 D-cache is the performance bottleneck for CNF workloads, not L2.**

### Bypassing logic

- On a tag store miss → insert into tag store with RC = 1 → bypass data store.
- On subsequent hits:
  - RC incremented.
  - If RC > threshold (e.g., 2), allocate data in the data store.
- Replacement:
  - Tag store uses LFU with aging (decays RC to evict stale entries).
- Data store uses existing GPU policies like LRU, RRIP.

![image](https://github.com/user-attachments/assets/01bb388a-a287-4514-b5b6-a628fc2453c2)

### 📊 Summary Table
Category	Behavior	Reuse Count	Reuse Distance	L1 Role	L2 Role
- CNF	Cache hurts	Mostly low (1–2)	Long (512–2048)	Bottleneck, polluted	Not bottleneck
- CI	Cache irrelevant	N/A	N/A	Irrelevant	Irrelevant
- CF	Cache helps	High	Short	Critical	Less relevant

### 🧠 Design Implications
- L1 D-cache should selectively cache data.
- A naive insert-everything policy causes: Thrashing in CNF apps.
- Performance degradation in CF apps if cache is bypassed.

The paper proposes a reuse-aware dynamic bypass mechanism that:
- Tracks Reference Count (RC).
- Filters accesses based on reuse patterns.

Add extra information in the tag. But not the data.

![image](https://github.com/user-attachments/assets/192f1f93-4aa2-4957-a8b8-fe96ffacc85f)

