---
title: GPU Cache Management
date: 2025-06-28
permalink: /pages/45897/
---

1. [22] Adaptive Memory-Side Last-Level GPU Caching
2. [83 2015] Locality-Driven Dynamic GPU Cache Bypassing
3. [9 2021] Analyzing and Leveraging Decoupled L1 Caches in GPUs

---

## 1. [22] Adaptive Memory-Side Last-Level GPU Caching

- Private LLCs, which replicate shared data across multiple slices, provide higher bandwidth but suffer from higher miss rates.
- Shared LLCs avoid redundancy, reducing miss rates, but suffer bandwidth contention under high sharing.


![image](https://github.com/user-attachments/assets/fe7ab2e6-f43a-4881-b154-0782bb84bafc)


In the shared LLC organization, an LLC slice is shared by all SMs.

The LLC slice for a given cache line is determined by a few address bits.

Collectively, all LLC slices associated with a given memory controller cache the entire memory address space served by the memory controller.

In the private LLC organization, an LLC slice is private to a cluster of SMs.

An LLC slice caches the entire memory partition served by the respective memory controller for only a single cluster of SMs.

The LLC slice for a cache line is thus determined by the cluster ID.


![image](https://github.com/user-attachments/assets/86193f2f-2ad3-45c3-a78d-4b4c79db370d)


### Dynamic Reconfiguration Rules

- Switch to private if:
  - Miss rate remains comparable (within 2%)
  - Bandwidth gain outweighs miss rate penalty
- Revert to shared:
  - At new kernel or time epoch

How to profile/

Set Dueling

---
## 2. [83 2015] Locality-Driven Dynamic GPU Cache Bypassing

### Categories of Applications
The paper classifies GPU applications into three categories based on how they benefit (or suffer) from the L1 D-cache:

#### 1. Cache-Unfriendly (CNF)
Definition: Applications that perform better when L1 D-cache is bypassed.

Cause:
- Low data reuse.
- Long reuse distances.

Leads to cache pollution and resource contention.

Impact:
- Memory pipeline stalls.
- Unnecessary eviction of useful lines.

Examples:
- NW, SD2, LUD, HS, PTF, BH, SSSP

Performance gain: Up to 36% IPC improvement by bypassing L1.

#### 2. Cache-Insensitive (CI)
Definition: Applications for which enabling/disabling L1 D-cache has little to no effect.

Cause:
- Heavy use of shared memory.
- Minimal or no global memory accesses.

Low memory intensity or high control divergence.

Impact:
- Cache behavior does not affect IPC.

Examples:
- CFD, MYC, FFT, GS, PF, LFK

#### 3. Cache-Friendly (CF)
Definition: Applications that benefit from L1 D-cache.

Cause:
- High data reuse.
- Short reuse distances.

Impact:
- Disabling L1 severely degrades performance.

Examples:
- **MM, HT, SD1, BT, BP**

Performance loss: Up to 77% IPC drop when bypassed.
- 🔁 Reuse Behavior Analysis
- 🔢 Reuse Count

What it shows: Number of times a memory address is reused.

![image](https://github.com/user-attachments/assets/fcc652e0-5464-423b-af2b-e7dcf4b4c21b)

Observation:
- CNF apps have few high-reuse accesses.
- Example: >60% of accesses in NW, LUD are reused fewer than 3 times.

#### 📏 Reuse Distance (Figure 4)
Definition: The number of unique memory accesses between two accesses to the same address.

Example: Pattern A–B–C–A → reuse distance = 2.

Observation:
- CNF apps have long reuse distances: often 512–2048.

**These accesses cannot fit in L1 (e.g., 128B lines × 512 = 64KB).**

🧠 L1 vs. L2 Cache Bottlenecks
Experimental Setup (Figure 2)
- The authors increase associativity and capacity of both L1 and L2 caches.

Findings:

![image](https://github.com/user-attachments/assets/4dc6d668-e824-43ee-b0fd-ca229beb6ced)

Cache Level	Observation for CNF Apps
- **L2	Performance is insensitive to L2 size and associativity. L2 is not a bottleneck.**
- **L1	Performance improves with larger/more associative L1. But needs impractically large L1 (e.g., 128-way, 16MB) to be effective. Still insufficient for some apps**.

Conclusion: **The L1 D-cache is the performance bottleneck for CNF workloads, not L2.**

### Bypassing logic

- On a tag store miss → insert into tag store with RC = 1 → bypass data store.
- On subsequent hits:
  - RC incremented.
  - If RC > threshold (e.g., 2), allocate data in the data store.
- Replacement:
  - Tag store uses LFU with aging (decays RC to evict stale entries).
- Data store uses existing GPU policies like LRU, RRIP.

![image](https://github.com/user-attachments/assets/01bb388a-a287-4514-b5b6-a628fc2453c2)

### 📊 Summary Table
Category	Behavior	Reuse Count	Reuse Distance	L1 Role	L2 Role
- CNF	Cache hurts	Mostly low (1–2)	Long (512–2048)	Bottleneck, polluted	Not bottleneck
- CI	Cache irrelevant	N/A	N/A	Irrelevant	Irrelevant
- CF	Cache helps	High	Short	Critical	Less relevant

### 🧠 Design Implications
- L1 D-cache should selectively cache data.
- A naive insert-everything policy causes: Thrashing in CNF apps.
- Performance degradation in CF apps if cache is bypassed.

The paper proposes a reuse-aware dynamic bypass mechanism that:
- Tracks Reference Count (RC).
- Filters accesses based on reuse patterns.

Add extra information in the tag. But not the data.

![image](https://github.com/user-attachments/assets/192f1f93-4aa2-4957-a8b8-fe96ffacc85f)


---

### 3. [9 2021] Analyzing and Leveraging Decoupled L1 Caches in GPUs

#### 📌 Motivation
Modern GPUs use private, tightly-coupled L1 caches per core. While this cache hierarchy helps address the memory wall by providing on-chip bandwidth, it introduces two key inefficiencies:

Data Replication: Multiple cores may cache the same data, wasting total L1 capacity.

Underutilized L1 Bandwidth: Many-to-few traffic from many L1s to fewer L2s causes poor L1 bandwidth utilization.

#### 🚀 Proposed Solution: DC-L1 (DeCoupled L1) Cache Architecture



![image](https://github.com/user-attachments/assets/9e418409-59b4-4b91-8157-527bdf49be7c)



Core Idea:

**Physically decouple L1 caches from the cores and aggregate them into a flexible organization called DC-L1, enabling:**
- Reduced data replication.
- Improved bandwidth utilization.
- Tunable design between latency and capacity benefits.

![image](https://github.com/user-attachments/assets/6d38f1d0-873b-440e-88ed-90b7f166f151)


#### 📐 Architectural Designs Explored
1. **Private DC-L1 (PrY):**
   
Each DC-L1 is shared by a group of cores.

Configurable aggregation granularity: e.g., Pr80 (1 core per DC-L1), Pr40 (2 cores/DC-L1), … Pr10 (8 cores/DC-L1).

Benefit: reduces replication as cores share cache lines.

🧠 Tradeoff: Larger group size → better replication reduction but worse bandwidth (more contention, fewer ports).


![image](https://github.com/user-attachments/assets/d0cfb8b3-43c1-406f-824f-dd599d34952f)


2. **Fully-Shared DC-L1 (Sh40):**


![image](https://github.com/user-attachments/assets/ff16cb91-c07a-4e60-8649-14067003b670)

   
All DC-L1 caches are part of a single logically shared cache space.

Each DC-L1 owns a distinct part of the address space (similar to L2 bank slicing).

Eliminates replication completely.

🔴 Problem: Requires a large 80×40 NoC crossbar → high area and power overheads.

3. **Clustered Shared DC-L1 (ShY+CZ):**

Divide GPU cores and DC-L1s into clusters (e.g., 10 clusters of 8 cores and 4 DC-L1s each).

Enable intra-cluster sharing, but allow inter-cluster replication.

Tradeoff: reduced replication and reduced NoC cost.


![image](https://github.com/user-attachments/assets/3b3352e5-4cf3-4407-8b84-d28c647191cc)


✅ Chosen configuration: Sh40+C10 (40 DC-L1s, 10 clusters) → balanced performance and cost.

4. **Sh40+C10+Boost:**

Same as above, but doubles the frequency of small crossbars in NoC#1.

Leverages the small crossbar size to overcome latency/bandwidth loss from decoupling.

#### Simulation Set up

We estimated such latency under the evaluated applications with Sh40+C10+Boost, and observed an overhead of **54 cycles**, on average.

Another source of latency overhead is the aggregation of the DC-L1s.

Specifically, with Sh40+C10+Boost, each DC-L1 cache is double the size of the baseline L1 cache, which adds a 7% increase in the DC-L1 access latency.

Specifically, the DC-L1s with Sh40+C10+Boost have an access latency of 30 cycles, compared to **28 cycles** L1 access latency in the baseline.

#### Insights & Analysis
- Replication Sensitivity Classification
- Applications were deemed replication-sensitive if:
 - Replication ratio > 25%
 - L1 miss rate > 50%
 - 5% IPC gain with 16× L1 size

12 such applications (e.g., T-AlexNet, C-BFS) showed substantial gains from reducing L1 replication.


### Ideas
#### 1. Workload-aware DC-L1 Partitioning for Multi-programmed GPUs
**Problem**: The paper evaluates replication and bandwidth under single workloads. In multi-tenant settings, different applications compete for DC-L1 capacity and bandwidth.

**Research Idea**: Dynamically partition DC-L1 clusters based on workload characteristics (e.g., memory intensity, data reuse, prefetch aggressiveness).

Use runtime metrics (e.g., replication rate, MPKI) to drive partition reconfiguration.

**Contribution**: Design a low-overhead runtime or compiler hint system for adaptive DC-L1 clustering and mapping under multi-workload scenarios.

#### 2. Replication-aware Compressed DC-L1 Caches
**Problem**: Even clustered DC-L1s have limited capacity and still experience intra-cluster replication.

**Research Idea**: Implement base-delta or frequent-value compression in DC-L1 caches.

- Exploit inter-core redundancy for cross-core dictionary compression or address-delta reuse.

**Twist**: Co-design compression with replication-tracking to avoid storing multiple compressed versions of the same line.

**Accel-Sim Angle**: Add lightweight modeling of decompression latency and tag expansion overheads in the L1 cache pipeline.

#### 3. Prefetch Filtering and Scheduling in DC-L1s for Irregular Workloads
**Problem:** DC-L1 nodes create new prefetch timing and routing bottlenecks.

**Research Idea:** Introduce a prefetch scheduler in DC-L1 nodes that:
- Differentiates between "core-locality" and "remote-sharing" prefetches.
- Uses reuse-distance or MSHR saturation tracking to decide eviction or forwarding.

**New Angle**: Combine prefetch confidence with replication sensitivity to filter harmful prefetches that pollute shared DC-L1s.

#### 4. Topology-aware Workload-to-DC-L1 Mapping in Multi-GPU Systems
**Problem:** DC-L1s require networked routing; current cluster mapping is static.

**Research Idea:** In a multi-GPU setup (Accel-Sim or MCM-GPU-style), design dynamic workload mapping policies that:
- Cluster memory-sharing CTAs to minimize DC-L1 hop distance.
- Balance NoC load across shared DC-L1s.

**Extension:** Apply machine learning (e.g., reinforcement learning) to learn optimal DC-L1 routing and core affinity over time.




