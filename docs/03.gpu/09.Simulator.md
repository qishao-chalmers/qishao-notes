---
title: GPU Simulator
date: 2024-01-01
permalink: /pages/458721/
---

1. DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis [Citation 39]
2. Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA] :+1:
3. [120 2015] Anatomy of GPU Memory System for Multi-Application Execution :+1:
4. CRISP: Concurrent Rendering and Compute Simulation Platform for GPUs :+1:
5. Parallelizing a modern GPU simulator
6. 

---
## 1.DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis
Still reading in process.

---

## 2.Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]
Done.
Gem5 GPU Introduction.

Reference Materials

AMD_gem5_APU_simulator_isca_2018_gem5_wiki.pdf

---

## 3. [120] Anatomy of GPU Memory System for Multi-Application Execution

**MAFIA: Multiple Application Framework for GPUs**

This is a opensource work based on gpgpu-sim.

https://github.com/adwaitjog/mafia

---

## 3. CRISP: Concurrent Rendering and Compute Simulation Platform for GPUs :+1:

:+1: Source code: https://github.com/JRPan/crisp-artifact

![image](https://github.com/user-attachments/assets/07e94c89-67f2-4b96-8782-7b5204aa9682)

We extended Accel-Sim and GPGPU-Sim to support advanced GPU partition methods.\
By default, the simulator supports concurrent kernel execution but launches thread blocks (CTAs) from one kernel exhaustively before switching to the next kernel.\
This means if a kernel is large enough and has enough warps to fill all the SMs, there is no concurrent execution.\
We updated the CTA scheduler and added the following partition methods:\
- Multi-Process Service (MPS)
- Multi-instance GPU (MiG)
- a Fine-grained intra-SM Partition (FG) similar to the async compute feature in Vulkan.

At a high level, MPS and MiG represent coarse-grained inter-SM partitioning methods where each SM is dedicated to either graphics rendering or general computing tasks.\
In the MPS model, only the SMs are partitioned, while the L2 cache and higher-level memory spaces remain shared across tasks.\
The MiG model partitions all resources, and each SM only accesses a designated subset of memory sub-partitions and controllers.

In fine-grained intra-SM partitioning (FG), each SM is partitioned to run both tasks.\
Instead of issuing as many CTAs as possible, the CTA scheduler only issues CTAs within the limits of partitioned resources.\
Partitioned resources include thread slots, shared memory, and registers.\
At the CTA issue stage, the CTA scheduler checks the CTAâ€™s resource requirements with the remaining resources on the SM.\
If all resource constraints are met, the CTA is issued.\
At CTA commit, resources occupied by the CTA are freed and can be used again for future CTAs.\
However, static partitioning leads to inefficiency since different kernel pairs exhibit divergent characteristics.\
For example, one kernel may be register-heavy while another uses a lot of shared memory.\
**Therefore, the partition ratio can be changed dynamically to maximize resource utilization.**

When the partition ratio changes dynamically, and on-chip resources must be reassigned to reflect the updated ratio.\
For example, each CTA from kernel A has 128 threads, while CTAs from kernel B have 256 threads.\
Resources freed by one CTA from kernel A are not enough for a CTA from kernel B.\
In this case, the CTA scheduler stops issuing CTAs from kernel A and waits until two CTAs from kernel A commit, then starts issuing CTAs from kernel B.

![image](https://github.com/user-attachments/assets/ee6cd302-0e24-4a63-818f-c62bc6f9bd8c)

