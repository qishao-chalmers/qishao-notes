---
title: GPU GEMM
date: 2025-05-08
permalink: /pages/45892/
---


1. [Bachelor Thesis] Developing an auto-tunable GEMM kernel that utilizes Tensor Cores
2. [NVIDIA BLOG] https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html
3. Understanding GEMM Performance and Energy on NVIDIA Ada Lovelace: A Machine Learning-Based Analytical Approach

---
## 1. [Bachelor Thesis] Developing an auto-tunable GEMM kernel that utilizes Tensor Cores

![image](https://github.com/user-attachments/assets/8fc44f80-eb79-4dfd-80a4-60a80343d087)

**Thread Block Tile**

Here it can be observed that the optimum lies somewhere between the blue colours, which represent values of 64 and 128.

**Warp Tile**

For both GPUs, it is visible that for the 32 × 8 × 16 shaped WMMA operation, a higher WMMA_COLS value performs better, and vice-versa for 8 × 32 × 16 and WMMA_ROWS.

This experiments are done on NVIDIA A4000 Ada.

---


## 2. [NVIDIA BLOG] Matrix Multiplication Background User's Guide

### Tile for Thread Block

For cuBLAS GEMMs, thread block tile sizes typically but not necessarily use power-of-two dimensions.

Different tile sizes might be used for different use cases, but as a starting point, the following tiles are available:
- 256x128 and 128x256 (most efficient)
- 128x128
- 256x64 and 64x256
- 128x64 and 64x128
- 64x64 (least efficient)

![image](https://github.com/user-attachments/assets/7fb2ff1b-f665-484e-b44b-9c869b89fd02)


---

## 3. Understanding GEMM Performance and Energy on NVIDIA Ada Lovelace: A Machine Learning-Based Analytical Approach

This papers seems that they didnt consider double buffer pingpong.

![image](https://github.com/user-attachments/assets/14c8b856-689f-4c9c-9e6b-804571ab5382)



### Matrix Operation Performance Analysis

The experimental results demonstrate several significant correlations between matrix operations and performance characteristics. Our analysis introduces key performance metrics:

![image](https://github.com/user-attachments/assets/71311a09-619e-49b7-96bf-1c67becc1673)

1) Matrix Dimensionality Impact: Correlation analysis revealed significant relationships:
   
- Matrix volume (M × N × K) shows strong correlation with runtime (r = 0.98)
- Output dimensions (M × N) demonstrate higher correlation with power consumption (r = 0.80) than compute
dimension K
- TFLOPS efficiency exhibits negative correlation with matrix size (r = −0.41), following:

![image](https://github.com/user-attachments/assets/aac518d6-4a49-4896-a2f6-8aa77b760356)


2) Tile Size Performance Impact: Analysis of tile size effects revealed optimization patterns:

![image](https://github.com/user-attachments/assets/7fe0ec65-da9c-4679-b530-9378219ac875)

Key findings include:
- Optimal tile size of 16 × 16 minimizes runtime across dimensions
- Power consumption stabilization occurs with larger tile sizes
- Performance plateau observed beyond 16 × 16 tiles due to shared memory constraints


![image](https://github.com/user-attachments/assets/e489e08c-57c5-496b-abee-7c0a26af305c)

But if we calculate the shared memory usage for tiling 16*16, it would be 2*16*16*4 = 2kB and the threadblock per SM is 6, so **it only use 12KB shared memory**...why?

