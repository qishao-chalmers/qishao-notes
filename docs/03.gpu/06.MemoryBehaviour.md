---
title: Memory Behaviour Paper List
date: 2023-11-21
permalink: /pages/45871e/
---

1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior 2014
2. [86 Y2014] Application-aware Memory System for Fair and Efficient Execution of Concurrent GPGPU Applications


---
### 1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior

CPU cores must extract very wide ILP in order to expose MLP to the memory hierarchy, and this MLP can be limited to lower levels of the memory hierarchy due to L1 cache locality. On the other hand, GPU cores and caches aim to mitigate MLP limitations, allowing the programmer to focus their efforts on leveraging the available MLP.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/da7d850e-93ba-4a22-b8bf-cd723c124b5b)

Rodinia is categorized into the above features.
- Pipleline
- Iterative
- Head Reads(I/c)

A fairly common factor in compute and memory op count differences between system configurations is due to register handling. For x86 CPU applications, the small architected register set (16) can cause register spilling to the stack and recomputation of previously computed values. 

In contrast, GPU cores have some flexibility in register use due to their core multithreading. By running fewer GPU threads per core and late binding register specifiers to physical registers, there is more flexibility for each thread to access more registers, which can avoid spilling and recomputation.

**Memory Access Pattern**

CPU cores use a small set of deep per-thread instruction windows, and high-frequency pipelines and caches to expose parallel memory accesses. In contrast, GPU cores expose parallel memory accesses by executing 100s–1000s more threads at lower frequencies, and threads are grouped for smaller perthread instruction windows and memory request coalescing.

while CPU cores rely heavily on L1 caches to capture locality, GPU cores capture most locality with coalescing and lessen the L1 cache responsibilities by providing scratch memory. 

Beyond the L1 caches, the memory systems tend to capture very similar locality. Further, we see that different core threading and cache filtering result in extreme differences in instantaneous memory access rates; CPU caches tend to filter accesses down to regular intervals, while GPU cores tend to issue bursts of accesses.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/1752db95-a2e2-42dc-9be1-4396cab944bb)

Scratch memory: GPU cores provide scratch memory, which can function as local storage for groups of threads to expand the space of local storage with register-like accessibility. In CUDA benchmarks that use the GPU scratch memory, kernels are typically organized into three stages: 
(1) read a small portion of data from global memory into the scratch memory,
(2) compute on the data in scratch memory, and 
(3) write results back to global memory.

Since GPU request coalescing behaves similarly to CPU single-instruction, multiple-data (SIMD) vectorization,  vectorization reduces the total number of memory accesses by 1.32–1.69× (1.44× geometric mean), and that most of the eliminated accesses are to heap data.

Overall, GPU **scratch memory and request coalescing** reduce the number of global memory accesses by 18–100× compared to CPU applications (27× in the geometric mean). 
 
Compared to CPU cores, this reduction alleviates pressure on caches, which in turn allows GPU cores to operate at lower frequencies while still serving data to threads at rates comparable to or greater than CPU cores.

**Spatial Locality**

CPU threads have extremely high spatial locality, typically striding through all elements in a heap cache line in subsequent algorithm loop iterations. These access patterns, which also include accesses to stack/local memory that is persistent over many loop iterations, result in high L1 cache hit rates that even exceed those expected by simple strided read memory access.

For GPU, small number of remaining spatially local accesses is likely due to separate thread groups accessing the same data rather than thread groups being unable to fully coalesce accesses.

**Temporal Locality**

For **CPU**, this leaves the L2 caches mostly responsible for capturing temporally local accesses to data shared across cores rather than temporally or spatially local accesses to data previously evicted from the L1 caches due to limited capacity.

For **GPU**, This indicates that instead of competing for L1 capacity, GPU threads from separate cores are generating most of the temporally local accesses to single cache lines, similar to the CPU L2.

Based on the above observations, we find that CPU and GPU L1 caches have very different importance, though their filtering roles are similar.

In the aggregate for data-parallel workloads, CPU L1 caches have many responsibilities; they must be designed to capture both the spatial locality for heap data accesses and the temporal locality of stack accesses. Fortunately for data-parallel workloads, these responsibilities rarely conflict given sufficient L1 capacity, so CPU L1s are quite effective and important for capturing locality. 

For GPU applications, register and scratch memory can shift local variable accesses away from the caches, which eliminates the L1 responsibility for capturing temporally local stack requests. Further, GPU coalescing greatly reduces the importance of spatial locality across separate heap accesses, so the **L1 caches are mostly responsible for capturing the small number of temporally local accesses from separate GPU threads on the same core**(this is different from cpu), diminishing the overall responsibility of the GPU L1s compared to CPU L1s.

**Bandwidth Demands**

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/c505aff3-e4f4-4442-925e-a1c14e05042c)

The key takeaway here is that GPU burst access behavior results from the way that GPUs group and launch threads. Specifically, at the beginning of a kernel, all capable thread block contexts begin executing at roughly the same time, so this can cause very large bursts of independent accesses. 

Following this initial burst, smaller but still significant access bursts occur each time a new thread block begins executing or when thread groups pass synchronization events.

By contrast, CPU cache access filtering tends to modulate the core’s ability to issue nearly as many parallel accesses to off-chip memory.

**Latency Sensitivity and Bandwidth Sensitivity**

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/17346822-3929-4c90-a850-d64781f719dd)

This Figue show CPU is sensitive to latency, but gpu to bandwidth.

**Interesting thoughts***

-cache shared by CPU and GPU
-interconnect and off-chip memory scheduling

These should take different characteristic of CPU and GPU into consideration.

---

## 2. [86 Y2014] Application-aware Memory System for Fair and Efficient Execution of Concurrent GPGPU Applications

They agument application-awareness to the memorysystem logic and propose the First-ready Round-robin FCFS (FR-RR-FCFS) memory scheduler, which schedules memory requests originating from different applications in a roundrobin (RR) fashion, while preserving the benefits of FR-FCFS scheduling.

![image](https://github.com/user-attachments/assets/225a4a73-904e-4b7a-9eac-3702b4381522)

When running two applications in GPU at the same time, the weighed speedup for each application might not be the same, which means unfairness.

![image](https://github.com/user-attachments/assets/5b3b6550-aaf7-4457-8569-4843dd71e287)


**Interesting Experiments**

![image](https://github.com/user-attachments/assets/ddb22b85-9ca9-4b85-9df0-08cbc2a7ba9d)

(B) Wasted-BW: the relative portion of DRAM cycles during which no data is transferred over the DRAM interface, but there are pending memory requests in memory controller.

This is because of DRAM timing constraints; improving DRAM page hit rates, and bank-level parallelism can reduce this Wasted-BW,


We propose to equip the widely known FR-FCFS memory scheduler with application awareness by choosing requests from different applications in round-robin (RR) fashion.

The advantage of this memory scheduling approach is that the bandwidth-intensive application would not be able to starve its co-scheduled applications for a long period time.

![image](https://github.com/user-attachments/assets/72d1319c-7809-4e6f-9dd4-9e2c14ab9b6d)

The request prioritization order of FR-RR-FCFS is: 1) row-buffer-hit requests over all other requests, 2) requests from the application next in the round-robin scheduling order, 3) older requests over younger ones.








