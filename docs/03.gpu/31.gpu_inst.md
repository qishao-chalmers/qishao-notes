---
title: GPU Insturctions
date: 2025-04-02
permalink: /pages/45890/
---

1. [20] Decoding CUDA Binary

---

1. [20] Decoding CUDA Binary
2. [7 2024 ASPLOS] A Journey of a 1,000 Kernels Begins with a Single Step A Restrospective of Deep Learning on GPUs
3. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking
4. Dissecting the NVidia Turing T4 GPU via Microbenchmarking


### Compiling Flow

When every thread in the warp has reached a re-convergence command - either a .S modifier or a SYNC instruction, 
depending on the architecture - it will wait until the thread warp reaches the instruction whose address is specified by the
SSY instruction, and then return to running in lock-step.

![image](https://github.com/user-attachments/assets/66319026-1b78-410c-a6c5-68d09375da76)

### Assemble Code
64 bits or 128 bits
![image](https://github.com/user-attachments/assets/86e4c66a-1b8f-4b18-8efd-bdb40638b80d)

### Load/Store Instruction and Control Flow of Divergence
![image](https://github.com/user-attachments/assets/6c3891a8-c1ee-4eef-9526-febbd597188a)

### Compile-Time Scheduling
As of Compute Capability 3.0, *instruction scheduling is handled by the compiler* rather than by the hardware.

On this architecture every 8−th instruction, rather than being a real instruction, is a set of scheduling codes inserted by the compiler.

These scheduling codes dictate the minimum number of cycles that the thread must wait between every two consecutive instructions in the following seven instructions in order to satisfy dependence constraints.

Starting with Compute Capability 5.0, NVIDIA moved even more control logic away from the hardware, saving power and space.

Thus instruction-level barrier has been added to the scheduling codes generated by the compiler.

The scheduling codes on Compute Capabilities 5.x and 6.x occur in place of every fourth instruction.

As of Compute Capability 7.0, **they are embedded into each individual instruction**, rather than controlling larger blocks of instructions.

#### Instruction with Operand

![image](https://github.com/user-attachments/assets/c12c2fae-dd53-420a-96f8-e05615ef0035)


Although instructions are of fixed length, NVIDIA’s instruction sets lack the relative simplicity of a RISC architecture.

It includes complicated instructions such as multiplication-and-addition, multi-function operation that performs trigonometric functions including sine and cosine, and so on.

Although we can make generalizations about which bits are used for which components of the instruction, there are few consistent rules across different instructions.

Check PSETP, it has 3 source operands.
![image](https://github.com/user-attachments/assets/b38e7590-f494-45a8-a029-643a7cdf7941)

![image](https://github.com/user-attachments/assets/54cd8f74-c1bc-4496-b175-8214972bef0c)


### Reverse Engineering Tool
nvdisasm and sass2ptx

### Instruction Format Basics
#### Instruction Length:

NVIDIA GPU instructions are typically 8 or 16 bytes in length (i.e., 64 or 128 bits), depending on the generation and specific instruction.

Most common instructions are encoded in 8 bytes, but certain instructions may require 16 bytes for additional fields (larger immediate values, special modifiers, etc.).

#### Predicate Bits:

Each instruction can be conditionally executed based on a predicate register (e.g., @P0 or @!P0).

The instruction encoding typically reserves a few bits for specifying which predicate is used, whether it’s negated, and whether the instruction updates that predicate or only tests it.

#### Opcode and Sub-Op Fields:

A chunk of bits is used to identify the primary operation (e.g., FADD for floating-point add, IMUL for integer multiply, LDG for global memory load, etc.).

Some instructions have “sub-ops” or “specialization bits” that further refine the operation (e.g., specifying data type, rounding mode, or variant of the operation).

#### Source and Destination Registers:

SASS instructions typically encode up to four source operands and one or two destinations (though most commonly one destination).

The register indices (e.g., R0, R1, R2, etc.) appear in dedicated fields.

Depending on the instruction, immediate operands (e.g., a constant offset) may replace a register operand.

#### Modifiers and Flags:

Many instructions have bits for modifiers (e.g., .CC to set condition codes, .SAT to enable saturation, etc.).

Reuse flags (discussed in your previous question) are also stored in a few bits in the encoding.

Additional bits might control things like whether a memory operation is cache-specific (.E for eviction policy, .L1 or .L2 usage, etc.), or whether an instruction is uniform across a warp, and so on.

#### Scheduling Information:

Modern NVIDIA architectures embed scheduling information in the instruction to help the hardware’s instruction scheduler. You might see references to “stall” counts or “read dependency” codes. In short:

A few bits can indicate how many cycles to wait before reading certain registers, or how many cycles to wait before issuing the next instruction.

This is sometimes referred to as “scheduling” or “scoreboarding” fields.

#### Operand Encoding and Immediate Values
Register Operands: Typically specified by a field that directly encodes the register number (e.g., 7 bits for the register index if up to 128 registers).

Immediate Operands: Some instructions support small inline immediates.

The immediate field is part of the instruction encoding, using a certain number of bits.

If the immediate is too large to fit, a 16-byte (128-bit) encoding might be used, or the compiler may materialize the immediate in a register first.

Addressing Modes: Memory instructions (LDG, STG, LDS, etc.) often encode an offset or a base+offset form. Some bits specify how to interpret those, e.g., 8-bit or 20-bit offset, sign extension, scaled by data type size, etc.

#### Reuse Flags and the 2-Way Associative CAM
Reuse Flags: Each instruction can mark which of its first four source registers should be saved in a small local cache, so that subsequent instructions can reuse them without accessing the main register file.

These flags are 4 bits in the SASS encoding (one per operand), typically in the lower part of the 8-byte instruction encoding.

This is a micro-architectural feature that helps reduce register file pressure and bank conflicts.

#### Predicate and Condition Code Fields
One or two bits designate whether an instruction is predicated (e.g., @P0, @!P0, etc.).

Another small field may specify which predicate register is used (since GPUs can have multiple predicate registers).

Some instructions also set condition codes (e.g., for subsequent instructions to test), which the hardware might encode in a “condition code” sub-field.

#### Example Layout (Hypothetical)
Below is a hypothetical 64-bit (8-byte) SASS instruction breakdown (not official, but a conceptual approximation):

![image](https://github.com/user-attachments/assets/22bb7fc6-1272-41cd-a662-843609fe2782)

- Opcode (7 bits): Identifies the core instruction (e.g., FADD, IMUL, LDG).
- Source Registers (5 bits each): Up to three or four sources, each needing enough bits to address the register file.
- Destination Register (5 bits): Usually one, possibly two in some instructions (like a multiply-add that writes an extra output).
- Modifiers / Flags: Several bits for controlling instruction behavior (e.g., rounding modes, type specifiers, etc.).
- Reuse Flags (4 bits): One bit per operand position, marking which registers to cache.
- Predicate / Condition Code: Often stored in either the high bits or low bits, depending on generation.
- Scheduling Info: Usually a small field that the compiler sets to help with instruction issuing/stalling.

Different architectures shift these fields around or allocate more/less bits, but the principle remains similar.


### Predicate

NVIDIA GPUs use predicate registers and explicit branch instructions (e.g., BRA) to handle conditional logic at the SASS (assembly) level.

In concert with the hardware’s warp execution model, this mechanism can create control flow divergence when different threads of the same warp take different paths.

Below is a more detailed explanation of how predicates, branching, and divergence work together.

#### The Basic Idea of Predication
A predicate (e.g., P0, P1, etc.) is a 1-bit register that can be set or cleared by a comparison instruction. For example:

```
PSETP.EQ.U32 P0, PT, R4, RZ, PT;
```

This sets predicate P0 to 1 if R4 == 0; otherwise P0 = 0.

PT means “always pass” (no predicate on that comparison itself).

RZ is the “zero register.”

Once a predicate is set, any subsequent instruction can be predicated—i.e., guarded—by referencing that predicate. For example:

```
@P0 IADD R5, R5, R6;
```

Reads as: “Perform IADD R5, R5, R6 only if P0 == 1; otherwise do nothing.”

However, you can also have a predicated branch:

```
@P0 BRA 0x210;
```

Means: “If P0 == 1, jump to the instruction at address 0x210; if P0 == 0, continue sequentially.”

#### Divergence and the Active Mask
A key point about GPU execution is that an entire warp (32 threads on most NVIDIA hardware) executes in lockstep on a single instruction stream. If a branch is taken by some threads but not others, the warp must diverge:
- The hardware splits the warp’s threads into multiple “subsets,” one subset that takes the branch and another that doesn’t.
- The warp serially executes each subset’s path, with the other subset of threads masked out (inactive).

At the end, the warp reconverges at a known instruction (e.g., the instruction pointed to by SSY).

This is how a single warp can handle different control flow paths for its 32 threads.

####  The Role of SSY (Set SYnchronization)
When the compiler emits:

```
SSY 0x238
@P0 BRA 0x210
...
SYNC
```
SSY 0x238 instructs the hardware to push a reconvergence point (address 0x238) onto the hardware’s “divergence stack.”

The next branch—@P0 BRA 0x210—can cause divergence: some threads branch, others continue.

After each subset of threads has finished, the hardware automatically goes to 0x238 (the SSY target) to reconverge the warp, so all threads proceed together again.

#### Using Predicates to Create Conditional Branches
Example Flow
Let’s say your high-level code is:

```
if (tid != 0) {
    sum += 4;
}
```

The compiler might produce:

```
// 1) Compare tid != 0, store result in P0
PSETP.NE.U32 P0, PT, R9, RZ, PT;  // if (R9 != 0) P0=1; else P0=0

// 2) Set the reconvergence point after the IF block
SSY targetAddr

// 3) Predicated branch: jump if P0=1
@P0 BRA insideIf

// (fallthrough path: if P0=0, skip the IF block)
BRA endIf   // or a SYNC, depending on code structure

insideIf:
 IADD32I R8, R8, 4;   // sum += 4
 SYNC

endIf:
// warp reconverges here
```

Here, the hardware uses P0 to decide which threads branch and which do not. The warp may need multiple passes if some threads are P0=1 and others are P0=0.

#### (tid) and (tid % 4): Why You Don’t Always See Explicit Instructions
In high-level CUDA (C/C++), you write:

```
int tid = threadIdx.x;
if (tid != 0) ...
if (tid % 4 == 0) ...
```

But the final SASS might not show a direct “integer modulo” or “compare to zero” instruction. Instead:

- **Thread ID is Already in a Register**
  - At the start of the kernel, the compiler loads threadIdx.x into a register (R9 or something).
  - That code might be hoisted well above the snippet you’re looking at.

- Modulo 4 = Bitwise Test
  - tid % 4 == 0 is the same as (tid & 3) == 0.
  - The compiler can use a single bitwise AND or LOP3 (logical operation) to check the two least significant bits. Then it sets a predicate based on the result.
  - So you’ll see something like LOP3.LUT P1, R9, RZ, ... 0x... or I2I P1, R9, AND, 3 or some variant that directly sets P1.

- Predicate is Checked at Branch
  - Instead of a standalone branch if (tid != 0), the hardware does PSETP.NE.U32 P0, R9, RZ followed by @P0 BRA label.
  - This merges “compare” + “branch” logic with the warp’s active mask concept.

Hence, you rarely see an explicit “%4” machine instruction or a standalone CMP R9 != 0 or “TID instruction.”

The predicate logic folds these checks into specialized SASS instructions that set P0 or P1, and then uses predicated branching or predicated instructions.

#### Putting It All Together
**Predicate Computation** A special compare (e.g., PSETP) sets P0 based on (tid != 0), (tid & 3) == 0, etc.
**Control Divergence** If some threads in the warp have P0=1 and others have P0=0, the warp diverges when it hits @P0 BRA somewhere.

Reconvergence: An SSY target before the branch and a SYNC (or matching BRA) after the branch help the hardware manage warp subsets and eventually bring them back together at the same program counter.

In short, predicates let each thread in a warp conditionally execute code. If different threads disagree on the condition, the warp temporarily serializes the different paths but eventually merges (reconverges) again at an SSY target. This is how GPUs handle “if” statements, loops, etc., across thousands of parallel threads.

---

**Summary of the Paper** 
The paper investigates the evolution of deep learning (DL) applications on GPUs by examining a diverse range of state-of-the-art applications and hardware across three NVIDIA GPU generations: **P100** , **V100** , and **A100** . It provides a comprehensive analysis at three levels:
 
- **Framework Level**  (TensorFlow/PyTorch)
 
- **Device API Level**  (e.g., cuDNN, CUDA kernels)
 
- **Hardware and Microarchitecture Level**

The authors develop a benchmarking suite named **CaSiO**  (covering applications from domains like computer vision, physical simulation, language processing, etc.) to capture a wide range of realistic production workloads that are broader and more diverse than conventional benchmarks like MLPerf.


---
## [7 2024] A Journey of a 1,000 Kernels Begins with a Single Step A Restrospective of Deep Learning on GPUs

### **Key Objectives and Questions Addressed** 

The paper aims to answer three major questions:

- **Application Scaling:** 
*How does the behavior of real-world DL applications scale across GPU hardware generations?*
- **Hardware-Software Interactions:** 
*What are the software-compiler-hardware interactions that either enable or limit generational speedup?*
- **Future Directions:** 
*What insights can we gain from current trends to inform the development of future GPU architectures?*

### **Main Insights and Findings** 
#### **Application-Level Observations** 
- **Operator Diversity:**  Modern DL applications are diverse and require a large set of operators beyond simple matrix multiplications or convolutions.

![image](https://github.com/user-attachments/assets/975da5b1-c112-4368-a883-4202ce0da33a)

- **GEMM Decline:**  While GEMM-based kernels (e.g., matrix multiplications) have traditionally dominated GPU computations, their relative importance decreases with newer hardware generations.
- **Specialization and Complexity:**  The software ecosystem must support an extensive set of shape-specialized kernels to maximize hardware utilization.

![image](https://github.com/user-attachments/assets/a59a404c-bba3-4aaf-b529-fd495e1ead03)

#### **Detailed GEMM Analysis** 
 - **Shape Specialization:**  Significant performance improvements (on V100 over P100) were driven by specialized kernels tailored for specific GEMM shapes. However, the further generational speedups (A100 over V100) were modest.
 - **Utilization:**  Hardware utilization for GEMM operations peaked on V100 but began declining on the A100 due to challenges in effectively utilizing massively parallel hardware for smaller or irregular shapes.

#### **Hardware Execution States** 
 - The authors defined a taxonomy based on three key hardware execution characteristics:
  - **Thread parallelism**  (low, medium, high)
  - **Compute utilization**  (SM utilization) 
  - **Memory utilization**  (DRAM bandwidth usage)
They found:
- Substantial under-utilization of resources even for seemingly optimized GEMM-heavy workloads.
- Frequent switching between execution states, highlighting the dynamic and diverse nature of modern AI applications.

### **Hardware Performance Analysis** 
- **P100 → V100:**  Significant speedups due to the introduction of TensorCores (specialized GEMM units).
- **V100 → A100:**  Modest gains indicating diminishing returns on GEMM acceleration alone.

![image](https://github.com/user-attachments/assets/826ea8d5-ea36-4e5c-b335-9c580c7bef52)

### **Insights into Future Hardware Directions** 
Based on their observations, the authors highlight three major areas for future hardware optimization:

#### **Compute Orchestration:** 
Optimizing hardware execution units to better handle diverse and irregular shapes of GEMMs, especially smaller and less regular computations.

![image](https://github.com/user-attachments/assets/ed5550cf-b903-42b5-a5e6-a9f0ed41c0d6)

 
#### **Data Orchestration:** 
Improving data movement operations in memory systems, since many kernels spend significant execution time on memory-bound or data-movement-heavy operations.
 
#### **Dependence Orchestration:** 
Better leveraging the rapidly changing execution states and complex dependencies in DL algorithms to maximize hardware resource occupancy.

The authors present a case study demonstrating that by focusing on these behaviors, future architectures could achieve around **2.3X geometric mean speedup**  over current GPU architectures.

### **Contributions and Resources** 
The authors provide a curated set of real-world DL applications, their infrastructure (**CaSiO**  suite), and detailed performance data at different levels for public access to aid future research.
 
- GitHub Repository: [CaSiO Suite](https://github.com/VerticalResearchGroup/casio)
 
- Detailed GEMM analysis data: [GEMM Shape Performance Data](https://github.com/VerticalResearchGroup/casio-gemms/blob/main/gemms.csv)
    - Large M,N,K provide consistent performance
    - At least one of M,N,K is very small
      With very small K, the inner loop doesn’t run enough, causing under-utilization of faster compute resources.
      With Large M or N but small K, utilization could still be high. but if both M and N are low, the utilization will be low.

![image](https://github.com/user-attachments/assets/0b06a53a-ecac-4dcf-b8b8-4c8dd88ef36c)


### **Significance** 
This paper is significant because:
- It systematically addresses the limits and future potentials of GPU-based accelerators for DL workloads.
- It provides empirical insights into how actual, complex DL workloads interact with evolving hardware, guiding future designs.
- It challenges the conventional wisdom of GEMM-centric acceleration by demonstrating diminishing returns and highlighting the need for more diverse hardware strategies.

### **Conclusion and Impact** 

The paper underscores a critical transition point in AI hardware development, clearly demonstrating that the era of straightforward GEMM-based acceleration has reached diminishing returns.

It makes a strong case for a more nuanced, algorithm-aware hardware architecture that can dynamically adapt to a wide variety of computational patterns and data orchestration needs.

By providing both detailed empirical evidence and actionable insights, the paper offers valuable guidance for architects and system designers developing next-generation hardware platforms tailored to real-world DL workloads.
