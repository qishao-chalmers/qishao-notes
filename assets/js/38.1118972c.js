(window.webpackJsonp=window.webpackJsonp||[]).push([[38],{492:function(t,e,n){"use strict";n.r(e);var a=n(8),i=Object(a.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"mlir-compiling-flow-of-transformer-decoder"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#mlir-compiling-flow-of-transformer-decoder"}},[t._v("#")]),t._v(" MLIR Compiling Flow of Transformer-Decoder")]),t._v(" "),e("blockquote",[e("p",[t._v("This is generated by ChatGPT.")])]),t._v(" "),e("p",[t._v("This document details the MLIR compilation flow for a "),e("strong",[t._v("Transformer decoder")]),t._v(", focusing on key computations such as:")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Self-attention")]),t._v(" (QKV projections, matmul, softmax, output projection)")]),t._v(" "),e("li",[e("strong",[t._v("MLP")]),t._v(" (Feed-forward layers with matmul, activation functions)")]),t._v(" "),e("li",[e("strong",[t._v("Layer Normalization")]),t._v(" (Reduction and elementwise operations)")])]),t._v(" "),e("p",[t._v("The compilation follows the pipeline:")]),t._v(" "),e("ol",[e("li",[e("strong",[t._v("Frontend Import (Torch Dialect)")]),t._v(": Convert TorchScript to the "),e("code",[t._v("torch")]),t._v(" dialect, preserving dynamic semantics.")]),t._v(" "),e("li",[e("strong",[t._v("Computational Pattern Lowering (Linalg Dialect)")]),t._v(": Convert tensor computations (matmul, elementwise ops) to structured "),e("code",[t._v("linalg")]),t._v(" operations.")]),t._v(" "),e("li",[e("strong",[t._v("Iteration Space Optimization (Affine Dialect)")]),t._v(": Introduce affine transformations for loop nest optimization, tiling, and dependency analysis.")]),t._v(" "),e("li",[e("strong",[t._v("Lowering to Vectorization and SCF")]),t._v(": Apply vectorization, loop transformations, and parallelize structured loops ("),e("code",[t._v("scf")]),t._v(" dialect).")]),t._v(" "),e("li",[e("strong",[t._v("Final GPU Lowering (GPU Dialect)")]),t._v(": Map computation to GPU blocks/threads, introduce shared memory, and lower to NVGPU/LLVM.")])]),t._v(" "),e("p",[t._v("Each stage includes "),e("strong",[t._v("MLIR code snippets")]),t._v(" demonstrating how IR evolves during compilation.")]),t._v(" "),e("h2",{attrs:{id:"_1-frontend-import-torchscript-to-torch-dialect"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-frontend-import-torchscript-to-torch-dialect"}},[t._v("#")]),t._v(" 1. Frontend Import: TorchScript to Torch Dialect")]),t._v(" "),e("p",[t._v("The Transformer decoder is first exported from PyTorch as a TorchScript model and converted into MLIR’s "),e("code",[t._v("torch")]),t._v(" dialect. This preserves dynamic shapes and PyTorch semantics:")]),t._v(" "),e("div",{staticClass:"language-mlir line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("%q = torch.aten.matmul %input, %weight_q : (!torch.vtensor<[?,64],f32>, !torch.vtensor<[64,64],f32>) -> !torch.vtensor<[?,64],f32>\n%k = torch.aten.matmul %input, %weight_k : (!torch.vtensor<[?,64],f32>, !torch.vtensor<[64,64],f32>) -> !torch.vtensor<[?,64],f32>\n%v = torch.aten.matmul %input, %weight_v : (!torch.vtensor<[?,64],f32>, !torch.vtensor<[64,64],f32>) -> !torch.vtensor<[?,64],f32>\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br"),e("span",{staticClass:"line-number"},[t._v("2")]),e("br"),e("span",{staticClass:"line-number"},[t._v("3")]),e("br")])]),e("p",[t._v("At this stage, operations like matrix multiplications, softmax, and elementwise operations remain in "),e("code",[t._v("torch.aten")]),t._v(" form.")]),t._v(" "),e("h2",{attrs:{id:"_2-computational-pattern-lowering-torch-to-linalg-dialect"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-computational-pattern-lowering-torch-to-linalg-dialect"}},[t._v("#")]),t._v(" 2. Computational Pattern Lowering: Torch to Linalg Dialect")]),t._v(" "),e("p",[t._v("Next, "),e("code",[t._v("torch.aten")]),t._v(" operations are lowered into "),e("code",[t._v("linalg")]),t._v(" operations, which define structured computations over tensors. This allows for explicit loop optimizations.")]),t._v(" "),e("div",{staticClass:"language-mlir line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("%q = linalg.matmul ins(%input, %weight_q) outs(%q_init) -> tensor<?x64xf32>\n%k = linalg.matmul ins(%input, %weight_k) outs(%k_init) -> tensor<?x64xf32>\n%v = linalg.matmul ins(%input, %weight_v) outs(%v_init) -> tensor<?x64xf32>\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br"),e("span",{staticClass:"line-number"},[t._v("2")]),e("br"),e("span",{staticClass:"line-number"},[t._v("3")]),e("br")])]),e("p",[t._v("Additionally, elementwise functions (e.g., softmax, GeLU) are converted into "),e("code",[t._v("linalg.generic")]),t._v(":")]),t._v(" "),e("div",{staticClass:"language-mlir line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v('%softmax_out = linalg.generic { indexing_maps = [...], iterator_types = ["parallel"] }\n  ins(%logits) outs(%softmax_init) {\n    ^bb(%arg0: f32):\n    %exp = math.exp %arg0 : f32\n    linalg.yield %exp : f32\n  } -> tensor<?x?xf32>\n')])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br"),e("span",{staticClass:"line-number"},[t._v("2")]),e("br"),e("span",{staticClass:"line-number"},[t._v("3")]),e("br"),e("span",{staticClass:"line-number"},[t._v("4")]),e("br"),e("span",{staticClass:"line-number"},[t._v("5")]),e("br"),e("span",{staticClass:"line-number"},[t._v("6")]),e("br")])]),e("p",[t._v("This explicit representation enables further transformations such as tiling and vectorization.")]),t._v(" "),e("h2",{attrs:{id:"_3-iteration-space-optimization-affine-dialect"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-iteration-space-optimization-affine-dialect"}},[t._v("#")]),t._v(" 3. Iteration Space Optimization: Affine Dialect")]),t._v(" "),e("p",[t._v("Affine transformations introduce "),e("strong",[t._v("loop nest optimization")]),t._v(", "),e("strong",[t._v("tiling")]),t._v(", and "),e("strong",[t._v("dependency analysis")]),t._v(":")]),t._v(" "),e("div",{staticClass:"language-mlir line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("scf.for %i = 0 to %M step 16 {\n  scf.for %j = 0 to %N step 16 {\n    %tile_q = affine.load %Q[%i, %j] : memref<?x64xf32>\n    %tile_k = affine.load %K[%i, %j] : memref<?x64xf32>\n    %tile_v = affine.load %V[%i, %j] : memref<?x64xf32>\n    %out = linalg.matmul %tile_q, %tile_k : tensor<16x64xf32>\n  }\n}\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br"),e("span",{staticClass:"line-number"},[t._v("2")]),e("br"),e("span",{staticClass:"line-number"},[t._v("3")]),e("br"),e("span",{staticClass:"line-number"},[t._v("4")]),e("br"),e("span",{staticClass:"line-number"},[t._v("5")]),e("br"),e("span",{staticClass:"line-number"},[t._v("6")]),e("br"),e("span",{staticClass:"line-number"},[t._v("7")]),e("br"),e("span",{staticClass:"line-number"},[t._v("8")]),e("br")])]),e("p",[t._v("The affine dialect provides static scheduling guarantees, allowing aggressive compiler optimizations.")]),t._v(" "),e("h2",{attrs:{id:"_4-lowering-to-vectorization-and-scf"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-lowering-to-vectorization-and-scf"}},[t._v("#")]),t._v(" 4. Lowering to Vectorization and SCF")]),t._v(" "),e("p",[t._v("At this stage, computations are "),e("strong",[t._v("vectorized")]),t._v(" and "),e("strong",[t._v("parallelized")]),t._v(" using structured loops ("),e("code",[t._v("scf.for")]),t._v("):")]),t._v(" "),e("div",{staticClass:"language-mlir line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("vector.transfer_read %A[%i, %j] : memref<?x64xf32> -> vector<4xf32>\n%result = vector.fma %vecA, %vecB, %vecC : vector<4xf32>\nvector.transfer_write %result, %C[%i, %j] : memref<?x64xf32>\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br"),e("span",{staticClass:"line-number"},[t._v("2")]),e("br"),e("span",{staticClass:"line-number"},[t._v("3")]),e("br")])]),e("p",[t._v("By vectorizing the inner loops, MLIR ensures that matrix multiplications and elementwise ops map efficiently to hardware vector units.")]),t._v(" "),e("h2",{attrs:{id:"_5-final-gpu-lowering-mapping-to-gpu-blocks-threads"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-final-gpu-lowering-mapping-to-gpu-blocks-threads"}},[t._v("#")]),t._v(" 5. Final GPU Lowering: Mapping to GPU Blocks/Threads")]),t._v(" "),e("p",[t._v("The final stage maps computations to GPU thread blocks, introduces shared memory buffers, and replaces operations with hardware-specific intrinsics:")]),t._v(" "),e("div",{staticClass:"language-mlir line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("gpu.launch blocks(%grid_x, %grid_y, %c1) threads(%block_x, %block_y, %c1) {\n  %q_tile = memref.alloc() : memref<16x16xf32, #gpu.address_space<workgroup>>\n  %k_tile = memref.alloc() : memref<16x16xf32, #gpu.address_space<workgroup>>\n  nvgpu.device_async_copy %q_global, %q_tile\n  nvgpu.device_async_wait\n  %acc = nvgpu.mma.sync %q_tile, %k_tile, %acc_init : tensor<16x16xf32>\n  gpu.barrier\n}\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br"),e("span",{staticClass:"line-number"},[t._v("2")]),e("br"),e("span",{staticClass:"line-number"},[t._v("3")]),e("br"),e("span",{staticClass:"line-number"},[t._v("4")]),e("br"),e("span",{staticClass:"line-number"},[t._v("5")]),e("br"),e("span",{staticClass:"line-number"},[t._v("6")]),e("br"),e("span",{staticClass:"line-number"},[t._v("7")]),e("br"),e("span",{staticClass:"line-number"},[t._v("8")]),e("br")])]),e("p",[t._v("Here, we:")]),t._v(" "),e("ul",[e("li",[t._v("Allocate "),e("strong",[t._v("shared memory tiles")]),t._v(" ("),e("code",[t._v("memref.alloc")]),t._v(" in "),e("code",[t._v("workgroup")]),t._v(" memory space).")]),t._v(" "),e("li",[e("strong",[t._v("Insert async memory copies")]),t._v(" from global to shared memory ("),e("code",[t._v("nvgpu.device_async_copy")]),t._v(").")]),t._v(" "),e("li",[e("strong",[t._v("Use Tensor Core instructions")]),t._v(" ("),e("code",[t._v("nvgpu.mma.sync")]),t._v(") for efficient matrix multiplications.")]),t._v(" "),e("li",[t._v("Introduce "),e("strong",[t._v("synchronization")]),t._v(" ("),e("code",[t._v("gpu.barrier")]),t._v(").")])]),t._v(" "),e("h2",{attrs:{id:"conclusion"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[t._v("#")]),t._v(" Conclusion")]),t._v(" "),e("p",[t._v("This pipeline enables efficient compilation of Transformer decoder computations for GPUs. By leveraging MLIR’s "),e("strong",[t._v("structured dialects")]),t._v(", "),e("strong",[t._v("tiling")]),t._v(", "),e("strong",[t._v("vectorization")]),t._v(", and "),e("strong",[t._v("hardware mapping")]),t._v(", we systematically lower high-level computations into efficient GPU kernels. The final MLIR IR is ready for conversion into "),e("strong",[t._v("LLVM/NVPTX")]),t._v(" for execution on NVIDIA GPUs, ensuring high performance and optimized memory usage.")])])}),[],!1,null,null,null);e.default=i.exports}}]);