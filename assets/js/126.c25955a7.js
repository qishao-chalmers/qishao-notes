(window.webpackJsonp=window.webpackJsonp||[]).push([[126],{581:function(e,t,a){"use strict";a.r(t);var o=a(9),n=Object(o.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("[Y2024] Understanding Performance Implications of LLM Inference on CPUs")]),e._v(" "),t("li",[e._v("[75 Y2021] FLAT: An Optimized Dataflow forMitigating Attention Bottlenecks")])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_1-y2024-understanding-performance-implications-of-llm-inference-on-cpus"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-y2024-understanding-performance-implications-of-llm-inference-on-cpus"}},[e._v("#")]),e._v(" 1. [Y2024] Understanding Performance Implications of LLM Inference on CPUs")]),e._v(" "),t("p",[t("strong",[e._v("Flops")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/43b30c0a-14ac-4239-9cd1-61c65fb0d21f",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("Intel AMX Architecture")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/fe5d81c7-97bc-4029-ac73-118328ae5e6a",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("Model Size")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/b5dbd3ab-f3e0-4772-a8c8-eb43bc777afa",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("KV Cache Size")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/1d7eabab-388f-4cd8-b7aa-9351dc79420e",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("CPU Servers")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/3d5b683a-86d2-4fff-9093-8322c3448027",alt:"image"}})]),e._v(" "),t("p",[e._v("The memory of CPU is much bigger than GPU.")]),e._v(" "),t("p",[e._v("These normalized results highlight the performance benefits gained from the use of both the matrix multiplication accelerator and high-bandwidth memory on the SPR Max CPU.")]),e._v(" "),t("p",[e._v("The significant reduction in latency and improvement in throughput during the prefill phase is due to AMX support on the SPR Max CPU.")]),e._v(" "),t("p",[e._v("The throughput improvement in the memorybound decode phase is made possible by the higher memory bandwidth provided by HBM.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/c75e9230-f6c0-4087-b349-e563d3f4ebb5",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("With larger batch sizes, both models exhibit a decrease in LLC MPKI and an increase in core utilization, indicating a shift towards a more compute-bound execution.")])]),e._v(" "),t("p",[t("strong",[e._v("Key Finding#1")])]),e._v(" "),t("p",[e._v("With AMX support, larger cores and cache, and HBM integration, the SPR Max CPU significantly reduces latency and increases throughput for BF16 LLM inference compared to the ICL CPU.")]),e._v(" "),t("p",[t("strong",[e._v("Key Finding#2")])]),e._v(" "),t("p",[e._v("Proper memory and clustering configurations are essential for optimizing performance.")]),e._v(" "),t("p",[e._v("The Flat memory mode with Quadrant clustering offers the best latency and throughput for LLM inference.")]),e._v(" "),t("p",[t("strong",[e._v("Key Finding#3")])]),e._v(" "),t("p",[e._v("Using 48 SPR cores with HBM maximizes core utilization and minimizes inter-socket communication,resulting in the best performance across models.")]),e._v(" "),t("p",[t("strong",[e._v("GPU")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/98d80cd2-4d22-448a-bda7-48cd7be9cc7d",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("Key Finding#4")])]),e._v(" "),t("p",[e._v("Overall, GPUs outperform CPUs in LLM inference, but AMX-enabled CPUs can achieve lower latency and higher throughput for larger models requiring offloading.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/ac3a7b7d-ebc3-4a9f-8c13-a113f04bd62b",alt:"image"}})]),e._v(" "),t("p",[e._v("As the number of input tokens increases, GPU latency and throughput remain stable, while the SPR Max 9468 CPU shows more variability.")]),e._v(" "),t("p",[e._v("This is due to the CPU‚Äôs lower compute throughput and memory bandwidth, resulting in less favorable performance scalability.")]),e._v(" "),t("p",[e._v("Interestingly, for larger models such as LLaMA2-70B, the CPU outperforms the GPU in both latency and throughput across all sequence lengths.")]),e._v(" "),t("p",[e._v("This is primarily due to the "),t("strong",[e._v("significant time spent on data loading via the PCIe bus when the batch size is set to 1, as shown in Figure 18")]),e._v(".")]),e._v(" "),t("p",[e._v("As the batch size increases to 16, the performance gap between CPUs and GPUs widens, particularly for smaller models.")]),e._v(" "),t("p",[e._v("For larger models such as LLaMA2-70B, we observed that at sequence lengths of 256 or more, the H100 GPU even when using offloading-based LLM inference‚Äîachieves lower latency compared to the CPU.")]),e._v(" "),t("p",[e._v("This is because, "),t("strong",[e._v("at these longer sequence lengths, the CPU‚Äôs LLM inference throughput continues to decline, resulting in lower performance than the H100")]),e._v(".")]),e._v(" "),t("p",[e._v("Ratio of latency in memory access and gpu compute")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/32a860da-f86a-4b3e-a4aa-35ce8ff3cd34",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("However, in the case of the A100 GPU, the CPU outperforms the GPU across all sequence lengths.")])]),e._v(" "),t("p",[e._v("This demonstrates that lower PCIe bandwidth significantly degrades the performance of offloading-based LLM serving systems.")]),e._v(" "),t("p",[t("strong",[e._v("Key Finding#5")])]),e._v(" "),t("p",[e._v("For larger batch sizes, GPUs outperform CPUs in small models. Even in larger models that require offloading, CPUs may underperform at longer sequence lengths due to lower compute throughput.")]),e._v(" "),t("p",[t("strong",[e._v("We also note that new Grace-Hopper Superchip would see lower overheads for offloading from DRAM to the integrated H100 due to its higher NVLink bandwidth (900 GB/s versus PCIe 5.0‚Äôs 128 GB/s), albeit at a cost of ‚àº4x of the SPR CPU\nand DDR5 [40].")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/54409207-cc8b-48aa-b0c5-67365acf1b4e",alt:"image"}})]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_2-75-y2021-flat-an-optimized-dataflow-formitigating-attention-bottlenecks"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-75-y2021-flat-an-optimized-dataflow-formitigating-attention-bottlenecks"}},[e._v("#")]),e._v(" 2. [75 Y2021] FLAT: An Optimized Dataflow forMitigating Attention Bottlenecks")]),e._v(" "),t("p",[e._v("The main operators within attention layers exhibit distinct compute and memory characteristics posing notable bottlenecks on off-chip memory bandwidth compared to CONV and FC.")]),e._v(" "),t("ul",[t("li",[e._v("Significantly low operational intensity.")])]),e._v(" "),t("p",[e._v("Inherently low data reuse in activation-activation operators significantly reduces the operational density of such operators in attention layers.")]),e._v(" "),t("p",[e._v("This inherently low operational density subsequently makes the activation-activation operators fundamentally memory-bound.")]),e._v(" "),t("p",[t("strong",[e._v("While prior work on intra-operator dataflow optimization, such as loop transformation and scheduling techniques, targets CONV and batched FC operators by leveraging the ample intrinsic data reuse, which are not well-suited for activation-activation operators lacking data reuse opportunity.")])]),e._v(" "),t("ul",[t("li",[e._v("Complex many-to-many operators.")])]),e._v(" "),t("p",[t("strong",[e._v("The main attention operators have many-to-many relation, obscuring opportunities to use operator fusion in attention operators.")])]),e._v(" "),t("p",[e._v("That is because operator fusion in conventional ML compilers mainly target operations such as CONV and FC with one-to-one (i.e.element-wise) relation.")]),e._v(" "),t("ul",[t("li",[e._v("Prohibitively large intermediate tensors.")])]),e._v(" "),t("p",[e._v("The size of intermediate tensors in attention layers grows quadratically‚Äîùí™(ùëÅ2))‚Äîwith the sequence length.")]),e._v(" "),t("p",[e._v("This quadratic growth imposes a significant pressure on on-chip memory capacity and prohibits opportunities to store the intermediate results on-chip and improve the compute utilization, a common practice in CNN accelerators.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/210ccfaa-15d2-4b94-ae48-17a3143a35b0",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/c9fba01a-6450-4589-ad5f-d5ac810660df",alt:"image"}})]),e._v(" "),t("p",[e._v("CONV operators lie in the compute-bound region. FC operators scatter across both memory and compute bound region; however, with the increase in batch size, their operation.")]),e._v(" "),t("p",[e._v("We can see that CONV operators lie in the compute-bound region. FC operators scatter across both memory and compute bound region; however, with the increase in batch size, their operation intensity increases and can become compute-bound.")]),e._v(" "),t("p",[e._v("This demonstrates why batching is a popular technique for FClayers. In contrast, L andAoperators sit at memory-bound and low-performance region, and batch size increase is not effective in these operators.")]),e._v(" "),t("p",[t("strong",[e._v("Low operational intensity of the L/A operators makes them fundamentally memory-bound, and any dataflow/mapping exploration at the individual operator level cannot further improve performance.")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/29eb92e1-7326-4113-ba7a-06a35cc098a3",alt:"image"}})]),e._v(" "),t("p",[e._v("Element-Element fusion (E-E) is the simplest fusion optimization.")]),e._v(" "),t("p",[e._v("With increased interest in operation fusion, moreML compilers/frameworks today support Tensor-Element (T-E) or Element-Tensor (E-T) fusion, where MatMul operators (i.e., CONV or FC) are often fused with element-wise operators (such as ReLu or Add), reshapes, or shuffling operators.")]),e._v(" "),t("p",[e._v("However, Tensor-Tensor Fusion (T-T) is not done automatically.")]),e._v(" "),t("p",[e._v("The key reason is that T.O. is a many-to-many operator.")]),e._v(" "),t("p",[e._v("While it is often straightforward and a simple engineering exercise to fuse a T.O. with one-to-one operator like E.O., how to fuse many-to-many with other many-to-many and whether it is beneficial to fuse them is\nstill a research question.")]),e._v(" "),t("p",[t("strong",[e._v("This is a work even before flashAttention.")])]),e._v(" "),t("p",[e._v("To address this challenge, we design a specialized inter-operator dataflow that not only considers the data dependency of two large tensor operators but also tackles the complex data dependency incurred by the many-to-many intermediate activation")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/284efaa4-1a96-4c21-9ff9-5c8ae20e3228",alt:"image"}})])])}),[],!1,null,null,null);t.default=n.exports}}]);