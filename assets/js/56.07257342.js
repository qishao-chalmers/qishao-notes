(window.webpackJsonp=window.webpackJsonp||[]).push([[56],{512:function(e,a,t){"use strict";t.r(a);var s=t(8),i=Object(s.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("ol",[a("li",[e._v("[90] Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring")]),e._v(" "),a("li",[e._v("[6] SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs")]),e._v(" "),a("li",[e._v("[117] Observations and Opportunities in Architecting Shared Virtual Memory for Heterogeneous Systems üëç üëç üëç üëç üë¥")]),e._v(" "),a("li",[e._v("[2023] TunneLs for Bootlegging: Fully Reverse-Engineering GPU TLBs  for Challenging Isolation Guarantees of NVIDIA MIG üëç üëç üëç")]),e._v(" "),a("li",[e._v("[31] Big data causing big (TLB) problems: taming random memory accesses on the GPU üëç üëç üëç")]),e._v(" "),a("li",[e._v("[248] Dissecting GPU Memory Hierarchy through Microbenchmarking")]),e._v(" "),a("li",[e._v("[2023 HPCA] Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding")]),e._v(" "),a("li",[e._v("[2020 PACT] Enhancing Address Translations in Throughput Processors via Compression üåö")]),e._v(" "),a("li",[e._v("[2024 MICRO] Improving Multi-Instance GPU Efficiency via Sub-Entry Sharing TLB Design")]),e._v(" "),a("li",[e._v("[137 MICRO] Mosaic: A GPU Memory Manager with Application-Transparent Support for Multiple Page Sizes")]),e._v(" "),a("li",[e._v("[109 ASPLOS] MASK: Redesigning the GPU Memory Hierarchy to Support Multi-Application Concurrency")])]),e._v(" "),a("hr"),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/e013760d-117e-474c-ac32-69361ead08f0",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/4659c5a5-6a99-4dda-acf2-5768fd5d28ff",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"_1-dissecting-the-nvidia-volta-gpu-architecture-via-microbenchmaring"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-dissecting-the-nvidia-volta-gpu-architecture-via-microbenchmaring"}},[e._v("#")]),e._v(" 1. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring")]),e._v(" "),a("p",[e._v("On Volta and on all other architectures we examined:")]),e._v(" "),a("ul",[a("li",[e._v("the L1 data cache is indexed by virtual addresses;")]),e._v(" "),a("li",[e._v("the L2 data cache is indexed by physical addresses")])]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_2-snakebyte-a-tlb-design-with-adaptive-and-recursive-page-merging-in-gpus"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-snakebyte-a-tlb-design-with-adaptive-and-recursive-page-merging-in-gpus"}},[e._v("#")]),e._v(" 2. SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs")]),e._v(" "),a("h4",{attrs:{id:"idea"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#idea"}},[e._v("#")]),e._v(" Idea")]),e._v(" "),a("p",[e._v("SnakeByte allows multiple equal-sized pages coalescing into a page table entry (PTE)."),a("br"),e._v("\nIt records the validity of pages to be merged using a bit vector, and few bits are annexed to indicate the size of merged pages.")]),e._v(" "),a("h4",{attrs:{id:"tlb-ptw-gmmu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tlb-ptw-gmmu"}},[e._v("#")]),e._v(" TLB & PTW & GMMU")]),e._v(" "),a("p",[e._v("Departing from conventional paging schemes of CPUs that heavily rely on operating systems, hardware-based GPU memory management units (GMMUs) are essential to effectively separate device memory management from host\nCPUs."),a("br"),e._v("\nOtherwise, GPUs require the frequent intervention of OS to handle page table walks (PTWs) and TLB misses, which significantly penalize the GPU performance.")]),e._v(" "),a("p",[e._v("Observations:")]),e._v(" "),a("ul",[a("li",[e._v("GPU workloads demand a large number of TLB entries (e.g., 32K to 256K entries) to handle sizable working sets, but conventional TLBs cannot provide sufficient coverage.")]),e._v(" "),a("li",[e._v("GPU workloads have variable ranges of page contiguity.")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/ca8c2089-866b-4c16-a853-3a0f2fc792bc",alt:"image"}})]),e._v(" "),a("h5",{attrs:{id:"paper-idea"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#paper-idea"}},[e._v("#")]),e._v(" Paper Idea")]),e._v(" "),a("p",[e._v("If contiguity exists, valid bits are accordingly set in the bit vector. When all pages in the page group are allocated with contiguity (i.e., all valid bits set), the first PTE of the page group called base PTE is promoted to be further coalesced into a larger page group.")]),e._v(" "),a("h5",{attrs:{id:"address-translation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#address-translation"}},[e._v("#")]),e._v(" Address Translation")]),e._v(" "),a("p",[e._v("An L1 TLB is private to a streaming multiprocessor (SM), and an L2 TLB is shared among SMs [41], [42]."),a("br"),e._v("\nOn a last-level TLB miss, a request is sent to a centralized GMMU [18], [41], [42] to walk through page tables, and the GMMU concurrently handles multiple PTW requests (e.g., 8-16 PTWs)."),a("br"),e._v("\nTo amortize the latency cost of PTWs, GPUs employ page walk caches that store recently used translations at different levels of page tables."),a("br"),e._v("\nImportantly, the GMMU execution has to be independent of host-side operations unlike the conventional paging schemes of CPUs that heavily rely on operating systems. "),a("br"),e._v("\nOtherwise, GPUs involve frequent OS interventions, which significantly penalize the GPU performance [44], [54].")]),e._v(" "),a("p",[e._v("This observation is the primary motivation of SnakeByte that can flexibly manage variable-sized page groups and maximize TLB reach.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/28e7240f-6b4a-4832-996b-70450bbef038",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/0661a5c0-910f-4f72-b1df-368ecf94e376",alt:"image"}})]),e._v(" "),a("p",[e._v("When eight 4KB pages are allocated with contiguity, the page group is promoted to be coalesced into the next level of page group.")]),e._v(" "),a("p",[a("strong",[e._v("At the new page allocation, SnakeByte checks the contiguity of the new PTE with others in the page group.")])]),e._v(" "),a("h5",{attrs:{id:"simulation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#simulation"}},[e._v("#")]),e._v(" Simulation")]),e._v(" "),a("ul",[a("li",[e._v("By recursively coalescing PTEs, SnakeByte inevitably loses fine-grained controls on the A/D bits for individual pages."),a("br"),e._v("\nSnakeByte adds 8-bit access and dirty fields to a TLB entry to trace A/D states within a page group.")]),e._v(" "),a("li",[e._v("GPUs have long shootdown delays (4.2us).")]),e._v(" "),a("li",[e._v("The TLB hierarchy consists of a private L1 TLB per SM, a shared L2 TLB, and miss status holding registers (MSHRs)."),a("br"),e._v("\nAn MSHR in an L1 TLB merges up to 16 misses.")]),e._v(" "),a("li",[e._v("16 page table walkers can concurrently access four-level page tables, and a page walk cache per page table level stores up to 16 recently used translations.")]),e._v(" "),a("li",[e._v("When a new page is allocated, a sequential page prefetcher allocates 16 consecutive pages (total 64KB) at a time.")]),e._v(" "),a("li",[e._v("To analyze the effect of page migration latency [9], [55], we add a 20us latency overhead for each 4KB page fault [55] with 8.48GB/s bandwidth for a 64KB prefetcher [18].")])]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_4-tunnels-for-bootlegging-fully-reverse-engineering-gpu-tlbs-for-challenging-isolation-guarantees-of-nvidia-mig"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-tunnels-for-bootlegging-fully-reverse-engineering-gpu-tlbs-for-challenging-isolation-guarantees-of-nvidia-mig"}},[e._v("#")]),e._v(" 4. TunneLs for Bootlegging: Fully Reverse-Engineering GPU TLBs  for Challenging Isolation Guarantees of NVIDIA MIG")]),e._v(" "),a("p",[e._v("However, we surprisingly find that MIG does not partitation the last-level TLB, which is shared by all the compute units in a GPU.")]),e._v(" "),a("h4",{attrs:{id:"uvm-managed-pages"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#uvm-managed-pages"}},[e._v("#")]),e._v(" UVM-Managed Pages")]),e._v(" "),a("p",[e._v("A module in the NVIDIA driver is in charge of allocating UVM-managed pages."),a("br"),e._v("\nThe cudaMallocManaged() function registers a virtual address subspace for UVM use."),a("br"),e._v("\nThe UVM module allocates pages when the GPU accesses addresses in the registered address space."),a("br"),e._v("\nAlthough three page sizes are supported (see Figure 1), we find that UVM actually only "),a("strong",[e._v("allocates 64KB and 2MB pages")]),e._v(".")]),e._v(" "),a("p",[e._v("UVM starts with allocating 64KB pages, but it will merge the 64KB pages within a 2MB page into the 2MB page if the residency reaches certain conditions."),a("br"),e._v("\nFor example, if the first 17 or more 64KB pages in a 2MB page are present on GPU, the page table entries for these 64KB pages will be purged and replaced with a 2MB entry;"),a("br"),e._v("\nbut if just the first 16 64KB pages are used, the merging operation will not be triggered."),a("br"),e._v("\nWe find that some other residency patterns with less than 17 pages can also trigger the merging."),a("br"),e._v("\nFor instance, if every other 64KB page is used (i.e., 16 ones as there are 32 64KB pages in a 2MB page), the merging will also happen.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/45e7eaca-8a31-447a-a6f8-7520e28f2109",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"tlb-sub-entries"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tlb-sub-entries"}},[e._v("#")]),e._v(" TLB Sub-Entries")]),e._v(" "),a("p",[e._v("In [26], Nayak et al. claim that NVIDIA GPUs enforce TLB coalescing, which combines 16 address translations to occupy just one TLB entry if the corresponding virtual page numbers are consecutive and the mapped physical page frames are also contiguous."),a("br"),e._v("\nHowever, "),a("em",[e._v("the results of our experiments do not agree with this claim")]),e._v(".")]),e._v(" "),a("p",[e._v("We notice that address translations reside in one L2-uTLB or L3-uTLB entry as long as the virtual base addresses of the corresponding pages\nare:")]),e._v(" "),a("ul",[a("li",[e._v("within the same 1MB-aligned address range if the pages are 64KB")]),e._v(" "),a("li",[e._v("within the same 32MB-aligned range if the pages are 2MB.")])]),e._v(" "),a("p",[e._v("This observation disproves the existence of dynamically coalescing TLB entries and explains why we separate the base addresses by 0x100000 (i.e., 1MB) and 0x2000000 (i.e., 32MB) when using sequences of 64KB and 2MB pages respectively to perform the above-mentioned tasks.")]),e._v(" "),a("p",[a("strong",[e._v("Instead of TLB coalescing, we conjecture that there are 16 sub-entries in one L2-uTLB or L3-uTLB entry")]),e._v(","),a("br"),e._v("\nand they have a one-to-one mapping relationship with the address translations for 16 pages of size 64KB or 2MB located in the same 1MB- or 32MB-aligned range."),a("br"),e._v("\nIf any sub-entry encounters an eviction, the rest of them are also invalidated."),a("br"),e._v(" "),a("strong",[e._v("Interestingly, we find that the entries of L1-iTLB and L1-dTLB do not have such sub-entries.")])]),e._v(" "),a("p",[a("em",[e._v("Inclusivity and Exclusivity.")]),a("br"),e._v("\nWe find that the L2-uTLB is neither inclusive nor exclusive in all the inspected GPUs. The same is also true for the L3-uTLB.")]),e._v(" "),a("p",[a("em",[e._v("Reinsertion.")]),e._v("\nWe find that an L2-uTLB hit is reinserted into the L1 and an L3-uTLB hit is also reinserted into the L2 and L1.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/7a67a196-7a7e-4e10-9fb3-4e1a55768aa4",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"observations"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#observations"}},[e._v("#")]),e._v(" Observations")]),e._v(" "),a("p",[e._v("We observe that the execution of the infinite loop on all the tested GPUs is not affected after modifying the address translation for the code page, which implies that (at least) the "),a("em",[e._v("L1 TLB of a GPU is split into an L1-iTLB and an L1-dTLB.")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/d3f79741-1f3f-45f5-915c-95fcce02c021",alt:"image"}}),e._v(" "),a("em",[e._v("From [6]")])]),e._v(" "),a("p",[e._v("we can infer that the L1-iTLB of these GPUs has 16 entries and is fully-associative (otherwise, the smallest ùëÅ evicting the target address translation should differ from 16 occasionally).")]),e._v(" "),a("p",[e._v("Exchanging the above roles played by code and data pages, we can learn that the L1-dTLB of all these GPUs also has 16 entries and is fully-associative.")]),e._v(" "),a("p",[e._v("We find that the L1-iTLB and the L1-dTLB are private to each SM in Turing GPUs (e.g., RTX 2080), but they are shared between the two SMs of each TPC in Ampere GPUs (e.g., RTX 3080 and A100).")]),e._v(" "),a("p",[e._v("This observation indicates that at least two levels of unified TLBs, which we call L2-uTLB and L3-uTLB.")]),e._v(" "),a("p",[e._v("L2-uTLB is 8-way set-associative in all the tested GPUs.")]),e._v(" "),a("p",[e._v("These observations lead to the conclusion that the L3-uTLB in MIG-supported GPUs is still 8-way set-associative and it is (physically or just logically) split into two slices; and each slice has an 8-entry victim buffer shared by all the TLB sets in the slice.")]),e._v(" "),a("p",[e._v("L3-uTLB is shared by all the SMs.")]),e._v(" "),a("h3",{attrs:{id:"_5-big-data-causing-big-tlb-problems-taming-random-memory-accesses-on-the-gpu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-big-data-causing-big-tlb-problems-taming-random-memory-accesses-on-the-gpu"}},[e._v("#")]),e._v(" 5. Big data causing big (TLB) problems: taming random memory accesses on the GPU")]),e._v(" "),a("p",[e._v("If the data accesses are irregular, like hash table accesses or random sampling, the GPU performance can suffer."),a("br"),e._v("\nEspecially when scaling such accesses beyond 2GB of data, a performance decrease of an order of magnitude is encountered."),a("br"),e._v("\nThis is paper analyzes the source of the slowdown through extensive micro-benchmarking, attributing the root cause to the Translation Lookaside Buffer (TLB).")]),e._v(" "),a("h4",{attrs:{id:"introduction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),a("p",[e._v("GPU data larger than 2GB, which, in some cases, may result in a ‚âà13.3x runtime decrease."),a("br"),e._v("\nwe identified the Translation Lookaside Buffer (TLB) as the  source of this slowdown, where TLB misses cost hundreds of cycles per memory access.")]),e._v(" "),a("ul",[a("li",[e._v("NVIDIA Kepler [15]")]),e._v(" "),a("li",[e._v("NVIDIA Pascal [16]")])]),e._v(" "),a("p",[e._v("the P100 shows a significantly better performance than the K80, as it has a newer hardware architecture."),a("br"),e._v("\nHowever, the slowdown for memory accesses >2GB is still significant with factors of 4.3x for random sampling and 3.3x for grouping.")]),e._v(" "),a("h4",{attrs:{id:"benchmark"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#benchmark"}},[e._v("#")]),e._v(" Benchmark")]),e._v(" "),a("h5",{attrs:{id:"virtual-memory"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#virtual-memory"}},[e._v("#")]),e._v(" Virtual Memory")]),e._v(" "),a("p",[e._v("The reasons why GPU use virtual address"),a("br"),e._v("\n(1) Isolation: The indirection controls a program‚Äôs memory accesses and, thus, keeps it from disallowed memory accesses to internal\ndevice data or to data of other applications using the same GPU."),a("br"),e._v("\n(2) Fragmentation: Memory fragmentation can be hidden with virtual pages, allowing a large consecutive region of virtual memory to be scattered across many positions in physical memory."),a("br"),e._v("\nThis can also increase memory bandwidth if physical memory is scattered to multiple memory chips, which then can be accessed in parallel.")]),e._v(" "),a("h5",{attrs:{id:"benchmark-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#benchmark-2"}},[e._v("#")]),e._v(" Benchmark")]),e._v(" "),a("p",[a("em",[e._v("pointer chasing with stride distance")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/ffc46301-1aa9-43b0-814d-f93332aba085",alt:"image"}})]),e._v(" "),a("p",[e._v("Every stride size smaller than the page size behaves like (1/2)*X: showing lower cycle counts but experiences the first TLB miss at the same position.")]),e._v(" "),a("h4",{attrs:{id:"observation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#observation"}},[e._v("#")]),e._v(" Observation")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/00839b1e-4e68-44a9-9202-94819117e199",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/3dd1f116-73ef-4947-8689-4b600cfb2f70",alt:"image"}})]),e._v(" "),a("h5",{attrs:{id:"summary"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#summary"}},[e._v("#")]),e._v(" Summary")]),e._v(" "),a("p",[e._v("(1) We found three levels of TLBs for the K80 and two levels for the P100."),a("br"),e._v("\n(2) For both GPUs, the different TLB levels apparently use different page sizes, where the L1 TLB uses a small page size and the L2/L3 TLB use a 16x larger page size."),a("br"),e._v("\n(3) Compared to K80, the P100 always has 16x larger pages."),a("br"),e._v("\n(4) For data larger than 2GB, the K80 has a total delay of 241 cycles, while the P100 only has a 119 cycle delay.")]),e._v(" "),a("h4",{attrs:{id:"plausibility-and-validation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#plausibility-and-validation"}},[e._v("#")]),e._v(" Plausibility and Validation")]),e._v(" "),a("ul",[a("li",[e._v("First, the sizes of the L1 TLB (16 entries) and L2 TLB (65 entries) for Kepler GPUs (K80).")]),e._v(" "),a("li",[e._v("We can confirm this for the L1 TLB, while the K80 already uses 2MB pages for the L2 and L3 TLB (as shown by [10]).")]),e._v(" "),a("li",[e._v("Third, every TPC has its own L1 TLB and every GPC has its own L2 TLB, while the L3 TLB is shared for all SMs.")]),e._v(" "),a("li",[e._v("Fourth, we can see a significant performance drop in our investigated database operations when we access more data than ‚âà2GB."),a("br"),e._v("\nEven with different page sizes for both GPUs, we can pinpoint the problem to the L3 TLB on the K80 and the L2 TLB on the P100."),a("br"),e._v("\nWe can even identify the L2 TLB boundary on the K80, where performance problems start at ‚âà130MB.")]),e._v(" "),a("li",[e._v("Fifth, in [6], the performance of a grouping operator on Kepler GPUs was improved by reducing the number of threads to <1000\ninstead of multiple thousands for data accesses beyond 2GB."),a("br"),e._v("\nWith our results, we can explain that this is benefinicial because each thread can load one page translation in the L3 TLB (1032 entries)."),a("br"),e._v("\nThe page translations stay in the TLB.")])]),e._v(" "),a("h4",{attrs:{id:"argument-for-unconventional-properties"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#argument-for-unconventional-properties"}},[e._v("#")]),e._v(" Argument for Unconventional Properties")]),e._v(" "),a("p",[e._v("two unconventional results:"),a("br"),e._v("\n(1) TLB entry numbers not being the power of two"),a("br"),e._v("\n(2) different page sizes for different TLB levels.")]),e._v(" "),a("p",[e._v("We evaluated the allocation size and found that the smaller page size is always used for allocations (128KB on K80, 2MB on P100)."),a("br"),e._v("\nOne possible explanation for the apparently larger page sizes in the L2/L3 TLB could be a "),a("strong",[e._v("static pre-fetching algorithm")]),e._v(", which always loads 16 contiguous pages when a TLB miss occurs."),a("br"),e._v("\nThis would result in one TLB miss and 15 TLB hits, when using the small page size as traversal stride.")]),e._v(" "),a("h3",{attrs:{id:"_6-trans-fw-short-circuiting-page-table-walk-in-multi-gpu-systems-via-remote-forwarding"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-trans-fw-short-circuiting-page-table-walk-in-multi-gpu-systems-via-remote-forwarding"}},[e._v("#")]),e._v(" 6. Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/d21d7b38-670b-4bb9-8a93-a03ad1a90f64",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"address-translation-flow"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#address-translation-flow"}},[e._v("#")]),e._v(" Address Translation Flow")]),e._v(" "),a("ol",[a("li",[e._v("The memory requests generated by the same wavefront are first coalesced by the GPU memory coalescing unit.")]),e._v(" "),a("li",[e._v("the L1 data cache and the L1 TLB perform lookups in parallel in a virtually indexed physically tagged (VIPT) TLB-cache design.")]),e._v(" "),a("li",[e._v("Upon L1 TLB misses, the L1 Miss Status Holding Register (MSHR) is first checked to filter out repetitive requests, and the outstanding requests are forwarded to the L2 TLB for lookup.")]),e._v(" "),a("li",[e._v("Translations that miss in the L2 TLB and L2 MSHR are sent to the local PT-walk in the GMMU.")])]),e._v(" "),a("p",[e._v("Because there is limited number of PT-walk threads, L2 TLB misses may not be served immediately.\\")]),e._v(" "),a("ol",[a("li",[e._v("these translation requests will be stored in the PW-queue and wait for available PT-walk threads.")]),e._v(" "),a("li",[e._v("During the page table walking, the translation is first checked in the PW-cache;")]),e._v(" "),a("li",[e._v("if it misses the PW-cache, the GPU local page table is accessed, which can be expensive and involves multiple memory accesses ( 5 ).")]),e._v(" "),a("li",[e._v("If the page walk fails, a far fault is propagated to the GMMU and kept in a structure called GPU Fault Buffer [6], [7].")]),e._v(" "),a("li",[e._v("Each time a far fault arises, the GMMU sends an alert to the UMV-driver.")]),e._v(" "),a("li",[e._v("Upon the receipt of a "),a("strong",[e._v("far fault")]),e._v(", the UVM driver fetches the fault information and caches them on the host side.")]),e._v(" "),a("li",[e._v("The cached page faults are processed in batch granularity (the batch size is 256 [53]).")]),e._v(" "),a("li",[e._v("Per batch, the UVM-driver initiates threads to perform page table walks using the centralized page table, initiates data transfer, and updates the GPU local page tables [7].")]),e._v(" "),a("li",[e._v("The translation request is replayed after the far fault is resolved.")])]),e._v(" "),a("h4",{attrs:{id:"hardware-handled-far-faults"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hardware-handled-far-faults"}},[e._v("#")]),e._v(" Hardware Handled Far Faults")]),e._v(" "),a("p",[e._v("This is the flow of hardware-acclerated Far faults handling")]),e._v(" "),a("ol",[a("li",[e._v("When a far fault is generated, it is sent to the host and then handled by host MMU.")]),e._v(" "),a("li",[e._v("Specifically, upon receiving a translation request, the host MMU first performs a host MMU TLB lookup.")]),e._v(" "),a("li",[e._v("If the translation misses in the TLB, the request waits in the host MMU PW-queue for PT-walk")]),e._v(" "),a("li",[e._v("The PT-walk process in the host MMU is similar to the GPU local PT-walk, including host MMU PW-cache lookup")]),e._v(" "),a("li",[e._v("host MMU PT-walk for PW-cache misses")]),e._v(" "),a("li",[e._v("host MMU TLB update")])]),e._v(" "),a("p",[a("strong",[e._v("the address translation overhead of software is 4.5√ó higher than that of hardware in 32 GPUs.")])]),e._v(" "),a("p",[a("em",[e._v("three")]),e._v(" major latencies in the baseline address translation:")]),e._v(" "),a("ol",[a("li",[e._v("waiting time for available PT-walk threads in the PW-queue")]),e._v(" "),a("li",[e._v("additionally memory accesses after PW-cache misses")]),e._v(" "),a("li",[e._v("handling far faults caused by page sharing among multiple GPUs")])]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_9-improving-multi-instance-gpu-efficiency-via-sub-entry-sharing-tlb-design"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_9-improving-multi-instance-gpu-efficiency-via-sub-entry-sharing-tlb-design"}},[e._v("#")]),e._v(" 9. Improving Multi-Instance GPU Efficiency via Sub-Entry Sharing TLB Design")]),e._v(" "),a("p",[e._v("Specifically, in the L2 and L3 TLBs, an entry comprises 16 sub-entries, each directly corresponding to the address translation of 16 sequential\n64 KB pages within a contiguous 1 MB-aligned segment.")]),e._v(" "),a("p",[a("em",[e._v("By compressing multiple translations into a single TLB entry, the TLB can manage more data with fewer entries")]),e._v(", thereby reducing hardware overhead, while improving TLB efficiency and boosting overall performance.")]),e._v(" "),a("p",[e._v("these GPUs organize their L2 and L3 TLB entries differently to increase TLB reach [60]."),a("br"),e._v("\nSpecifically, each of these entries contains 16 sub-entries, which directly map to the address translations for 16 pages."),a("br"),e._v("\nThese pages can be either 64 KB or 2 MB in size, and all of them fall within an aligned range of either 1 MB or 32 MB in size, respectively."),a("br"),e._v("\nThat means each sub-entry in a TLB entry has a one-to-one relationship with a single page."),a("br"),e._v("\nNote that, in the sub-entry setting, if any TLB entry is evicted, all the 16 sub-entries associated with that TLB entry are zeroed.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/e2326fd1-7dab-48e8-9f65-c5b4355adfc7",alt:"image"}})]),e._v(" "),a("h2",{attrs:{id:"please-notice-that-in-the-above-picture-for-each-subpage-it-has-a-physical-address-thus-it-is-not-physically-consecutive-physical-page-address-could-be-randomly-located"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#please-notice-that-in-the-above-picture-for-each-subpage-it-has-a-physical-address-thus-it-is-not-physically-consecutive-physical-page-address-could-be-randomly-located"}},[e._v("#")]),e._v(" "),a("strong",[e._v("Please Notice that in the above picture, for each subpage, it has a physical address. Thus it is not physically consecutive. Physical page address could be randomly located.")])]),e._v(" "),a("h3",{attrs:{id:"_11-mask-redesigning-the-gpu-memory-hierarchy-to-support-multi-application-concurrency"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_11-mask-redesigning-the-gpu-memory-hierarchy-to-support-multi-application-concurrency"}},[e._v("#")]),e._v(" 11. MASK: Redesigning the GPU Memory Hierarchy to Support Multi-Application Concurrency")]),e._v(" "),a("h4",{attrs:{id:"main-idea"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#main-idea"}},[e._v("#")]),e._v(" Main Idea")]),e._v(" "),a("p",[e._v("Contention of shard TLB leads to frequent misses in the shared translation lookaside buffer (TLB), where a single miss can induce long-latency stalls for hundreds of threads."),a("br"),e._v("\nAs a result, the GPU often cannot schedule enough threads to successfully hide the stalls, which diminishes system throughput and becomes a\nfirst-order performance concern.")]),e._v(" "),a("h4",{attrs:{id:"contributions"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#contributions"}},[e._v("#")]),e._v(" Contributions")]),e._v(" "),a("p",[e._v("(1) a token-based technique to reduce TLB contention"),a("br"),e._v("\n(2) a bypassing mechanism to improve the effectiveness of cached address translations"),a("br"),e._v("\n(3) an application-aware memory scheduling scheme to reduce the interference between address translation and data requests.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/f5dc0080-a485-4548-b6fd-b730dd0070e1",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/ea172f22-625c-4eb4-8427-ea2119aeaccc",alt:"image"}})])])}),[],!1,null,null,null);a.default=i.exports}}]);