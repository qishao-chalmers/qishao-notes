(window.webpackJsonp=window.webpackJsonp||[]).push([[118],{572:function(e,t,i){"use strict";i.r(t);var a=i(9),n=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("[35] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models")]),e._v(" "),t("li",[e._v("[1 2024]When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models")]),e._v(" "),t("li",[e._v("[1] Huff-LLM: End-to-End Lossless Compression for Efficient LLM Inference")])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_35-minicache-kv-cache-compression-in-depth-dimension-for-large-language-models"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_35-minicache-kv-cache-compression-in-depth-dimension-for-large-language-models"}},[e._v("#")]),e._v(" [35] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models")]),e._v(" "),t("p",[e._v('The paper "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models" addresses the challenge of efficiently deploying large language models (LLMs) by proposing an innovative method for compressing the key-value (KV) cache.')]),e._v(" "),t("p",[e._v("Below is a detailed explanation of the paper, including key insights, contributions, and findings.")]),e._v(" "),t("p",[e._v("Additionally, since MiniCache is orthogonal to existing quantization techniques, it can achieve a compression ratio of up to "),t("strong",[e._v("5.02×")]),e._v(" when combined with the "),t("strong",[e._v("4-bit quantization")]),e._v(" technique.")]),e._v(" "),t("h3",{attrs:{id:"background-and-motivation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#background-and-motivation"}},[e._v("#")]),e._v(" Background and Motivation")]),e._v(" "),t("p",[e._v("Large Language Models (LLMs) like GPT and LLaMA rely heavily on autoregressive generation, where previously computed tokens' key-value pairs are cached to minimize redundant computation during inference.")]),e._v(" "),t("p",[e._v("However, the KV cache grows linearly with sequence length, becoming a substantial memory burden for long context tasks."),t("br"),e._v(" The authors identify an overlooked dimension—"),t("strong",[e._v("cross-layer redundancy")]),e._v(" , highlighting high similarities between the KV states of adjacent layers, particularly in middle-to-deep layers of LLMs.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/84d897bf-0c57-465e-84fe-831d4cdc56e2",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"key-insights"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#key-insights"}},[e._v("#")]),e._v(" Key Insights")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Cross-Layer Redundancy")]),t("br"),e._v("KV cache states across adjacent layers share significant redundancy, especially in deeper layers.")]),e._v(" "),t("li",[t("strong",[e._v("Unequal Mergeability")]),t("br"),e._v("Not all KV cache state pairs between adjacent layers are equally suitable for merging—some tokens show distinct semantic differences and thus should not be merged indiscriminately.")]),e._v(" "),t("li",[t("strong",[e._v("Reparameterization Approach")]),t("br"),e._v("Separating state vectors into magnitude and directional components allows effective merging via interpolation in polar coordinates, preserving crucial information and performance.")])]),e._v(" "),t("blockquote",[t("p",[t("strong",[e._v("Please notice this consine similarity in tokens index")])])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/8aca3d7b-3970-4d6f-a346-fe6dd9609db4",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"main-contributions"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#main-contributions"}},[e._v("#")]),e._v(" Main Contributions")]),e._v(" "),t("ol",[t("li",[t("strong",[e._v("MiniCache Framework")]),e._v(" "),t("br"),e._v("Introduces a novel cross-layer compression strategy that merges KV caches from adjacent layers starting from the middle layers of the LLM.")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/282c4d68-fc9c-443b-9ad5-69a231d5e863",alt:"image"}})]),e._v(" "),t("ol",{attrs:{start:"2"}},[t("li",[t("p",[t("strong",[e._v("Reparameterization-based Cache Merging")]),t("br"),e._v("Uses Spherical Linear Interpolation (SLERP) to merge the direction component of the KV cache vectors, preserving the original magnitude for minimal information loss.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Token Retention Strategy")]),t("br"),e._v("Identifies and retains critical, distinct token pairs to prevent semantic degradation during merging, ensuring accuracy with minimal overhead.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Orthogonality to Existing Methods")]),t("br"),e._v("MiniCache complements existing compression methods (e.g., quantization and sparsity), achieving superior compression rates when combined.")])])]),e._v(" "),t("h3",{attrs:{id:"methodology"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#methodology"}},[e._v("#")]),e._v(" Methodology")]),e._v(" "),t("p",[e._v("The method involves two main components:")]),e._v(" "),t("h4",{attrs:{id:"cross-layer-compression"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#cross-layer-compression"}},[e._v("#")]),e._v(" Cross-Layer Compression")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Identifying Optimal Layers")]),e._v("\nMerging begins at the midpoint of LLM layers, justified by observed higher redundancy at deeper layers.")]),e._v(" "),t("li",[t("strong",[e._v("Merge Function")]),e._v("\nKV pairs from adjacent layers are merged via a carefully designed function leveraging SLERP, preserving semantic integrity and directional properties.")])]),e._v(" "),t("h4",{attrs:{id:"cache-merging-and-restoration"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#cache-merging-and-restoration"}},[e._v("#")]),e._v(" Cache Merging and Restoration")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Reparameterization")]),e._v("\nKV caches are decomposed into directional vectors (normalized) and magnitudes. The directional component undergoes merging using SLERP, ensuring geometrically coherent interpolation.")]),e._v(" "),t("li",[t("strong",[e._v("Token Retention")]),e._v("\nIdentifies outliers—distinct KV pairs unsuitable for merging—based on angular distance, selectively retaining these tokens to minimize performance loss.")]),e._v(" "),t("li",[t("strong",[e._v("Restoration Process")]),e._v("\nMerged caches are restored by scaling merged directions with their original magnitudes and reintegrating the retained distinct tokens.")])]),e._v(" "),t("h3",{attrs:{id:"experimental-evaluation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#experimental-evaluation"}},[e._v("#")]),e._v(" Experimental Evaluation")]),e._v(" "),t("p",[e._v("The authors evaluate MiniCache extensively across several popular LLMs:")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Models")]),e._v("\nLLaMA-2, LLaMA-3, Mixtral, Phi-3.")]),e._v(" "),t("li",[t("strong",[e._v("Datasets")]),e._v("\nEvaluations include GSM8K (math problems), COQA (conversational Q&A), TruthfulQA, and LongBench (long-context tasks).")]),e._v(" "),t("li",[t("strong",[e._v("Baselines")]),e._v("\nComparisons include FP16 baseline (no compression), quantization-based methods (e.g., KIVI, SmoothQuant), and sparsity-driven methods.")])]),e._v(" "),t("h3",{attrs:{id:"results-and-findings"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#results-and-findings"}},[e._v("#")]),e._v(" Results and Findings:")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Memory Efficiency")]),e._v("\nMiniCache achieves up to 41% memory reduction and a compression ratio up to 5.02× when combined with 4-bit quantization.")]),e._v(" "),t("li",[t("strong",[e._v("Throughput Enhancement")]),e._v("\nImproves inference throughput by approximately 5× compared to the FP16 baseline due to reduced memory footprint, enabling larger batch sizes and faster generation.")]),e._v(" "),t("li",[t("strong",[e._v("Minimal Performance Drop")]),e._v("\nCompression with MiniCache leads to near-lossless performance, even under aggressive compression settings.")])]),e._v(" "),t("h3",{attrs:{id:"ablation-studies"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ablation-studies"}},[e._v("#")]),e._v(" Ablation Studies:")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Interpolation Parameter")]),e._v("\nA critical hyperparameter determining the balance in merging adjacent KV pairs—optimal around "),t("code",[e._v("t=0.6")]),e._v(".")]),e._v(" "),t("li",[t("strong",[e._v("Retention Threshold")]),e._v("\nOptimal token retention ("),t("code",[e._v("γ=0.05")]),e._v(") strikes the best balance between accuracy and memory usage.")])]),e._v(" "),t("h3",{attrs:{id:"contributions-summarized"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#contributions-summarized"}},[e._v("#")]),e._v(" Contributions Summarized")]),e._v(" "),t("ul",[t("li",[e._v("The paper proposes a novel depth-wise KV cache compression method, MiniCache.")]),e._v(" "),t("li",[e._v("It identifies cross-layer KV cache redundancy as a previously unexplored yet crucial dimension.")]),e._v(" "),t("li",[e._v("It introduces robust merging via SLERP interpolation and a targeted retention strategy for distinct tokens.")]),e._v(" "),t("li",[e._v("Experimental validation highlights substantial efficiency improvements, minimal accuracy degradation, and strong compatibility with existing methods.")])]),e._v(" "),t("h3",{attrs:{id:"limitations-and-future-work"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#limitations-and-future-work"}},[e._v("#")]),e._v(" Limitations and Future Work")]),e._v(" "),t("ul",[t("li",[e._v("The SLERP-based merging function is currently limited to pairwise merging.\nFuture extensions could explore simultaneous merging across multiple layers.")]),e._v(" "),t("li",[e._v("Further exploration into adaptive interpolation parameters based on the relative magnitude ratio of vectors is identified as promising.")])]),e._v(" "),t("h3",{attrs:{id:"conclusion"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[e._v("#")]),e._v(" Conclusion")]),e._v(" "),t("p",[e._v('"MiniCache" successfully identifies and exploits an important new dimension—depth-wise redundancy in KV caches of LLMs.'),t("br"),e._v("\nIts combination of reparameterization-based merging and selective token retention provides significant improvements in memory efficiency and inference throughput, establishing a new direction for research and practical optimization in deploying large-scale language models.")]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_2-1-2024-when-compression-meets-model-compression-memory-efficient-double-compression-for-large-language-models"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-2024-when-compression-meets-model-compression-memory-efficient-double-compression-for-large-language-models"}},[e._v("#")]),e._v(" 2. [1 2024]When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models")]),e._v(" "),t("h3",{attrs:{id:"insights"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#insights"}},[e._v("#")]),e._v(" Insights")]),e._v(" "),t("p",[e._v("Simply applying a scaling technique to model weights makes the quantized weight distribution more uneven, which can improve compressibility but reduce accuracy by approximately 60%.")]),e._v(" "),t("p",[e._v("To address this issue, we propose a compression-aware quantization and pruning approach that expands important values and reduces unimportant weight.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/383e24d5-432a-4f76-a251-799b4482cf85",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/b9801e28-ffd4-40bb-9e0d-1c667b09cdb6",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/743aee20-cc78-4ab7-ab23-b5936e031509",alt:"image"}})]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_3-1-huff-llm-end-to-end-lossless-compression-for-efficient-llm-inference"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-huff-llm-end-to-end-lossless-compression-for-efficient-llm-inference"}},[e._v("#")]),e._v(" 3. [1] Huff-LLM: End-to-End Lossless Compression for Efficient LLM Inference")]),e._v(" "),t("p",[e._v("Hardware implementation of Huffman Compression for weights.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/9ffda8c3-95b9-49da-a2d5-72a49be57e25",alt:"image"}})]),e._v(" "),t("p",[e._v("Interestingly, we find that if, instead of Huffman compressing FP16 weights directly, we separately compress the 5-bit exponent, the 5 higher-order and 5 lower-order bits of the mantissa, the total entropy is only slightly larger at\n10.61 bits/parameter.")]),e._v(" "),t("p",[t("strong",[e._v("We refer to this as {1, 5, 5, 5} compression.")])])])}),[],!1,null,null,null);t.default=n.exports}}]);