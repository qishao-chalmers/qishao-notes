(window.webpackJsonp=window.webpackJsonp||[]).push([[52],{462:function(e,t,n){"use strict";n.r(t);var a=n(5),i=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2023]")]),e._v(" "),t("li",[e._v("NVIDIA Mastering LLM Techniques")])]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_1-llm-as-os-agents-as-apps-envisioning-aios-agents-and-the-aios-agent-ecosystem-2012"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-llm-as-os-agents-as-apps-envisioning-aios-agents-and-the-aios-agent-ecosystem-2012"}},[e._v("#")]),e._v(" 1. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2012]")]),e._v(" "),t("p",[e._v("üëç\nAnalogy of LLM and OS.")]),e._v(" "),t("p",[e._v("Blog: https://huggingface.co/blog/shivance/illustrated-llm-os")]),e._v(" "),t("p",[e._v("Youtube: Andrej Karpathy https://www.youtube.com/watch?v=kCc8FmEb1nY")]),e._v(" "),t("p",[e._v("Parallel decoding(Multi threading): This is a technique that allows multiple decoding processes to occur simultaneously, which can speed up the decoding process. For example, instead of generating one token at a time, parallel decoding can generate several tokens in parallel, using different models or different parts of the same model. This can reduce the latency and increase the throughput of the decoding process. A recent paper by Apple researchers proposed a method called Parallel Speculative Sampling (PaSS) that introduces parallel decoding for LLMs, maintaining model quality while achieving remarkable speed.\n"),t("strong",[e._v("Related Paper")]),e._v(": Accelerating LLM Inference with Staged Speculative Decoding")]),e._v(" "),t("p",[e._v("Ensemble decoding(Multi processing): This is a technique that involves using multiple models to decode a single input sequence, which can improve the accuracy of the decoding process. For example, instead of relying on one model to generate the output, ensemble decoding can combine the outputs of several models, using methods such as voting, averaging, or reranking. This can increase the diversity and robustness of the decoding process. A common approach for ensemble decoding is to use models that have been trained with different architectures, hyperparameters, or data sources.")]),e._v(" "),t("p",[e._v("Speculative execution: This is a technique that involves predicting the outcome of a computation before it is actually executed, which can speed up the decoding process. For example, instead of waiting for the final hidden state of the model to generate the next token, speculative execution can use the early hidden states to predict the next token and execute the model in parallel on the predicted token. This can reduce the dependency between tokens and increase the parallelism of the decoding process. A recent paper by Berkeley researchers proposed a method called SPEED, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states.")]),e._v(" "),t("p",[t("strong",[e._v("Related Paper")]),e._v(": SPEED: Speculative Pipelined Execution for Efficient Decoding")]),e._v(" "),t("h3",{attrs:{id:"_2-nvidia-mastering-llm-techniques"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-nvidia-mastering-llm-techniques"}},[e._v("#")]),e._v(" 2. NVIDIA Mastering LLM Techniques")]),e._v(" "),t("p",[e._v("Link: https://developer.nvidia.com/blog/search-posts/?q=Mastering+LLM+Techniques")]),e._v(" "),t("ol",[t("li",[e._v("Customization")]),e._v(" "),t("li",[e._v("LLMOps")]),e._v(" "),t("li",[e._v("Training")]),e._v(" "),t("li",[e._v("Inference Optimization")]),e._v(" "),t("li")]),e._v(" "),t("h3",{attrs:{id:"_3-finetuning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-finetuning"}},[e._v("#")]),e._v(" 3. Finetuning")]),e._v(" "),t("ol",[t("li",[e._v("How RLHF Preference Model Tuning Works (And How Things May Go Wrong)\nBlog: https://www.assemblyai.com/blog/how-rlhf-preference-model-tuning-works-and-how-things-may-go-wrong/")])]),e._v(" "),t("p",[e._v("Paper: RRHF: Rank Responses to Align Language Models with Human Feedback without tears\n2.")]),e._v(" "),t("h3",{attrs:{id:"_4-function-calling"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-function-calling"}},[e._v("#")]),e._v(" 4. Function Calling")]),e._v(" "),t("ol",[t("li",[e._v("Blog: https://crunchingthedata.com/when-to-use-function-calling-for-llms/")]),e._v(" "),t("li",[e._v("Paper: An LLM Compiler for Parallel Function Calling")])])])}),[],!1,null,null,null);t.default=i.exports}}]);