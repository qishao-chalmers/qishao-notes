(window.webpackJsonp=window.webpackJsonp||[]).push([[48],{458:function(e,t,a){"use strict";a.r(t);var s=a(5),r=Object(s.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("[248] Dissecting GPU Memory Hierarchy through Microbenchmarking")]),e._v(" "),t("li",[e._v("[75] Benchmarking the Memory Hierarchy of Modern GPUs")]),e._v(" "),t("li",[e._v("[18] Benchmarking the GPU memory at the warp level")]),e._v(" "),t("li",[e._v("[90] Dissecting the NVidia Turing T4 GPU via Microbenchmarking")]),e._v(" "),t("li",[e._v("[38] Exploring Modern GPU Memory System Design Challenges through Accurate Modeling üëç üëç üëç")]),e._v(" "),t("li",[e._v("[9] OSM: Off-Chip Shared Memory for GPUs")]),e._v(" "),t("li",[e._v("[10] Demystifying GPU UVM Cost with Deep Runtime and Workload Analysis")])]),e._v(" "),t("hr"),e._v(" "),t("h1",{attrs:{id:"_1-dissecting-gpu-memory-hierarchy-through-microbenchmarking"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-dissecting-gpu-memory-hierarchy-through-microbenchmarking"}},[e._v("#")]),e._v(" 1. Dissecting GPU Memory Hierarchy through Microbenchmarking")]),e._v(" "),t("p",[e._v("A paper in 2015, profile memory in Fermi, Kepler and Maxwell")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/683d67af-3feb-4d35-9ecf-dfeafb814c37",alt:"image"}})]),e._v(" "),t("h4",{attrs:{id:"parameter"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#parameter"}},[e._v("#")]),e._v(" Parameter")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/08215b14-4856-4d3a-8c6a-b5050f905f02",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/5daed100-0155-4fed-9358-e26681294b2a",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/60213279-226b-4a30-aa05-36271e9ac0ff",alt:"image"}})]),e._v(" "),t("h4",{attrs:{id:"l1-data-cache"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#l1-data-cache"}},[e._v("#")]),e._v(" L1 Data Cache")]),e._v(" "),t("p",[e._v("On the Fermi and Kepler devices, the L1 data cache and shared memory are physically implemented together."),t("br"),e._v("\nOn the Maxwell devices, the L1 data cache is unified with the texture cache.")]),e._v(" "),t("p",[e._v("The 16 KB L1 cache has 128 cache lines mapped onto four cache ways."),t("br"),e._v("\nFor each cache way, 32 cache sets are divided into 8 major sets. Each major set contains 16 cache lines.")]),e._v(" "),t("p",[e._v("The data mapping is also unconventional."),t("br"),e._v("\nThe 12-13th bits in the memory address define the cache way, the 9-11th bits define the major set, and the 0-6th bits define the memory offset inside the cache line.\n"),t("img",{attrs:{src:"https://github.com/user-attachments/assets/f997bf94-4b5b-4948-882c-7f72dd7bd506",alt:"image"}})]),e._v(" "),t("p",[e._v("One distinctive feature of the Fermi L1 cache is that its replacement policy is not LRU, as pointed out by Meltzer et.al.\nAmong the four cache ways, cache way 2 is three times more likely to be replaced than the other three cache ways.")]),e._v(" "),t("p",[t("strong",[e._v("Another paper[4]")]),e._v(" We found that when the L1 data cache saturates, Turing randomly evicts 4 consecutive cache lines (128 B)."),t("br"),e._v("\nWe observed that once a block of cache lines are evicted, the second scan will cause more cache lines from the same set to be evicted.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/ae6a8abd-7d57-4e0c-98ea-12264a37ae75",alt:"image"}})]),e._v(" "),t("h4",{attrs:{id:"l2-data-cache"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#l2-data-cache"}},[e._v("#")]),e._v(" L2 Data Cache")]),e._v(" "),t("ul",[t("li",[e._v("The replacement policy of the L2 cache is not LRU")]),e._v(" "),t("li",[t("strong",[e._v("The L2 cache line size is 32 bytes")]),e._v(" by observing the memory access pattern of overflowing the cache and visiting array element one by one.")]),e._v(" "),t("li",[e._v("The data mapping is sophisticated and not conventional bits-defined")]),e._v(" "),t("li",[e._v("a hardware-level pre-fetching mechanism from the DRAM to the L2 data cache on all three platforms."),t("br"),e._v(" "),t("strong",[e._v("The pre-fetching size is about 2/3 of the L2 cache size and the prefetching is sequential. This is deduced from that if we load an array smaller than 2/3 of the L2 data cache size, there is no cold cache miss patterns.")]),t("br"),e._v("\nüôã(Maybe they can cover the gap just by prefetching sequential line.)")])]),e._v(" "),t("h4",{attrs:{id:"global-memory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#global-memory"}},[e._v("#")]),e._v(" Global Memory")]),e._v(" "),t("p",[e._v("global memory access involves accessing the DRAM, L1 and L2 data caches, TLBs and page tables.")]),e._v(" "),t("h5",{attrs:{id:"global-memory-throughput"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#global-memory-throughput"}},[e._v("#")]),e._v(" Global Memory Throughput")]),e._v(" "),t("p",[e._v("The theoretical bandwidth is calculated as fmem * bus width * DDR factor.\n"),t("img",{attrs:{src:"https://github.com/user-attachments/assets/dbb8cdc6-e0cd-4bc6-aec8-f9450ea6d0bf",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/707f8b05-88e6-40b2-b3e4-426f984d4405",alt:"image"}})]),e._v(" "),t("p",[e._v("the throughput of a larger ILP saturates faster.")]),e._v(" "),t("p",[e._v("The GTX780 has the highest throughput as it benefits from the highest bus width,"),t("br"),e._v("\nbut its convergence speed is the slowest, i.e., it requires the most memory requests to hide the pipeline latency.")]),e._v(" "),t("p",[t("strong",[e._v("This could be part of the reason that NVIDIA reduced the bus width back to 256 bits in Maxwell devices.")])]),e._v(" "),t("h5",{attrs:{id:"global-memory-latency"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#global-memory-latency"}},[e._v("#")]),e._v(" Global Memory Latency")]),e._v(" "),t("p",[t("strong",[e._v("The global memory access latency is the whole time accessing a data located in DRAM/L2 or L1 cache, including the latency of page table look-ups.")])]),e._v(" "),t("ul",[t("li",[e._v("very large s1 = 32 MB to construct the TLB/page table miss and cache miss (P5&P6)")]),e._v(" "),t("li",[e._v("set s2 = 1 MB to construct the L1 TLB hit but cache miss (P4)")]),e._v(" "),t("li",[e._v("After a total of 65 data accesses, 65 data lines are loaded into the cache."),t("br"),e._v("\nWe then visit the cached data lines with s1 again for several times, to construct cache hit but TLB miss (P2&P3).")]),e._v(" "),t("li",[e._v("set s3 = 1 element and repeatedly load the data in a cache line so that every memory access is a cache hit (P1).")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/e5c397e8-f4b4-46ba-b7ee-41c34fa08b33",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/1f616c8e-a758-4151-b1eb-61f15c810246",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/822fc563-d0dc-4708-afd2-89549adb7ec4",alt:"image"}})]),e._v(" "),t("ul",[t("li",[e._v("The Maxwell and Kepler devices have a unique memory access pattern (P6) for page table context switching. "),t("br"),e._v("\nWhen a kernel is launched, only memory page entries of 512 MB are activated. "),t("br"),e._v("\nIf the thread visits an inactivate page entry, the hardware needs a rather long time to switch between page tables."),t("br"),e._v("\nThis phenomena is also reported in [22] as page table ‚Äúmiss‚Äù.")]),e._v(" "),t("li",[e._v("The Maxwell L1 data cache addressing does not go through the TLBs or page tables."),t("br"),e._v("\nOn the GTX980, there is no TLB miss pattern (i.e., P2 and P3) when the L1 data cache is hit."),t("br"),e._v("\nOnce the L1 cache is missed, the access latency increases from tens of cycles to hundreds or even thousands of cycles.\n"),t("strong",[e._v("My comments: But if we look at GTX560Ti in P2, the latency is different with P1. So does this means that in Fermi, the memory request has to go through TLB first, and then access L1 DataCache? This might be the reason that the latency is longer. But this will degrade the performance....")])]),e._v(" "),t("li",[e._v("The TLBs are off-chip. we infer that the physical memory locations of the L1 TLB and L2 data cache are close. "),t("br"),e._v("\nThe physical memory locations of the L1 TLB and L2 TLB are also close, which means that the L1/L2 TLB and L2 data cache are shared off-chip by all SMs.")]),e._v(" "),t("li",[e._v("The GTX780 generally has the shortest global memory latencies, almost half that of the Fermi, with an access pattern of P2-P5."),t("br"),e._v("\nThe page table context switching of the GTX980 is also much more expensive than that of the GTX780.")])]),e._v(" "),t("p",[e._v("To summarize, the Maxwell device has "),t("em",[e._v("long global memory access latencies")]),e._v(" for cold cache misses and page table context switching."),t("br"),e._v("\nExcept for these rare access patterns, its access latency cycles are close to those of the Kepler device. "),t("br"),e._v("\nbecause the GTX980 has higher fmem than the GTX780, it actually offers the shortest global memory access time (P2-P4).")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/8d12e01f-1a6e-49e7-894c-28de28c9f864",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"shared-memory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory"}},[e._v("#")]),e._v(" Shared Memory")]),e._v(" "),t("p",[e._v("In CUDA programming, different CTAs assigned to the same SM have to share the same physical memory space."),t("br"),e._v("\nOn the Fermi and Kepler platforms, the shared memory is physically integrated with the L1 cache."),t("br"),e._v("\nOn the Maxwell platform, it occupies a separate memory space.\n"),t("strong",[e._v("Note that the shared memory and L1 cache are separated since Maxwell architecture.")])]),e._v(" "),t("p",[t("em",[e._v("Programmers")]),e._v(" move the data into and out of shared memory from global memory before and after arithmetic execution,"),t("br"),e._v("\nto avoid the frequent occurrence of long global memory access latencies.")]),e._v(" "),t("p",[t("strong",[e._v("We report a dramatic improvement in performance for the Maxwell device.")])]),e._v(" "),t("h5",{attrs:{id:"shared-memory-throughput"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-throughput"}},[e._v("#")]),e._v(" Shared Memory Throughput")]),e._v(" "),t("p",[e._v("the shared memory is organized as 32 memory banks [15]."),t("br"),e._v("\nThe bank width of the "),t("strong",[e._v("Fermi and Maxwell devices is 4 bytes")]),e._v(", while that of the Kepler device is 8 bytes.\nThe theoretical peak throughput of each SM (WSM) is calculated as fcore ‚àó Wbank ‚àó 32.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/3bf6da77-b196-4e13-b2fe-af410ee750a4",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("The achieved throughput per SM is calculated as 2 * fcore * sizeof(int) * (number of active threads per SM) * ILP / (total latency of each SM).")]),e._v("\nUsually a large value of ILP results in less active warps per SM."),t("br"),e._v("\nThe peak throughput W0SM denotes the respective maximum throughput of the abovecombinations."),t("br"),e._v("\nTwo key factors that affect the throughput are the number of active warps per SM and the ILP level.")]),e._v(" "),t("p",[e._v("The GTX980 reaches its peak throughput when the CTA size = 256, CTAs per SM = 2 and ILP = 8, i.e., 16 active warps per SM. The peak throughput is 137.41 GB/s, about "),t("em",[e._v("83.9%")]),e._v(" of the theoretical bandwidth.\nThe Maxwell device shows the best use of its shared memory bandwidth, and the Kepler device shows the worst.")]),e._v(" "),t("p",[e._v("GTX980 exhibits similar behavior as GTX780: high ILP is required to achieve high throughput for high SM occupancy.")]),e._v(" "),t("p",[e._v("According to Little‚Äôs Law, we roughly have: number of active warps * ILP = latency cycles * throughput.")]),e._v(" "),t("p",[t("strong",[e._v("GTX780 sucks in ILP = 1, since its limited 64 warps at most to be scheduled concurrently.")]),t("br"),e._v("\nWe consider this to be the main reason the achieved throughput of the GTX780 is poor compared with its designed value.")]),e._v(" "),t("h4",{attrs:{id:"shared-memory-latency"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-latency"}},[e._v("#")]),e._v(" Shared Memory Latency")]),e._v(" "),t("p",[t("strong",[e._v("The shared memory latencies on Fermi, Kepler and Maxwell devices are 50, 47 and 28 cycles, respectively.")])]),e._v(" "),t("p",[t("strong",[e._v("Fermi and Maxwell devices have the same number of potential bank conflicts because they have the same architecture.")])]),e._v(" "),t("p",[e._v("The shared memory space is divided into 32 banks."),t("br"),e._v("\nSuccessive words are allocated to successive banks."),t("br"),e._v("\nIf two threads in the same warp access memory spaces in the same bank, a 2-way bank conflict occurs.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/c5ef66d3-c05e-46b7-84d6-ace224aafeab",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/c8e4560b-68aa-4188-9621-05a9f90fca32",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/f8e02655-9d5d-4d80-9bfb-fb6e5aefde8f",alt:"image"}})]),e._v(" "),t("p",[e._v("For the Fermi and Kepler devices, where there is a 32-way bank conflict, it takes much longer to access shared memory than regular global memory (TLB hit, cache miss). "),t("br"),e._v("\nSurprisingly, the effect of a bank conflict on shared memory access latency on the Maxwell device is mild."),t("br"),e._v("\nEven the longest shared memory access latency is still at the same level as L1 data cache latency.")]),e._v(" "),t("p",[e._v("In summary, although the shared memory has very short access latency, it can be rather long if there are many ways of bank conflicts."),t("br"),e._v("\nThis is most obvious on the Fermi hardware."),t("br"),e._v("\nThe Kepler device tries to solve it by doubling the bank width of shared memory."),t("br"),e._v("\nCompared with the Fermi, the Kepler‚Äôs 4-byte mode shared memory halves the chance of bank conflict, and the 8-byte mode reduces it further.")]),e._v(" "),t("p",[e._v("However, we also find that the Kepler‚Äôs shared memory is inefficient in terms of throughput."),t("br"),e._v("\nThe Maxwell device has the best shared memory performance."),t("br"),e._v("\nWith the same architecture as the Fermi device, the Maxwell hardware shows a 2x size, 2x memory access speedup and achieves the highest throughput."),t("br"),e._v("\nMost importantly, the Maxwell device‚Äôs shared memory has been optimized to avoid the long latency caused by bank conflicts.")]),e._v(" "),t("h4",{attrs:{id:"conclusion"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[e._v("#")]),e._v(" Conclusion")]),e._v(" "),t("p",[e._v("The memory capacity is significantly enhanced in both Kepler and Maxwell as compared with Fermi."),t("br"),e._v("\nThe Kepler device is performance-oriented and incorporates several aggressive elements in its design, such as increasing the bus width of DRAM and doubling the bank width of shared memory."),t("br"),e._v("\nThese designs have some side-effects."),t("br"),e._v("\nThe theoretical bandwidths of both global memory and shared memory are difficult to saturate, and hardware resources are imbalanced with a low utilization rate."),t("br"),e._v("\nThe Maxwell device has a more efficient and conservative design."),t("br"),e._v("\nIt has a reduced bus width and bank width, and the on-chip cache architectures are adjusted, including doubling the shared memory size and the read-only data cache size."),t("br"),e._v("\nFurthermore, it sharply decreases the shared memory latency caused under bank conflicts.")]),e._v(" "),t("h3",{attrs:{id:"_4-dissecting-the-nvidia-turing-t4-gpu-via-microbenchmarking"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-dissecting-the-nvidia-turing-t4-gpu-via-microbenchmarking"}},[e._v("#")]),e._v(" 4. Dissecting the NVidia Turing T4 GPU via Microbenchmarking")]),e._v(" "),t("h4",{attrs:{id:"result"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#result"}},[e._v("#")]),e._v(" Result")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/eb21b04f-6ce8-44ef-8307-d26c35fa8a86",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/bae4d3b8-b2df-463b-a8f8-c095fbb53c9d",alt:"image"}})]),e._v(" "),t("h4",{attrs:{id:"shared-memory-latency-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-latency-2"}},[e._v("#")]),e._v(" Shared Memory Latency")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/ae7a4300-20fb-4be0-9404-e1c39a223d7d",alt:"image"}})]),e._v(" "),t("h4",{attrs:{id:"bandwidth"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#bandwidth"}},[e._v("#")]),e._v(" Bandwidth")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/74dc0901-e5a8-4084-b6a1-d8d71175926f",alt:"image"}})]),e._v(" "),t("h1",{attrs:{id:"_4-benchmarking-the-gpu-memory-at-the-warp-level"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-benchmarking-the-gpu-memory-at-the-warp-level"}},[e._v("#")]),e._v(" 4. Benchmarking the GPU memory at the warp level")]),e._v(" "),t("p",[e._v("In this work, we investigate the data accessing capability of a warp of threads: broadcasting and parallel accessing.\\")]),e._v(" "),t("ul",[t("li",[e._v("Broadcasting occurs when multiple threads access the same data element, i.e., multiple threads request a single data element (MTSD).")]),e._v(" "),t("li",[e._v("We refer the case of multiple threads accessing multiple distinct data elements (MTMD) as parallel accessing.")])]),e._v(" "),t("h5",{attrs:{id:"local-memory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#local-memory"}},[e._v("#")]),e._v(" Local Memory")]),e._v(" "),t("ul",[t("li",[e._v("For the simple memory access patterns, we should allocate a sufficient small array to guarantee that it is located in registers.")]),e._v(" "),t("li",[e._v("For the complex memory access patterns, we should simplify codes to exploit registers. For example, we merge a three-level loop into an one-level loop so that a larger temporal vector can be allocated in registers.")]),e._v(" "),t("li")]),e._v(" "),t("h5",{attrs:{id:"shared-memory-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-2"}},[e._v("#")]),e._v(" Shared Memory")]),e._v(" "),t("ul",[t("li",[e._v("Bank conflicts must be avoided by the ways of e.g., data padding.")]),e._v(" "),t("li",[e._v("Shared memory supports both broadcasting and parallel accessing.")]),e._v(" "),t("li",[e._v("Neither consecutively accessing nor aligned accessing is a must.")]),e._v(" "),t("li",[e._v("The latency decreases when the number of threads increase, and thus we should use a sufficiently large thread block.")]),e._v(" "),t("li",[e._v("Replacing global memory with shared memory, because the latency of shared memory is smaller than that of global memory.")]),e._v(" "),t("li",[e._v("Using shared memory bares an overhead (i.e., buffer allocation and data movement) and reusing data in it is a must for improved performance.")])]),e._v(" "),t("h5",{attrs:{id:"constant-memory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#constant-memory"}},[e._v("#")]),e._v(" Constant Memory")]),e._v(" "),t("p",[e._v("But constant memory does not support parallel accessing."),t("br"),e._v("\nThat is, constant memory can only be accessed serially when requesting different data elements."),t("br"),e._v("\nOn the one hand, constant memory is used to store a small amount of read-only data, which is not sensitive to bandwidth."),t("br"),e._v("\nSo parallel accessing is not a must for constant memory.")]),e._v(" "),t("ul",[t("li",[e._v("Constant memory supports the accessing capability of broadcasting.")]),e._v(" "),t("li",[e._v("Constant memory does not support parallel accessing, and satisfies parallel memory requests in a serial manner.")])]),e._v(" "),t("h5",{attrs:{id:"global-memory-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#global-memory-2"}},[e._v("#")]),e._v(" Global Memory")]),e._v(" "),t("ul",[t("li",[e._v("Global memory supports both broadcasting and parallel accessing.")]),e._v(" "),t("li",[e._v("The data types of 4 or 8 bytes can obtain the near upper-bounded bandwidth of global memory, while the data types cannot."),t("br"),e._v("\nSo the char data should be coalesced into the char4 type for improved bandwidth.")]),e._v(" "),t("li",[e._v("Global memory accesses should be consecutive, but aligned accessing is not necessary for global memory.")]),e._v(" "),t("li",[e._v("When memory accessing is non-consecutive, the latency changes with the number of threads, but not with the number of blocks.\nSo we should configure the thread dimensionality.")])]),e._v(" "),t("h3",{attrs:{id:"_5-exploring-modern-gpu-memory-system-design-challenges-through-accurate-modeling"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-exploring-modern-gpu-memory-system-design-challenges-through-accurate-modeling"}},[e._v("#")]),e._v(" 5. Exploring Modern GPU Memory System Design Challenges through Accurate Modeling")]),e._v(" "),t("p",[e._v("üëç üëç üëç")]),e._v(" "),t("h5",{attrs:{id:"memory-coalescer"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#memory-coalescer"}},[e._v("#")]),e._v(" Memory Coalescer")]),e._v(" "),t("p",[e._v("the eviction granularity of the cache is 128B, indicating that the L1 cache has 128B lines with 32B sectors."),t("br"),e._v("\nFurthermore, the coalescer operates across eight threads, i.e. the coalescer tries to coalesce each group of eight threads separately to generate sectored accesses.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/c3d84400-b121-4152-931c-c40074848909",alt:"image"}})]),e._v(" "),t("p",[e._v("When the stride=32, the memory access is converged, and all the threads within the same warp will access the same cache line,"),t("br"),e._v("\nhowever we receive four read accesses at L1 cache.")]),e._v(" "),t("p",[t("strong",[e._v("8 Thread register 32bit == 32Byte.")])]),e._v(" "),t("h5",{attrs:{id:"l2-cache"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#l2-cache"}},[e._v("#")]),e._v(" L2 Cache")]),e._v(" "),t("p",[e._v("L2 cache applies something similar to "),t("strong",[e._v("write-validate")]),e._v(" not "),t("strong",[e._v("fetch on write")]),e._v(".\\ üò±\nHowever, all the reads received by L2 caches from the coalescer are 32-byte sectored accesses."),t("br"),e._v("\nThus, the read access granularity (32 bytes) is different from the write access granularity (one byte)."),t("br"),e._v("\nTo handle this, the L2 cache applies a different write allocation policy, which we named lazy fetch-on-read, that is a compromise between write-validate and fetch-on-write.")]),e._v(" "),t("p",[e._v("When a sector read request is received to a modified sector, it first checks if the sector write-mask is complete, i.e. all the bytes have been written to and the line is\nfully readable."),t("br"),e._v("\nIf so, it reads the sector, otherwise, similar to fetch-on-write, it generates a read request for this sector and merges it with the modified bytes.")]),e._v(" "),t("h5",{attrs:{id:"streaming-throughput-oriented-l1-cache"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#streaming-throughput-oriented-l1-cache"}},[e._v("#")]),e._v(" Streaming Throughput-oriented L1 Cache")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/8edd3919-c6ff-40df-a207-a0853fcfa161",alt:"image"}})]),e._v(" "),t("p",[e._v("The L1 cache in Volta is what NVIDIA is calling a streaming cache [33]."),t("br"),e._v("\nIt is streaming because the documentation states that it allows "),t("strong",[e._v("unlimited cache misses")]),e._v(" to be in flight regardless the number of cache lines per cache set [10].")]),e._v(" "),t("p",[e._v("independent of the number of L1 configured size, the number of MSHRs available are the same, even if more of the on-chip SRAM storage is devoted to shared memory.")]),e._v(" "),t("p",[e._v("We believe that unified cache is a plain SRAM where sectored data blocks are shared between the L1D and the CUDA shared memory."),t("br"),e._v("\nIt can be configured adaptively by the driver as we discussed earlier."),t("br"),e._v("\nWe assume that the L1D‚Äôs TAG and MSHR merging functionality are combined together in a separate table structure (TAG-MSHR table)."),t("br"),e._v("\nSince, the filling policy is now ON FILL, we can have more TAG entries and outstanding requests than the assigned L1D cache lines.")]),e._v(" "),t("p",[e._v("If it is a hit to a reserved sector (i.e. the status is pending), it sets its corresponding warp bit in the merging mask (64 bits for 64 warps)."),t("br"),e._v("\nWhen the pending request comes back, it allocates a cache line/sector in the data block and sets the allocated block index in the table."),t("br"),e._v("\nThen, the merged warps access the sector, on a cycle-by-cyle basis.")]),e._v(" "),t("h1",{attrs:{id:"_6-osm-off-chip-shared-memory-for-gpus"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-osm-off-chip-shared-memory-for-gpus"}},[e._v("#")]),e._v(" 6. OSM: Off-Chip Shared Memory for GPUs")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/0fadce93-433a-4889-b130-e2c330d3b334",alt:"image"}})]),e._v(" "),t("p",[e._v("L1-D cache and shared memory use the same 32-bank memory structure (4 KB capacity per bank) as shown in Fig. 4;"),t("br"),e._v("\nhowever, they have some differences."),t("br"),e._v("\nWe can "),t("strong",[e._v("access 32-bit shared memory arrays via a thread-index directly")]),e._v(", while for accessing L1-D cache, we should read 128B (four 32B sectors) of the cache block."),t("br"),e._v("\nIn addition, L1 cache requires an extra hardware for managing tags and implementing LRU replacement policy.")]),e._v(" "),t("h1",{attrs:{id:"_7-demystifying-gpu-uvm-cost-with-deep-runtime-and-workload-analysis"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-demystifying-gpu-uvm-cost-with-deep-runtime-and-workload-analysis"}},[e._v("#")]),e._v(" 7. Demystifying GPU UVM Cost with Deep Runtime and Workload Analysis")]),e._v(" "),t("h2",{attrs:{id:"motivation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#motivation"}},[e._v("#")]),e._v(" Motivation")]),e._v(" "),t("ol",[t("li",[e._v("cumulative data access latency without prefetching generally increases one or more orders of magnitude with UVM in comparison to explicit\ndirect management by programmers")]),e._v(" "),t("li",[e._v("when all data fits in GPU global memory, prefetching reduces the cost significantly."),t("br"),e._v("\nbut the overall time can still be several times higher than the baseline")]),e._v(" "),t("li",[e._v("once the GPU global memory is oversubscribed, "),t("strong",[e._v("data access latency dramatically increases by another order of magnitude depending on access pattern")])]),e._v(" "),t("li",[e._v("prefetching can aggravate the performance issues after oversubscription.\n"),t("img",{attrs:{src:"https://github.com/user-attachments/assets/1f0b11d8-0f1d-4b05-889c-55cf9f01fd6b",alt:"image"}})])]),e._v(" "),t("h2",{attrs:{id:"cost-of-demand-paging"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#cost-of-demand-paging"}},[e._v("#")]),e._v(" Cost of Demand Paging")]),e._v(" "),t("p",[t("strong",[e._v("Far Fault")]),e._v(" "),t("br"),e._v("\nPaged migration moves data between devices in response to a page fault, maps the page into the faulter‚Äôs physical space, and unmaps from the previous location.")]),e._v(" "),t("p",[t("em",[e._v("Remote Mapping")]),e._v(" maps the requested data into the requester‚Äôs page tables without actually migrating it and accesses it using DMA or a related\nmechanism."),t("br"),e._v(" "),t("em",[e._v("Read-only duplication")]),e._v(" duplicates data at two or more physical devices and maps them locally to each device under the constraint that the data cannot be mutated.")]),e._v(" "),t("p",[e._v("UVM on-demand paging is implemented using GPU hardware and CPU software working in tandem."),t("br"),e._v("\nTo integrate with the host OS, the UVM driver is provided as a kernel module for the host OS to extend the virtual memory space and map it\nto GPU global memory utilizing the host memory layout.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/1a38d53a-c3fa-4afe-bc5d-3f4f1506191a",alt:"image"}})]),e._v(" "),t("p",[e._v("UVM uses a four-level hierarchy for memory address space: address spaces, virtual address ranges, virtual address blocks, and pages.")]),e._v(" "),t("p",[e._v("In general, a virtual address space is associated with an application."),t("br"),e._v("\nEach address space is composed of ‚Äúranges‚Äù, each corresponding to an arbitrarily sized memory allocation i.e. cudaMallocManaged() or related allocator."),t("br"),e._v("\nA range is broken up into 2MB sequential virtual address blocks, VABlocks."),t("br"),e._v("\nVABlocks are page-aligned and are composed of OS pages.")]),e._v(" "),t("h3",{attrs:{id:"flow"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#flow"}},[e._v("#")]),e._v(" Flow")]),e._v(" "),t("p",[e._v("operations of the UVM Driver in three groups: pre/post-processing, fault servicing, and fault replay policy.")]),e._v(" "),t("ul",[t("li",[t("p",[t("em",[e._v("pre-processing")]),e._v(" "),t("br"),e._v("\nthe driver stores page fault information read from the GPU fault buffer and sorts them locally."),t("br"),e._v("\nFaults are fetched until all the fault pointer queue is empty, the current batch of faults is full, or fault that is not ready is encountered, depending on policy."),t("br"),e._v("\nThe default batch size is 256 faults. Per batch, the driver groups page faults based on VABlocks and service the faults.")])]),e._v(" "),t("li",[t("p",[t("em",[e._v("Fault Servicing")]),e._v("\nFault servicing involves memory allocation, updating page tables, data transfer, and possibly issuing one or more fault replays or other operations, subject ot the fault replay policy.")])])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/d924b57e-8f52-4f63-b0b6-d234e1d6aefb",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"cost-overview"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#cost-overview"}},[e._v("#")]),e._v(" Cost Overview")]),e._v(" "),t("p",[e._v("Pre/post processing is shown to be negligible in cost, but functionally important for the fault servicing and replay implementation.")]),e._v(" "),t("p",[e._v("Pre-processing first gathers faults from the device, performs basic bookkeeping and logical checks, and sorts them into the appropriate VABlock bins."),t("br"),e._v("\nNVIDIA documentation indicates that the driver uses a circular device-side queue to store a fault pointer when a fault occurs [15]."),t("br"),e._v("\nThe host can read these pointers, which subsequently point to locations in the global GPU fault buffer that contain the full fault information."),t("br"),e._v("\nThe driver will generally read at least a full batch from the queue during every pass and cache the faults on the host to avoid having to make multiple remote updates to the queue."),t("br"),e._v("\nFaults may not be immediately available in the GPU fault buffer due to the asynchronicity."),t("br"),e._v("\nThus the driver may need to poll the buffer until the appropriate ‚Äúready‚Äù field is marked true or may be able to begin processing on previously fetched faults."),t("br"),e._v("\nSorting cost for batches is roughly constant due to the nature of sorting and the relatively small size of batches.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/f38bb8e3-12de-42eb-a10e-16a37a8620f6",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"sevice-cost-breakdown"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#sevice-cost-breakdown"}},[e._v("#")]),e._v(" Sevice Cost Breakdown")]),e._v(" "),t("p",[e._v("Fault servicing is a multi-step process that includes:")]),e._v(" "),t("ul",[t("li",[e._v("allocating physical space")]),e._v(" "),t("li",[e._v("zeroing out GPU pages")]),e._v(" "),t("li",[e._v("migrating data from the source to the destination")]),e._v(" "),t("li",[e._v("mapping pages and permissions, and a number of other tasks.")])]),e._v(" "),t("h4",{attrs:{id:"physical-memory-allocation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#physical-memory-allocation"}},[e._v("#")]),e._v(" Physical Memory Allocation")]),e._v(" "),t("p",[e._v("Physical memory allocation accounts for a large but variable quantity of service cost."),t("br"),e._v("\nThe UVM driver uses a physical memory allocator to track physical allocations on the GPU."),t("br"),e._v("\nAllocation is performed by "),t("strong",[e._v("calling into the main NVIDIA driver")]),e._v(".")]),e._v(" "),t("p",[e._v("The allocator over-allocates memory to cache it, knowing that the cost of each call is quite high."),t("br"),e._v(" "),t("strong",[e._v("This over-allocation and caching causes the allocation cost to remain relatively constant and negligible at large sizes")]),e._v(". "),t("br"),e._v("\nThis cost is actually contained within the greater ‚ÄúMigrate Pages‚Äù category, but is separated here as it is responsible for the ‚Äúconstant‚Äù dominating transfer cost within UVM at small sizes.")]),e._v(" "),t("h4",{attrs:{id:"page-migration"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#page-migration"}},[e._v("#")]),e._v(" Page Migration")]),e._v(" "),t("p",[e._v("Page migration involves:")]),e._v(" "),t("ul",[t("li",[e._v("permission checking and updates")]),e._v(" "),t("li",[e._v("memory allocation and zeroing of newly-allocated memory")]),e._v(" "),t("li",[e._v("copying data from the source location to staging locations")]),e._v(" "),t("li",[e._v("eventually issuing GPU instructions to copy data from the staging location to the final destination.\nOnce data is staged on the destination device, page duplication would be broken and unmapped from source locations."),t("br"),e._v("\nThe UVM driver initiates the memory copy command, and notifies the GPU to actually perform the data copy using DMA.")])]),e._v(" "),t("h4",{attrs:{id:"mapping-data"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#mapping-data"}},[e._v("#")]),e._v(" Mapping Data")]),e._v(" "),t("p",[e._v("Mapping data includes updating the local and remote page tables and issuing appropriate memory barriers to ensure consistency on the GPU."),t("br"),e._v("\nWhile updating the GPU page tables is part of the cost here, the importance of this step is in  "),t("u",[e._v("bookkeeping and ensuring data consistency and integrity")]),e._v(".")]),e._v(" "),t("p",[t("strong",[e._v("Important Insights")])]),e._v(" "),t("ul",[t("li",[e._v("First, the number of VABlocks in a batch has a great impact on service time.")]),e._v(" "),t("li",[e._v("Second, the batch size affects the cost and the optimal size depends on application access patterns and data requirement.\n"),t("em",[e._v("The appropriate tuning of batch size may differ on a per-application basis, and would be an interesting area of future study.")])])]),e._v(" "),t("h4",{attrs:{id:"replay-policy-cost"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#replay-policy-cost"}},[e._v("#")]),e._v(" Replay Policy Cost")]),e._v(" "),t("p",[e._v("Replayable faults do not block the faulting GPU compute unit, which can continue running non-faulting warps until a replay command is received [19].")]),e._v(" "),t("p",[e._v("The replay notification indicates that the original memory access should be tried again."),t("br"),e._v("\nNote that a single fault may need to be replayed multiple times due to hardware fault capacity limitations or software policy.")]),e._v(" "),t("ul",[t("li",[e._v("Block Policy")]),e._v(" "),t("li",[e._v("Batch Policy")]),e._v(" "),t("li",[e._v("Batch Flush Policy")]),e._v(" "),t("li",[e._v("Once Policy")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/dfa35487-d81c-4194-ac49-54708f20a969",alt:"image"}})]),e._v(" "),t("h2",{attrs:{id:"prefetching-challenges-and-page-level-gpu-access-patterns"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#prefetching-challenges-and-page-level-gpu-access-patterns"}},[e._v("#")]),e._v(" Prefetching Challenges and Page-level GPU Access Patterns")]),e._v(" "),t("p",[t("strong",[e._v("TODO")])]),e._v(" "),t("h3",{attrs:{id:"prefetching-design-constraints-and-the-tree-based-algorithm"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#prefetching-design-constraints-and-the-tree-based-algorithm"}},[e._v("#")]),e._v(" Prefetching Design Constraints and the Tree-Based Algorithm")]),e._v(" "),t("h3",{attrs:{id:"complex-gpu-access-patterns"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#complex-gpu-access-patterns"}},[e._v("#")]),e._v(" Complex GPU Access Patterns")]),e._v(" "),t("h3",{attrs:{id:"effectiveness-of-tree-based-algorithm"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#effectiveness-of-tree-based-algorithm"}},[e._v("#")]),e._v(" Effectiveness of Tree-Based Algorithm")]),e._v(" "),t("h3",{attrs:{id:"game-changer-oversubscription"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#game-changer-oversubscription"}},[e._v("#")]),e._v(" Game-ChangerÔºöOversubscription")]),e._v(" "),t("h4",{attrs:{id:"the-cost-of-eviction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#the-cost-of-eviction"}},[e._v("#")]),e._v(" The Cost of Eviction")]),e._v(" "),t("h5",{attrs:{id:"eviction-in-uvm"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#eviction-in-uvm"}},[e._v("#")]),e._v(" Eviction in UVM")]),e._v(" "),t("p",[e._v("The eviction mechanism is triggered whenever the driver attempts to allocate memory for a VABlock that does not have memory reserved on the GPU already, e.g. the first page fault."),t("br"),e._v("\nEvictions are performed at the VABlock level, mirroring allocation. When evicted, any modified pages are copied back to the host, and the  physical memory allocation for the VABlock is released.")]),e._v(" "),t("p",[e._v("The UVM driver uses least-recently-used eviction."),t("br"),e._v("\nThe LRU list is updated when a fault is handled from a VABlock.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/d41a9270-053d-49fa-93c4-574434d16795",alt:"image"}})]),e._v(" "),t("h5",{attrs:{id:"direct-and-indirect-costs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#direct-and-indirect-costs"}},[e._v("#")]),e._v(" Direct and Indirect Costs")]),e._v(" "),t("p",[t("strong",[e._v("Direct Costs")])]),e._v(" "),t("ul",[t("li",[e._v("First, the eviction itself has the same components as a device-to-host fault for a VABlock not present on the host."),t("br"),e._v("\nThe changed data needs to be migrated, involving data transfer, memory barrier, and page mapping/unmapping.")]),e._v(" "),t("li",[e._v("Second, due to the locking scheme in the driver, eviction causes the VABlock faulting path to start over, as the faulting block lock must be\ndropped while the evicted block lock is held.")])]),e._v(" "),t("p",[t("strong",[e._v("Indirect Costs")])]),e._v(" "),t("ul",[t("li",[e._v("Random Access Pattern: increased quantity of evict/map operations for small data sizes.")]),e._v(" "),t("li",[e._v("Eviction mechanism can evict data that is still being used."),t("br"),e._v("\nBecause the LRU function is only aware of page-faults, it is possible that the ‚Äúhottest‚Äù regions of data may also be the most likely to be\nevicted."),t("br"),e._v("\nThe data would quickly be migrated to the GPU and then never again updated in the LRU list.")])]),e._v(" "),t("p",[e._v("Prefetching can fetch data that will not be used prior to eviction.")]),e._v(" "),t("p",[e._v("Delays due to frequent eviction cause larger backlogs of faults in the fault buffer, requiring increased time to flush the fault buffer before replays and accumulating the time spent handling the fault replay policy.")]),e._v(" "),t("h5",{attrs:{id:"total-eviction-costs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#total-eviction-costs"}},[e._v("#")]),e._v(" Total Eviction Costs")]),e._v(" "),t("p",[t("strong",[e._v("It is important to understand that the impact of eviction is at its greatest when data access is irregular.")])]),e._v(" "),t("p",[e._v("The overhead of eviction mixed with the sheer number of additional faults and evictions from the poor access pattern account for the order-of-magnitude performance loss.")]),e._v(" "),t("p",[e._v("The overall cost depends on the measure of oversubscription as well as the access pattern itself..")]),e._v(" "),t("p",[e._v("The worst effect is noticeable when applications cross the threshold where local data no longer fits in-core, and data is evicted prior to being used.")]),e._v(" "),t("h2",{attrs:{id:"discussion-and-conclusions"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#discussion-and-conclusions"}},[e._v("#")]),e._v(" Discussion and Conclusions")]),e._v(" "),t("h3",{attrs:{id:"challenges-in-effective-prefetching-and-eviction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#challenges-in-effective-prefetching-and-eviction"}},[e._v("#")]),e._v(" Challenges in Effective Prefetching and Eviction")]),e._v(" "),t("h4",{attrs:{id:"prefetching"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#prefetching"}},[e._v("#")]),e._v(" Prefetching")]),e._v(" "),t("p",[e._v("Density prefetching has limitations when eviction is involved, because there is no guarantee that any prefetched data will actually be used. "),t("br"),e._v("\nWhile a key advantage of density prefetching is its ignorance of precise fault order, it loses a lot of information about spatial locality.")]),e._v(" "),t("p",[e._v("The primary cost of prefetching is in additional data migration.")]),e._v(" "),t("h4",{attrs:{id:"eviction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#eviction"}},[e._v("#")]),e._v(" Eviction")]),e._v(" "),t("p",[e._v("Eviction is a very difficult problem because it has its own algorithmic component, as well as a dependency on the memory allocation functionality.\\")]),e._v(" "),t("p",[e._v("Algorithmically, the implementation is still dependent on "),t("strong",[e._v("page fault information")]),e._v(", which is insufficient."),t("br"),e._v("\nThe granularity of evictions also impacts its performance, which is locked in UVM at the VABlock level.")]),e._v(" "),t("p",[t("strong",[e._v("This has two key implications:")])]),e._v(" "),t("ul",[t("li",[e._v("Data that is accessed on the GPU but does not cause a page fault because the page is present will not upgrade its location in the LRU list.")]),e._v(" "),t("li",[e._v("VABlocks that are fully resident on the GPU will never be upgraded in the LRU list until they are evicted and re-faulted.")])]),e._v(" "),t("p",[e._v("Addressing allocation granularity, 2MB blocks may be too coarse for allocations and evictions for irregular applications."),t("br"),e._v("\nWhile 4KB granularity is very small, irregular applications may not even have locality at the 4KB granularity.")]),e._v(" "),t("h3",{attrs:{id:"potential-paths-for-better-prefetching-and-eviction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#potential-paths-for-better-prefetching-and-eviction"}},[e._v("#")]),e._v(" Potential Paths for Better Prefetching and Eviction")]),e._v(" "),t("h4",{attrs:{id:"increased-fault-origin-information"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#increased-fault-origin-information"}},[e._v("#")]),e._v(" Increased fault origin information")]),e._v(" "),t("p",[e._v("The GPU presently provides quite a bit of information along with a GPU fault, primarily for tracing higher-level information about the origin\nof a fault.")]),e._v(" "),t("p",[e._v("Another level of information that offers SM ID, logical thread ID, or related information sufficient to pinpoint a specific area of execution.")]),e._v(" "),t("h4",{attrs:{id:"flexible-memory-allocation-granularity"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#flexible-memory-allocation-granularity"}},[e._v("#")]),e._v(" Flexible memory allocation granularity")]),e._v(" "),t("p",[e._v("irregular applications may benefit from a tuneable parameter allowing different sized memory allocations.")]),e._v(" "),t("p",[t("strong",[e._v("Overhead")]),e._v("\nmore complex ŒºTLB implementations and highly flexible driver implementation.")]),e._v(" "),t("h4",{attrs:{id:"gpu-memory-access-aware-eviction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gpu-memory-access-aware-eviction"}},[e._v("#")]),e._v(" GPU memory access-aware eviction")]),e._v(" "),t("p",[e._v("NVIDIA has included support for multiple-granularity access counters for GPU-level memory access on GPUs since the Volta architecture [27].")]),e._v(" "),t("p",[e._v("This idea is explored and simulated in Ganguly et al. [4], but has not been explored on a real system.")]),e._v(" "),t("h4",{attrs:{id:"adpative-prefetching"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#adpative-prefetching"}},[e._v("#")]),e._v(" Adpative prefetching")]),e._v(" "),t("p",[e._v("The existing mechanism has demonstrated that it is quite capable depending on the circumstance."),t("br"),e._v("\nUsing existing information, the driver could adapt some simple heuristics to adaptively tune prefetching.")])])}),[],!1,null,null,null);t.default=r.exports}}]);