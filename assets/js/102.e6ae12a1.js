(window.webpackJsonp=window.webpackJsonp||[]).push([[102],{555:function(t,e,r){"use strict";r.r(e);var a=r(8),o=Object(a.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"blogs-watcher"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#blogs-watcher"}},[t._v("#")]),t._v(" Blogs watcher")]),t._v(" "),e("h2",{attrs:{id:"pytorch-code-analysis"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#pytorch-code-analysis"}},[t._v("#")]),t._v(" Pytorch Code Analysis")]),t._v(" "),e("p",[t._v("ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘")]),t._v(" "),e("ol",[e("li",[e("p",[t._v("https://hurray0.com/menu/151/")])]),t._v(" "),e("li",[e("p",[t._v("https://hurray0.com/menu/152/")])]),t._v(" "),e("li",[e("p",[t._v("https://www.cnblogs.com/int-me-X/category/2371391.html")])]),t._v(" "),e("li",[e("p",[t._v("https://cloud.tencent.com/developer/article/2346580")])]),t._v(" "),e("li",[e("p",[t._v("https://github.com/search?q=repo%3Akeithyin%2Fread-pytorch-source-code%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0&type=code")])]),t._v(" "),e("li",[e("p",[t._v("https://mlgdg.github.io/2019/12/05/Pytorch%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/")])])]),t._v(" "),e("p",[t._v("ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘\nhttps://www.52coding.com.cn/2019/05/05/PyTorch0/")]),t._v(" "),e("p",[t._v("ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘\nPaper: SURVEY AND EVALUATION OF CONVERGING ARCHITECTURE IN LLMS BASED ON FOOTSTEPS OF OPERATIONS")]),t._v(" "),e("p",[t._v("ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘\n"),e("a",{attrs:{href:"https://blog.christianperone.com/2018/03/pytorch-internal-architecture-tour/",target:"_blank",rel:"noopener noreferrer"}},[t._v("PyTorch â€“ Internal Architecture Tour"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘\n"),e("a",{attrs:{href:"https://blog.christianperone.com/2019/02/pydata-montreal-slides-for-the-talk-pytorch-under-the-hood/",target:"_blank",rel:"noopener noreferrer"}},[t._v("PyData Montreal slides for the talk: PyTorch under the hood"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘\n"),e("a",{attrs:{href:"https://blog.christianperone.com/2023/12/pytorch-2-internals-talk/",target:"_blank",rel:"noopener noreferrer"}},[t._v("PyTorch 2 Internals â€“ Talk"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://www.cnblogs.com/learnhow/p/18447779",target:"_blank",rel:"noopener noreferrer"}},[t._v("llama.cpp source code analysis"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://forsworns.github.io/zh/blogs/20240623/",target:"_blank",rel:"noopener noreferrer"}},[t._v("llama.cpp source code analysis"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"cuda-optimization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#cuda-optimization"}},[t._v("#")]),t._v(" CUDA Optimization")]),t._v(" "),e("h3",{attrs:{id:"cuda-warp-level-primitives"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#cuda-warp-level-primitives"}},[t._v("#")]),t._v(" CUDA Warp-level Primitives")]),t._v(" "),e("p",[t._v("https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/")]),t._v(" "),e("p",[t._v("https://developer.nvidia.com/blog/cooperative-groups/")]),t._v(" "),e("p",[t._v("https://blog.csdn.net/kunhe0512/article/details/125492263")]),t._v(" "),e("h3",{attrs:{id:"cuda-kernel-optimization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#cuda-kernel-optimization"}},[t._v("#")]),t._v(" CUDA Kernel Optimization")]),t._v(" "),e("h4",{attrs:{id:"gemm"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#gemm"}},[t._v("#")]),t._v(" GEMM")]),t._v(" "),e("p",[t._v("ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘ "),e("a",{attrs:{href:"https://siboehm.com/articles/22/CUDA-MMM",target:"_blank",rel:"noopener noreferrer"}},[t._v("How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://xinyinicole.com/blogs/programming-tensor-cores-using-nvcuda-wmma/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Programming tensor cores using nvcuda-wmma"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("https://github.com/hitqshao/NVIDIA_SGEMM_PRACTICE")]),t._v(" "),e("p",[t._v("https://netfiles.pw/cuda-matrix-multiplication-performance-optimization-guide/")]),t._v(" "),e("p",[t._v("https://leimao.github.io/blog/NVIDIA-Tensor-Core-Programming/")]),t._v(" "),e("p",[t._v("https://leimao.github.io/blog/Row-Major-VS-Column-Major/")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/NVIDIA-developer-blog/code-samples/blob/master/posts/tensor-cores/simpleTensorCoreGEMM.cu",target:"_blank",rel:"noopener noreferrer"}},[t._v("NVIDIA-developer-blog code-samples"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("[KAUST] https://vccvisualization.org/teaching/CS380/CS380_fall2021_lecture_26.pdf")]),t._v(" "),e("p",[t._v("https://0mean1sigma.com/")]),t._v(" "),e("ul",[e("li",[t._v("Step2 Global Memory Calescing")]),t._v(" "),e("li",[t._v("Step3 GPU Shared Memory")]),t._v(" "),e("li",[t._v("Step4 1D Thread Coarsening using GPU Registers")]),t._v(" "),e("li",[t._v("Step5 2D Thread Coarsening using GPU Registers")]),t._v(" "),e("li",[t._v("Step6 Vectorized Memory Accesses")])]),t._v(" "),e("h4",{attrs:{id:"kernel-optimization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#kernel-optimization"}},[t._v("#")]),t._v(" Kernel Optimization")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://www.vrushankdes.ai/diffusion-policy-inference-optimization/part-v---1d-convolution-in-cuda-optimized",target:"_blank",rel:"noopener noreferrer"}},[t._v("Part V - 1D Convolution in CUDA (Optimized)"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://www.vrushankdes.ai/diffusion-policy-inference-optimization/part-ii---cuda-kernel-optimization-tips",target:"_blank",rel:"noopener noreferrer"}},[t._v("Part II - CUDA Kernel Optimization Tips"),e("OutboundLink")],1)]),t._v(" "),e("h3",{attrs:{id:"optimization-papers"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#optimization-papers"}},[t._v("#")]),t._v(" Optimization Papers")]),t._v(" "),e("p",[t._v("This paper mention a bit about float4 memory accessing performance.\n"),e("a",{attrs:{href:"https://lightsighter.org/pdfs/cudadma-sc11.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization"),e("OutboundLink")],1)]),t._v(" "),e("h3",{attrs:{id:"softmax"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#softmax"}},[t._v("#")]),t._v(" Softmax")]),t._v(" "),e("p",[t._v("Demo of diy softmax and import in pytorch\nhttps://github.com/fattorib/CudaSoftmax")]),t._v(" "),e("p",[t._v("ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘ ICS Paper on using tensor core to accelerate Reduction and Scan.\nhttps://arxiv.org/pdf/1811.09736")]),t._v(" "),e("p",[t._v("Volta Tensor Coreï¼š\nhttps://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Tensor-Cores.pdf")]),t._v(" "),e("h2",{attrs:{id:"papers-to-be-read"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#papers-to-be-read"}},[t._v("#")]),t._v(" Papers to be read")]),t._v(" "),e("ul",[e("li",[t._v("Optimization Principles and Application Performance Evaluation of a Multithreaded GPU Using CUDA")]),t._v(" "),e("li",[t._v("Performance Study of GPU applications using SYCL and CUDA on Tesla V100 GPU")]),t._v(" "),e("li",[t._v("A performance prediction model for the CUDA GPGPU platform")]),t._v(" "),e("li",[t._v("3.5-D Blocking Optimization for Stencil Computations on Modern CPUs and GPUs")]),t._v(" "),e("li",[t._v("Benchmarking Optimization Algorithms for Auto-Tuning GPU Kernels")]),t._v(" "),e("li",[t._v("[316] Auto-tuning a high-level language targeted to GPU codes")]),t._v(" "),e("li",[t._v("[91] Kernel Tuner: A search-optimizing GPU code auto-tuner")]),t._v(" "),e("li",[t._v("Meta-programming and auto-tuning in the search for high performance GPU code")]),t._v(" "),e("li",[t._v("[79] Autotuning in High-Performance Computing Applications")]),t._v(" "),e("li",[t._v("[105] Optimizing CUDA code by kernel fusion: application on BLAS")]),t._v(" "),e("li",[t._v("[24] A review of CUDA optimization techniques and tools for structured grid computing")])]),t._v(" "),e("h2",{attrs:{id:"llm-related"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#llm-related"}},[t._v("#")]),t._v(" LLM Related")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity")])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"https://wdndev.github.io/llm_interview_note/#/",target:"_blank",rel:"noopener noreferrer"}},[t._v("LLMsç›¸å…³çŸ¥è¯†åŠé¢è¯•é¢˜"),e("OutboundLink")],1)])]),t._v(" "),e("li",[e("p",[t._v("Gating Dropout: Communication-efficient Regularization for Sparsely Activated Transformers\nThis paper discuss about token routing to expert in another node. All-to-all communication.\n"),e("img",{attrs:{src:"https://github.com/user-attachments/assets/b958d7ab-261c-4a35-9550-237aa2e4d01e",alt:"image"}})])]),t._v(" "),e("li",[e("p",[t._v("PipeDream: inter-batch pipeline parallelism\n"),e("img",{attrs:{src:"https://github.com/user-attachments/assets/31828b88-2a1e-483c-bada-a838c23f5cc2",alt:"image"}})])]),t._v(" "),e("li",[e("p",[t._v("BPIPE: Memory-Balanced Pipeline Parallelism for Training Large Language Models\n"),e("img",{attrs:{src:"https://github.com/user-attachments/assets/95f482d2-2c0e-454c-b877-35a9efe1932a",alt:"image"}})])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"https://arxiv.org/pdf/2409.12517",target:"_blank",rel:"noopener noreferrer"}},[t._v("SCALING FP8 TRAINING TO TRILLION-TOKEN LLMS"),e("OutboundLink")],1)])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"https://arxiv.org/pdf/2310.18313",target:"_blank",rel:"noopener noreferrer"}},[t._v("FP8-LM: Training FP8 Large Language Models"),e("OutboundLink")],1)])])])])}),[],!1,null,null,null);e.default=o.exports}}]);