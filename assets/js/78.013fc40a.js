(window.webpackJsonp=window.webpackJsonp||[]).push([[78],{530:function(r,e,n){"use strict";n.r(e);var a=n(8),s=Object(a.a)({},(function(){var r=this,e=r._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":r.$parent.slotKey}},[e("ol",[e("li",[r._v("HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]")]),r._v(" "),e("li",[r._v("TurboTransformers: An Efficient GPU Serving System For Transformer Models [82]")]),r._v(" "),e("li",[r._v("Improving the Efficiency of Transformers for Resource-Constrained Devices [8]")]),r._v(" "),e("li",[r._v("Bag of Tricks for Optimizing Transformer Efficiency [5]")]),r._v(" "),e("li",[r._v("Making Transformer inference faster on GPUs[Blog]")]),r._v(" "),e("li",[r._v("Energy-efficient Inference Service of Transformer-based Deep Learning Models on GPUs [4]")]),r._v(" "),e("li",[r._v("Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs [TACO 2023 Ref 2]")]),r._v(" "),e("li",[r._v("hugging face https://huggingface.co/docs/transformers/performance")])]),r._v(" "),e("hr"),r._v(" "),e("h3",{attrs:{id:"_1-hat-hardware-aware-transformers-for-efficient-natural-language-processing-mit-247"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-hat-hardware-aware-transformers-for-efficient-natural-language-processing-mit-247"}},[r._v("#")]),r._v(" 1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]")]),r._v(" "),e("p",[r._v("üëç üëç üëç üëç")]),r._v(" "),e("h3",{attrs:{id:"_4-making-transformer-inference-faster-on-gpus-blog"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-making-transformer-inference-faster-on-gpus-blog"}},[r._v("#")]),r._v(" 4. Making Transformer inference faster on GPUs[Blog]")]),r._v(" "),e("p",[r._v("https://dev-discuss.pytorch.org/t/making-transformer-inference-faster-on-gpus/190")])])}),[],!1,null,null,null);e.default=s.exports}}]);