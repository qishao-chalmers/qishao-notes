(window.webpackJsonp=window.webpackJsonp||[]).push([[16],{429:function(e,a,t){"use strict";t.r(a);var s=t(5),i=Object(s.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("ol",[a("li",[e._v("Compresso: Pragmatic Main Memory Compression")]),e._v(" "),a("li",[e._v("Translation-optimized Memory Compression for Capacity")]),e._v(" "),a("li",[e._v("Touche: Towards Ideal and Efficient Cache Compression By Mitigating Tag Area Overheads")])]),e._v(" "),a("hr"),e._v(" "),a("h4",{attrs:{id:"_1-compresso-pragmatic-main-memory-compression"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-compresso-pragmatic-main-memory-compression"}},[e._v("#")]),e._v(" 1. Compresso: Pragmatic Main Memory Compression")]),e._v(" "),a("p",[e._v("Cite from the paper:\nWe propose Compresso, with optimizations to reduce compressed data movement in a hardware compressed memory, while maintaining high compression ratio by repacking data at the right time.")]),e._v(" "),a("p",[e._v("Compresso uses the modified BPC compression algorithm, achieving 1.85x average compression on a wide range of applications.")]),e._v(" "),a("p",[e._v("Compresso uses the compression granularity of 64B.")]),e._v(" "),a("p",[e._v("Compresso uses LinePack with 4 possible cache line sizes.")]),e._v(" "),a("p",[e._v("We compare variable-sized chunks (512B, 1KB, 2KB and 4KB) with 512B fixed-sized chunks. Compresso uses incremental allocation in 512B chunks,thereby allowing 8 page sizes (512B, 1KB, 1.5KB and so on).")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/d8d83212-ff2b-4f84-85c1-454255ee5e98",alt:"image"}})]),e._v(" "),a("p",[e._v("Additional Data Movement:")]),e._v(" "),a("ol",[a("li",[e._v("split-access cachelines")]),e._v(" "),a("li",[e._v("changes in compressibility(overflows)")]),e._v(" "),a("li",[e._v("metadata access")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/d04d18a4-5678-428c-a2cf-d1bc43ca2f3f",alt:"image"}})]),e._v(" "),a("p",[e._v("Difference in exception in LCP compression\nInstead, Compresso allows some number of such inflated cachelines to be stored uncompressed in the inflation room at the end of an MPA page, provided that there is space in that page (Fig. 5a). This is similar to the\nexception region in LCP, but is used for an entirely different reason—to reduce compression-related data movement, rather than to support a specific packing scheme.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/c97e0467-88e3-43a9-b25f-3d43477744d5",alt:"image"}})]),e._v(" "),a("p",[e._v("We present the first main-memory compression architecture that is designed to run an unmodified operating system.")]),e._v(" "),a("hr"),e._v(" "),a("h4",{attrs:{id:"_2-translation-optimized-memory-compression-for-capacity"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-translation-optimized-memory-compression-for-capacity"}},[e._v("#")]),e._v(" 2. Translation-optimized Memory Compression for Capacity")]),e._v(" "),a("p",[e._v("prior workscompress and pack/migrate data at a small - memory block-level - granularity; this introduces an additional block-level translation after the page-level virtual address translation. In general, the smaller the granularity of address translation, the higher the translation overhead.")]),e._v(" "),a("p",[e._v("A promising solution is to only save memory from cold (i.e.,less recently accessed) pages without saving memory from hot (i.e., more recently accessed) pages (e.g., keep the hot pages\nuncompressed).")]),e._v(" "),a("p",[e._v("Two challenges:")]),e._v(" "),a("ol",[a("li",[e._v("after a compressed cold page becomes hot again, migrating the page to a full 4KB DRAM location still adds another level (albeit page-level, instead of block-level) of translation on top of existing virtual address translation.\n"),a("strong",[e._v("Solution")]),e._v("\nwe propose compressing page table blocks in hardware to opportunistically embed compression translations into them in a software-transparent manner to effectively prefetch compression translations during a page walk, instead of serially fetching them after the walk.")])]),e._v(" "),a("p",[e._v("First, CTE misses typically occur after PTE misses in TLB because CTEs, especially the page-level CTEs under an OS-inspired approach, have similar translation reach as PTEs.\nSecond, we observe page table blocks (PTBs) are highly compressible because adjacent virtual pages often have identical status bits and the most significant bits in physical page numbers are unused.\nAs such, to hide the latency of CTE misses, TMCC transparently compresses each PTB in hardware to free up space in the PTB to embed the CTEs of the 4KB pages (i.e., either data pages or page table pages) that the PTB points to; this enables each page walk to also prefetch the matching CTE required for fetching from DRAM either the end data or the next PTB.")]),e._v(" "),a("ol",{attrs:{start:"2"}},[a("li",[e._v("only compressing cold data require compressing them very aggressively to achieve high overall memory savings.\n"),a("strong",[e._v("Solution")]),e._v("\nwe perform a large design space exploration across many hardware configurations and diverse workloads to derive and implement in HDL an ASIC Deflate that is specialized for memory.")])]),e._v(" "),a("p",[e._v("Prior new hardware managed translation entries as Compression Translation Entries (CTEs), as they are similar to OS page table entries (PTEs). Prior works cache CTEs in the memory controller via a dedicated CTE cache, similar to the TLBs dedicated to caching PTEs.")]),e._v(" "),a("p",[e._v("let hardware take on an OS-inspired approach: only save memory from cold (i.e., less recently accessed) pages without saving memory from hot (i.e., recently accessed) pages (e.g., keep the hot pages uncompressed), like OS memory compression.\nSolves the problem of")]),e._v(" "),a("ol",[a("li",[e._v("translation overheads that large and/or irregular workloads suffer from high PTE miss under hardware memory compression.")]),e._v(" "),a("li",[e._v("Fine-grained address translation")])]),e._v(" "),a("p",[e._v("Accesses to a compressed virtual page in ML2 incurs a page fault to wake up OS to pop a free physical page from ML1’s free list and migrate the virtual page to the page.")]),e._v(" "),a("p",[e._v("ML2 also keeps many free lists, each tracking sub-physical pages of a different size, to store any compressed virtual page in a practically ideal matching sub-physical page.")]),e._v(" "),a("p",[e._v("ML2 gracefully grows and shrinks relative to ML1 with increasing and decreasing memory usage. When everything can fit in memory uncompressed, ML2 shrinks to zero bytes in physical size so ML1 can have every physical page. Specifically, when ML2’s free list(s) get large (e.g., due to reducing memory usage), ML2 donates free physical pages from its free list(s) to ML1.\nOS also grows ML1 free list, when it gets small, by migrating cold virtual pages to ML2.\nMigrating a virtual page to ML2 shrinks one of ML2’s free lists. If a ML2 free list gets empty, ML1 gives cold victim physical pages to ML2 (i.e., track them in ML2 instead of ML1), so that ML2 can compress the virtual pages currently in the victim pages to free space in the victims to grow ML2’s free list(s).")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/a8a3f90b-4334-4d5b-a7ad-3a158b9f0966",alt:"image"}})]),e._v(" "),a("p",[e._v("Key Idea: Based on our observations, we propose transparently compressing each PTB in hardware to free up space in the PTB to embed the CTEs of the 4KB pages (i.e., either data pages or page table pages) that the PTB points to; this enables each page walk access to also prefetch the matching CTE required either for the next page walk access (i.e., to the next PTB) or for the actual data (or instruction) access after the walk.\n"),a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/58f6c74a-d910-4c85-9811-2f74a15aec07",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/744c2c42-6a02-42df-8d19-ccd707abe9ad",alt:"image"}})]),e._v(" "),a("p",[e._v("A practical challenge is that after migrating a page (e.g., from ML1 to ML2 after the page becomes cold), the corresponding CTE embedded in the page’s PTB should be updated. However, hardware has no easy way to use the PPN of the migrating page to find/access the page’s PTB(s).\nTMCC addresses this challenge by lazily updating the CTE in the PTB later around when the PTB is naturally accessed by the page walker, instead of updating it at the time of migrating the page.\nHowever, this means that for the first page walker access to the PTB after migrating one of the pages that the PTB points to, the corresponding CTE is out-of-date.\nTo ensure correctness, TMCC also accesses the correct CTE in DRAM (or in CTE cache) in parallel to verify the correctness of the DRAM access.\nFigure 8 compares and contrasts how TMCC serves an LLC miss that also misses in CTE cache with the baseline approach.\nFigure 9 provides n architectural overview of TMCC.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/d351dbe2-dc03-4069-8977-0c17dc3300e0",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/60eeac47-a9c5-4589-8ecb-e23578097575",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/6c31c55f-6120-42fe-882a-a707129eadcf",alt:"image"}})]),e._v(" "),a("p",[a("strong",[e._v("My Comment")]),e._v("\nWhen os access compressed page, that page is migrated from ML2 to ML1. Hardware cannot update the PTB easily. Thus it utilizes lazily update.\nDuring the page table walk, it will buffer the piggybacked CTE into CTE buffer. And when data miss req happens,  L2 extracts the PPN from the received request to lookup the CTE Buffer to obtain the CTE for MC to translate the PPN.")]),e._v(" "),a("hr"),e._v(" "),a("h4",{attrs:{id:"_3-touche-towards-ideal-and-efficient-cache-compression-by-mitigating-tag-area-overheads"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-touche-towards-ideal-and-efficient-cache-compression-by-mitigating-tag-area-overheads"}},[e._v("#")]),e._v(" 3. Touche: Towards Ideal and Efficient Cache Compression By Mitigating Tag Area Overheads")]),e._v(" "),a("p",[e._v("The first component, called the “Signature” (SIGN) engine, creates shortened signatures from the tag addresses of compressed blocks. Due to this, the SIGN engine can store multiple signatures in each tag entry. On a cache access, the physical cacheline is accessed only if there is a signature match (which has a negligible probability of false positive).\nThe second component, called the “Tag Appended Data” (TADA) mechanism, stores the full tag addresses with data. TADA enables Touch´e to detect false positive signature matches by ensuring that the actual tag address is available for comparison.\nThe third component, called the “Superblock Marker” (SMARK) mechanism, uses a unique marker in the tag entry to indicate the occurrence of compressed cache blocks from neighboring physical addresses in the same cacheline.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/c359f253-63dd-4354-8a41-4fb91975d2a9",alt:"image"}})]),e._v(" "),a("p",[e._v("On average, 55% of the blocks can be compressed to less than 48 bytes in size. Furthermore, 17% of the lines can be compressed to be less than 16 bytes in size. Therefore, several workloads tend to have blocks with low entropy and can benefit from compression.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/3f38d55a-e469-41f7-b102-f06d63432298",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/7b258b8a-8488-4b8b-b009-bd3c8e17bc51",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/3e2a0253-22ba-4ae0-8a60-bc14179c1b52",alt:"image"}})]),e._v(" "),a("p",[e._v("For instance, a cacheline cannot be marked both invalid and dirty at the same time. The tag manager uses this unused state to flag cachelines that contains compressed blocks. Thereafter, for a cacheline that stores compressed blocks, the 1st and 2nd bits of the tag address encodes its valid bit and dirty bit.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/e3b82585-7f61-44ef-b7b7-c19bc021a522",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/ef49a85e-7589-4d5d-a6d2-f01167d19fbe",alt:"image"}})]),e._v(" "),a("p",[e._v("The tag manager then retrieves the 16-bit marker from the SMARK mechanism. It then informs the SIGN engine to ignore the last 2-bits (corresponding to four neighboring addresses) of the full tag address to generate a unique 9-bit signature.")]),e._v(" "),a("p",[e._v("This SMARK generate a random 16-bit marker and concated with signature. Since non-superblocks use 3 signature to identify blks, it should also use a tag to compare not just 0.")]),e._v(" "),a("p",[e._v("If this paper doest not support superblock 4 compressed blks in a super block, it can only store 3* 16B compressed block or 48B + 64B block, due to extra real tag stored in data, 43bit for each data.")])])}),[],!1,null,null,null);a.default=i.exports}}]);