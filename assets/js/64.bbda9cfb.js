(window.webpackJsonp=window.webpackJsonp||[]).push([[64],{524:function(e,a,t){"use strict";t.r(a);var r=t(8),n=Object(r.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("ol",[a("li",[e._v("[354] Benchmarking TPU, GPU, and CPU Platforms for Deep Learning")]),e._v(" "),a("li",[e._v("[59 Year:2019] Characterizing Deep Learning Training Workloads on Alibaba-PAI")]),e._v(" "),a("li",[e._v("[12] Effective Elastic Scaling of Deep Learning Workloads")]),e._v(" "),a("li",[e._v("[30 Year:2024] LLM Inference Unveiled: Survey and Roofline Model Insights")]),e._v(" "),a("li",[e._v("[1] Performance Modeling and Workload Analysis of Distributed Large Language Model Training and Inference")]),e._v(" "),a("li",[e._v("[Blog] LLM Inference Series: 5. Dissecting model performance")]),e._v(" "),a("li",[e._v("[47 Year:2024] Understanding LLMs: A Comprehensive Overview from Training to Inference")]),e._v(" "),a("li",[e._v("[37 Year:2022] Reveal training performance mystery between TensorFlow and PyTorch in the single GPU environment üëç  üëç  üëç  üëç  üëç")]),e._v(" "),a("li",[e._v("[46 Year:2019] Performance Characterization of DNN Training using TensorFlow and PyTorch on Modern Clusters")]),e._v(" "),a("li",[e._v("[163 Year:2020] Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference")]),e._v(" "),a("li",[e._v("[204 Year:2016] Fathom: Reference Workloads for Modern Deep Learning Methods")]),e._v(" "),a("li",[e._v("[1: Year: 2018] ¬µ-cuDNN Accelerating Deep Learning Frameworks with Micro-Batching")]),e._v(" "),a("li",[e._v("[89 Year: 2019] Restructuring Batch Normalization to Accelerate CNN Training")]),e._v(" "),a("li",[e._v("[29 Year: 2018] Characterizing the Microarchitectural Implications of a Convolutional Neural Network (CNN) Execution on GPUs")]),e._v(" "),a("li",[e._v("[13 Year: 2022] cuConv: CUDA implementation of convolution for CNN inference")]),e._v(" "),a("li",[e._v("[10 Year: 2018] Performance Analysis of Different Convolution Algorithms in GPU Environment")]),e._v(" "),a("li",[e._v("[0 Year:2024] GPU Performance Optimization via Intergroup Cache Cooperation")]),e._v(" "),a("li",[e._v("[2024] Accelerating ML Workloads using GPU Tensor Cores: The Good, the Bad, and the Ugly :+1Ôºö"),a("em",[e._v("love the name")])])]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_1-benchmarking-tpu-gpu-and-cpu-platforms-for-deep-learning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-benchmarking-tpu-gpu-and-cpu-platforms-for-deep-learning"}},[e._v("#")]),e._v(" 1. Benchmarking TPU, GPU, and CPU Platforms for Deep Learning")]),e._v(" "),a("p",[e._v("Paper from Harvard")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/110ddbc0-1ddf-40fa-b360-9e3f589494c6",alt:"image"}})]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_2-59-year-2019-characterizing-deep-learning-training-workloads-on-alibaba-pai"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-59-year-2019-characterizing-deep-learning-training-workloads-on-alibaba-pai"}},[e._v("#")]),e._v(" 2 [59 Year:2019] Characterizing Deep Learning Training Workloads on Alibaba-PAI")]),e._v(" "),a("p",[e._v("This paper compares early ml, like single node, single master multi worker, nvlinkd is just introduced to Alibaba at that time.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/8e9eb798-a1bf-48da-aa17-d30c7e6973fc",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/ec5cdae5-2150-48aa-9626-37869115bff7",alt:"image"}})]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_10-inducing-and-exploiting-activation-sparsity-for-fast-neural-network-inference"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_10-inducing-and-exploiting-activation-sparsity-for-fast-neural-network-inference"}},[e._v("#")]),e._v(" 10. Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference")]),e._v(" "),a("p",[e._v("Sparsity across channels:")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/d0ab98ae-adcd-438a-97fe-db8c31becb3f",alt:"image"}})]),e._v(" "),a("p",[e._v("Sparsity across layers:\n"),a("img",{attrs:{src:"https://github.com/user-attachments/assets/68c5159a-f536-4714-aadf-0772b6c73dde",alt:"image"}})]),e._v(" "),a("p",[a("em",[a("strong",[e._v("CSCC: Convolution Split Compression Calculation Algorithm for Deep Neural Network")])]),e._v("\nSparsity across layers:\n"),a("img",{attrs:{src:"https://github.com/user-attachments/assets/0496984d-6cad-4727-8ee7-f63023a1656c",alt:"image"}})]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_11-fathom-reference-workloads-for-modern-deep-learning-methods"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_11-fathom-reference-workloads-for-modern-deep-learning-methods"}},[e._v("#")]),e._v(" 11.Fathom: Reference Workloads for Modern Deep Learning Methods")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/04833dea-d804-416e-914f-d59409ab0f5e",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/4394bb14-fd24-46de-a604-f34b692fd139",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/39728745-788b-4ac1-b777-937f25b9083d",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"_12-Œº-cudnn-accelerating-deep-learning-frameworks-with-micro-batching"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-Œº-cudnn-accelerating-deep-learning-frameworks-with-micro-batching"}},[e._v("#")]),e._v(" 12. ¬µ-cuDNN Accelerating Deep Learning Frameworks with Micro-Batching")]),e._v(" "),a("p",[e._v("Memory requirements of different matrix multiplication algorithm and their execution time.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/6d533da4-a5a3-46e4-b6ad-61dfa0c21d71",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"_13-restructuring-batch-normalization-to-accelerate-cnn-training"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_13-restructuring-batch-normalization-to-accelerate-cnn-training"}},[e._v("#")]),e._v(" 13. Restructuring Batch Normalization to Accelerate CNN Training")]),e._v(" "),a("p",[e._v("Batch normalization is essential in Densenet.\n"),a("img",{attrs:{src:"https://github.com/user-attachments/assets/679ff5f7-0a9d-424e-b1e7-2a9ac9d9842c",alt:"image"}})]),e._v(" "),a("p",[e._v("The non-CONV layers of DenseNet-121 are mostly bottlenecked by the peak mainmemory bandwidth of the system we use (230.4GB/s),"),a("br"),e._v("\nwhereas the CONV layers underutilize the available bandwidth (only up to 120GB/s).")]),e._v(" "),a("p",[e._v("non-Conv layers have less data locality and computation intensity, which makes loop blocking techniques less effective,"),a("br"),e._v("\nleading to higher demand in memory bandwidth.")]),e._v(" "),a("p",[e._v("Memory accesses in ReLU and BN layers mostly come from reading and writing ifmaps and ofmaps."),a("br"),e._v("\nSince BN layers have strict data dependency, cross-layer data reuse is prohibited.")]),e._v(" "),a("ul",[a("li",[e._v("In the forward pass of a BN layer, all pixels of ifmaps that belong to a mini-batch should be retrieved to get per-channel mean and variance values prior to normalizing individual pixels.")]),e._v(" "),a("li",[e._v("In backpropagation, calculating the partial derivatives on Œ≥ (scaling factors) and Œ≤ (shift factors) accompanies sweeping the partial derivatives on ofmaps; this should precede computing the partial derivatives on ifmaps.")])]),e._v(" "),a("p",[e._v("Because these dependencies make data reuse distance far in BN, it is difficult to apply the data reuse techniques that were previously proposed for CONV\nlayers to these non-CONV layers.")]),e._v(" "),a("p",[e._v("They split batch normalization into two parts:")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/27a1b975-a383-4ecb-9d39-545ed4069388",alt:"image"}})]),e._v(" "),a("p",[a("strong",[e._v("what is convential kernel fusion of conv+bn about? is that fusion conv+bn totally?")])]),e._v(" "),a("h3",{attrs:{id:"_14-characterizing-the-microarchitectural-implications-of-a-convolutional-neural-network-cnn-execution-on-gpus"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_14-characterizing-the-microarchitectural-implications-of-a-convolutional-neural-network-cnn-execution-on-gpus"}},[e._v("#")]),e._v(" 14. Characterizing the Microarchitectural Implications of a Convolutional Neural Network (CNN) Execution on GPUs")]),e._v(" "),a("p",[e._v("üëç üëç üëç üëç üëç")]),e._v(" "),a("p",[e._v("Quantization analysis of CNN on GPUs")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/e91ea58d-414d-4d28-8209-5397b7b7dfd3",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/2526f66e-008f-44b9-aa37-21796af5c1b2",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"results-on-k40"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#results-on-k40"}},[e._v("#")]),e._v(" Results on K40")]),e._v(" "),a("h5",{attrs:{id:"conv"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#conv"}},[e._v("#")]),e._v(" Conv")]),e._v(" "),a("ul",[a("li",[e._v("stall exec dependency: the intrinsic program characteristics of this layer")]),e._v(" "),a("li",[e._v("stall not selected : the warp is not selected to run since the scheduler selects competing warps")])]),e._v(" "),a("p",[e._v("bounded by computing")]),e._v(" "),a("h5",{attrs:{id:"relu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#relu"}},[e._v("#")]),e._v(" relu")]),e._v(" "),a("ul",[a("li",[e._v("stall_memory_throttle")]),e._v(" "),a("li",[e._v("stall_memory_dependency\nMemory bound")])]),e._v(" "),a("h4",{attrs:{id:"layer-normalization-pooling-and-softmax"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#layer-normalization-pooling-and-softmax"}},[e._v("#")]),e._v(" layer normalization, pooling and softmax")]),e._v(" "),a("p",[e._v("compute and memory bound"),a("br"),e._v("\nwhen it changes to GTX1080, it is only memory-bound. GTX1080 better compute performance/")]),e._v(" "),a("h4",{attrs:{id:"fully-connect-layer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fully-connect-layer"}},[e._v("#")]),e._v(" fully connect layer:")]),e._v(" "),a("p",[e._v("fc6_w is somewhat special in that it is partially bounded by memory and "),a("em",[e._v("partially bounded by instruction fetch")]),e._v(".")]),e._v(" "),a("h4",{attrs:{id:"memory-access-behaviour"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#memory-access-behaviour"}},[e._v("#")]),e._v(" Memory Access Behaviour")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/160a0fe3-6b9b-4663-8fb1-6b242c410924",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"memory-access-behaviour-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#memory-access-behaviour-2"}},[e._v("#")]),e._v(" Memory Access Behaviour")]),e._v(" "),a("ul",[a("li",[e._v("layers of the linear data transformation make good use of the texture cache.\nThis can be explained since the computations in both the convolution and fully-connected layers exhibit a high degree of spatial locality."),a("br"),e._v("\nThe texture cache is designed in such a way as to take advantage of spatial locality.")]),e._v(" "),a("li",[e._v("Even the activation layer, which has "),a("strong",[e._v("no temporal locality")]),e._v(", also makes use of the texture cache to exploit "),a("strong",[e._v("spatial locality")]),e._v(".")]),e._v(" "),a("li",[e._v("the linear data transformation layers rely heavily on shared memory and the texture cache.\nAs indicated in the cache hit rate figures, both the convolution and fully-connected layers possess high temporal and spatial locality, given that data accessed within a region is repeatedly accessed."),a("br"),e._v("\nAs a result, there are a large number of memory transactions issued to these two memory levels, especially read requests.")]),e._v(" "),a("li",[e._v("for other layers, the utilization of shared memory and texture cache is very limited.\nEven for the pooling and LRN layers, the data reuse rate is very low."),a("br"),e._v("\nFor the other layers, including pooling, LRN, activation, and Softmax, the number of memory transactions does not vary significantly across the memory hierarchy")]),e._v(" "),a("li",[e._v("shared memory usually takes 38 cycles to read, while the texture cache takes 436-443 cycles.")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/caf5d7b9-27a6-48ac-ac92-ad84cc2eed40",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"potential-optimization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#potential-optimization"}},[e._v("#")]),e._v(" Potential Optimization")]),e._v(" "),a("ul",[a("li",[a("p",[e._v("convolution is compute power hungary instead of DRAM bandwidth.\nIncreasing the DRAM bandwidth on the GTX1080 will not benefit the CNN throughput very much.\nInstead, if we increase the bandwidth of the texture cache, we should see much better performance.")])]),e._v(" "),a("li",[a("p",[e._v("L1 cache is essentially unused in most of the layers.")])])]),e._v(" "),a("p",[e._v("we can enable "),a("strong",[e._v("L1 cache bypassing[27]")]),e._v(" for selected layers to avoid unnecessary data requests to the L1 cache.\nWhen we re-run our application with the L1 cache disabled for both reads and writes, we observe a speedup in some layers for both forward and backward propagation.")]),e._v(" "),a("ul",[a("li",[e._v("apply kernel fusion[28] for the linear data transformation layers and the non-linear activation.\n"),a("img",{attrs:{src:"https://github.com/user-attachments/assets/3552f82e-7519-435b-9404-e544d92c7ecc",alt:"image"}})])]),e._v(" "),a("h3",{attrs:{id:"_15-cuconv-cuda-implementation-of-convolution-for-cnn-inference"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_15-cuconv-cuda-implementation-of-convolution-for-cnn-inference"}},[e._v("#")]),e._v(" 15. cuConv: CUDA implementation of convolution for CNN inference")]),e._v(" "),a("p",[e._v("They design a new implementaion of convolution.")]),e._v(" "),a("p",[e._v("They discucss about GEMM, Winograd and FFT and design a new two-stage convolution scheme, two-stage convolution.")]),e._v(" "),a("h3",{attrs:{id:"_16-performance-analysis-of-different-convolution-algorithms-in-gpu-environment"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_16-performance-analysis-of-different-convolution-algorithms-in-gpu-environment"}},[e._v("#")]),e._v(" 16. Performance Analysis of Different Convolution Algorithms in GPU Environment")]),e._v(" "),a("p",[e._v("They compare the memory size and performance of FFT, gemm, Winograd.\n"),a("img",{attrs:{src:"https://github.com/user-attachments/assets/2ad332b7-d0dc-449d-a663-a85d3b920acf",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"_17-gpu-performance-optimization-via-intergroup-cache-cooperation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_17-gpu-performance-optimization-via-intergroup-cache-cooperation"}},[e._v("#")]),e._v(" 17. GPU Performance Optimization via Intergroup Cache Cooperation")]),e._v(" "),a("p",[e._v("This paper designed cache for gpu and also analyze different level of ache behaviour on different benchmarks.\\")]),e._v(" "),a("p",[e._v("L1 hit ratio")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/47ea33de-8ddb-41b9-9e97-c930ac1ac87d",alt:"image"}})]),e._v(" "),a("p",[e._v("Duplicate data")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/6cd548eb-4994-4a83-8239-d07898d696b7",alt:"image"}})])])}),[],!1,null,null,null);a.default=n.exports}}]);