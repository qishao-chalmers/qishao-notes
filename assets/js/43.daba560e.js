(window.webpackJsonp=window.webpackJsonp||[]).push([[43],{497:function(e,t,n){"use strict";n.r(t);var a=n(8),i=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("Efficient Memory Management for Large Language Model Serving with PagedAttention [2023]")]),e._v(" "),t("li",[e._v("LLM in a flash: Efficient Large Language Model Inference with Limited Memory [Apple 2023]")]),e._v(" "),t("li",[e._v("[591 Year: 2021] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM "),t("strong",[e._v("Not Read")])])]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_1-efficient-memory-management-for-large-language-model-serving-with-pagedattention"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-efficient-memory-management-for-large-language-model-serving-with-pagedattention"}},[e._v("#")]),e._v(" 1. Efficient Memory Management for Large Language Model Serving with PagedAttention")]),e._v(" "),t("p",[e._v("Disscussed the GEMM in prompt and GEMV in auto regression.\nIn GEMV, LLM is memory bound. There is lot of fragment in KVCache.\nIt also quantize the memory necessity for parameter in KV Cache.\nThey came up the method similar to paging in OS to manage KV in KV cache, reducing the fragment.")]),e._v(" "),t("h3",{attrs:{id:"_2-llm-in-a-flash-efficient-large-language-model-inference-with-limited-memory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-llm-in-a-flash-efficient-large-language-model-inference-with-limited-memory"}},[e._v("#")]),e._v(" 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory")]),e._v(" "),t("p",[e._v("Upproject matrix and downprojection matrix:\nhttps://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\nRelated paper:\nParameter-Efficient Transfer Learning for NLP\nThis introduce low-rank.\n"),t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/14c7f4b6-7709-48b9-9871-77296acb8e19",alt:"image"}})]),e._v(" "),t("p",[e._v("sliding window.\n"),t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/b3d007ea-743d-445b-8236-8e1aaaea816e",alt:"image"}})]),e._v(" "),t("ol",[t("li",[t("p",[e._v("high sparsity in FeedForward Layers, more than 90%\nSelectively only load parameters from memory either no-zero input or predicted have non-zero output")])]),e._v(" "),t("li",[t("p",[e._v("Minimize data transfer and maximize flash memory throughout\n"),t("strong",[e._v("Window sliding")]),e._v(": Load parameters for only the past few tokens, reusing activations from recently computed tokens. This sliding window approach reduces the number of IO requests to load weights.\n"),t("strong",[e._v("Row-column bundling")]),e._v(": We store a concatenated row and column of the up-projection and down-projection layers to read bigger contiguous chunks from flash memory. This increases throughput by reading larger chunks.")])]),e._v(" "),t("li",[t("p",[e._v("Predict FFN sparsity and avoid loading zeroed-out parameter\nto minimize the number of weights to be transferred from flash memory to DRAM.")])]),e._v(" "),t("li",[t("p",[e._v("Static memory preallocation")])])]),e._v(" "),t("p",[e._v("Also a model to predict the tradeoff between loading less data and reading larger chunks")]),e._v(" "),t("p",[e._v("Load only 2% of FFN layer from flash")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Larger chunk\nAlthough throughput growth is not linear (larger chunks take longer to transfer), the latency for the initial byte becomes a smaller fraction of the total request time, resulting in more efficient data reading.\n"),t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/de3b1b16-fc7e-4ce9-bba6-aa6ad2bccf34",alt:"image"}})])]),e._v(" "),t("li",[t("p",[e._v("Load From Flash")])])]),e._v(" "),t("p",[e._v("2.1  inherent sparsity found in Feed-Forward Network (FFN) model")]),e._v(" "),t("p",[t("strong",[e._v("Selective Persistence Strategy")]),e._v("\nRetain the embeddings and matrices within the attention mechanism of the transformer constant.Attentions weights 1/3 of the model size.For the Feed-Forward Network (FFN) portions, only the non-sparse segments are dynamically loaded into DRAM as needed.")]),e._v(" "),t("p",[t("strong",[e._v("Anticipating ReLU Sparsity")]),t("br"),e._v("\nRelu activation can induce 90% sparsity. Optimize preceding layer, up project by low-rank predictor to identify the zeroed elements post-ReLU."),t("br"),e._v("\nIn contrast to their work, our predictor needs only the output of the current layer’s attention module and not the previous layer’s FFN module.\n"),t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/85f3c2dc-a352-4506-b4d4-08abb896e8c7",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("Neuron Data Management via Sliding Window Technique")]),e._v("\nOur approach focuses on managing neuron data by employing a Sliding Window Technique. This methodology entails maintaining neuron data only for a recent subset of input tokens in the memory."),t("br"),e._v("\nThe key aspect of this technique is the "),t("strong",[e._v("selective loading of neuron data that differs between the current input token and its immediate predecessors")]),e._v("."),t("br"),e._v("\nFrees up memory resources previously allocated to neuron data from older tokens that are no longer within the sliding window\n"),t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/10b9ef42-5aac-4826-9247-b90ad9a171e7",alt:"image"}})]),e._v(" "),t("p",[e._v("Let sagg(k) denote the cumulative use of neuron data across a sequence of k input tokens.\nThis reduction in data loading is counterbalanced by the memory cost associated with storing sagg(k). In determining the size of the sliding window, the aim is to maximize it within the constraints imposed by the available memory capacity.")]),e._v(" "),t("p",[e._v("2.2 Improve Transfer Throughput with Increased Chunk Sizes")]),e._v(" "),t("p",[t("strong",[e._v("Bundling Columns and Rows")]),e._v(" for upward and downward projection")]),e._v(" "),t("p",[t("strong",[e._v("Bundling Based on Co-activation")]),e._v(" fetch neuron with its cloest friend. But there is WARM-GUY problem.")]),e._v(" "),t("p",[e._v("2.3 Optimized Data Management in DRAM")]),e._v(" "),t("p",[e._v("When a substantial portion (approximately 25%) of the Feed-Forward Networks (FFNs) in DRAM needs to be rewritten.")]),e._v(" "),t("p",[e._v("When introducing data for new neurons, reallocating the matrix and appending new matrices can lead to significant overhead due to the need for rewriting existing neurons data in DRAM. This involves the preallocation of all necessary memory and the establishment of a corresponding data structure for efficient management.\n"),t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/39b7adfe-fad8-4bd3-9be3-f084c260fcec",alt:"image"}})]),e._v(" "),t("hr"),e._v(" "),t("p",[e._v("LLM Principles")]),e._v(" "),t("h3",{attrs:{id:"_1-a-survey-on-hallucination-in-large-language-models-principles-taxonomy-challenges-and-open-questions"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-a-survey-on-hallucination-in-large-language-models-principles-taxonomy-challenges-and-open-questions"}},[e._v("#")]),e._v(" 1. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/35d4a401-2b5f-43ea-8070-8d1043f9d8e2",alt:"image"}})])])}),[],!1,null,null,null);t.default=i.exports}}]);