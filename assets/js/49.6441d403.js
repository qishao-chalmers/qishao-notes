(window.webpackJsonp=window.webpackJsonp||[]).push([[49],{460:function(e,a,t){"use strict";t.r(a);var s=t(5),r=Object(s.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("ol",[a("li",[e._v("[90] Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring")]),e._v(" "),a("li",[e._v("[6] SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs")]),e._v(" "),a("li",[e._v("[117] Observations and Opportunities in Architecting Shared Virtual Memory for Heterogeneous Systems ðŸ‘ ðŸ‘ ðŸ‘ ðŸ‘ ðŸ‘´")]),e._v(" "),a("li",[e._v("[2023] TunneLs for Bootlegging: Fully Reverse-Engineering GPU TLBs  for Challenging Isolation Guarantees of NVIDIA MIG ðŸ‘ ðŸ‘ ðŸ‘")]),e._v(" "),a("li",[e._v("[31] Big data causing big (TLB) problems: taming random memory accesses on the GPU ðŸ‘ ðŸ‘ ðŸ‘")])]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_1-dissecting-the-nvidia-volta-gpu-architecture-via-microbenchmaring"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-dissecting-the-nvidia-volta-gpu-architecture-via-microbenchmaring"}},[e._v("#")]),e._v(" 1. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring")]),e._v(" "),a("p",[e._v("On Volta and on all other architectures we examined:")]),e._v(" "),a("ul",[a("li",[e._v("the L1 data cache is indexed by virtual addresses;")]),e._v(" "),a("li",[e._v("the L2 data cache is indexed by physical addresses")])]),e._v(" "),a("h3",{attrs:{id:"_2-snakebyte-a-tlb-design-with-adaptive-and-recursive-page-merging-in-gpus"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-snakebyte-a-tlb-design-with-adaptive-and-recursive-page-merging-in-gpus"}},[e._v("#")]),e._v(" 2. SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs")]),e._v(" "),a("h4",{attrs:{id:"idea"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#idea"}},[e._v("#")]),e._v(" Idea")]),e._v(" "),a("p",[e._v("SnakeByte allows multiple equal-sized pages coalescing into a page table entry (PTE)."),a("br"),e._v("\nIt records the validity of pages to be merged using a bit vector, and few bits are annexed to indicate the size of merged pages.")]),e._v(" "),a("h4",{attrs:{id:"tlb-ptw-gmmu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tlb-ptw-gmmu"}},[e._v("#")]),e._v(" TLB & PTW & GMMU")]),e._v(" "),a("p",[e._v("Departing from conventional paging schemes of CPUs that heavily rely on operating systems, hardware-based GPU memory management units (GMMUs) are essential to effectively separate device memory management from host\nCPUs."),a("br"),e._v("\nOtherwise, GPUs require the frequent intervention of OS to handle page table walks (PTWs) and TLB misses, which significantly penalize the GPU performance.")]),e._v(" "),a("p",[e._v("Observations:")]),e._v(" "),a("ul",[a("li",[e._v("GPU workloads demand a large number of TLB entries (e.g., 32K to 256K entries) to handle sizable working sets, but conventional TLBs cannot provide sufficient coverage.")]),e._v(" "),a("li",[e._v("GPU workloads have variable ranges of page contiguity.")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/ca8c2089-866b-4c16-a853-3a0f2fc792bc",alt:"image"}})]),e._v(" "),a("h5",{attrs:{id:"paper-idea"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#paper-idea"}},[e._v("#")]),e._v(" Paper Idea")]),e._v(" "),a("p",[e._v("If contiguity exists, valid bits are accordingly set in the bit vector. When all pages in the page group are allocated with contiguity (i.e., all valid bits set), the first PTE of the page group called base PTE is promoted to be further coalesced into a larger page group.")]),e._v(" "),a("h5",{attrs:{id:"address-translation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#address-translation"}},[e._v("#")]),e._v(" Address Translation")]),e._v(" "),a("p",[e._v("An L1 TLB is private to a streaming multiprocessor (SM), and an L2 TLB is shared among SMs [41], [42]."),a("br"),e._v("\nOn a last-level TLB miss, a request is sent to a centralized GMMU [18], [41], [42] to walk through page tables, and the GMMU concurrently handles multiple PTW requests (e.g., 8-16 PTWs)."),a("br"),e._v("\nTo amortize the latency cost of PTWs, GPUs employ page walk caches that store recently used translations at different levels of page tables."),a("br"),e._v("\nImportantly, the GMMU execution has to be independent of host-side operations unlike the conventional paging schemes of CPUs that heavily rely on operating systems. "),a("br"),e._v("\nOtherwise, GPUs involve frequent OS interventions, which significantly penalize the GPU performance [44], [54].")]),e._v(" "),a("p",[e._v("This observation is the primary motivation of SnakeByte that can flexibly manage variable-sized page groups and maximize TLB reach.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/28e7240f-6b4a-4832-996b-70450bbef038",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/0661a5c0-910f-4f72-b1df-368ecf94e376",alt:"image"}})]),e._v(" "),a("p",[e._v("When eight 4KB pages are allocated with contiguity, the page group is promoted to be coalesced into the next level of page group.")]),e._v(" "),a("p",[a("strong",[e._v("At the new page allocation, SnakeByte checks the contiguity of the new PTE with others in the page group.")])]),e._v(" "),a("h5",{attrs:{id:"simulation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#simulation"}},[e._v("#")]),e._v(" Simulation")]),e._v(" "),a("ul",[a("li",[e._v("By recursively coalescing PTEs, SnakeByte inevitably loses fine-grained controls on the A/D bits for individual pages."),a("br"),e._v("\nSnakeByte adds 8-bit access and dirty fields to a TLB entry to trace A/D states within a page group.")]),e._v(" "),a("li",[e._v("GPUs have long shootdown delays (4.2us).")]),e._v(" "),a("li",[e._v("The TLB hierarchy consists of a private L1 TLB per SM, a shared L2 TLB, and miss status holding registers (MSHRs)."),a("br"),e._v("\nAn MSHR in an L1 TLB merges up to 16 misses.")]),e._v(" "),a("li",[e._v("16 page table walkers can concurrently access four-level page tables, and a page walk cache per page table level stores up to 16 recently used translations.")]),e._v(" "),a("li",[e._v("When a new page is allocated, a sequential page prefetcher allocates 16 consecutive pages (total 64KB) at a time.")]),e._v(" "),a("li",[e._v("To analyze the effect of page migration latency [9], [55], we add a 20us latency overhead for each 4KB page fault [55] with 8.48GB/s bandwidth for a 64KB prefetcher [18].")])]),e._v(" "),a("h3",{attrs:{id:"_5-big-data-causing-big-tlb-problems-taming-random-memory-accesses-on-the-gpu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-big-data-causing-big-tlb-problems-taming-random-memory-accesses-on-the-gpu"}},[e._v("#")]),e._v(" 5. Big data causing big (TLB) problems: taming random memory accesses on the GPU")]),e._v(" "),a("p",[e._v("If the data accesses are irregular, like hash table accesses or random sampling, the GPU performance can suffer."),a("br"),e._v("\nEspecially when scaling such accesses beyond 2GB of data, a performance decrease of an order of magnitude is encountered."),a("br"),e._v("\nThis is paper analyzes the source of the slowdown through extensive micro-benchmarking, attributing the root cause to the Translation Lookaside Buffer (TLB).")]),e._v(" "),a("h4",{attrs:{id:"introduction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),a("p",[e._v("GPU data larger than 2GB, which, in some cases, may result in a â‰ˆ13.3x runtime decrease."),a("br"),e._v("\nwe identified the Translation Lookaside Buffer (TLB) as the  source of this slowdown, where TLB misses cost hundreds of cycles per memory access.")]),e._v(" "),a("ul",[a("li",[e._v("NVIDIA Kepler [15]")]),e._v(" "),a("li",[e._v("NVIDIA Pascal [16]")])]),e._v(" "),a("p",[e._v("the P100 shows a significantly better performance than the K80, as it has a newer hardware architecture."),a("br"),e._v("\nHowever, the slowdown for memory accesses >2GB is still significant with factors of 4.3x for random sampling and 3.3x for grouping.")]),e._v(" "),a("h4",{attrs:{id:"benchmark"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#benchmark"}},[e._v("#")]),e._v(" Benchmark")]),e._v(" "),a("h5",{attrs:{id:"virtual-memory"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#virtual-memory"}},[e._v("#")]),e._v(" Virtual Memory")]),e._v(" "),a("p",[e._v("The reasons why GPU use virtual address"),a("br"),e._v("\n(1) Isolation: The indirection controls a programâ€™s memory accesses and, thus, keeps it from disallowed memory accesses to internal\ndevice data or to data of other applications using the same GPU."),a("br"),e._v("\n(2) Fragmentation: Memory fragmentation can be hidden with virtual pages, allowing a large consecutive region of virtual memory to be scattered across many positions in physical memory."),a("br"),e._v("\nThis can also increase memory bandwidth if physical memory is scattered to multiple memory chips, which then can be accessed in parallel.")]),e._v(" "),a("h5",{attrs:{id:"benchmark-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#benchmark-2"}},[e._v("#")]),e._v(" Benchmark")]),e._v(" "),a("p",[a("em",[e._v("pointer chasing with stride distance")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/ffc46301-1aa9-43b0-814d-f93332aba085",alt:"image"}})]),e._v(" "),a("p",[e._v("Every stride size smaller than the page size behaves like (1/2)*X: showing lower cycle counts but experiences the first TLB miss at the same position.")]),e._v(" "),a("h4",{attrs:{id:"observation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#observation"}},[e._v("#")]),e._v(" Observation")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/00839b1e-4e68-44a9-9202-94819117e199",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/3dd1f116-73ef-4947-8689-4b600cfb2f70",alt:"image"}})]),e._v(" "),a("h5",{attrs:{id:"summary"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#summary"}},[e._v("#")]),e._v(" Summary")]),e._v(" "),a("p",[e._v("(1) We found three levels of TLBs for the K80 and two levels for the P100."),a("br"),e._v("\n(2) For both GPUs, the different TLB levels apparently use different page sizes, where the L1 TLB uses a small page size and the L2/L3 TLB use a 16x larger page size."),a("br"),e._v("\n(3) Compared to K80, the P100 always has 16x larger pages."),a("br"),e._v("\n(4) For data larger than 2GB, the K80 has a total delay of 241 cycles, while the P100 only has a 119 cycle delay.")]),e._v(" "),a("h4",{attrs:{id:"plausibility-and-validation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#plausibility-and-validation"}},[e._v("#")]),e._v(" Plausibility and Validation")]),e._v(" "),a("ul",[a("li",[e._v("First, the sizes of the L1 TLB (16 entries) and L2 TLB (65 entries) for Kepler GPUs (K80).")]),e._v(" "),a("li",[e._v("We can confirm this for the L1 TLB, while the K80 already uses 2MB pages for the L2 and L3 TLB (as shown by [10]).")]),e._v(" "),a("li",[e._v("Third, every TPC has its own L1 TLB and every GPC has its own L2 TLB, while the L3 TLB is shared for all SMs.")]),e._v(" "),a("li",[e._v("Fourth, we can see a significant performance drop in our investigated database operations when we access more data than â‰ˆ2GB."),a("br"),e._v("\nEven with different page sizes for both GPUs, we can pinpoint the problem to the L3 TLB on the K80 and the L2 TLB on the P100."),a("br"),e._v("\nWe can even identify the L2 TLB boundary on the K80, where performance problems start at â‰ˆ130MB.")]),e._v(" "),a("li",[e._v("Fifth, in [6], the performance of a grouping operator on Kepler GPUs was improved by reducing the number of threads to <1000\ninstead of multiple thousands for data accesses beyond 2GB."),a("br"),e._v("\nWith our results, we can explain that this is benefinicial because each thread can load one page translation in the L3 TLB (1032 entries)."),a("br"),e._v("\nThe page translations stay in the TLB.")])]),e._v(" "),a("h4",{attrs:{id:"argument-for-unconventional-properties"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#argument-for-unconventional-properties"}},[e._v("#")]),e._v(" Argument for Unconventional Properties")]),e._v(" "),a("p",[e._v("two unconventional results:"),a("br"),e._v("\n(1) TLB entry numbers not being the power of two"),a("br"),e._v("\n(2) different page sizes for different TLB levels.")]),e._v(" "),a("p",[e._v("We evaluated the allocation size and found that the smaller page size is always used for allocations (128KB on K80,2MB on P100)."),a("br"),e._v("\nOne possible explanation for the apparently larger page sizes in the L2/L3 TLB could be a "),a("strong",[e._v("static pre-fetching algorithm")]),e._v(", which always loads 16 contiguous pages when a TLB miss occurs."),a("br"),e._v("\nThis would result in one TLB miss and 15 TLB hits, when using the small page size as traversal stride.")])])}),[],!1,null,null,null);a.default=r.exports}}]);