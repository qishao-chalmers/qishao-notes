(window.webpackJsonp=window.webpackJsonp||[]).push([[101],{554:function(e,t,a){"use strict";a.r(t);var i=a(9),r=Object(i.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"llm-training-time"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#llm-training-time"}},[e._v("#")]),e._v(" LLM Training time")]),e._v(" "),t("p",[e._v("Llama3 405B")]),e._v(" "),t("p",[e._v("C = 6 * N * D")]),e._v(" "),t("ul",[t("li",[e._v("C compute")]),e._v(" "),t("li",[e._v("N parameter number")]),e._v(" "),t("li",[e._v("D token number")])]),e._v(" "),t("p",[e._v("C = 6 * 405 * 10^9 * 15*10^12 = 3.6 * 10^25")]),e._v(" "),t("p",[e._v("C/(16KGPU*TFlops)")]),e._v(" "),t("p",[e._v("C/(16 * 1000 * 400 * 10^12) = 97 days")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://www.factorialfunds.com/blog/thoughts-on-llama-3",target:"_blank",rel:"noopener noreferrer"}},[e._v("Source"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("same as:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/723e951d-9eab-40b9-b42c-916c5c3084cc",alt:"image"}})]),e._v(" "),t("p",[t("a",{attrs:{href:"%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"}},[e._v("Page125")])]),e._v(" "),t("p",[e._v("Also in")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/4635d893-655c-403d-82f5-fc16e972a670",alt:"image"}})]),e._v(" "),t("p",[t("a",{attrs:{href:"https://arxiv.org/pdf/2305.10403#page=36&zoom=100,63,210",target:"_blank",rel:"noopener noreferrer"}},[e._v("Source"),t("OutboundLink")],1),e._v("\nFrom paper "),t("em",[e._v("PaLM 2 Technical Report")])]),e._v(" "),t("p",[e._v("This is far different from:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/e18267cb-ad5f-465a-b19f-1a13eef6d49f",alt:"image"}})]),e._v(" "),t("p",[t("em",[e._v("Source:Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM")])]),e._v(" "),t("p",[e._v("Still needs time to check.")]),e._v(" "),t("h2",{attrs:{id:"why-it-is-6-fold"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#why-it-is-6-fold"}},[e._v("#")]),e._v(" why it is 6 fold?")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://medium.com/@kailaspsudheer/the-transformers-arithmetic-527111099527",target:"_blank",rel:"noopener noreferrer"}},[e._v("reference"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/79df3f9d-e6d0-4b97-8461-b7b1ddf9659c",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("In short, for backward pass, the back propagation loss needs to be multiplied by weight to pass get the input-loss for privous layer.")]),e._v(" "),t("strong",[e._v("It also needs to multiplied by the input of this layer to get the derivative of the weights for updating the weight.")])]),e._v(" "),t("p",[e._v("Those two above is multiplication operation, it also needs another two add operation to sum up input-loss for privous layer and update the weigth, one for each.")]),e._v(" "),t("p",[e._v("Paper waiting to be read:")]),e._v(" "),t("ul",[t("li",[e._v("Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM")]),e._v(" "),t("li",[e._v("ZeRO: Memory Optimizations Toward Training Trillion Parameter Models")]),e._v(" "),t("li",[e._v("How Does Critical Batch Size Scale in Pre-training?")]),e._v(" "),t("li",[e._v("Parallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory Management üëç\n"),t("img",{attrs:{src:"https://github.com/user-attachments/assets/77b9b55d-54c7-46e4-b66a-70d11305704e",alt:"image"}})]),e._v(" "),t("li",[e._v("Comparative Study of Large Language Model Architectures on Frontier")]),e._v(" "),t("li",[e._v("Optimizing Distributed Training on Frontier for Large Language Models")]),e._v(" "),t("li",[e._v("Deep Optimizer States: Towards Scalable Training of Transformer Models Using Interleaved Offloading")]),e._v(" "),t("li",[e._v("The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities")]),e._v(" "),t("li",[e._v("Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies")]),e._v(" "),t("li",[e._v("Towards Scalable Automated Alignment of LLMs: A Survey")]),e._v(" "),t("li",[e._v("MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length")]),e._v(" "),t("li",[e._v("Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models")]),e._v(" "),t("li",[e._v("[C1731] Training Compute-Optimal Large Language Models üëç")]),e._v(" "),t("li",[e._v("[C138] Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers")]),e._v(" "),t("li",[e._v("[C1082] Scaling Language Models: Methods, Analysis & Insights from Training Gopher")])])])}),[],!1,null,null,null);t.default=r.exports}}]);