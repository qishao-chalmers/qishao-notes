(window.webpackJsonp=window.webpackJsonp||[]).push([[37],{491:function(e,n,s){"use strict";s.r(n);var t=s(8),a=Object(t.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"mlir-compiling-flow-of-conv2d"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#mlir-compiling-flow-of-conv2d"}},[e._v("#")]),e._v(" MLIR Compiling Flow of Conv2D")]),e._v(" "),n("blockquote",[n("p",[e._v("This is generated by ChatGPT.")])]),e._v(" "),n("h2",{attrs:{id:"_1-high-level-dialect-tosa-or-mhlo"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_1-high-level-dialect-tosa-or-mhlo"}},[e._v("#")]),e._v(" 1. High-Level Dialect (TOSA or MHLO)")]),e._v(" "),n("p",[e._v("At the highest level, the convolution typically appears as a single-purpose op, e.g., TOSA’s tosa.conv2d or MHLO’s mhlo.convolution. Below is a TOSA-style example:")]),e._v(" "),n("details",[n("summary",[e._v("Code")]),e._v(" "),n("div",{staticClass:"language-mlir line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("# Simple 2D conv: stride=1, no padding, no dilation\n# Input:  [N=4, H=32, W=32, C_in=3]\n# Filter: [KH=3, KW=3, C_in=3, C_out=8]\n# Output: [N=4, H=30, W=30, C_out=8]\ntosa.conv2d(\n  %input,    // tensor<4x32x32x3xf32>\n  %filter,   // tensor<3x3x3x8xf32>\n  %bias,     // tensor<8xf32>, optional\n  strides = [1, 1],\n  pad = [0, 0, 0, 0],\n  dilation = [1, 1]\n) : (tensor<4x32x32x3xf32>,\n     tensor<3x3x3x8xf32>,\n     tensor<8xf32>) -> tensor<4x30x30x8xf32>\n")])]),e._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[e._v("1")]),n("br"),n("span",{staticClass:"line-number"},[e._v("2")]),n("br"),n("span",{staticClass:"line-number"},[e._v("3")]),n("br"),n("span",{staticClass:"line-number"},[e._v("4")]),n("br"),n("span",{staticClass:"line-number"},[e._v("5")]),n("br"),n("span",{staticClass:"line-number"},[e._v("6")]),n("br"),n("span",{staticClass:"line-number"},[e._v("7")]),n("br"),n("span",{staticClass:"line-number"},[e._v("8")]),n("br"),n("span",{staticClass:"line-number"},[e._v("9")]),n("br"),n("span",{staticClass:"line-number"},[e._v("10")]),n("br"),n("span",{staticClass:"line-number"},[e._v("11")]),n("br"),n("span",{staticClass:"line-number"},[e._v("12")]),n("br"),n("span",{staticClass:"line-number"},[e._v("13")]),n("br"),n("span",{staticClass:"line-number"},[e._v("14")]),n("br")])])]),e._v(" "),n("p",[e._v("At this stage, the operation is still “high level”: the compiler knows it’s a convolution with certain attributes but hasn’t expanded it into explicit loops yet.")]),e._v(" "),n("h2",{attrs:{id:"_2-lowering-to-linalg-structured-ops"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2-lowering-to-linalg-structured-ops"}},[e._v("#")]),e._v(" 2. Lowering to Linalg (Structured Ops)")]),e._v(" "),n("p",[e._v("A specialized Linalg op may replace the TOSA conv2d. MLIR has built-in named ops like linalg.conv_2d_nhwc_hwcf, but one can also lower to a more generic linalg.generic. Below is a named Linalg form that captures the same convolution:")]),e._v(" "),n("details",[n("summary",[e._v("Code")]),e._v(" "),n("div",{staticClass:"language-mlir line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("%output = linalg.conv_2d_nhwc_hwcf\n    ins(%input, %filter : tensor<4x32x32x3xf32>, tensor<3x3x3x8xf32>)\n    outs(%init : tensor<4x30x30x8xf32>) -> tensor<4x30x30x8xf32>\n{\n  // The internal region is typically auto-generated. \n  // The iteration spaces (N, H_out, W_out, C_out, KH, KW, C_in) \n  // are implied by this op’s definition.\n}\n")])]),e._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[e._v("1")]),n("br"),n("span",{staticClass:"line-number"},[e._v("2")]),n("br"),n("span",{staticClass:"line-number"},[e._v("3")]),n("br"),n("span",{staticClass:"line-number"},[e._v("4")]),n("br"),n("span",{staticClass:"line-number"},[e._v("5")]),n("br"),n("span",{staticClass:"line-number"},[e._v("6")]),n("br"),n("span",{staticClass:"line-number"},[e._v("7")]),n("br"),n("span",{staticClass:"line-number"},[e._v("8")]),n("br")])])]),e._v(" "),n("p",[e._v("Alternatively, you may see an expanded linalg.generic with explicit indexing maps that show the iteration space. Here’s a solid example: we define 7 iteration dimensions:")]),e._v(" "),n("ul",[n("li",[e._v("4 “parallel” dims: (n, oh, ow, co)")]),e._v(" "),n("li",[e._v("3 “reduction” dims: (kh, kw, ci)")])]),e._v(" "),n("p",[e._v("We then set up affine maps to read from (n, oh + kh, ow + kw, ci) and (kh, kw, ci, co), accumulating into (n, oh, ow, co):")]),e._v(" "),n("details",[n("summary",[e._v("Code")]),e._v(" "),n("div",{staticClass:"language-mlir line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v('%conv_result = linalg.generic\n    // Define how to map the 7 loop indices to each tensor’s coordinates\n    { indexing_maps = [\n        // Input:  (n, oh, ow, co, kh, kw, ci) -> (n, oh + kh, ow + kw, ci)\n        affine_map<(n, oh, ow, co, kh, kw, ci) -> (n, oh + kh, ow + kw, ci)>,\n\n        // Filter: (n, oh, ow, co, kh, kw, ci) -> (kh, kw, ci, co)\n        affine_map<(n, oh, ow, co, kh, kw, ci) -> (kh, kw, ci, co)>,\n\n        // Output: (n, oh, ow, co, kh, kw, ci) -> (n, oh, ow, co)\n        affine_map<(n, oh, ow, co, kh, kw, ci) -> (n, oh, ow, co)>\n      ],\n      iterator_types = [\n        "parallel", "parallel", "parallel", "parallel", // n, oh, ow, co\n        "reduction", "reduction", "reduction"           // kh, kw, ci\n      ]\n    }\n    ins(%input, %filter : tensor<4x32x32x3xf32>, tensor<3x3x3x8xf32>)\n    outs(%init_out : tensor<4x30x30x8xf32>) -> tensor<4x30x30x8xf32>\n  {\n    // This block is the "fused operation" over each point in the iteration space\n    ^bb0(%in_val: f32, %fil_val: f32, %acc_val: f32):\n      %mul = arith.mulf %in_val, %fil_val : f32\n      %res = arith.addf %acc_val, %mul : f32\n      linalg.yield %res : f32\n  }\n')])]),e._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[e._v("1")]),n("br"),n("span",{staticClass:"line-number"},[e._v("2")]),n("br"),n("span",{staticClass:"line-number"},[e._v("3")]),n("br"),n("span",{staticClass:"line-number"},[e._v("4")]),n("br"),n("span",{staticClass:"line-number"},[e._v("5")]),n("br"),n("span",{staticClass:"line-number"},[e._v("6")]),n("br"),n("span",{staticClass:"line-number"},[e._v("7")]),n("br"),n("span",{staticClass:"line-number"},[e._v("8")]),n("br"),n("span",{staticClass:"line-number"},[e._v("9")]),n("br"),n("span",{staticClass:"line-number"},[e._v("10")]),n("br"),n("span",{staticClass:"line-number"},[e._v("11")]),n("br"),n("span",{staticClass:"line-number"},[e._v("12")]),n("br"),n("span",{staticClass:"line-number"},[e._v("13")]),n("br"),n("span",{staticClass:"line-number"},[e._v("14")]),n("br"),n("span",{staticClass:"line-number"},[e._v("15")]),n("br"),n("span",{staticClass:"line-number"},[e._v("16")]),n("br"),n("span",{staticClass:"line-number"},[e._v("17")]),n("br"),n("span",{staticClass:"line-number"},[e._v("18")]),n("br"),n("span",{staticClass:"line-number"},[e._v("19")]),n("br"),n("span",{staticClass:"line-number"},[e._v("20")]),n("br"),n("span",{staticClass:"line-number"},[e._v("21")]),n("br"),n("span",{staticClass:"line-number"},[e._v("22")]),n("br"),n("span",{staticClass:"line-number"},[e._v("23")]),n("br"),n("span",{staticClass:"line-number"},[e._v("24")]),n("br"),n("span",{staticClass:"line-number"},[e._v("25")]),n("br"),n("span",{staticClass:"line-number"},[e._v("26")]),n("br")])])]),e._v(" "),n("p",[e._v("This explicitly shows the iteration space (n, oh, ow, co, kh, kw, ci) and how each tensor is indexed. The convolution is essentially a 7D loop nest: 4 parallel loops, 3 reduction loops.")]),e._v(" "),n("h2",{attrs:{id:"_3-bufferization-memref-dialect"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_3-bufferization-memref-dialect"}},[e._v("#")]),e._v(" 3. Bufferization & MemRef Dialect")]),e._v(" "),n("p",[e._v("Before we can generate code for GPUs, we usually transform from “tensor forms” (value semantics) into memref forms (explicit pointers). For instance, each tensor might become a memref<4x32x32x3xf32> allocated in GPU memory. A simplified result might look like:")]),e._v(" "),n("details",[n("summary",[e._v("Code")]),e._v(" "),n("div",{staticClass:"language-mlir line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("// Example function using memrefs for input/output/filter:\nfunc.func @conv2d_main(%input: memref<4x32x32x3xf32>, \n                       %filter: memref<3x3x3x8xf32>, \n                       %output: memref<4x30x30x8xf32>) {\n  // The linalg op is now in buffer form:\n  call @linalg_conv_2d_bufferized(%input, %filter, %output)\n\n  func.return\n}\n")])]),e._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[e._v("1")]),n("br"),n("span",{staticClass:"line-number"},[e._v("2")]),n("br"),n("span",{staticClass:"line-number"},[e._v("3")]),n("br"),n("span",{staticClass:"line-number"},[e._v("4")]),n("br"),n("span",{staticClass:"line-number"},[e._v("5")]),n("br"),n("span",{staticClass:"line-number"},[e._v("6")]),n("br"),n("span",{staticClass:"line-number"},[e._v("7")]),n("br"),n("span",{staticClass:"line-number"},[e._v("8")]),n("br"),n("span",{staticClass:"line-number"},[e._v("9")]),n("br")])])]),e._v(" "),n("p",[e._v("Internally, linalg_conv_2d_bufferized will do loads and stores (memref.load, memref.store) or eventually llvm.load/llvm.store once further lowered. Bufferization clarifies memory addressing and layouts.")]),e._v(" "),n("h2",{attrs:{id:"_4-tiling-scheduling-and-vectorization"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_4-tiling-scheduling-and-vectorization"}},[e._v("#")]),e._v(" 4. Tiling, Scheduling, and Vectorization")]),e._v(" "),n("p",[e._v("At this stage (still in Linalg land or in the GPU dialect), a series of passes will:")]),e._v(" "),n("p",[e._v("Tile the convolution (e.g., break the iteration space into tile sizes that fit GPU blocks/threads).\nSchedule the loops to map them to GPU block IDs and thread IDs.\nOptionally vectorize (using the vector dialect) or fuse elementwise ops (like a subsequent ReLU).\nConceptually, this transforms the single big convolution nest into multiple smaller “tiled” computations that run in parallel on the GPU.")]),e._v(" "),n("h2",{attrs:{id:"_5-lowering-to-gpu-dialect-and-then-to-llvm-dialect"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_5-lowering-to-gpu-dialect-and-then-to-llvm-dialect"}},[e._v("#")]),e._v(" 5. Lowering to GPU Dialect (and then to LLVM Dialect)")]),e._v(" "),n("p",[e._v("After tiling and scheduling, we insert ops from the GPU dialect—these specify things like gpu.launch, gpu.block_id, gpu.thread_id. For example:")]),e._v(" "),n("details",[n("summary",[e._v("Code")]),e._v(" "),n("div",{staticClass:"language-mlir line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v('gpu.func @conv2d_kernel(%input: memref<4x32x32x3xf32, 1>,\n                        %filter: memref<3x3x3x8xf32, 1>,\n                        %output: memref<4x30x30x8xf32, 1>) {\n  %bx = gpu.block_id x\n  %tx = gpu.thread_id x\n  // Possibly compute global indices = bx * blockSize + tx, etc.\n\n  // "scf.for" or "gpu.for" loops for the tile\n  // Perform loads, FMAs, stores\n  gpu.return\n}\n')])]),e._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[e._v("1")]),n("br"),n("span",{staticClass:"line-number"},[e._v("2")]),n("br"),n("span",{staticClass:"line-number"},[e._v("3")]),n("br"),n("span",{staticClass:"line-number"},[e._v("4")]),n("br"),n("span",{staticClass:"line-number"},[e._v("5")]),n("br"),n("span",{staticClass:"line-number"},[e._v("6")]),n("br"),n("span",{staticClass:"line-number"},[e._v("7")]),n("br"),n("span",{staticClass:"line-number"},[e._v("8")]),n("br"),n("span",{staticClass:"line-number"},[e._v("9")]),n("br"),n("span",{staticClass:"line-number"},[e._v("10")]),n("br"),n("span",{staticClass:"line-number"},[e._v("11")]),n("br")])])]),e._v(" "),n("p",[e._v("Once in the GPU dialect, a pass lowers it to NVVM or ROCDL dialect ops, which are then turned into the LLVM dialect. For instance, we might see:")]),e._v(" "),n("details",[n("summary",[e._v("Code")]),e._v(" "),n("div",{staticClass:"language-mlir line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("// NVVM dialect or final LLVM dialect snippet (illustrative)\nllvm.func @conv2d_kernel_gpu(...) {\n  %0 = llvm.getelementptr ...\n  %1 = llvm.load %0 : !llvm.ptr<f32>\n  %2 = llvm.fmul %1, %some_other_val : f32\n  ...\n  llvm.store %2, %dest_ptr : !llvm.ptr<f32>\n  llvm.return\n}\n")])]),e._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[e._v("1")]),n("br"),n("span",{staticClass:"line-number"},[e._v("2")]),n("br"),n("span",{staticClass:"line-number"},[e._v("3")]),n("br"),n("span",{staticClass:"line-number"},[e._v("4")]),n("br"),n("span",{staticClass:"line-number"},[e._v("5")]),n("br"),n("span",{staticClass:"line-number"},[e._v("6")]),n("br"),n("span",{staticClass:"line-number"},[e._v("7")]),n("br"),n("span",{staticClass:"line-number"},[e._v("8")]),n("br"),n("span",{staticClass:"line-number"},[e._v("9")]),n("br")])])]),e._v(" "),n("h2",{attrs:{id:"_6-final-ptx-generation"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_6-final-ptx-generation"}},[e._v("#")]),e._v(" 6. Final PTX Generation")]),e._v(" "),n("p",[e._v("Finally, the LLVM dialect is handed off to LLVM’s NVPTX backend, which emits PTX code. In practice, you might see (in textual PTX form):")]),e._v(" "),n("details",[n("summary",[e._v("Code")]),e._v(" "),n("div",{staticClass:"language-mlir line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("    // A tiny snippet of PTX from the compiled kernel\n    // load r0, [rd0];\n    // ...\n    mul.f32  %f2, %f1, %f0;\n    add.f32  %f3, %f2, %f4;\n    // ...\n    ret;\n")])]),e._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[e._v("1")]),n("br"),n("span",{staticClass:"line-number"},[e._v("2")]),n("br"),n("span",{staticClass:"line-number"},[e._v("3")]),n("br"),n("span",{staticClass:"line-number"},[e._v("4")]),n("br"),n("span",{staticClass:"line-number"},[e._v("5")]),n("br"),n("span",{staticClass:"line-number"},[e._v("6")]),n("br"),n("span",{staticClass:"line-number"},[e._v("7")]),n("br")])])]),e._v(" "),n("p",[e._v("This PTX can be JIT-compiled by the CUDA driver into GPU microcode (cubin). Once this step completes, your conv2d kernel is ready to run on NVIDIA hardware.")]),e._v(" "),n("h2",{attrs:{id:"putting-it-all-together-conv2d-→-ptx"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#putting-it-all-together-conv2d-→-ptx"}},[e._v("#")]),e._v(" Putting It All Together (Conv2D → PTX)")]),e._v(" "),n("p",[n("strong",[e._v("High-Level Dialect (TOSA, MHLO)")])]),e._v(" "),n("ul",[n("li",[e._v("Single op: tosa.conv2d / mhlo.convolution.")])]),e._v(" "),n("p",[n("strong",[e._v("Linalg (Structured Ops)")])]),e._v(" "),n("ul",[n("li",[e._v("Named op: linalg.conv_2d_nhwc_hwcf, or")]),e._v(" "),n("li",[e._v("Expanded op: linalg.generic with explicit (n, oh, ow, co, kh, kw, ci) iteration.")])]),e._v(" "),n("p",[n("strong",[e._v("Bufferization → MemRef dialect")])]),e._v(" "),n("ul",[n("li",[e._v("Turn tensors into explicit memory references (memref<...>).")])]),e._v(" "),n("p",[n("strong",[e._v("Tiling & Scheduling")])]),e._v(" "),n("ul",[n("li",[e._v("Transform loops for GPU block/thread distribution, fuse ops, vectorize.")])]),e._v(" "),n("p",[n("strong",[e._v("GPU Dialect → NVVM / LLVM Dialect")])]),e._v(" "),n("ul",[n("li",[e._v("Introduce gpu.launch, thread IDs, eventually become LLVM IR.")])]),e._v(" "),n("p",[n("strong",[e._v("LLVM NVPTX Backend → PTX")])]),e._v(" "),n("ul",[n("li",[e._v("Final codegen to PTX, run on the NVIDIA GPU.")])]),e._v(" "),n("p",[e._v("In short, each stage refines the representation from an abstract “conv2d” operation down to the PTX instructions that physically perform the multiply-accumulate loops on the GPU.")])])}),[],!1,null,null,null);n.default=a.exports}}]);