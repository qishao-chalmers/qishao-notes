(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(e){function n(n){for(var a,o,s=n[0],l=n[1],c=n[2],h=0,m=[];h<s.length;h++)o=s[h],Object.prototype.hasOwnProperty.call(i,o)&&i[o]&&m.push(i[o][0]),i[o]=0;for(a in l)Object.prototype.hasOwnProperty.call(l,a)&&(e[a]=l[a]);for(d&&d(n);m.length;)m.shift()();return r.push.apply(r,c||[]),t()}function t(){for(var e,n=0;n<r.length;n++){for(var t=r[n],a=!0,s=1;s<t.length;s++){var l=t[s];0!==i[l]&&(a=!1)}a&&(r.splice(n--,1),e=o(o.s=t[0]))}return e}var a={},i={1:0},r=[];function o(n){if(a[n])return a[n].exports;var t=a[n]={i:n,l:!1,exports:{}};return e[n].call(t.exports,t,t.exports,o),t.l=!0,t.exports}o.e=function(e){var n=[],t=i[e];if(0!==t)if(t)n.push(t[2]);else{var a=new Promise((function(n,a){t=i[e]=[n,a]}));n.push(t[2]=a);var r,s=document.createElement("script");s.charset="utf-8",s.timeout=120,o.nc&&s.setAttribute("nonce",o.nc),s.src=function(e){return o.p+"assets/js/"+({}[e]||e)+"."+{2:"6d8a25ce",3:"a8431c6b",4:"bae99c2a",5:"8b03d117",6:"2827b429",7:"5d95b052",8:"f90e1643",9:"d1b8e683",10:"c53d023c",11:"b6f7d42d",12:"dc4136f0",13:"33791185",14:"fdb905c9",15:"18b8bfd8",16:"8dd9af70",17:"b688542f",18:"ba9d4baf",19:"daaab02a",20:"78b7313e",21:"91aeba31",22:"58ec3c1c",23:"9a151267",24:"9bd10ddd",25:"db192854",26:"f10809d3",27:"65953f3e",28:"c0382aef",29:"a9c55c08",30:"a37871e7",31:"7304d22e",32:"317fcf55",33:"02dd6ae5",34:"bc72af98",35:"b2e3df35",36:"21d1843c",37:"ece62693",38:"f16032dc",39:"c924945e",40:"89b382c5",41:"c81aff9d",42:"0ae91820",43:"f5fc0a4e",44:"3cc58cc5",45:"dde88fd3",46:"4edc7d96",47:"12a88187",48:"4a1ed3f4",49:"b95dad0e",50:"b803c07a",51:"c400caa3",52:"28e64713",53:"bfe1dda8",54:"9480e434",55:"9a082998",56:"a3b517e3",57:"e6a3f4e4",58:"e1d3c447",59:"fe5b5324",60:"39abe135",61:"f32429ee",62:"6190f2a9",63:"19c15ad2",64:"b0f3a491",65:"0e13cd4c",66:"021a0ef2",67:"52e249a1",68:"80f9f6ee",69:"51699562",70:"3b4ae10c",71:"5b06b360",72:"bc22dca0",73:"3a589a2f",74:"8deb1a9c",75:"83e128af",76:"cd96c90f",77:"9c8ec88e",78:"ce7c920c",79:"45ce4740",80:"7280a683",81:"9cab858c",82:"f2525f33",83:"f4d43dbc",84:"59d6bbd6",85:"53c6d81d",86:"e0896c8e",87:"284b7d73",88:"665c7088",89:"c94ca959",90:"201ebb72",91:"4e398438",92:"c48adf91",93:"9dfed52c",94:"2c93e18c",95:"2bba59c2",96:"0a0aa512",97:"80b8e0a2",98:"6294ae42",99:"71ed026c",100:"a6846821",101:"d2879d9e",102:"be2face5",103:"e14139ad",104:"b501a732",105:"61edaaa1",106:"44b6e691",107:"1d6d4d7c",108:"f8ee38f0",109:"d64b47e0",110:"1746f701",111:"2a5ca088",112:"22ae62ee",113:"d67ed6e3",114:"d90ae064",115:"dd514d98",116:"313ed29f",117:"111bb439",118:"46020aaf",119:"6b330049",120:"ec76960f",121:"e5e3eca8",122:"e5b75833",123:"017be12d",124:"e086b106",125:"854f02c0",126:"398ad1ae",127:"e220f313"}[e]+".js"}(e);var l=new Error;r=function(n){s.onerror=s.onload=null,clearTimeout(c);var t=i[e];if(0!==t){if(t){var a=n&&("load"===n.type?"missing":n.type),r=n&&n.target&&n.target.src;l.message="Loading chunk "+e+" failed.\n("+a+": "+r+")",l.name="ChunkLoadError",l.type=a,l.request=r,t[1](l)}i[e]=void 0}};var c=setTimeout((function(){r({type:"timeout",target:s})}),12e4);s.onerror=s.onload=r,document.head.appendChild(s)}return Promise.all(n)},o.m=e,o.c=a,o.d=function(e,n,t){o.o(e,n)||Object.defineProperty(e,n,{enumerable:!0,get:t})},o.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},o.t=function(e,n){if(1&n&&(e=o(e)),8&n)return e;if(4&n&&"object"==typeof e&&e&&e.__esModule)return e;var t=Object.create(null);if(o.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:e}),2&n&&"string"!=typeof e)for(var a in e)o.d(t,a,function(n){return e[n]}.bind(null,a));return t},o.n=function(e){var n=e&&e.__esModule?function(){return e.default}:function(){return e};return o.d(n,"a",n),n},o.o=function(e,n){return Object.prototype.hasOwnProperty.call(e,n)},o.p="/qishao-notes/",o.oe=function(e){throw console.error(e),e};var s=window.webpackJsonp=window.webpackJsonp||[],l=s.push.bind(s);s.push=n,s=s.slice();for(var c=0;c<s.length;c++)n(s[c]);var d=l;r.push([158,0]),t()}([function(e,n,t){"use strict";var a="object"==typeof document&&document.all;e.exports=void 0===a&&void 0!==a?function(e){return"function"==typeof e||e===a}:function(e){return"function"==typeof e}},function(e,n,t){"use strict";var a=t(10),i=String,r=TypeError;e.exports=function(e){if(a(e))return e;throw new r(i(e)+" is not an object")}},function(e,n,t){"use strict";var a=function(e){return e&&e.Math===Math&&e};e.exports=a("object"==typeof globalThis&&globalThis)||a("object"==typeof window&&window)||a("object"==typeof self&&self)||a("object"==typeof global&&global)||a("object"==typeof this&&this)||function(){return this}()||Function("return this")()},function(e,n,t){"use strict";e.exports=function(e){try{return!!e()}catch(e){return!0}}},function(e,n,t){"use strict";var a=t(137),i=Object.prototype.toString;function r(e){return"[object Array]"===i.call(e)}function o(e){return void 0===e}function s(e){return null!==e&&"object"==typeof e}function l(e){if("[object Object]"!==i.call(e))return!1;var n=Object.getPrototypeOf(e);return null===n||n===Object.prototype}function c(e){return"[object Function]"===i.call(e)}function d(e,n){if(null!=e)if("object"!=typeof e&&(e=[e]),r(e))for(var t=0,a=e.length;t<a;t++)n.call(null,e[t],t,e);else for(var i in e)Object.prototype.hasOwnProperty.call(e,i)&&n.call(null,e[i],i,e)}e.exports={isArray:r,isArrayBuffer:function(e){return"[object ArrayBuffer]"===i.call(e)},isBuffer:function(e){return null!==e&&!o(e)&&null!==e.constructor&&!o(e.constructor)&&"function"==typeof e.constructor.isBuffer&&e.constructor.isBuffer(e)},isFormData:function(e){return"undefined"!=typeof FormData&&e instanceof FormData},isArrayBufferView:function(e){return"undefined"!=typeof ArrayBuffer&&ArrayBuffer.isView?ArrayBuffer.isView(e):e&&e.buffer&&e.buffer instanceof ArrayBuffer},isString:function(e){return"string"==typeof e},isNumber:function(e){return"number"==typeof e},isObject:s,isPlainObject:l,isUndefined:o,isDate:function(e){return"[object Date]"===i.call(e)},isFile:function(e){return"[object File]"===i.call(e)},isBlob:function(e){return"[object Blob]"===i.call(e)},isFunction:c,isStream:function(e){return s(e)&&c(e.pipe)},isURLSearchParams:function(e){return"undefined"!=typeof URLSearchParams&&e instanceof URLSearchParams},isStandardBrowserEnv:function(){return("undefined"==typeof navigator||"ReactNative"!==navigator.product&&"NativeScript"!==navigator.product&&"NS"!==navigator.product)&&("undefined"!=typeof window&&"undefined"!=typeof document)},forEach:d,merge:function e(){var n={};function t(t,a){l(n[a])&&l(t)?n[a]=e(n[a],t):l(t)?n[a]=e({},t):r(t)?n[a]=t.slice():n[a]=t}for(var a=0,i=arguments.length;a<i;a++)d(arguments[a],t);return n},extend:function(e,n,t){return d(n,(function(n,i){e[i]=t&&"function"==typeof n?a(n,t):n})),e},trim:function(e){return e.trim?e.trim():e.replace(/^\s+|\s+$/g,"")},stripBOM:function(e){return 65279===e.charCodeAt(0)&&(e=e.slice(1)),e}}},function(e,n,t){"use strict";var a=t(12),i=t(2),r=t(173),o=t(1),s=t(0),l=t(96),c=t(155),d=t(175),h=t(3),m=t(11),u=t(17),p=t(97).IteratorPrototype,g=t(7),f=t(25),y=u("toStringTag"),b=TypeError,v=i.Iterator,w=f||!s(v)||v.prototype!==p||!h((function(){v({})})),_=function(){if(r(this,p),l(this)===p)throw new b("Abstract class Iterator not directly constructable")},k=function(e,n){g?c(p,e,{configurable:!0,get:function(){return n},set:function(n){if(o(this),this===p)throw new b("You can't redefine this property");m(this,e)?this[e]=n:d(this,e,n)}}):p[e]=n};m(p,y)||k(y,"Iterator"),!w&&m(p,"constructor")&&p.constructor!==Object||k("constructor",_),_.prototype=p,a({global:!0,constructor:!0,forced:w},{Iterator:_})},function(e,n,t){"use strict";var a=t(35),i=Function.prototype,r=i.call,o=a&&i.bind.bind(r,r);e.exports=a?o:function(e){return function(){return r.apply(e,arguments)}}},function(e,n,t){"use strict";var a=t(3);e.exports=!a((function(){return 7!==Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(e,n,t){"use strict";function a(e,n,t,a,i,r,o,s){var l,c="function"==typeof e?e.options:e;if(n&&(c.render=n,c.staticRenderFns=t,c._compiled=!0),a&&(c.functional=!0),r&&(c._scopeId="data-v-"+r),o?(l=function(e){(e=e||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(e=__VUE_SSR_CONTEXT__),i&&i.call(this,e),e&&e._registeredComponents&&e._registeredComponents.add(o)},c._ssrRegister=l):i&&(l=s?function(){i.call(this,(c.functional?this.parent:this).$root.$options.shadowRoot)}:i),l)if(c.functional){c._injectStyles=l;var d=c.render;c.render=function(e,n){return l.call(n),d(e,n)}}else{var h=c.beforeCreate;c.beforeCreate=h?[].concat(h,l):[l]}return{exports:e,options:c}}t.d(n,"a",(function(){return a}))},function(e,n,t){"use strict";var a=t(0),i=t(54),r=TypeError;e.exports=function(e){if(a(e))return e;throw new r(i(e)+" is not a function")}},function(e,n,t){"use strict";var a=t(0);e.exports=function(e){return"object"==typeof e?null!==e:a(e)}},function(e,n,t){"use strict";var a=t(6),i=t(33),r=a({}.hasOwnProperty);e.exports=Object.hasOwn||function(e,n){return r(i(e),n)}},function(e,n,t){"use strict";var a=t(2),i=t(79).f,r=t(26),o=t(50),s=t(56),l=t(93),c=t(172);e.exports=function(e,n){var t,d,h,m,u,p=e.target,g=e.global,f=e.stat;if(t=g?a:f?a[p]||s(p,{}):a[p]&&a[p].prototype)for(d in n){if(m=n[d],h=e.dontCallGetSet?(u=i(t,d))&&u.value:t[d],!c(g?d:p+(f?".":"#")+d,e.forced)&&void 0!==h){if(typeof m==typeof h)continue;l(m,h)}(e.sham||h&&h.sham)&&r(m,"sham",!0),o(t,d,m,e)}}},function(e,n,t){"use strict";var a=t(12),i=t(15),r=t(9),o=t(1),s=t(29),l=t(98),c=t(99),d=t(25),h=l((function(){for(var e,n,t=this.iterator,a=this.predicate,r=this.next;;){if(e=o(i(r,t)),this.done=!!e.done)return;if(n=e.value,c(t,a,[n,this.counter++],!0))return n}}));a({target:"Iterator",proto:!0,real:!0,forced:d},{filter:function(e){return o(this),r(e),new h(s(this),{predicate:e})}})},function(e,n){var t=Array.isArray;e.exports=t},function(e,n,t){"use strict";var a=t(35),i=Function.prototype.call;e.exports=a?i.bind(i):function(){return i.apply(i,arguments)}},function(e,n,t){var a=t(105),i="object"==typeof self&&self&&self.Object===Object&&self,r=a||i||Function("return this")();e.exports=r},function(e,n,t){"use strict";var a=t(2),i=t(86),r=t(11),o=t(87),s=t(83),l=t(82),c=a.Symbol,d=i("wks"),h=l?c.for||c:c&&c.withoutSetter||o;e.exports=function(e){return r(d,e)||(d[e]=s&&r(c,e)?c[e]:h("Symbol."+e)),d[e]}},function(e,n,t){"use strict";var a=t(7),i=t(88),r=t(90),o=t(1),s=t(80),l=TypeError,c=Object.defineProperty,d=Object.getOwnPropertyDescriptor;n.f=a?r?function(e,n,t){if(o(e),n=s(n),o(t),"function"==typeof e&&"prototype"===n&&"value"in t&&"writable"in t&&!t.writable){var a=d(e,n);a&&a.writable&&(e[n]=t.value,t={configurable:"configurable"in t?t.configurable:a.configurable,enumerable:"enumerable"in t?t.enumerable:a.enumerable,writable:!1})}return c(e,n,t)}:c:function(e,n,t){if(o(e),n=s(n),o(t),i)try{return c(e,n,t)}catch(e){}if("get"in t||"set"in t)throw new l("Accessors not supported");return"value"in t&&(e[n]=t.value),e}},function(e,n,t){var a=t(226),i=t(229);e.exports=function(e,n){var t=i(e,n);return a(t)?t:void 0}},function(e,n,t){var a=t(339),i=t(135),r=/[T ]/,o=/:/,s=/^(\d{2})$/,l=[/^([+-]\d{2})$/,/^([+-]\d{3})$/,/^([+-]\d{4})$/],c=/^(\d{4})/,d=[/^([+-]\d{4})/,/^([+-]\d{5})/,/^([+-]\d{6})/],h=/^-(\d{2})$/,m=/^-?(\d{3})$/,u=/^-?(\d{2})-?(\d{2})$/,p=/^-?W(\d{2})$/,g=/^-?W(\d{2})-?(\d{1})$/,f=/^(\d{2}([.,]\d*)?)$/,y=/^(\d{2}):?(\d{2}([.,]\d*)?)$/,b=/^(\d{2}):?(\d{2}):?(\d{2}([.,]\d*)?)$/,v=/([Z+-].*)$/,w=/^(Z)$/,_=/^([+-])(\d{2})$/,k=/^([+-])(\d{2}):?(\d{2})$/;function x(e,n,t){n=n||0,t=t||0;var a=new Date(0);a.setUTCFullYear(e,0,4);var i=7*n+t+1-(a.getUTCDay()||7);return a.setUTCDate(a.getUTCDate()+i),a}e.exports=function(e,n){if(i(e))return new Date(e.getTime());if("string"!=typeof e)return new Date(e);var t=(n||{}).additionalDigits;t=null==t?2:Number(t);var T=function(e){var n,t={},a=e.split(r);o.test(a[0])?(t.date=null,n=a[0]):(t.date=a[0],n=a[1]);if(n){var i=v.exec(n);i?(t.time=n.replace(i[1],""),t.timezone=i[1]):t.time=n}return t}(e),z=function(e,n){var t,a=l[n],i=d[n];if(t=c.exec(e)||i.exec(e)){var r=t[1];return{year:parseInt(r,10),restDateString:e.slice(r.length)}}if(t=s.exec(e)||a.exec(e)){var o=t[1];return{year:100*parseInt(o,10),restDateString:e.slice(o.length)}}return{year:null}}(T.date,t),M=z.year,P=function(e,n){if(null===n)return null;var t,a,i,r;if(0===e.length)return(a=new Date(0)).setUTCFullYear(n),a;if(t=h.exec(e))return a=new Date(0),i=parseInt(t[1],10)-1,a.setUTCFullYear(n,i),a;if(t=m.exec(e)){a=new Date(0);var o=parseInt(t[1],10);return a.setUTCFullYear(n,0,o),a}if(t=u.exec(e)){a=new Date(0),i=parseInt(t[1],10)-1;var s=parseInt(t[2],10);return a.setUTCFullYear(n,i,s),a}if(t=p.exec(e))return r=parseInt(t[1],10)-1,x(n,r);if(t=g.exec(e)){r=parseInt(t[1],10)-1;var l=parseInt(t[2],10)-1;return x(n,r,l)}return null}(z.restDateString,M);if(P){var C,A=P.getTime(),I=0;if(T.time&&(I=function(e){var n,t,a;if(n=f.exec(e))return(t=parseFloat(n[1].replace(",",".")))%24*36e5;if(n=y.exec(e))return t=parseInt(n[1],10),a=parseFloat(n[2].replace(",",".")),t%24*36e5+6e4*a;if(n=b.exec(e)){t=parseInt(n[1],10),a=parseInt(n[2],10);var i=parseFloat(n[3].replace(",","."));return t%24*36e5+6e4*a+1e3*i}return null}(T.time)),T.timezone)C=6e4*function(e){var n,t;if(n=w.exec(e))return 0;if(n=_.exec(e))return t=60*parseInt(n[2],10),"+"===n[1]?-t:t;if(n=k.exec(e))return t=60*parseInt(n[2],10)+parseInt(n[3],10),"+"===n[1]?-t:t;return 0}(T.timezone);else{var S=A+I,L=new Date(S);C=a(L);var U=new Date(S);U.setDate(L.getDate()+1);var q=a(U)-a(L);q>0&&(C+=q)}return new Date(A+I+C)}return new Date(e)}},function(e,n,t){"use strict";t.d(n,"e",(function(){return a})),t.d(n,"b",(function(){return r})),t.d(n,"j",(function(){return o})),t.d(n,"g",(function(){return l})),t.d(n,"h",(function(){return c})),t.d(n,"i",(function(){return d})),t.d(n,"c",(function(){return h})),t.d(n,"f",(function(){return m})),t.d(n,"l",(function(){return u})),t.d(n,"m",(function(){return p})),t.d(n,"d",(function(){return f})),t.d(n,"k",(function(){return y})),t.d(n,"n",(function(){return b})),t.d(n,"a",(function(){return w}));t(49),t(5),t(13),t(23),t(22);const a=/#.*$/,i=/\.(md|html)$/,r=/\/$/,o=/^[a-z]+:/i;function s(e){return decodeURI(e).replace(a,"").replace(i,"")}function l(e){return o.test(e)}function c(e){return/^mailto:/.test(e)}function d(e){return/^tel:/.test(e)}function h(e){if(l(e))return e;if(!e)return"404";const n=e.match(a),t=n?n[0]:"",i=s(e);return r.test(i)?e:i+".html"+t}function m(e,n){const t=e.hash,i=function(e){const n=e&&e.match(a);if(n)return n[0]}(n);if(i&&t!==i)return!1;return s(e.path)===s(n)}function u(e,n,t){if(l(n))return{type:"external",path:n};t&&(n=function(e,n,t){const a=e.charAt(0);if("/"===a)return e;if("?"===a||"#"===a)return n+e;const i=n.split("/");t&&i[i.length-1]||i.pop();const r=e.replace(/^\//,"").split("/");for(let e=0;e<r.length;e++){const n=r[e];".."===n?i.pop():"."!==n&&i.push(n)}""!==i[0]&&i.unshift("");return i.join("/")}(n,t));const a=s(n);for(let n=0;n<e.length;n++)if(s(e[n].regularPath)===a)return Object.assign({},e[n],{type:"page",path:h(e[n].path)});return console.error(`[vuepress] No matching page found for sidebar item "${n}"`),{}}function p(e,n,t,a){const{pages:i,themeConfig:r}=t,o=a&&r.locales&&r.locales[a]||r;if("auto"===(e.frontmatter.sidebar||o.sidebar||r.sidebar))return g(e);const s=o.sidebar||r.sidebar;if(s){const{base:t,config:a}=function(e,n){if(Array.isArray(n))return{base:"/",config:n};for(const a in n)if(0===(t=e,/(\.html|\/)$/.test(t)?t:t+"/").indexOf(encodeURI(a)))return{base:a,config:n[a]};var t;return{}}(n,s);return"auto"===a?g(e):a?a.map(e=>function e(n,t,a,i=1){if("string"==typeof n)return u(t,n,a);if(Array.isArray(n))return Object.assign(u(t,n[0],a),{title:n[1]});{i>3&&console.error("[vuepress] detected a too deep nested sidebar group.");const r=n.children||[];return 0===r.length&&n.path?Object.assign(u(t,n.path,a),{title:n.title}):{type:"group",path:n.path,title:n.title,sidebarDepth:n.sidebarDepth,initialOpenGroupIndex:n.initialOpenGroupIndex,children:r.map(n=>e(n,t,a,i+1)),collapsable:!1!==n.collapsable}}}(e,i,t)):[]}return[]}function g(e){const n=f(e.headers||[]);return[{type:"group",collapsable:!1,title:e.title,path:null,children:n.map(n=>({type:"auto",title:n.title,basePath:e.path,path:e.path+"#"+n.slug,children:n.children||[]}))}]}function f(e){let n;return(e=e.map(e=>Object.assign({},e))).forEach(e=>{2===e.level?n=e:n&&(n.children||(n.children=[])).push(e)}),e.filter(e=>2===e.level)}function y(e){return Object.assign(e,{type:e.items&&e.items.length?"links":"link"})}function b(e){return Object.prototype.toString.call(e).match(/\[object (.*?)\]/)[1].toLowerCase()}function v(e){let n=e.frontmatter.date||e.lastUpdated||new Date,t=new Date(n);return"Invalid Date"==t&&n&&(t=new Date(n.replace(/-/g,"/"))),t.getTime()}function w(e,n){return v(n)-v(e)}},function(e,n,t){"use strict";var a=t(12),i=t(181);a({target:"Iterator",proto:!0,real:!0,forced:t(25)},{map:i})},function(e,n,t){"use strict";var a=t(12),i=t(51),r=t(9),o=t(1),s=t(29);a({target:"Iterator",proto:!0,real:!0},{forEach:function(e){o(this),r(e);var n=s(this),t=0;i(n,(function(n){e(n,t++)}),{IS_RECORD:!0})}})},function(e,n){e.exports=function(e){return null!=e&&"object"==typeof e}},function(e,n,t){"use strict";e.exports=!1},function(e,n,t){"use strict";var a=t(7),i=t(18),r=t(36);e.exports=a?function(e,n,t){return i.f(e,n,r(1,t))}:function(e,n,t){return e[n]=t,e}},function(e,n,t){"use strict";e.exports=TypeError},function(e,n,t){"use strict";var a=t(308),i=t(337),r=t(72);e.exports={formats:r,parse:i,stringify:a}},function(e,n,t){"use strict";e.exports=function(e){return{iterator:e,next:e.next,done:!1}}},function(e,n,t){var a=t(32),i=t(211),r=t(212),o=a?a.toStringTag:void 0;e.exports=function(e){return null==e?void 0===e?"[object Undefined]":"[object Null]":o&&o in Object(e)?i(e):r(e)}},function(e,n,t){"use strict";var a=t(6),i=a({}.toString),r=a("".slice);e.exports=function(e){return r(i(e),8,-1)}},function(e,n,t){var a=t(16).Symbol;e.exports=a},function(e,n,t){"use strict";var a=t(52),i=Object;e.exports=function(e){return i(a(e))}},function(e,n,t){"use strict";var a=t(170);e.exports=function(e){return a(e.length)}},function(e,n,t){"use strict";var a=t(3);e.exports=!a((function(){var e=function(){}.bind();return"function"!=typeof e||e.hasOwnProperty("prototype")}))},function(e,n,t){"use strict";e.exports=function(e,n){return{enumerable:!(1&e),configurable:!(2&e),writable:!(4&e),value:n}}},function(e,n,t){"use strict";var a=t(75),i=t(52);e.exports=function(e){return a(i(e))}},function(e,n,t){"use strict";var a=t(2),i=t(0),r=function(e){return i(e)?e:void 0};e.exports=function(e,n){return arguments.length<2?r(a[e]):a[e]&&a[e][n]}},function(e,n,t){"use strict";var a=t(6);e.exports=a({}.isPrototypeOf)},function(e,n,t){"use strict";var a=t(9),i=t(53);e.exports=function(e,n){var t=e[n];return i(t)?void 0:a(t)}},function(e,n,t){var a=t(216),i=t(217),r=t(218),o=t(219),s=t(220);function l(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var a=e[n];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=i,l.prototype.get=r,l.prototype.has=o,l.prototype.set=s,e.exports=l},function(e,n,t){var a=t(107);e.exports=function(e,n){for(var t=e.length;t--;)if(a(e[t][0],n))return t;return-1}},function(e,n,t){var a=t(19)(Object,"create");e.exports=a},function(e,n,t){var a=t(238);e.exports=function(e,n){var t=e.__data__;return a(n)?t["string"==typeof n?"string":"hash"]:t.map}},function(e,n,t){var a=t(68);e.exports=function(e){if("string"==typeof e||a(e))return e;var n=e+"";return"0"==n&&1/e==-1/0?"-0":n}},function(e,n,t){var a="function"==typeof Map&&Map.prototype,i=Object.getOwnPropertyDescriptor&&a?Object.getOwnPropertyDescriptor(Map.prototype,"size"):null,r=a&&i&&"function"==typeof i.get?i.get:null,o=a&&Map.prototype.forEach,s="function"==typeof Set&&Set.prototype,l=Object.getOwnPropertyDescriptor&&s?Object.getOwnPropertyDescriptor(Set.prototype,"size"):null,c=s&&l&&"function"==typeof l.get?l.get:null,d=s&&Set.prototype.forEach,h="function"==typeof WeakMap&&WeakMap.prototype?WeakMap.prototype.has:null,m="function"==typeof WeakSet&&WeakSet.prototype?WeakSet.prototype.has:null,u="function"==typeof WeakRef&&WeakRef.prototype?WeakRef.prototype.deref:null,p=Boolean.prototype.valueOf,g=Object.prototype.toString,f=Function.prototype.toString,y=String.prototype.match,b=String.prototype.slice,v=String.prototype.replace,w=String.prototype.toUpperCase,_=String.prototype.toLowerCase,k=RegExp.prototype.test,x=Array.prototype.concat,T=Array.prototype.join,z=Array.prototype.slice,M=Math.floor,P="function"==typeof BigInt?BigInt.prototype.valueOf:null,C=Object.getOwnPropertySymbols,A="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?Symbol.prototype.toString:null,I="function"==typeof Symbol&&"object"==typeof Symbol.iterator,S="function"==typeof Symbol&&Symbol.toStringTag&&(typeof Symbol.toStringTag===I||"symbol")?Symbol.toStringTag:null,L=Object.prototype.propertyIsEnumerable,U=("function"==typeof Reflect?Reflect.getPrototypeOf:Object.getPrototypeOf)||([].__proto__===Array.prototype?function(e){return e.__proto__}:null);function q(e,n){if(e===1/0||e===-1/0||e!=e||e&&e>-1e3&&e<1e3||k.call(/e/,n))return n;var t=/[0-9](?=(?:[0-9]{3})+(?![0-9]))/g;if("number"==typeof e){var a=e<0?-M(-e):M(e);if(a!==e){var i=String(a),r=b.call(n,i.length+1);return v.call(i,t,"$&_")+"."+v.call(v.call(r,/([0-9]{3})/g,"$&_"),/_$/,"")}}return v.call(n,t,"$&_")}var E=t(310),R=E.custom,D=H(R)?R:null,O={__proto__:null,double:'"',single:"'"},G={__proto__:null,double:/(["\\])/g,single:/(['\\])/g};function F(e,n,t){var a=t.quoteStyle||n,i=O[a];return i+e+i}function B(e){return v.call(String(e),/"/g,"&quot;")}function N(e){return!S||!("object"==typeof e&&(S in e||void 0!==e[S]))}function j(e){return"[object Array]"===K(e)&&N(e)}function V(e){return"[object RegExp]"===K(e)&&N(e)}function H(e){if(I)return e&&"object"==typeof e&&e instanceof Symbol;if("symbol"==typeof e)return!0;if(!e||"object"!=typeof e||!A)return!1;try{return A.call(e),!0}catch(e){}return!1}e.exports=function e(n,t,a,i){var s=t||{};if(W(s,"quoteStyle")&&!W(O,s.quoteStyle))throw new TypeError('option "quoteStyle" must be "single" or "double"');if(W(s,"maxStringLength")&&("number"==typeof s.maxStringLength?s.maxStringLength<0&&s.maxStringLength!==1/0:null!==s.maxStringLength))throw new TypeError('option "maxStringLength", if provided, must be a positive integer, Infinity, or `null`');var l=!W(s,"customInspect")||s.customInspect;if("boolean"!=typeof l&&"symbol"!==l)throw new TypeError("option \"customInspect\", if provided, must be `true`, `false`, or `'symbol'`");if(W(s,"indent")&&null!==s.indent&&"\t"!==s.indent&&!(parseInt(s.indent,10)===s.indent&&s.indent>0))throw new TypeError('option "indent" must be "\\t", an integer > 0, or `null`');if(W(s,"numericSeparator")&&"boolean"!=typeof s.numericSeparator)throw new TypeError('option "numericSeparator", if provided, must be `true` or `false`');var g=s.numericSeparator;if(void 0===n)return"undefined";if(null===n)return"null";if("boolean"==typeof n)return n?"true":"false";if("string"==typeof n)return function e(n,t){if(n.length>t.maxStringLength){var a=n.length-t.maxStringLength,i="... "+a+" more character"+(a>1?"s":"");return e(b.call(n,0,t.maxStringLength),t)+i}var r=G[t.quoteStyle||"single"];return r.lastIndex=0,F(v.call(v.call(n,r,"\\$1"),/[\x00-\x1f]/g,X),"single",t)}(n,s);if("number"==typeof n){if(0===n)return 1/0/n>0?"0":"-0";var w=String(n);return g?q(n,w):w}if("bigint"==typeof n){var k=String(n)+"n";return g?q(n,k):k}var M=void 0===s.depth?5:s.depth;if(void 0===a&&(a=0),a>=M&&M>0&&"object"==typeof n)return j(n)?"[Array]":"[Object]";var C=function(e,n){var t;if("\t"===e.indent)t="\t";else{if(!("number"==typeof e.indent&&e.indent>0))return null;t=T.call(Array(e.indent+1)," ")}return{base:t,prev:T.call(Array(n+1),t)}}(s,a);if(void 0===i)i=[];else if(Q(i,n)>=0)return"[Circular]";function R(n,t,r){if(t&&(i=z.call(i)).push(t),r){var o={depth:s.depth};return W(s,"quoteStyle")&&(o.quoteStyle=s.quoteStyle),e(n,o,a+1,i)}return e(n,s,a+1,i)}if("function"==typeof n&&!V(n)){var $=function(e){if(e.name)return e.name;var n=y.call(f.call(e),/^function\s*([\w$]+)/);if(n)return n[1];return null}(n),te=ne(n,R);return"[Function"+($?": "+$:" (anonymous)")+"]"+(te.length>0?" { "+T.call(te,", ")+" }":"")}if(H(n)){var ae=I?v.call(String(n),/^(Symbol\(.*\))_[^)]*$/,"$1"):A.call(n);return"object"!=typeof n||I?ae:Y(ae)}if(function(e){if(!e||"object"!=typeof e)return!1;if("undefined"!=typeof HTMLElement&&e instanceof HTMLElement)return!0;return"string"==typeof e.nodeName&&"function"==typeof e.getAttribute}(n)){for(var ie="<"+_.call(String(n.nodeName)),re=n.attributes||[],oe=0;oe<re.length;oe++)ie+=" "+re[oe].name+"="+F(B(re[oe].value),"double",s);return ie+=">",n.childNodes&&n.childNodes.length&&(ie+="..."),ie+="</"+_.call(String(n.nodeName))+">"}if(j(n)){if(0===n.length)return"[]";var se=ne(n,R);return C&&!function(e){for(var n=0;n<e.length;n++)if(Q(e[n],"\n")>=0)return!1;return!0}(se)?"["+ee(se,C)+"]":"[ "+T.call(se,", ")+" ]"}if(function(e){return"[object Error]"===K(e)&&N(e)}(n)){var le=ne(n,R);return"cause"in Error.prototype||!("cause"in n)||L.call(n,"cause")?0===le.length?"["+String(n)+"]":"{ ["+String(n)+"] "+T.call(le,", ")+" }":"{ ["+String(n)+"] "+T.call(x.call("[cause]: "+R(n.cause),le),", ")+" }"}if("object"==typeof n&&l){if(D&&"function"==typeof n[D]&&E)return E(n,{depth:M-a});if("symbol"!==l&&"function"==typeof n.inspect)return n.inspect()}if(function(e){if(!r||!e||"object"!=typeof e)return!1;try{r.call(e);try{c.call(e)}catch(e){return!0}return e instanceof Map}catch(e){}return!1}(n)){var ce=[];return o&&o.call(n,(function(e,t){ce.push(R(t,n,!0)+" => "+R(e,n))})),J("Map",r.call(n),ce,C)}if(function(e){if(!c||!e||"object"!=typeof e)return!1;try{c.call(e);try{r.call(e)}catch(e){return!0}return e instanceof Set}catch(e){}return!1}(n)){var de=[];return d&&d.call(n,(function(e){de.push(R(e,n))})),J("Set",c.call(n),de,C)}if(function(e){if(!h||!e||"object"!=typeof e)return!1;try{h.call(e,h);try{m.call(e,m)}catch(e){return!0}return e instanceof WeakMap}catch(e){}return!1}(n))return Z("WeakMap");if(function(e){if(!m||!e||"object"!=typeof e)return!1;try{m.call(e,m);try{h.call(e,h)}catch(e){return!0}return e instanceof WeakSet}catch(e){}return!1}(n))return Z("WeakSet");if(function(e){if(!u||!e||"object"!=typeof e)return!1;try{return u.call(e),!0}catch(e){}return!1}(n))return Z("WeakRef");if(function(e){return"[object Number]"===K(e)&&N(e)}(n))return Y(R(Number(n)));if(function(e){if(!e||"object"!=typeof e||!P)return!1;try{return P.call(e),!0}catch(e){}return!1}(n))return Y(R(P.call(n)));if(function(e){return"[object Boolean]"===K(e)&&N(e)}(n))return Y(p.call(n));if(function(e){return"[object String]"===K(e)&&N(e)}(n))return Y(R(String(n)));if("undefined"!=typeof window&&n===window)return"{ [object Window] }";if("undefined"!=typeof globalThis&&n===globalThis||"undefined"!=typeof global&&n===global)return"{ [object globalThis] }";if(!function(e){return"[object Date]"===K(e)&&N(e)}(n)&&!V(n)){var he=ne(n,R),me=U?U(n)===Object.prototype:n instanceof Object||n.constructor===Object,ue=n instanceof Object?"":"null prototype",pe=!me&&S&&Object(n)===n&&S in n?b.call(K(n),8,-1):ue?"Object":"",ge=(me||"function"!=typeof n.constructor?"":n.constructor.name?n.constructor.name+" ":"")+(pe||ue?"["+T.call(x.call([],pe||[],ue||[]),": ")+"] ":"");return 0===he.length?ge+"{}":C?ge+"{"+ee(he,C)+"}":ge+"{ "+T.call(he,", ")+" }"}return String(n)};var $=Object.prototype.hasOwnProperty||function(e){return e in this};function W(e,n){return $.call(e,n)}function K(e){return g.call(e)}function Q(e,n){if(e.indexOf)return e.indexOf(n);for(var t=0,a=e.length;t<a;t++)if(e[t]===n)return t;return-1}function X(e){var n=e.charCodeAt(0),t={8:"b",9:"t",10:"n",12:"f",13:"r"}[n];return t?"\\"+t:"\\x"+(n<16?"0":"")+w.call(n.toString(16))}function Y(e){return"Object("+e+")"}function Z(e){return e+" { ? }"}function J(e,n,t,a){return e+" ("+n+") {"+(a?ee(t,a):T.call(t,", "))+"}"}function ee(e,n){if(0===e.length)return"";var t="\n"+n.prev+n.base;return t+T.call(e,","+t)+"\n"+n.prev}function ne(e,n){var t=j(e),a=[];if(t){a.length=e.length;for(var i=0;i<e.length;i++)a[i]=W(e,i)?n(e[i],e):""}var r,o="function"==typeof C?C(e):[];if(I){r={};for(var s=0;s<o.length;s++)r["$"+o[s]]=o[s]}for(var l in e)W(e,l)&&(t&&String(Number(l))===l&&l<e.length||I&&r["$"+l]instanceof Symbol||(k.call(/[^\w$]/,l)?a.push(n(l,e)+": "+n(e[l],e)):a.push(l+": "+n(e[l],e))));if("function"==typeof C)for(var c=0;c<o.length;c++)L.call(e,o[c])&&a.push("["+n(o[c])+"]: "+n(e[o[c]],e));return a}},function(e,n,t){"use strict";var a=t(332);e.exports=Function.prototype.bind||a},function(e,n,t){var a,i;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(i="function"==typeof(a=function(){var e,n,t={version:"0.2.0"},a=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function i(e,n,t){return e<n?n:e>t?t:e}function r(e){return 100*(-1+e)}t.configure=function(e){var n,t;for(n in e)void 0!==(t=e[n])&&e.hasOwnProperty(n)&&(a[n]=t);return this},t.status=null,t.set=function(e){var n=t.isStarted();e=i(e,a.minimum,1),t.status=1===e?null:e;var l=t.render(!n),c=l.querySelector(a.barSelector),d=a.speed,h=a.easing;return l.offsetWidth,o((function(n){""===a.positionUsing&&(a.positionUsing=t.getPositioningCSS()),s(c,function(e,n,t){var i;return(i="translate3d"===a.positionUsing?{transform:"translate3d("+r(e)+"%,0,0)"}:"translate"===a.positionUsing?{transform:"translate("+r(e)+"%,0)"}:{"margin-left":r(e)+"%"}).transition="all "+n+"ms "+t,i}(e,d,h)),1===e?(s(l,{transition:"none",opacity:1}),l.offsetWidth,setTimeout((function(){s(l,{transition:"all "+d+"ms linear",opacity:0}),setTimeout((function(){t.remove(),n()}),d)}),d)):setTimeout(n,d)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var e=function(){setTimeout((function(){t.status&&(t.trickle(),e())}),a.trickleSpeed)};return a.trickle&&e(),this},t.done=function(e){return e||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(e){var n=t.status;return n?("number"!=typeof e&&(e=(1-n)*i(Math.random()*n,.1,.95)),n=i(n+e,0,.994),t.set(n)):t.start()},t.trickle=function(){return t.inc(Math.random()*a.trickleRate)},e=0,n=0,t.promise=function(a){return a&&"resolved"!==a.state()?(0===n&&t.start(),e++,n++,a.always((function(){0==--n?(e=0,t.done()):t.set((e-n)/e)})),this):this},t.render=function(e){if(t.isRendered())return document.getElementById("nprogress");c(document.documentElement,"nprogress-busy");var n=document.createElement("div");n.id="nprogress",n.innerHTML=a.template;var i,o=n.querySelector(a.barSelector),l=e?"-100":r(t.status||0),d=document.querySelector(a.parent);return s(o,{transition:"all 0 linear",transform:"translate3d("+l+"%,0,0)"}),a.showSpinner||(i=n.querySelector(a.spinnerSelector))&&m(i),d!=document.body&&c(d,"nprogress-custom-parent"),d.appendChild(n),n},t.remove=function(){d(document.documentElement,"nprogress-busy"),d(document.querySelector(a.parent),"nprogress-custom-parent");var e=document.getElementById("nprogress");e&&m(e)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var e=document.body.style,n="WebkitTransform"in e?"Webkit":"MozTransform"in e?"Moz":"msTransform"in e?"ms":"OTransform"in e?"O":"";return n+"Perspective"in e?"translate3d":n+"Transform"in e?"translate":"margin"};var o=function(){var e=[];function n(){var t=e.shift();t&&t(n)}return function(t){e.push(t),1==e.length&&n()}}(),s=function(){var e=["Webkit","O","Moz","ms"],n={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(e,n){return n.toUpperCase()})),n[t]||(n[t]=function(n){var t=document.body.style;if(n in t)return n;for(var a,i=e.length,r=n.charAt(0).toUpperCase()+n.slice(1);i--;)if((a=e[i]+r)in t)return a;return n}(t))}function a(e,n,a){n=t(n),e.style[n]=a}return function(e,n){var t,i,r=arguments;if(2==r.length)for(t in n)void 0!==(i=n[t])&&n.hasOwnProperty(t)&&a(e,t,i);else a(e,r[1],r[2])}}();function l(e,n){return("string"==typeof e?e:h(e)).indexOf(" "+n+" ")>=0}function c(e,n){var t=h(e),a=t+n;l(t,n)||(e.className=a.substring(1))}function d(e,n){var t,a=h(e);l(e,n)&&(t=a.replace(" "+n+" "," "),e.className=t.substring(1,t.length-1))}function h(e){return(" "+(e.className||"")+" ").replace(/\s+/gi," ")}function m(e){e&&e.parentNode&&e.parentNode.removeChild(e)}return t})?a.call(n,t,n,e):a)||(e.exports=i)},function(e,n,t){"use strict";var a=t(12),i=t(33),r=t(34),o=t(205),s=t(207);a({target:"Array",proto:!0,arity:1,forced:t(3)((function(){return 4294967297!==[].push.call({length:4294967296},1)}))||!function(){try{Object.defineProperty([],"length",{writable:!1}).push()}catch(e){return e instanceof TypeError}}()},{push:function(e){var n=i(this),t=r(n),a=arguments.length;s(t+a);for(var l=0;l<a;l++)n[t]=arguments[l],t++;return o(n,t),t}})},function(e,n,t){"use strict";var a=t(0),i=t(18),r=t(91),o=t(56);e.exports=function(e,n,t,s){s||(s={});var l=s.enumerable,c=void 0!==s.name?s.name:n;if(a(t)&&r(t,c,s),s.global)l?e[n]=t:o(n,t);else{try{s.unsafe?e[n]&&(l=!0):delete e[n]}catch(e){}l?e[n]=t:i.f(e,n,{value:t,enumerable:!1,configurable:!s.nonConfigurable,writable:!s.nonWritable})}return e}},function(e,n,t){"use strict";var a=t(156),i=t(15),r=t(1),o=t(54),s=t(188),l=t(34),c=t(39),d=t(189),h=t(101),m=t(60),u=TypeError,p=function(e,n){this.stopped=e,this.result=n},g=p.prototype;e.exports=function(e,n,t){var f,y,b,v,w,_,k,x=t&&t.that,T=!(!t||!t.AS_ENTRIES),z=!(!t||!t.IS_RECORD),M=!(!t||!t.IS_ITERATOR),P=!(!t||!t.INTERRUPTED),C=a(n,x),A=function(e){return f&&m(f,"normal",e),new p(!0,e)},I=function(e){return T?(r(e),P?C(e[0],e[1],A):C(e[0],e[1])):P?C(e,A):C(e)};if(z)f=e.iterator;else if(M)f=e;else{if(!(y=h(e)))throw new u(o(e)+" is not iterable");if(s(y)){for(b=0,v=l(e);v>b;b++)if((w=I(e[b]))&&c(g,w))return w;return new p(!1)}f=d(e,y)}for(_=z?e.next:f.next;!(k=i(_,f)).done;){try{w=I(k.value)}catch(e){m(f,"throw",e)}if("object"==typeof w&&w&&c(g,w))return w}return new p(!1)}},function(e,n,t){"use strict";var a=t(53),i=TypeError;e.exports=function(e){if(a(e))throw new i("Can't call method on "+e);return e}},function(e,n,t){"use strict";e.exports=function(e){return null==e}},function(e,n,t){"use strict";var a=String;e.exports=function(e){try{return a(e)}catch(e){return"Object"}}},function(e,n,t){"use strict";var a=t(25),i=t(2),r=t(56),o=e.exports=i["__core-js_shared__"]||r("__core-js_shared__",{});(o.versions||(o.versions=[])).push({version:"3.41.0",mode:a?"pure":"global",copyright:"© 2014-2025 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.41.0/LICENSE",source:"https://github.com/zloirock/core-js"})},function(e,n,t){"use strict";var a=t(2),i=Object.defineProperty;e.exports=function(e,n){try{i(a,e,{value:n,configurable:!0,writable:!0})}catch(t){a[e]=n}return n}},function(e,n,t){"use strict";var a=t(86),i=t(87),r=a("keys");e.exports=function(e){return r[e]||(r[e]=i(e))}},function(e,n,t){"use strict";e.exports={}},function(e,n,t){"use strict";e.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(e,n,t){"use strict";var a=t(15),i=t(1),r=t(40);e.exports=function(e,n,t){var o,s;i(e);try{if(!(o=r(e,"return"))){if("throw"===n)throw t;return t}o=a(o,e)}catch(e){s=!0,o=e}if("throw"===n)throw t;if(s)throw o;return i(o),t}},function(e,n,t){var a=t(210),i=t(24),r=Object.prototype,o=r.hasOwnProperty,s=r.propertyIsEnumerable,l=a(function(){return arguments}())?a:function(e){return i(e)&&o.call(e,"callee")&&!s.call(e,"callee")};e.exports=l},function(e,n,t){var a=t(19)(t(16),"Map");e.exports=a},function(e,n){e.exports=function(e){var n=typeof e;return null!=e&&("object"==n||"function"==n)}},function(e,n,t){var a=t(230),i=t(237),r=t(239),o=t(240),s=t(241);function l(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var a=e[n];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=i,l.prototype.get=r,l.prototype.has=o,l.prototype.set=s,e.exports=l},function(e,n){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e){t[++n]=e})),t}},function(e,n){e.exports=function(e){return"number"==typeof e&&e>-1&&e%1==0&&e<=9007199254740991}},function(e,n,t){var a=t(14),i=t(68),r=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,o=/^\w*$/;e.exports=function(e,n){if(a(e))return!1;var t=typeof e;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=e&&!i(e))||(o.test(e)||!r.test(e)||null!=n&&e in Object(n))}},function(e,n,t){var a=t(30),i=t(24);e.exports=function(e){return"symbol"==typeof e||i(e)&&"[object Symbol]"==a(e)}},function(e,n){e.exports=function(e){return e}},function(e,n,t){"use strict";var a=t(127),i=t(312),r=t(313),o=t(314),s=t(315),l=t(316),c=t(27),d=t(317),h=t(318),m=t(319),u=t(320),p=t(321),g=t(322),f=t(323),y=t(324),b=Function,v=function(e){try{return b('"use strict"; return ('+e+").constructor;")()}catch(e){}},w=t(128),_=t(327),k=function(){throw new c},x=w?function(){try{return k}catch(e){try{return w(arguments,"callee").get}catch(e){return k}}}():k,T=t(328)(),z=t(330),M=t(130),P=t(129),C=t(132),A=t(71),I={},S="undefined"!=typeof Uint8Array&&z?z(Uint8Array):void 0,L={__proto__:null,"%AggregateError%":"undefined"==typeof AggregateError?void 0:AggregateError,"%Array%":Array,"%ArrayBuffer%":"undefined"==typeof ArrayBuffer?void 0:ArrayBuffer,"%ArrayIteratorPrototype%":T&&z?z([][Symbol.iterator]()):void 0,"%AsyncFromSyncIteratorPrototype%":void 0,"%AsyncFunction%":I,"%AsyncGenerator%":I,"%AsyncGeneratorFunction%":I,"%AsyncIteratorPrototype%":I,"%Atomics%":"undefined"==typeof Atomics?void 0:Atomics,"%BigInt%":"undefined"==typeof BigInt?void 0:BigInt,"%BigInt64Array%":"undefined"==typeof BigInt64Array?void 0:BigInt64Array,"%BigUint64Array%":"undefined"==typeof BigUint64Array?void 0:BigUint64Array,"%Boolean%":Boolean,"%DataView%":"undefined"==typeof DataView?void 0:DataView,"%Date%":Date,"%decodeURI%":decodeURI,"%decodeURIComponent%":decodeURIComponent,"%encodeURI%":encodeURI,"%encodeURIComponent%":encodeURIComponent,"%Error%":i,"%eval%":eval,"%EvalError%":r,"%Float16Array%":"undefined"==typeof Float16Array?void 0:Float16Array,"%Float32Array%":"undefined"==typeof Float32Array?void 0:Float32Array,"%Float64Array%":"undefined"==typeof Float64Array?void 0:Float64Array,"%FinalizationRegistry%":"undefined"==typeof FinalizationRegistry?void 0:FinalizationRegistry,"%Function%":b,"%GeneratorFunction%":I,"%Int8Array%":"undefined"==typeof Int8Array?void 0:Int8Array,"%Int16Array%":"undefined"==typeof Int16Array?void 0:Int16Array,"%Int32Array%":"undefined"==typeof Int32Array?void 0:Int32Array,"%isFinite%":isFinite,"%isNaN%":isNaN,"%IteratorPrototype%":T&&z?z(z([][Symbol.iterator]())):void 0,"%JSON%":"object"==typeof JSON?JSON:void 0,"%Map%":"undefined"==typeof Map?void 0:Map,"%MapIteratorPrototype%":"undefined"!=typeof Map&&T&&z?z((new Map)[Symbol.iterator]()):void 0,"%Math%":Math,"%Number%":Number,"%Object%":a,"%Object.getOwnPropertyDescriptor%":w,"%parseFloat%":parseFloat,"%parseInt%":parseInt,"%Promise%":"undefined"==typeof Promise?void 0:Promise,"%Proxy%":"undefined"==typeof Proxy?void 0:Proxy,"%RangeError%":o,"%ReferenceError%":s,"%Reflect%":"undefined"==typeof Reflect?void 0:Reflect,"%RegExp%":RegExp,"%Set%":"undefined"==typeof Set?void 0:Set,"%SetIteratorPrototype%":"undefined"!=typeof Set&&T&&z?z((new Set)[Symbol.iterator]()):void 0,"%SharedArrayBuffer%":"undefined"==typeof SharedArrayBuffer?void 0:SharedArrayBuffer,"%String%":String,"%StringIteratorPrototype%":T&&z?z(""[Symbol.iterator]()):void 0,"%Symbol%":T?Symbol:void 0,"%SyntaxError%":l,"%ThrowTypeError%":x,"%TypedArray%":S,"%TypeError%":c,"%Uint8Array%":"undefined"==typeof Uint8Array?void 0:Uint8Array,"%Uint8ClampedArray%":"undefined"==typeof Uint8ClampedArray?void 0:Uint8ClampedArray,"%Uint16Array%":"undefined"==typeof Uint16Array?void 0:Uint16Array,"%Uint32Array%":"undefined"==typeof Uint32Array?void 0:Uint32Array,"%URIError%":d,"%WeakMap%":"undefined"==typeof WeakMap?void 0:WeakMap,"%WeakRef%":"undefined"==typeof WeakRef?void 0:WeakRef,"%WeakSet%":"undefined"==typeof WeakSet?void 0:WeakSet,"%Function.prototype.call%":A,"%Function.prototype.apply%":C,"%Object.defineProperty%":_,"%Object.getPrototypeOf%":M,"%Math.abs%":h,"%Math.floor%":m,"%Math.max%":u,"%Math.min%":p,"%Math.pow%":g,"%Math.round%":f,"%Math.sign%":y,"%Reflect.getPrototypeOf%":P};if(z)try{null.error}catch(e){var U=z(z(e));L["%Error.prototype%"]=U}var q={__proto__:null,"%ArrayBufferPrototype%":["ArrayBuffer","prototype"],"%ArrayPrototype%":["Array","prototype"],"%ArrayProto_entries%":["Array","prototype","entries"],"%ArrayProto_forEach%":["Array","prototype","forEach"],"%ArrayProto_keys%":["Array","prototype","keys"],"%ArrayProto_values%":["Array","prototype","values"],"%AsyncFunctionPrototype%":["AsyncFunction","prototype"],"%AsyncGenerator%":["AsyncGeneratorFunction","prototype"],"%AsyncGeneratorPrototype%":["AsyncGeneratorFunction","prototype","prototype"],"%BooleanPrototype%":["Boolean","prototype"],"%DataViewPrototype%":["DataView","prototype"],"%DatePrototype%":["Date","prototype"],"%ErrorPrototype%":["Error","prototype"],"%EvalErrorPrototype%":["EvalError","prototype"],"%Float32ArrayPrototype%":["Float32Array","prototype"],"%Float64ArrayPrototype%":["Float64Array","prototype"],"%FunctionPrototype%":["Function","prototype"],"%Generator%":["GeneratorFunction","prototype"],"%GeneratorPrototype%":["GeneratorFunction","prototype","prototype"],"%Int8ArrayPrototype%":["Int8Array","prototype"],"%Int16ArrayPrototype%":["Int16Array","prototype"],"%Int32ArrayPrototype%":["Int32Array","prototype"],"%JSONParse%":["JSON","parse"],"%JSONStringify%":["JSON","stringify"],"%MapPrototype%":["Map","prototype"],"%NumberPrototype%":["Number","prototype"],"%ObjectPrototype%":["Object","prototype"],"%ObjProto_toString%":["Object","prototype","toString"],"%ObjProto_valueOf%":["Object","prototype","valueOf"],"%PromisePrototype%":["Promise","prototype"],"%PromiseProto_then%":["Promise","prototype","then"],"%Promise_all%":["Promise","all"],"%Promise_reject%":["Promise","reject"],"%Promise_resolve%":["Promise","resolve"],"%RangeErrorPrototype%":["RangeError","prototype"],"%ReferenceErrorPrototype%":["ReferenceError","prototype"],"%RegExpPrototype%":["RegExp","prototype"],"%SetPrototype%":["Set","prototype"],"%SharedArrayBufferPrototype%":["SharedArrayBuffer","prototype"],"%StringPrototype%":["String","prototype"],"%SymbolPrototype%":["Symbol","prototype"],"%SyntaxErrorPrototype%":["SyntaxError","prototype"],"%TypedArrayPrototype%":["TypedArray","prototype"],"%TypeErrorPrototype%":["TypeError","prototype"],"%Uint8ArrayPrototype%":["Uint8Array","prototype"],"%Uint8ClampedArrayPrototype%":["Uint8ClampedArray","prototype"],"%Uint16ArrayPrototype%":["Uint16Array","prototype"],"%Uint32ArrayPrototype%":["Uint32Array","prototype"],"%URIErrorPrototype%":["URIError","prototype"],"%WeakMapPrototype%":["WeakMap","prototype"],"%WeakSetPrototype%":["WeakSet","prototype"]},E=t(47),R=t(335),D=E.call(A,Array.prototype.concat),O=E.call(C,Array.prototype.splice),G=E.call(A,String.prototype.replace),F=E.call(A,String.prototype.slice),B=E.call(A,RegExp.prototype.exec),N=/[^%.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|%$))/g,j=/\\(\\)?/g,V=function(e){var n=F(e,0,1),t=F(e,-1);if("%"===n&&"%"!==t)throw new l("invalid intrinsic syntax, expected closing `%`");if("%"===t&&"%"!==n)throw new l("invalid intrinsic syntax, expected opening `%`");var a=[];return G(e,N,(function(e,n,t,i){a[a.length]=t?G(i,j,"$1"):n||e})),a},H=function(e,n){var t,a=e;if(R(q,a)&&(a="%"+(t=q[a])[0]+"%"),R(L,a)){var i=L[a];if(i===I&&(i=function e(n){var t;if("%AsyncFunction%"===n)t=v("async function () {}");else if("%GeneratorFunction%"===n)t=v("function* () {}");else if("%AsyncGeneratorFunction%"===n)t=v("async function* () {}");else if("%AsyncGenerator%"===n){var a=e("%AsyncGeneratorFunction%");a&&(t=a.prototype)}else if("%AsyncIteratorPrototype%"===n){var i=e("%AsyncGenerator%");i&&z&&(t=z(i.prototype))}return L[n]=t,t}(a)),void 0===i&&!n)throw new c("intrinsic "+e+" exists, but is not available. Please file an issue!");return{alias:t,name:a,value:i}}throw new l("intrinsic "+e+" does not exist!")};e.exports=function(e,n){if("string"!=typeof e||0===e.length)throw new c("intrinsic name must be a non-empty string");if(arguments.length>1&&"boolean"!=typeof n)throw new c('"allowMissing" argument must be a boolean');if(null===B(/^%?[^%]*%?$/,e))throw new l("`%` may not be present anywhere but at the beginning and end of the intrinsic name");var t=V(e),a=t.length>0?t[0]:"",i=H("%"+a+"%",n),r=i.name,o=i.value,s=!1,d=i.alias;d&&(a=d[0],O(t,D([0,1],d)));for(var h=1,m=!0;h<t.length;h+=1){var u=t[h],p=F(u,0,1),g=F(u,-1);if(('"'===p||"'"===p||"`"===p||'"'===g||"'"===g||"`"===g)&&p!==g)throw new l("property names with quotes must have matching quotes");if("constructor"!==u&&m||(s=!0),R(L,r="%"+(a+="."+u)+"%"))o=L[r];else if(null!=o){if(!(u in o)){if(!n)throw new c("base intrinsic for "+e+" exists, but the property is not available.");return}if(w&&h+1>=t.length){var f=w(o,u);o=(m=!!f)&&"get"in f&&!("originalValue"in f.get)?f.get:o[u]}else m=R(o,u),o=o[u];m&&!s&&(L[r]=o)}}return o}},function(e,n,t){"use strict";e.exports=Function.prototype.call},function(e,n,t){"use strict";var a=String.prototype.replace,i=/%20/g,r="RFC1738",o="RFC3986";e.exports={default:o,formatters:{RFC1738:function(e){return a.call(e,i,"+")},RFC3986:function(e){return String(e)}},RFC1738:r,RFC3986:o}},function(e,n,t){var a=t(344);e.exports=function(e){return a(e,{weekStartsOn:1})}},function(e,n,t){"use strict";var a=t(4),i=t(356),r=t(139),o={"Content-Type":"application/x-www-form-urlencoded"};function s(e,n){!a.isUndefined(e)&&a.isUndefined(e["Content-Type"])&&(e["Content-Type"]=n)}var l,c={transitional:{silentJSONParsing:!0,forcedJSONParsing:!0,clarifyTimeoutError:!1},adapter:(("undefined"!=typeof XMLHttpRequest||"undefined"!=typeof process&&"[object process]"===Object.prototype.toString.call(process))&&(l=t(140)),l),transformRequest:[function(e,n){return i(n,"Accept"),i(n,"Content-Type"),a.isFormData(e)||a.isArrayBuffer(e)||a.isBuffer(e)||a.isStream(e)||a.isFile(e)||a.isBlob(e)?e:a.isArrayBufferView(e)?e.buffer:a.isURLSearchParams(e)?(s(n,"application/x-www-form-urlencoded;charset=utf-8"),e.toString()):a.isObject(e)||n&&"application/json"===n["Content-Type"]?(s(n,"application/json"),function(e,n,t){if(a.isString(e))try{return(n||JSON.parse)(e),a.trim(e)}catch(e){if("SyntaxError"!==e.name)throw e}return(t||JSON.stringify)(e)}(e)):e}],transformResponse:[function(e){var n=this.transitional,t=n&&n.silentJSONParsing,i=n&&n.forcedJSONParsing,o=!t&&"json"===this.responseType;if(o||i&&a.isString(e)&&e.length)try{return JSON.parse(e)}catch(e){if(o){if("SyntaxError"===e.name)throw r(e,this,"E_JSON_PARSE");throw e}}return e}],timeout:0,xsrfCookieName:"XSRF-TOKEN",xsrfHeaderName:"X-XSRF-TOKEN",maxContentLength:-1,maxBodyLength:-1,validateStatus:function(e){return e>=200&&e<300}};c.headers={common:{Accept:"application/json, text/plain, */*"}},a.forEach(["delete","get","head"],(function(e){c.headers[e]={}})),a.forEach(["post","put","patch"],(function(e){c.headers[e]=a.merge(o)})),e.exports=c},function(e,n,t){"use strict";var a=t(6),i=t(3),r=t(31),o=Object,s=a("".split);e.exports=i((function(){return!o("z").propertyIsEnumerable(0)}))?function(e){return"String"===r(e)?s(e,""):o(e)}:o},function(e,n,t){"use strict";var a,i=t(1),r=t(176),o=t(59),s=t(58),l=t(178),c=t(89),d=t(57),h=d("IE_PROTO"),m=function(){},u=function(e){return"<script>"+e+"<\/script>"},p=function(e){e.write(u("")),e.close();var n=e.parentWindow.Object;return e=null,n},g=function(){try{a=new ActiveXObject("htmlfile")}catch(e){}var e,n;g="undefined"!=typeof document?document.domain&&a?p(a):((n=c("iframe")).style.display="none",l.appendChild(n),n.src=String("javascript:"),(e=n.contentWindow.document).open(),e.write(u("document.F=Object")),e.close(),e.F):p(a);for(var t=o.length;t--;)delete g.prototype[o[t]];return g()};s[h]=!0,e.exports=Object.create||function(e,n){var t;return null!==e?(m.prototype=i(e),t=new m,m.prototype=null,t[h]=e):t=g(),void 0===n?t:r.f(t,n)}},function(e,n){e.exports=function(e){return e.webpackPolyfill||(e.deprecate=function(){},e.paths=[],e.children||(e.children=[]),Object.defineProperty(e,"loaded",{enumerable:!0,get:function(){return e.l}}),Object.defineProperty(e,"id",{enumerable:!0,get:function(){return e.i}}),e.webpackPolyfill=1),e}},function(e,n){var t=/^\s+|\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,i=/^0b[01]+$/i,r=/^0o[0-7]+$/i,o=parseInt,s="object"==typeof global&&global&&global.Object===Object&&global,l="object"==typeof self&&self&&self.Object===Object&&self,c=s||l||Function("return this")(),d=Object.prototype.toString,h=Math.max,m=Math.min,u=function(){return c.Date.now()};function p(e){var n=typeof e;return!!e&&("object"==n||"function"==n)}function g(e){if("number"==typeof e)return e;if(function(e){return"symbol"==typeof e||function(e){return!!e&&"object"==typeof e}(e)&&"[object Symbol]"==d.call(e)}(e))return NaN;if(p(e)){var n="function"==typeof e.valueOf?e.valueOf():e;e=p(n)?n+"":n}if("string"!=typeof e)return 0===e?e:+e;e=e.replace(t,"");var s=i.test(e);return s||r.test(e)?o(e.slice(2),s?2:8):a.test(e)?NaN:+e}e.exports=function(e,n,t){var a,i,r,o,s,l,c=0,d=!1,f=!1,y=!0;if("function"!=typeof e)throw new TypeError("Expected a function");function b(n){var t=a,r=i;return a=i=void 0,c=n,o=e.apply(r,t)}function v(e){return c=e,s=setTimeout(_,n),d?b(e):o}function w(e){var t=e-l;return void 0===l||t>=n||t<0||f&&e-c>=r}function _(){var e=u();if(w(e))return k(e);s=setTimeout(_,function(e){var t=n-(e-l);return f?m(t,r-(e-c)):t}(e))}function k(e){return s=void 0,y&&a?b(e):(a=i=void 0,o)}function x(){var e=u(),t=w(e);if(a=arguments,i=this,l=e,t){if(void 0===s)return v(l);if(f)return s=setTimeout(_,n),b(l)}return void 0===s&&(s=setTimeout(_,n)),o}return n=g(n)||0,p(t)&&(d=!!t.leading,r=(f="maxWait"in t)?h(g(t.maxWait)||0,n):r,y="trailing"in t?!!t.trailing:y),x.cancel=function(){void 0!==s&&clearTimeout(s),c=0,a=l=i=s=void 0},x.flush=function(){return void 0===s?o:k(u())},x}},function(e,n,t){"use strict";var a=t(7),i=t(15),r=t(159),o=t(36),s=t(37),l=t(80),c=t(11),d=t(88),h=Object.getOwnPropertyDescriptor;n.f=a?h:function(e,n){if(e=s(e),n=l(n),d)try{return h(e,n)}catch(e){}if(c(e,n))return o(!i(r.f,e,n),e[n])}},function(e,n,t){"use strict";var a=t(160),i=t(81);e.exports=function(e){var n=a(e,"string");return i(n)?n:n+""}},function(e,n,t){"use strict";var a=t(38),i=t(0),r=t(39),o=t(82),s=Object;e.exports=o?function(e){return"symbol"==typeof e}:function(e){var n=a("Symbol");return i(n)&&r(n.prototype,s(e))}},function(e,n,t){"use strict";var a=t(83);e.exports=a&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(e,n,t){"use strict";var a=t(84),i=t(3),r=t(2).String;e.exports=!!Object.getOwnPropertySymbols&&!i((function(){var e=Symbol("symbol detection");return!r(e)||!(Object(e)instanceof Symbol)||!Symbol.sham&&a&&a<41}))},function(e,n,t){"use strict";var a,i,r=t(2),o=t(85),s=r.process,l=r.Deno,c=s&&s.versions||l&&l.version,d=c&&c.v8;d&&(i=(a=d.split("."))[0]>0&&a[0]<4?1:+(a[0]+a[1])),!i&&o&&(!(a=o.match(/Edge\/(\d+)/))||a[1]>=74)&&(a=o.match(/Chrome\/(\d+)/))&&(i=+a[1]),e.exports=i},function(e,n,t){"use strict";var a=t(2).navigator,i=a&&a.userAgent;e.exports=i?String(i):""},function(e,n,t){"use strict";var a=t(55);e.exports=function(e,n){return a[e]||(a[e]=n||{})}},function(e,n,t){"use strict";var a=t(6),i=0,r=Math.random(),o=a(1..toString);e.exports=function(e){return"Symbol("+(void 0===e?"":e)+")_"+o(++i+r,36)}},function(e,n,t){"use strict";var a=t(7),i=t(3),r=t(89);e.exports=!a&&!i((function(){return 7!==Object.defineProperty(r("div"),"a",{get:function(){return 7}}).a}))},function(e,n,t){"use strict";var a=t(2),i=t(10),r=a.document,o=i(r)&&i(r.createElement);e.exports=function(e){return o?r.createElement(e):{}}},function(e,n,t){"use strict";var a=t(7),i=t(3);e.exports=a&&i((function(){return 42!==Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(e,n,t){"use strict";var a=t(6),i=t(3),r=t(0),o=t(11),s=t(7),l=t(162).CONFIGURABLE,c=t(163),d=t(92),h=d.enforce,m=d.get,u=String,p=Object.defineProperty,g=a("".slice),f=a("".replace),y=a([].join),b=s&&!i((function(){return 8!==p((function(){}),"length",{value:8}).length})),v=String(String).split("String"),w=e.exports=function(e,n,t){"Symbol("===g(u(n),0,7)&&(n="["+f(u(n),/^Symbol\(([^)]*)\).*$/,"$1")+"]"),t&&t.getter&&(n="get "+n),t&&t.setter&&(n="set "+n),(!o(e,"name")||l&&e.name!==n)&&(s?p(e,"name",{value:n,configurable:!0}):e.name=n),b&&t&&o(t,"arity")&&e.length!==t.arity&&p(e,"length",{value:t.arity});try{t&&o(t,"constructor")&&t.constructor?s&&p(e,"prototype",{writable:!1}):e.prototype&&(e.prototype=void 0)}catch(e){}var a=h(e);return o(a,"source")||(a.source=y(v,"string"==typeof n?n:"")),e};Function.prototype.toString=w((function(){return r(this)&&m(this).source||c(this)}),"toString")},function(e,n,t){"use strict";var a,i,r,o=t(164),s=t(2),l=t(10),c=t(26),d=t(11),h=t(55),m=t(57),u=t(58),p=s.TypeError,g=s.WeakMap;if(o||h.state){var f=h.state||(h.state=new g);f.get=f.get,f.has=f.has,f.set=f.set,a=function(e,n){if(f.has(e))throw new p("Object already initialized");return n.facade=e,f.set(e,n),n},i=function(e){return f.get(e)||{}},r=function(e){return f.has(e)}}else{var y=m("state");u[y]=!0,a=function(e,n){if(d(e,y))throw new p("Object already initialized");return n.facade=e,c(e,y,n),n},i=function(e){return d(e,y)?e[y]:{}},r=function(e){return d(e,y)}}e.exports={set:a,get:i,has:r,enforce:function(e){return r(e)?i(e):a(e,{})},getterFor:function(e){return function(n){var t;if(!l(n)||(t=i(n)).type!==e)throw new p("Incompatible receiver, "+e+" required");return t}}}},function(e,n,t){"use strict";var a=t(11),i=t(165),r=t(79),o=t(18);e.exports=function(e,n,t){for(var s=i(n),l=o.f,c=r.f,d=0;d<s.length;d++){var h=s[d];a(e,h)||t&&a(t,h)||l(e,h,c(n,h))}}},function(e,n,t){"use strict";var a=t(6),i=t(11),r=t(37),o=t(167).indexOf,s=t(58),l=a([].push);e.exports=function(e,n){var t,a=r(e),c=0,d=[];for(t in a)!i(s,t)&&i(a,t)&&l(d,t);for(;n.length>c;)i(a,t=n[c++])&&(~o(d,t)||l(d,t));return d}},function(e,n,t){"use strict";var a=t(169);e.exports=function(e){var n=+e;return n!=n||0===n?0:a(n)}},function(e,n,t){"use strict";var a=t(11),i=t(0),r=t(33),o=t(57),s=t(174),l=o("IE_PROTO"),c=Object,d=c.prototype;e.exports=s?c.getPrototypeOf:function(e){var n=r(e);if(a(n,l))return n[l];var t=n.constructor;return i(t)&&n instanceof t?t.prototype:n instanceof c?d:null}},function(e,n,t){"use strict";var a,i,r,o=t(3),s=t(0),l=t(10),c=t(76),d=t(96),h=t(50),m=t(17),u=t(25),p=m("iterator"),g=!1;[].keys&&("next"in(r=[].keys())?(i=d(d(r)))!==Object.prototype&&(a=i):g=!0),!l(a)||o((function(){var e={};return a[p].call(e)!==e}))?a={}:u&&(a=c(a)),s(a[p])||h(a,p,(function(){return this})),e.exports={IteratorPrototype:a,BUGGY_SAFARI_ITERATORS:g}},function(e,n,t){"use strict";var a=t(15),i=t(76),r=t(26),o=t(179),s=t(17),l=t(92),c=t(40),d=t(97).IteratorPrototype,h=t(180),m=t(60),u=s("toStringTag"),p=l.set,g=function(e){var n=l.getterFor(e?"WrapForValidIterator":"IteratorHelper");return o(i(d),{next:function(){var t=n(this);if(e)return t.nextHandler();if(t.done)return h(void 0,!0);try{var a=t.nextHandler();return t.returnHandlerResult?a:h(a,t.done)}catch(e){throw t.done=!0,e}},return:function(){var t=n(this),i=t.iterator;if(t.done=!0,e){var r=c(i,"return");return r?a(r,i):h(void 0,!0)}if(t.inner)try{m(t.inner.iterator,"normal")}catch(e){return m(i,"throw",e)}return i&&m(i,"normal"),h(void 0,!0)}})},f=g(!0),y=g(!1);r(y,u,"Iterator Helper"),e.exports=function(e,n,t){var a=function(a,i){i?(i.iterator=a.iterator,i.next=a.next):i=a,i.type=n?"WrapForValidIterator":"IteratorHelper",i.returnHandlerResult=!!t,i.nextHandler=e,i.counter=0,i.done=!1,p(this,i)};return a.prototype=n?f:y,a}},function(e,n,t){"use strict";var a=t(1),i=t(60);e.exports=function(e,n,t,r){try{return r?n(a(t)[0],t[1]):n(t)}catch(n){i(e,"throw",n)}}},function(e,n,t){"use strict";e.exports={}},function(e,n,t){"use strict";var a=t(102),i=t(40),r=t(53),o=t(100),s=t(17)("iterator");e.exports=function(e){if(!r(e))return i(e,s)||i(e,"@@iterator")||o[a(e)]}},function(e,n,t){"use strict";var a=t(190),i=t(0),r=t(31),o=t(17)("toStringTag"),s=Object,l="Arguments"===r(function(){return arguments}());e.exports=a?r:function(e){var n,t,a;return void 0===e?"Undefined":null===e?"Null":"string"==typeof(t=function(e,n){try{return e[n]}catch(e){}}(n=s(e),o))?t:l?r(n):"Object"===(a=r(n))&&i(n.callee)?"Arguments":a}},function(e,n,t){"use strict";var a=t(195),i=t(10),r=t(52),o=t(196);e.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var e,n=!1,t={};try{(e=a(Object.prototype,"__proto__","set"))(t,[]),n=t instanceof Array}catch(e){}return function(t,a){return r(t),o(a),i(t)?(n?e(t,a):t.__proto__=a,t):t}}():void 0)},function(e,n){e.exports=function(e,n){for(var t=-1,a=n.length,i=e.length;++t<a;)e[i+t]=n[t];return e}},function(e,n){var t="object"==typeof global&&global&&global.Object===Object&&global;e.exports=t},function(e,n,t){var a=t(41),i=t(221),r=t(222),o=t(223),s=t(224),l=t(225);function c(e){var n=this.__data__=new a(e);this.size=n.size}c.prototype.clear=i,c.prototype.delete=r,c.prototype.get=o,c.prototype.has=s,c.prototype.set=l,e.exports=c},function(e,n){e.exports=function(e,n){return e===n||e!=e&&n!=n}},function(e,n,t){var a=t(30),i=t(63);e.exports=function(e){if(!i(e))return!1;var n=a(e);return"[object Function]"==n||"[object GeneratorFunction]"==n||"[object AsyncFunction]"==n||"[object Proxy]"==n}},function(e,n){var t=Function.prototype.toString;e.exports=function(e){if(null!=e){try{return t.call(e)}catch(e){}try{return e+""}catch(e){}}return""}},function(e,n,t){var a=t(242),i=t(24);e.exports=function e(n,t,r,o,s){return n===t||(null==n||null==t||!i(n)&&!i(t)?n!=n&&t!=t:a(n,t,r,o,e,s))}},function(e,n,t){var a=t(112),i=t(245),r=t(113);e.exports=function(e,n,t,o,s,l){var c=1&t,d=e.length,h=n.length;if(d!=h&&!(c&&h>d))return!1;var m=l.get(e),u=l.get(n);if(m&&u)return m==n&&u==e;var p=-1,g=!0,f=2&t?new a:void 0;for(l.set(e,n),l.set(n,e);++p<d;){var y=e[p],b=n[p];if(o)var v=c?o(b,y,p,n,e,l):o(y,b,p,e,n,l);if(void 0!==v){if(v)continue;g=!1;break}if(f){if(!i(n,(function(e,n){if(!r(f,n)&&(y===e||s(y,e,t,o,l)))return f.push(n)}))){g=!1;break}}else if(y!==b&&!s(y,b,t,o,l)){g=!1;break}}return l.delete(e),l.delete(n),g}},function(e,n,t){var a=t(64),i=t(243),r=t(244);function o(e){var n=-1,t=null==e?0:e.length;for(this.__data__=new a;++n<t;)this.add(e[n])}o.prototype.add=o.prototype.push=i,o.prototype.has=r,e.exports=o},function(e,n){e.exports=function(e,n){return e.has(n)}},function(e,n,t){var a=t(255),i=t(261),r=t(118);e.exports=function(e){return r(e)?a(e):i(e)}},function(e,n,t){(function(e){var a=t(16),i=t(257),r=n&&!n.nodeType&&n,o=r&&"object"==typeof e&&e&&!e.nodeType&&e,s=o&&o.exports===r?a.Buffer:void 0,l=(s?s.isBuffer:void 0)||i;e.exports=l}).call(this,t(77)(e))},function(e,n){var t=/^(?:0|[1-9]\d*)$/;e.exports=function(e,n){var a=typeof e;return!!(n=null==n?9007199254740991:n)&&("number"==a||"symbol"!=a&&t.test(e))&&e>-1&&e%1==0&&e<n}},function(e,n,t){var a=t(258),i=t(259),r=t(260),o=r&&r.isTypedArray,s=o?i(o):a;e.exports=s},function(e,n,t){var a=t(108),i=t(66);e.exports=function(e){return null!=e&&i(e.length)&&!a(e)}},function(e,n,t){var a=t(19)(t(16),"Set");e.exports=a},function(e,n,t){var a=t(63);e.exports=function(e){return e==e&&!a(e)}},function(e,n){e.exports=function(e,n){return function(t){return null!=t&&(t[e]===n&&(void 0!==n||e in Object(t)))}}},function(e,n,t){var a=t(123),i=t(45);e.exports=function(e,n){for(var t=0,r=(n=a(n,e)).length;null!=e&&t<r;)e=e[i(n[t++])];return t&&t==r?e:void 0}},function(e,n,t){var a=t(14),i=t(67),r=t(272),o=t(275);e.exports=function(e,n){return a(e)?e:i(e,n)?[e]:r(o(e))}},function(e,n,t){},function(e,n,t){},function(e,n,t){"use strict";var a=t(70),i=t(133),r=t(46),o=t(27),s=a("%Map%",!0),l=i("Map.prototype.get",!0),c=i("Map.prototype.set",!0),d=i("Map.prototype.has",!0),h=i("Map.prototype.delete",!0),m=i("Map.prototype.size",!0);e.exports=!!s&&function(){var e,n={assert:function(e){if(!n.has(e))throw new o("Side channel does not contain "+r(e))},delete:function(n){if(e){var t=h(e,n);return 0===m(e)&&(e=void 0),t}return!1},get:function(n){if(e)return l(e,n)},has:function(n){return!!e&&d(e,n)},set:function(n,t){e||(e=new s),c(e,n,t)}};return n}},function(e,n,t){"use strict";e.exports=Object},function(e,n,t){"use strict";var a=t(326);if(a)try{a([],"length")}catch(e){a=null}e.exports=a},function(e,n,t){"use strict";e.exports="undefined"!=typeof Reflect&&Reflect.getPrototypeOf||null},function(e,n,t){"use strict";var a=t(127);e.exports=a.getPrototypeOf||null},function(e,n,t){"use strict";var a=t(47),i=t(27),r=t(71),o=t(333);e.exports=function(e){if(e.length<1||"function"!=typeof e[0])throw new i("a function is required");return o(a,r,e)}},function(e,n,t){"use strict";e.exports=Function.prototype.apply},function(e,n,t){"use strict";var a=t(70),i=t(131),r=i([a("%String.prototype.indexOf%")]);e.exports=function(e,n){var t=a(e,!!n);return"function"==typeof t&&r(e,".prototype.")>-1?i([t]):t}},function(e,n,t){"use strict";var a=t(72),i=Object.prototype.hasOwnProperty,r=Array.isArray,o=function(){for(var e=[],n=0;n<256;++n)e.push("%"+((n<16?"0":"")+n.toString(16)).toUpperCase());return e}(),s=function(e,n){for(var t=n&&n.plainObjects?{__proto__:null}:{},a=0;a<e.length;++a)void 0!==e[a]&&(t[a]=e[a]);return t};e.exports={arrayToObject:s,assign:function(e,n){return Object.keys(n).reduce((function(e,t){return e[t]=n[t],e}),e)},combine:function(e,n){return[].concat(e,n)},compact:function(e){for(var n=[{obj:{o:e},prop:"o"}],t=[],a=0;a<n.length;++a)for(var i=n[a],o=i.obj[i.prop],s=Object.keys(o),l=0;l<s.length;++l){var c=s[l],d=o[c];"object"==typeof d&&null!==d&&-1===t.indexOf(d)&&(n.push({obj:o,prop:c}),t.push(d))}return function(e){for(;e.length>1;){var n=e.pop(),t=n.obj[n.prop];if(r(t)){for(var a=[],i=0;i<t.length;++i)void 0!==t[i]&&a.push(t[i]);n.obj[n.prop]=a}}}(n),e},decode:function(e,n,t){var a=e.replace(/\+/g," ");if("iso-8859-1"===t)return a.replace(/%[0-9a-f]{2}/gi,unescape);try{return decodeURIComponent(a)}catch(e){return a}},encode:function(e,n,t,i,r){if(0===e.length)return e;var s=e;if("symbol"==typeof e?s=Symbol.prototype.toString.call(e):"string"!=typeof e&&(s=String(e)),"iso-8859-1"===t)return escape(s).replace(/%u[0-9a-f]{4}/gi,(function(e){return"%26%23"+parseInt(e.slice(2),16)+"%3B"}));for(var l="",c=0;c<s.length;c+=1024){for(var d=s.length>=1024?s.slice(c,c+1024):s,h=[],m=0;m<d.length;++m){var u=d.charCodeAt(m);45===u||46===u||95===u||126===u||u>=48&&u<=57||u>=65&&u<=90||u>=97&&u<=122||r===a.RFC1738&&(40===u||41===u)?h[h.length]=d.charAt(m):u<128?h[h.length]=o[u]:u<2048?h[h.length]=o[192|u>>6]+o[128|63&u]:u<55296||u>=57344?h[h.length]=o[224|u>>12]+o[128|u>>6&63]+o[128|63&u]:(m+=1,u=65536+((1023&u)<<10|1023&d.charCodeAt(m)),h[h.length]=o[240|u>>18]+o[128|u>>12&63]+o[128|u>>6&63]+o[128|63&u])}l+=h.join("")}return l},isBuffer:function(e){return!(!e||"object"!=typeof e)&&!!(e.constructor&&e.constructor.isBuffer&&e.constructor.isBuffer(e))},isRegExp:function(e){return"[object RegExp]"===Object.prototype.toString.call(e)},maybeMap:function(e,n){if(r(e)){for(var t=[],a=0;a<e.length;a+=1)t.push(n(e[a]));return t}return n(e)},merge:function e(n,t,a){if(!t)return n;if("object"!=typeof t&&"function"!=typeof t){if(r(n))n.push(t);else{if(!n||"object"!=typeof n)return[n,t];(a&&(a.plainObjects||a.allowPrototypes)||!i.call(Object.prototype,t))&&(n[t]=!0)}return n}if(!n||"object"!=typeof n)return[n].concat(t);var o=n;return r(n)&&!r(t)&&(o=s(n,a)),r(n)&&r(t)?(t.forEach((function(t,r){if(i.call(n,r)){var o=n[r];o&&"object"==typeof o&&t&&"object"==typeof t?n[r]=e(o,t,a):n.push(t)}else n[r]=t})),n):Object.keys(t).reduce((function(n,r){var o=t[r];return i.call(n,r)?n[r]=e(n[r],o,a):n[r]=o,n}),o)}}},function(e,n){e.exports=function(e){return e instanceof Date}},function(e,n,t){var a=t(20),i=t(73);e.exports=function(e){var n=a(e),t=n.getFullYear(),r=new Date(0);r.setFullYear(t+1,0,4),r.setHours(0,0,0,0);var o=i(r),s=new Date(0);s.setFullYear(t,0,4),s.setHours(0,0,0,0);var l=i(s);return n.getTime()>=o.getTime()?t+1:n.getTime()>=l.getTime()?t:t-1}},function(e,n,t){"use strict";e.exports=function(e,n){return function(){for(var t=new Array(arguments.length),a=0;a<t.length;a++)t[a]=arguments[a];return e.apply(n,t)}}},function(e,n,t){"use strict";var a=t(4);function i(e){return encodeURIComponent(e).replace(/%3A/gi,":").replace(/%24/g,"$").replace(/%2C/gi,",").replace(/%20/g,"+").replace(/%5B/gi,"[").replace(/%5D/gi,"]")}e.exports=function(e,n,t){if(!n)return e;var r;if(t)r=t(n);else if(a.isURLSearchParams(n))r=n.toString();else{var o=[];a.forEach(n,(function(e,n){null!=e&&(a.isArray(e)?n+="[]":e=[e],a.forEach(e,(function(e){a.isDate(e)?e=e.toISOString():a.isObject(e)&&(e=JSON.stringify(e)),o.push(i(n)+"="+i(e))})))})),r=o.join("&")}if(r){var s=e.indexOf("#");-1!==s&&(e=e.slice(0,s)),e+=(-1===e.indexOf("?")?"?":"&")+r}return e}},function(e,n,t){"use strict";e.exports=function(e,n,t,a,i){return e.config=n,t&&(e.code=t),e.request=a,e.response=i,e.isAxiosError=!0,e.toJSON=function(){return{message:this.message,name:this.name,description:this.description,number:this.number,fileName:this.fileName,lineNumber:this.lineNumber,columnNumber:this.columnNumber,stack:this.stack,config:this.config,code:this.code}},e}},function(e,n,t){"use strict";var a=t(4),i=t(357),r=t(358),o=t(138),s=t(359),l=t(362),c=t(363),d=t(141);e.exports=function(e){return new Promise((function(n,t){var h=e.data,m=e.headers,u=e.responseType;a.isFormData(h)&&delete m["Content-Type"];var p=new XMLHttpRequest;if(e.auth){var g=e.auth.username||"",f=e.auth.password?unescape(encodeURIComponent(e.auth.password)):"";m.Authorization="Basic "+btoa(g+":"+f)}var y=s(e.baseURL,e.url);function b(){if(p){var a="getAllResponseHeaders"in p?l(p.getAllResponseHeaders()):null,r={data:u&&"text"!==u&&"json"!==u?p.response:p.responseText,status:p.status,statusText:p.statusText,headers:a,config:e,request:p};i(n,t,r),p=null}}if(p.open(e.method.toUpperCase(),o(y,e.params,e.paramsSerializer),!0),p.timeout=e.timeout,"onloadend"in p?p.onloadend=b:p.onreadystatechange=function(){p&&4===p.readyState&&(0!==p.status||p.responseURL&&0===p.responseURL.indexOf("file:"))&&setTimeout(b)},p.onabort=function(){p&&(t(d("Request aborted",e,"ECONNABORTED",p)),p=null)},p.onerror=function(){t(d("Network Error",e,null,p)),p=null},p.ontimeout=function(){var n="timeout of "+e.timeout+"ms exceeded";e.timeoutErrorMessage&&(n=e.timeoutErrorMessage),t(d(n,e,e.transitional&&e.transitional.clarifyTimeoutError?"ETIMEDOUT":"ECONNABORTED",p)),p=null},a.isStandardBrowserEnv()){var v=(e.withCredentials||c(y))&&e.xsrfCookieName?r.read(e.xsrfCookieName):void 0;v&&(m[e.xsrfHeaderName]=v)}"setRequestHeader"in p&&a.forEach(m,(function(e,n){void 0===h&&"content-type"===n.toLowerCase()?delete m[n]:p.setRequestHeader(n,e)})),a.isUndefined(e.withCredentials)||(p.withCredentials=!!e.withCredentials),u&&"json"!==u&&(p.responseType=e.responseType),"function"==typeof e.onDownloadProgress&&p.addEventListener("progress",e.onDownloadProgress),"function"==typeof e.onUploadProgress&&p.upload&&p.upload.addEventListener("progress",e.onUploadProgress),e.cancelToken&&e.cancelToken.promise.then((function(e){p&&(p.abort(),t(e),p=null)})),h||(h=null),p.send(h)}))}},function(e,n,t){"use strict";var a=t(139);e.exports=function(e,n,t,i,r){var o=new Error(e);return a(o,n,t,i,r)}},function(e,n,t){"use strict";e.exports=function(e){return!(!e||!e.__CANCEL__)}},function(e,n,t){"use strict";var a=t(4);e.exports=function(e,n){n=n||{};var t={},i=["url","method","data"],r=["headers","auth","proxy","params"],o=["baseURL","transformRequest","transformResponse","paramsSerializer","timeout","timeoutMessage","withCredentials","adapter","responseType","xsrfCookieName","xsrfHeaderName","onUploadProgress","onDownloadProgress","decompress","maxContentLength","maxBodyLength","maxRedirects","transport","httpAgent","httpsAgent","cancelToken","socketPath","responseEncoding"],s=["validateStatus"];function l(e,n){return a.isPlainObject(e)&&a.isPlainObject(n)?a.merge(e,n):a.isPlainObject(n)?a.merge({},n):a.isArray(n)?n.slice():n}function c(i){a.isUndefined(n[i])?a.isUndefined(e[i])||(t[i]=l(void 0,e[i])):t[i]=l(e[i],n[i])}a.forEach(i,(function(e){a.isUndefined(n[e])||(t[e]=l(void 0,n[e]))})),a.forEach(r,c),a.forEach(o,(function(i){a.isUndefined(n[i])?a.isUndefined(e[i])||(t[i]=l(void 0,e[i])):t[i]=l(void 0,n[i])})),a.forEach(s,(function(a){a in n?t[a]=l(e[a],n[a]):a in e&&(t[a]=l(void 0,e[a]))}));var d=i.concat(r).concat(o).concat(s),h=Object.keys(e).concat(Object.keys(n)).filter((function(e){return-1===d.indexOf(e)}));return a.forEach(h,c),t}},function(e,n,t){"use strict";function a(e){this.message=e}a.prototype.toString=function(){return"Cancel"+(this.message?": "+this.message:"")},a.prototype.__CANCEL__=!0,e.exports=a},function(e,n,t){},function(e,n,t){},function(e,n,t){var a=t(208),i=t(213),r=t(284),o=t(292),s=t(301),l=t(157),c=r((function(e){var n=l(e);return s(n)&&(n=void 0),o(a(e,1,s,!0),i(n,2))}));e.exports=c},function(e,n,t){var a=t(338),i=t(343),r=t(136),o=t(20),s=t(346),l=t(347);var c={M:function(e){return e.getMonth()+1},MM:function(e){return m(e.getMonth()+1,2)},Q:function(e){return Math.ceil((e.getMonth()+1)/3)},D:function(e){return e.getDate()},DD:function(e){return m(e.getDate(),2)},DDD:function(e){return a(e)},DDDD:function(e){return m(a(e),3)},d:function(e){return e.getDay()},E:function(e){return e.getDay()||7},W:function(e){return i(e)},WW:function(e){return m(i(e),2)},YY:function(e){return m(e.getFullYear(),4).substr(2)},YYYY:function(e){return m(e.getFullYear(),4)},GG:function(e){return String(r(e)).substr(2)},GGGG:function(e){return r(e)},H:function(e){return e.getHours()},HH:function(e){return m(e.getHours(),2)},h:function(e){var n=e.getHours();return 0===n?12:n>12?n%12:n},hh:function(e){return m(c.h(e),2)},m:function(e){return e.getMinutes()},mm:function(e){return m(e.getMinutes(),2)},s:function(e){return e.getSeconds()},ss:function(e){return m(e.getSeconds(),2)},S:function(e){return Math.floor(e.getMilliseconds()/100)},SS:function(e){return m(Math.floor(e.getMilliseconds()/10),2)},SSS:function(e){return m(e.getMilliseconds(),3)},Z:function(e){return h(e.getTimezoneOffset(),":")},ZZ:function(e){return h(e.getTimezoneOffset())},X:function(e){return Math.floor(e.getTime()/1e3)},x:function(e){return e.getTime()}};function d(e){return e.match(/\[[\s\S]/)?e.replace(/^\[|]$/g,""):e.replace(/\\/g,"")}function h(e,n){n=n||"";var t=e>0?"-":"+",a=Math.abs(e),i=a%60;return t+m(Math.floor(a/60),2)+n+m(i,2)}function m(e,n){for(var t=Math.abs(e).toString();t.length<n;)t="0"+t;return t}e.exports=function(e,n,t){var a=n?String(n):"YYYY-MM-DDTHH:mm:ss.SSSZ",i=(t||{}).locale,r=l.format.formatters,h=l.format.formattingTokensRegExp;i&&i.format&&i.format.formatters&&(r=i.format.formatters,i.format.formattingTokensRegExp&&(h=i.format.formattingTokensRegExp));var m=o(e);return s(m)?function(e,n,t){var a,i,r=e.match(t),o=r.length;for(a=0;a<o;a++)i=n[r[a]]||c[r[a]],r[a]=i||d(r[a]);return function(e){for(var n="",t=0;t<o;t++)r[t]instanceof Function?n+=r[t](e,c):n+=r[t];return n}}(a,r,h)(m):"Invalid Date"}},function(e,n,t){e.exports=t(351)},function(e,n,t){"use strict";
/**
 * @file Embedded JavaScript templating engine. {@link http://ejs.co}
 * @author Matthew Eernisse <mde@fleegix.org>
 * @author Tiancheng "Timothy" Gu <timothygu99@gmail.com>
 * @project EJS
 * @license {@link http://www.apache.org/licenses/LICENSE-2.0 Apache License, Version 2.0}
 */var a=t(369),i=t(370),r=t(371),o=!1,s=t(372).version,l=["delimiter","scope","context","debug","compileDebug","client","_with","rmWhitespace","strict","filename","async"],c=l.concat("cache"),d=/^\uFEFF/,h=/^[a-zA-Z_$][0-9a-zA-Z_$]*$/;function m(e,t){var i;if(t.some((function(t){return i=n.resolveInclude(e,t,!0),a.existsSync(i)})))return i}function u(e,t){var a,i=e.filename,r=arguments.length>1;if(e.cache){if(!i)throw new Error("cache option requires a filename");if(a=n.cache.get(i))return a;r||(t=g(i).toString().replace(d,""))}else if(!r){if(!i)throw new Error("Internal EJS error: no file name or template provided");t=g(i).toString().replace(d,"")}return a=n.compile(t,e),e.cache&&n.cache.set(i,a),a}function p(e,t,a){var i;if(!a){if("function"==typeof n.promiseImpl)return new n.promiseImpl((function(n,a){try{n(i=u(e)(t))}catch(e){a(e)}}));throw new Error("Please provide a callback function")}try{i=u(e)(t)}catch(e){return a(e)}a(null,i)}function g(e){return n.fileLoader(e)}function f(e,t){var i=r.shallowCopy(r.createNullProtoObjWherePossible(),t);if(i.filename=function(e,t){var i,r,o=t.views,s=/^[A-Za-z]+:\\|^\//.exec(e);if(s&&s.length)e=e.replace(/^\/*/,""),i=Array.isArray(t.root)?m(e,t.root):n.resolveInclude(e,t.root||"/",!0);else if(t.filename&&(r=n.resolveInclude(e,t.filename),a.existsSync(r)&&(i=r)),!i&&Array.isArray(o)&&(i=m(e,o)),!i&&"function"!=typeof t.includer)throw new Error('Could not find the include file "'+t.escapeFunction(e)+'"');return i}(e,i),"function"==typeof t.includer){var o=t.includer(e,i.filename);if(o&&(o.filename&&(i.filename=o.filename),o.template))return u(i,o.template)}return u(i)}function y(e,n,t,a,i){var r=n.split("\n"),o=Math.max(a-3,0),s=Math.min(r.length,a+3),l=i(t),c=r.slice(o,s).map((function(e,n){var t=n+o+1;return(t==a?" >> ":"    ")+t+"| "+e})).join("\n");throw e.path=l,e.message=(l||"ejs")+":"+a+"\n"+c+"\n\n"+e.message,e}function b(e){return e.replace(/;(\s*$)/,"$1")}function v(e,t){var a=r.hasOwnOnlyObject(t),i=r.createNullProtoObjWherePossible();this.templateText=e,this.mode=null,this.truncate=!1,this.currentLine=1,this.source="",i.client=a.client||!1,i.escapeFunction=a.escape||a.escapeFunction||r.escapeXML,i.compileDebug=!1!==a.compileDebug,i.debug=!!a.debug,i.filename=a.filename,i.openDelimiter=a.openDelimiter||n.openDelimiter||"<",i.closeDelimiter=a.closeDelimiter||n.closeDelimiter||">",i.delimiter=a.delimiter||n.delimiter||"%",i.strict=a.strict||!1,i.context=a.context,i.cache=a.cache||!1,i.rmWhitespace=a.rmWhitespace,i.root=a.root,i.includer=a.includer,i.outputFunctionName=a.outputFunctionName,i.localsName=a.localsName||n.localsName||"locals",i.views=a.views,i.async=a.async,i.destructuredLocals=a.destructuredLocals,i.legacyInclude=void 0===a.legacyInclude||!!a.legacyInclude,i.strict?i._with=!1:i._with=void 0===a._with||a._with,this.opts=i,this.regex=this.createRegex()}n.cache=r.cache,n.fileLoader=a.readFileSync,n.localsName="locals",n.promiseImpl=new Function("return this;")().Promise,n.resolveInclude=function(e,n,t){var a=i.dirname,r=i.extname,o=(0,i.resolve)(t?n:a(n),e);return r(e)||(o+=".ejs"),o},n.compile=function(e,n){return n&&n.scope&&(o||(console.warn("`scope` option is deprecated and will be removed in EJS 3"),o=!0),n.context||(n.context=n.scope),delete n.scope),new v(e,n).compile()},n.render=function(e,n,t){var a=n||r.createNullProtoObjWherePossible(),i=t||r.createNullProtoObjWherePossible();return 2==arguments.length&&r.shallowCopyFromList(i,a,l),u(i,e)(a)},n.renderFile=function(){var e,n,t,a=Array.prototype.slice.call(arguments),i=a.shift(),o={filename:i};return"function"==typeof arguments[arguments.length-1]&&(e=a.pop()),a.length?(n=a.shift(),a.length?r.shallowCopy(o,a.pop()):(n.settings&&(n.settings.views&&(o.views=n.settings.views),n.settings["view cache"]&&(o.cache=!0),(t=n.settings["view options"])&&r.shallowCopy(o,t)),r.shallowCopyFromList(o,n,c)),o.filename=i):n=r.createNullProtoObjWherePossible(),p(o,n,e)},n.Template=v,n.clearCache=function(){n.cache.reset()},v.modes={EVAL:"eval",ESCAPED:"escaped",RAW:"raw",COMMENT:"comment",LITERAL:"literal"},v.prototype={createRegex:function(){var e="(<%%|%%>|<%=|<%-|<%_|<%#|<%|%>|-%>|_%>)",n=r.escapeRegExpChars(this.opts.delimiter),t=r.escapeRegExpChars(this.opts.openDelimiter),a=r.escapeRegExpChars(this.opts.closeDelimiter);return e=e.replace(/%/g,n).replace(/</g,t).replace(/>/g,a),new RegExp(e)},compile:function(){var e,n,t,a=this.opts,o="",s="",l=a.escapeFunction,c=a.filename?JSON.stringify(a.filename):"undefined";if(!this.source){if(this.generateSource(),o+='  var __output = "";\n  function __append(s) { if (s !== undefined && s !== null) __output += s }\n',a.outputFunctionName){if(!h.test(a.outputFunctionName))throw new Error("outputFunctionName is not a valid JS identifier.");o+="  var "+a.outputFunctionName+" = __append;\n"}if(a.localsName&&!h.test(a.localsName))throw new Error("localsName is not a valid JS identifier.");if(a.destructuredLocals&&a.destructuredLocals.length){for(var d="  var __locals = ("+a.localsName+" || {}),\n",m=0;m<a.destructuredLocals.length;m++){var u=a.destructuredLocals[m];if(!h.test(u))throw new Error("destructuredLocals["+m+"] is not a valid JS identifier.");m>0&&(d+=",\n  "),d+=u+" = __locals."+u}o+=d+";\n"}!1!==a._with&&(o+="  with ("+a.localsName+" || {}) {\n",s+="  }\n"),s+="  return __output;\n",this.source=o+this.source+s}e=a.compileDebug?"var __line = 1\n  , __lines = "+JSON.stringify(this.templateText)+"\n  , __filename = "+c+";\ntry {\n"+this.source+"} catch (e) {\n  rethrow(e, __lines, __filename, __line, escapeFn);\n}\n":this.source,a.client&&(e="escapeFn = escapeFn || "+l.toString()+";\n"+e,a.compileDebug&&(e="rethrow = rethrow || "+y.toString()+";\n"+e)),a.strict&&(e='"use strict";\n'+e),a.debug&&console.log(e),a.compileDebug&&a.filename&&(e=e+"\n//# sourceURL="+c+"\n");try{if(a.async)try{t=new Function("return (async function(){}).constructor;")()}catch(e){throw e instanceof SyntaxError?new Error("This environment does not support async/await"):e}else t=Function;n=new t(a.localsName+", escapeFn, include, rethrow",e)}catch(e){throw e instanceof SyntaxError&&(a.filename&&(e.message+=" in "+a.filename),e.message+=" while compiling ejs\n\n",e.message+="If the above error is not helpful, you may want to try EJS-Lint:\n",e.message+="https://github.com/RyanZim/EJS-Lint",a.async||(e.message+="\n",e.message+="Or, if you meant to create an async function, pass `async: true` as an option.")),e}var p=a.client?n:function(e){return n.apply(a.context,[e||r.createNullProtoObjWherePossible(),l,function(n,t){var i=r.shallowCopy(r.createNullProtoObjWherePossible(),e);return t&&(i=r.shallowCopy(i,t)),f(n,a)(i)},y])};if(a.filename&&"function"==typeof Object.defineProperty){var g=a.filename,b=i.basename(g,i.extname(g));try{Object.defineProperty(p,"name",{value:b,writable:!1,enumerable:!1,configurable:!0})}catch(e){}}return p},generateSource:function(){this.opts.rmWhitespace&&(this.templateText=this.templateText.replace(/[\r\n]+/g,"\n").replace(/^\s+|\s+$/gm,"")),this.templateText=this.templateText.replace(/[ \t]*<%_/gm,"<%_").replace(/_%>[ \t]*/gm,"_%>");var e=this,n=this.parseTemplateText(),t=this.opts.delimiter,a=this.opts.openDelimiter,i=this.opts.closeDelimiter;n&&n.length&&n.forEach((function(r,o){var s;if(0===r.indexOf(a+t)&&0!==r.indexOf(a+t+t)&&(s=n[o+2])!=t+i&&s!="-"+t+i&&s!="_"+t+i)throw new Error('Could not find matching close tag for "'+r+'".');e.scanLine(r)}))},parseTemplateText:function(){for(var e,n=this.templateText,t=this.regex,a=t.exec(n),i=[];a;)0!==(e=a.index)&&(i.push(n.substring(0,e)),n=n.slice(e)),i.push(a[0]),n=n.slice(a[0].length),a=t.exec(n);return n&&i.push(n),i},_addOutput:function(e){if(this.truncate&&(e=e.replace(/^(?:\r\n|\r|\n)/,""),this.truncate=!1),!e)return e;e=(e=(e=(e=e.replace(/\\/g,"\\\\")).replace(/\n/g,"\\n")).replace(/\r/g,"\\r")).replace(/"/g,'\\"'),this.source+='    ; __append("'+e+'")\n'},scanLine:function(e){var n,t=this.opts.delimiter,a=this.opts.openDelimiter,i=this.opts.closeDelimiter;switch(n=e.split("\n").length-1,e){case a+t:case a+t+"_":this.mode=v.modes.EVAL;break;case a+t+"=":this.mode=v.modes.ESCAPED;break;case a+t+"-":this.mode=v.modes.RAW;break;case a+t+"#":this.mode=v.modes.COMMENT;break;case a+t+t:this.mode=v.modes.LITERAL,this.source+='    ; __append("'+e.replace(a+t+t,a+t)+'")\n';break;case t+t+i:this.mode=v.modes.LITERAL,this.source+='    ; __append("'+e.replace(t+t+i,t+i)+'")\n';break;case t+i:case"-"+t+i:case"_"+t+i:this.mode==v.modes.LITERAL&&this._addOutput(e),this.mode=null,this.truncate=0===e.indexOf("-")||0===e.indexOf("_");break;default:if(this.mode){switch(this.mode){case v.modes.EVAL:case v.modes.ESCAPED:case v.modes.RAW:e.lastIndexOf("//")>e.lastIndexOf("\n")&&(e+="\n")}switch(this.mode){case v.modes.EVAL:this.source+="    ; "+e+"\n";break;case v.modes.ESCAPED:this.source+="    ; __append(escapeFn("+b(e)+"))\n";break;case v.modes.RAW:this.source+="    ; __append("+b(e)+")\n";break;case v.modes.COMMENT:break;case v.modes.LITERAL:this._addOutput(e)}}else this._addOutput(e)}this.opts.compileDebug&&n&&(this.currentLine+=n,this.source+="    ; __line = "+this.currentLine+"\n")}},n.escapeXML=r.escapeXML,n.__express=n.renderFile,n.VERSION=s,n.name="ejs","undefined"!=typeof window&&(window.ejs=n)},function(e,n,t){"use strict";t.r(n);var a={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},i=(t(304),t(8)),r=Object(i.a)(a,(function(){return(0,this._self._c)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);n.default=r.exports},function(e,n,t){"use strict";t.r(n);t(5),t(13),t(23),t(22);var a={name:"CodeGroup",data:()=>({codeTabs:[],activeCodeTabIndex:-1}),watch:{activeCodeTabIndex(e){this.codeTabs.forEach(e=>{e.elm.classList.remove("theme-code-block__active")}),this.codeTabs[e].elm.classList.add("theme-code-block__active")}},mounted(){this.codeTabs=(this.$slots.default||[]).filter(e=>Boolean(e.componentOptions)).map((e,n)=>(""===e.componentOptions.propsData.active&&(this.activeCodeTabIndex=n),{title:e.componentOptions.propsData.title,elm:e.elm})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab(e){this.activeCodeTabIndex=e}}},i=(t(305),t(8)),r=Object(i.a)(a,(function(){var e=this,n=e._self._c;return n("div",{staticClass:"theme-code-group"},[n("div",{staticClass:"theme-code-group__nav"},[n("ul",{staticClass:"theme-code-group__ul"},e._l(e.codeTabs,(function(t,a){return n("li",{key:t.title,staticClass:"theme-code-group__li"},[n("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":a===e.activeCodeTabIndex},on:{click:function(n){return e.changeCodeTab(a)}}},[e._v("\n            "+e._s(t.title)+"\n          ")])])})),0)]),e._v(" "),e._t("default"),e._v(" "),e.codeTabs.length<1?n("pre",{staticClass:"pre-blank"},[e._v("// Make sure to add code blocks to your code group")]):e._e()],2)}),[],!1,null,"2f5f1757",null);n.default=r.exports},function(e,n,t){"use strict";var a=t(12),i=t(51),r=t(9),o=t(1),s=t(29);a({target:"Iterator",proto:!0,real:!0},{some:function(e){o(this),r(e);var n=s(this),t=0;return i(n,(function(n,a){if(e(n,t++))return a()}),{IS_RECORD:!0,INTERRUPTED:!0}).stopped}})},function(e,n,t){"use strict";var a=t(102),i=String;e.exports=function(e){if("Symbol"===a(e))throw new TypeError("Cannot convert a Symbol value to a string");return i(e)}},function(e,n,t){"use strict";var a=t(91),i=t(18);e.exports=function(e,n,t){return t.get&&a(t.get,n,{getter:!0}),t.set&&a(t.set,n,{setter:!0}),i.f(e,n,t)}},function(e,n,t){"use strict";var a=t(187),i=t(9),r=t(35),o=a(a.bind);e.exports=function(e,n){return i(e),void 0===n?e:r?o(e,n):function(){return e.apply(n,arguments)}}},function(e,n){e.exports=function(e){var n=null==e?0:e.length;return n?e[n-1]:void 0}},function(e,n,t){e.exports=t(376)},function(e,n,t){"use strict";var a={}.propertyIsEnumerable,i=Object.getOwnPropertyDescriptor,r=i&&!a.call({1:2},1);n.f=r?function(e){var n=i(this,e);return!!n&&n.enumerable}:a},function(e,n,t){"use strict";var a=t(15),i=t(10),r=t(81),o=t(40),s=t(161),l=t(17),c=TypeError,d=l("toPrimitive");e.exports=function(e,n){if(!i(e)||r(e))return e;var t,l=o(e,d);if(l){if(void 0===n&&(n="default"),t=a(l,e,n),!i(t)||r(t))return t;throw new c("Can't convert object to primitive value")}return void 0===n&&(n="number"),s(e,n)}},function(e,n,t){"use strict";var a=t(15),i=t(0),r=t(10),o=TypeError;e.exports=function(e,n){var t,s;if("string"===n&&i(t=e.toString)&&!r(s=a(t,e)))return s;if(i(t=e.valueOf)&&!r(s=a(t,e)))return s;if("string"!==n&&i(t=e.toString)&&!r(s=a(t,e)))return s;throw new o("Can't convert object to primitive value")}},function(e,n,t){"use strict";var a=t(7),i=t(11),r=Function.prototype,o=a&&Object.getOwnPropertyDescriptor,s=i(r,"name"),l=s&&"something"===function(){}.name,c=s&&(!a||a&&o(r,"name").configurable);e.exports={EXISTS:s,PROPER:l,CONFIGURABLE:c}},function(e,n,t){"use strict";var a=t(6),i=t(0),r=t(55),o=a(Function.toString);i(r.inspectSource)||(r.inspectSource=function(e){return o(e)}),e.exports=r.inspectSource},function(e,n,t){"use strict";var a=t(2),i=t(0),r=a.WeakMap;e.exports=i(r)&&/native code/.test(String(r))},function(e,n,t){"use strict";var a=t(38),i=t(6),r=t(166),o=t(171),s=t(1),l=i([].concat);e.exports=a("Reflect","ownKeys")||function(e){var n=r.f(s(e)),t=o.f;return t?l(n,t(e)):n}},function(e,n,t){"use strict";var a=t(94),i=t(59).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(e){return a(e,i)}},function(e,n,t){"use strict";var a=t(37),i=t(168),r=t(34),o=function(e){return function(n,t,o){var s=a(n),l=r(s);if(0===l)return!e&&-1;var c,d=i(o,l);if(e&&t!=t){for(;l>d;)if((c=s[d++])!=c)return!0}else for(;l>d;d++)if((e||d in s)&&s[d]===t)return e||d||0;return!e&&-1}};e.exports={includes:o(!0),indexOf:o(!1)}},function(e,n,t){"use strict";var a=t(95),i=Math.max,r=Math.min;e.exports=function(e,n){var t=a(e);return t<0?i(t+n,0):r(t,n)}},function(e,n,t){"use strict";var a=Math.ceil,i=Math.floor;e.exports=Math.trunc||function(e){var n=+e;return(n>0?i:a)(n)}},function(e,n,t){"use strict";var a=t(95),i=Math.min;e.exports=function(e){var n=a(e);return n>0?i(n,9007199254740991):0}},function(e,n,t){"use strict";n.f=Object.getOwnPropertySymbols},function(e,n,t){"use strict";var a=t(3),i=t(0),r=/#|\.prototype\./,o=function(e,n){var t=l[s(e)];return t===d||t!==c&&(i(n)?a(n):!!n)},s=o.normalize=function(e){return String(e).replace(r,".").toLowerCase()},l=o.data={},c=o.NATIVE="N",d=o.POLYFILL="P";e.exports=o},function(e,n,t){"use strict";var a=t(39),i=TypeError;e.exports=function(e,n){if(a(n,e))return e;throw new i("Incorrect invocation")}},function(e,n,t){"use strict";var a=t(3);e.exports=!a((function(){function e(){}return e.prototype.constructor=null,Object.getPrototypeOf(new e)!==e.prototype}))},function(e,n,t){"use strict";var a=t(7),i=t(18),r=t(36);e.exports=function(e,n,t){a?i.f(e,n,r(0,t)):e[n]=t}},function(e,n,t){"use strict";var a=t(7),i=t(90),r=t(18),o=t(1),s=t(37),l=t(177);n.f=a&&!i?Object.defineProperties:function(e,n){o(e);for(var t,a=s(n),i=l(n),c=i.length,d=0;c>d;)r.f(e,t=i[d++],a[t]);return e}},function(e,n,t){"use strict";var a=t(94),i=t(59);e.exports=Object.keys||function(e){return a(e,i)}},function(e,n,t){"use strict";var a=t(38);e.exports=a("document","documentElement")},function(e,n,t){"use strict";var a=t(50);e.exports=function(e,n,t){for(var i in n)a(e,i,n[i],t);return e}},function(e,n,t){"use strict";e.exports=function(e,n){return{value:e,done:n}}},function(e,n,t){"use strict";var a=t(15),i=t(9),r=t(1),o=t(29),s=t(98),l=t(99),c=s((function(){var e=this.iterator,n=r(a(this.next,e));if(!(this.done=!!n.done))return l(e,this.mapper,[n.value,this.counter++],!0)}));e.exports=function(e){return r(this),i(e),new c(o(this),{mapper:e})}},function(e,n,t){"use strict";var a=t(12),i=t(183).left,r=t(184),o=t(84);a({target:"Array",proto:!0,forced:!t(185)&&o>79&&o<83||!r("reduce")},{reduce:function(e){var n=arguments.length;return i(this,e,n,n>1?arguments[1]:void 0)}})},function(e,n,t){"use strict";var a=t(9),i=t(33),r=t(75),o=t(34),s=TypeError,l="Reduce of empty array with no initial value",c=function(e){return function(n,t,c,d){var h=i(n),m=r(h),u=o(h);if(a(t),0===u&&c<2)throw new s(l);var p=e?u-1:0,g=e?-1:1;if(c<2)for(;;){if(p in m){d=m[p],p+=g;break}if(p+=g,e?p<0:u<=p)throw new s(l)}for(;e?p>=0:u>p;p+=g)p in m&&(d=t(d,m[p],p,h));return d}};e.exports={left:c(!1),right:c(!0)}},function(e,n,t){"use strict";var a=t(3);e.exports=function(e,n){var t=[][e];return!!t&&a((function(){t.call(null,n||function(){return 1},1)}))}},function(e,n,t){"use strict";var a=t(186);e.exports="NODE"===a},function(e,n,t){"use strict";var a=t(2),i=t(85),r=t(31),o=function(e){return i.slice(0,e.length)===e};e.exports=o("Bun/")?"BUN":o("Cloudflare-Workers")?"CLOUDFLARE":o("Deno/")?"DENO":o("Node.js/")?"NODE":a.Bun&&"string"==typeof Bun.version?"BUN":a.Deno&&"object"==typeof Deno.version?"DENO":"process"===r(a.process)?"NODE":a.window&&a.document?"BROWSER":"REST"},function(e,n,t){"use strict";var a=t(31),i=t(6);e.exports=function(e){if("Function"===a(e))return i(e)}},function(e,n,t){"use strict";var a=t(17),i=t(100),r=a("iterator"),o=Array.prototype;e.exports=function(e){return void 0!==e&&(i.Array===e||o[r]===e)}},function(e,n,t){"use strict";var a=t(15),i=t(9),r=t(1),o=t(54),s=t(101),l=TypeError;e.exports=function(e,n){var t=arguments.length<2?s(e):n;if(i(t))return r(a(t,e));throw new l(o(e)+" is not iterable")}},function(e,n,t){"use strict";var a={};a[t(17)("toStringTag")]="z",e.exports="[object z]"===String(a)},function(e,n,t){"use strict";var a=t(12),i=t(51),r=t(9),o=t(1),s=t(29),l=TypeError;a({target:"Iterator",proto:!0,real:!0},{reduce:function(e){o(this),r(e);var n=s(this),t=arguments.length<2,a=t?void 0:arguments[1],c=0;if(i(n,(function(n){t?(t=!1,a=n):a=e(a,n,c),c++}),{IS_RECORD:!0}),t)throw new l("Reduce of empty iterator with no initial value");return a}})},function(e,n,t){"use strict";var a=t(12),i=t(2),r=t(193),o=t(194),s=i.WebAssembly,l=7!==new Error("e",{cause:7}).cause,c=function(e,n){var t={};t[e]=o(e,n,l),a({global:!0,constructor:!0,arity:1,forced:l},t)},d=function(e,n){if(s&&s[e]){var t={};t[e]=o("WebAssembly."+e,n,l),a({target:"WebAssembly",stat:!0,constructor:!0,arity:1,forced:l},t)}};c("Error",(function(e){return function(n){return r(e,this,arguments)}})),c("EvalError",(function(e){return function(n){return r(e,this,arguments)}})),c("RangeError",(function(e){return function(n){return r(e,this,arguments)}})),c("ReferenceError",(function(e){return function(n){return r(e,this,arguments)}})),c("SyntaxError",(function(e){return function(n){return r(e,this,arguments)}})),c("TypeError",(function(e){return function(n){return r(e,this,arguments)}})),c("URIError",(function(e){return function(n){return r(e,this,arguments)}})),d("CompileError",(function(e){return function(n){return r(e,this,arguments)}})),d("LinkError",(function(e){return function(n){return r(e,this,arguments)}})),d("RuntimeError",(function(e){return function(n){return r(e,this,arguments)}}))},function(e,n,t){"use strict";var a=t(35),i=Function.prototype,r=i.apply,o=i.call;e.exports="object"==typeof Reflect&&Reflect.apply||(a?o.bind(r):function(){return o.apply(r,arguments)})},function(e,n,t){"use strict";var a=t(38),i=t(11),r=t(26),o=t(39),s=t(103),l=t(93),c=t(198),d=t(199),h=t(200),m=t(201),u=t(202),p=t(7),g=t(25);e.exports=function(e,n,t,f){var y=f?2:1,b=e.split("."),v=b[b.length-1],w=a.apply(null,b);if(w){var _=w.prototype;if(!g&&i(_,"cause")&&delete _.cause,!t)return w;var k=a("Error"),x=n((function(e,n){var t=h(f?n:e,void 0),a=f?new w(e):new w;return void 0!==t&&r(a,"message",t),u(a,x,a.stack,2),this&&o(_,this)&&d(a,this,x),arguments.length>y&&m(a,arguments[y]),a}));if(x.prototype=_,"Error"!==v?s?s(x,k):l(x,k,{name:!0}):p&&"stackTraceLimit"in w&&(c(x,w,"stackTraceLimit"),c(x,w,"prepareStackTrace")),l(x,w),!g)try{_.name!==v&&r(_,"name",v),_.constructor=x}catch(e){}return x}}},function(e,n,t){"use strict";var a=t(6),i=t(9);e.exports=function(e,n,t){try{return a(i(Object.getOwnPropertyDescriptor(e,n)[t]))}catch(e){}}},function(e,n,t){"use strict";var a=t(197),i=String,r=TypeError;e.exports=function(e){if(a(e))return e;throw new r("Can't set "+i(e)+" as a prototype")}},function(e,n,t){"use strict";var a=t(10);e.exports=function(e){return a(e)||null===e}},function(e,n,t){"use strict";var a=t(18).f;e.exports=function(e,n,t){t in e||a(e,t,{configurable:!0,get:function(){return n[t]},set:function(e){n[t]=e}})}},function(e,n,t){"use strict";var a=t(0),i=t(10),r=t(103);e.exports=function(e,n,t){var o,s;return r&&a(o=n.constructor)&&o!==t&&i(s=o.prototype)&&s!==t.prototype&&r(e,s),e}},function(e,n,t){"use strict";var a=t(154);e.exports=function(e,n){return void 0===e?arguments.length<2?"":n:a(e)}},function(e,n,t){"use strict";var a=t(10),i=t(26);e.exports=function(e,n){a(n)&&"cause"in n&&i(e,"cause",n.cause)}},function(e,n,t){"use strict";var a=t(26),i=t(203),r=t(204),o=Error.captureStackTrace;e.exports=function(e,n,t,s){r&&(o?o(e,n):a(e,"stack",i(t,s)))}},function(e,n,t){"use strict";var a=t(6),i=Error,r=a("".replace),o=String(new i("zxcasd").stack),s=/\n\s*at [^:]*:[^\n]*/,l=s.test(o);e.exports=function(e,n){if(l&&"string"==typeof e&&!i.prepareStackTrace)for(;n--;)e=r(e,s,"");return e}},function(e,n,t){"use strict";var a=t(3),i=t(36);e.exports=!a((function(){var e=new Error("a");return!("stack"in e)||(Object.defineProperty(e,"stack",i(1,7)),7!==e.stack)}))},function(e,n,t){"use strict";var a=t(7),i=t(206),r=TypeError,o=Object.getOwnPropertyDescriptor,s=a&&!function(){if(void 0!==this)return!0;try{Object.defineProperty([],"length",{writable:!1}).length=1}catch(e){return e instanceof TypeError}}();e.exports=s?function(e,n){if(i(e)&&!o(e,"length").writable)throw new r("Cannot set read only .length");return e.length=n}:function(e,n){return e.length=n}},function(e,n,t){"use strict";var a=t(31);e.exports=Array.isArray||function(e){return"Array"===a(e)}},function(e,n,t){"use strict";var a=TypeError;e.exports=function(e){if(e>9007199254740991)throw a("Maximum allowed index exceeded");return e}},function(e,n,t){var a=t(104),i=t(209);e.exports=function e(n,t,r,o,s){var l=-1,c=n.length;for(r||(r=i),s||(s=[]);++l<c;){var d=n[l];t>0&&r(d)?t>1?e(d,t-1,r,o,s):a(s,d):o||(s[s.length]=d)}return s}},function(e,n,t){var a=t(32),i=t(61),r=t(14),o=a?a.isConcatSpreadable:void 0;e.exports=function(e){return r(e)||i(e)||!!(o&&e&&e[o])}},function(e,n,t){var a=t(30),i=t(24);e.exports=function(e){return i(e)&&"[object Arguments]"==a(e)}},function(e,n,t){var a=t(32),i=Object.prototype,r=i.hasOwnProperty,o=i.toString,s=a?a.toStringTag:void 0;e.exports=function(e){var n=r.call(e,s),t=e[s];try{e[s]=void 0;var a=!0}catch(e){}var i=o.call(e);return a&&(n?e[s]=t:delete e[s]),i}},function(e,n){var t=Object.prototype.toString;e.exports=function(e){return t.call(e)}},function(e,n,t){var a=t(214),i=t(270),r=t(69),o=t(14),s=t(281);e.exports=function(e){return"function"==typeof e?e:null==e?r:"object"==typeof e?o(e)?i(e[0],e[1]):a(e):s(e)}},function(e,n,t){var a=t(215),i=t(269),r=t(121);e.exports=function(e){var n=i(e);return 1==n.length&&n[0][2]?r(n[0][0],n[0][1]):function(t){return t===e||a(t,e,n)}}},function(e,n,t){var a=t(106),i=t(110);e.exports=function(e,n,t,r){var o=t.length,s=o,l=!r;if(null==e)return!s;for(e=Object(e);o--;){var c=t[o];if(l&&c[2]?c[1]!==e[c[0]]:!(c[0]in e))return!1}for(;++o<s;){var d=(c=t[o])[0],h=e[d],m=c[1];if(l&&c[2]){if(void 0===h&&!(d in e))return!1}else{var u=new a;if(r)var p=r(h,m,d,e,n,u);if(!(void 0===p?i(m,h,3,r,u):p))return!1}}return!0}},function(e,n){e.exports=function(){this.__data__=[],this.size=0}},function(e,n,t){var a=t(42),i=Array.prototype.splice;e.exports=function(e){var n=this.__data__,t=a(n,e);return!(t<0)&&(t==n.length-1?n.pop():i.call(n,t,1),--this.size,!0)}},function(e,n,t){var a=t(42);e.exports=function(e){var n=this.__data__,t=a(n,e);return t<0?void 0:n[t][1]}},function(e,n,t){var a=t(42);e.exports=function(e){return a(this.__data__,e)>-1}},function(e,n,t){var a=t(42);e.exports=function(e,n){var t=this.__data__,i=a(t,e);return i<0?(++this.size,t.push([e,n])):t[i][1]=n,this}},function(e,n,t){var a=t(41);e.exports=function(){this.__data__=new a,this.size=0}},function(e,n){e.exports=function(e){var n=this.__data__,t=n.delete(e);return this.size=n.size,t}},function(e,n){e.exports=function(e){return this.__data__.get(e)}},function(e,n){e.exports=function(e){return this.__data__.has(e)}},function(e,n,t){var a=t(41),i=t(62),r=t(64);e.exports=function(e,n){var t=this.__data__;if(t instanceof a){var o=t.__data__;if(!i||o.length<199)return o.push([e,n]),this.size=++t.size,this;t=this.__data__=new r(o)}return t.set(e,n),this.size=t.size,this}},function(e,n,t){var a=t(108),i=t(227),r=t(63),o=t(109),s=/^\[object .+?Constructor\]$/,l=Function.prototype,c=Object.prototype,d=l.toString,h=c.hasOwnProperty,m=RegExp("^"+d.call(h).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");e.exports=function(e){return!(!r(e)||i(e))&&(a(e)?m:s).test(o(e))}},function(e,n,t){var a,i=t(228),r=(a=/[^.]+$/.exec(i&&i.keys&&i.keys.IE_PROTO||""))?"Symbol(src)_1."+a:"";e.exports=function(e){return!!r&&r in e}},function(e,n,t){var a=t(16)["__core-js_shared__"];e.exports=a},function(e,n){e.exports=function(e,n){return null==e?void 0:e[n]}},function(e,n,t){var a=t(231),i=t(41),r=t(62);e.exports=function(){this.size=0,this.__data__={hash:new a,map:new(r||i),string:new a}}},function(e,n,t){var a=t(232),i=t(233),r=t(234),o=t(235),s=t(236);function l(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var a=e[n];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=i,l.prototype.get=r,l.prototype.has=o,l.prototype.set=s,e.exports=l},function(e,n,t){var a=t(43);e.exports=function(){this.__data__=a?a(null):{},this.size=0}},function(e,n){e.exports=function(e){var n=this.has(e)&&delete this.__data__[e];return this.size-=n?1:0,n}},function(e,n,t){var a=t(43),i=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;if(a){var t=n[e];return"__lodash_hash_undefined__"===t?void 0:t}return i.call(n,e)?n[e]:void 0}},function(e,n,t){var a=t(43),i=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;return a?void 0!==n[e]:i.call(n,e)}},function(e,n,t){var a=t(43);e.exports=function(e,n){var t=this.__data__;return this.size+=this.has(e)?0:1,t[e]=a&&void 0===n?"__lodash_hash_undefined__":n,this}},function(e,n,t){var a=t(44);e.exports=function(e){var n=a(this,e).delete(e);return this.size-=n?1:0,n}},function(e,n){e.exports=function(e){var n=typeof e;return"string"==n||"number"==n||"symbol"==n||"boolean"==n?"__proto__"!==e:null===e}},function(e,n,t){var a=t(44);e.exports=function(e){return a(this,e).get(e)}},function(e,n,t){var a=t(44);e.exports=function(e){return a(this,e).has(e)}},function(e,n,t){var a=t(44);e.exports=function(e,n){var t=a(this,e),i=t.size;return t.set(e,n),this.size+=t.size==i?0:1,this}},function(e,n,t){var a=t(106),i=t(111),r=t(246),o=t(249),s=t(265),l=t(14),c=t(115),d=t(117),h="[object Object]",m=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,u,p,g){var f=l(e),y=l(n),b=f?"[object Array]":s(e),v=y?"[object Array]":s(n),w=(b="[object Arguments]"==b?h:b)==h,_=(v="[object Arguments]"==v?h:v)==h,k=b==v;if(k&&c(e)){if(!c(n))return!1;f=!0,w=!1}if(k&&!w)return g||(g=new a),f||d(e)?i(e,n,t,u,p,g):r(e,n,b,t,u,p,g);if(!(1&t)){var x=w&&m.call(e,"__wrapped__"),T=_&&m.call(n,"__wrapped__");if(x||T){var z=x?e.value():e,M=T?n.value():n;return g||(g=new a),p(z,M,t,u,g)}}return!!k&&(g||(g=new a),o(e,n,t,u,p,g))}},function(e,n){e.exports=function(e){return this.__data__.set(e,"__lodash_hash_undefined__"),this}},function(e,n){e.exports=function(e){return this.__data__.has(e)}},function(e,n){e.exports=function(e,n){for(var t=-1,a=null==e?0:e.length;++t<a;)if(n(e[t],t,e))return!0;return!1}},function(e,n,t){var a=t(32),i=t(247),r=t(107),o=t(111),s=t(248),l=t(65),c=a?a.prototype:void 0,d=c?c.valueOf:void 0;e.exports=function(e,n,t,a,c,h,m){switch(t){case"[object DataView]":if(e.byteLength!=n.byteLength||e.byteOffset!=n.byteOffset)return!1;e=e.buffer,n=n.buffer;case"[object ArrayBuffer]":return!(e.byteLength!=n.byteLength||!h(new i(e),new i(n)));case"[object Boolean]":case"[object Date]":case"[object Number]":return r(+e,+n);case"[object Error]":return e.name==n.name&&e.message==n.message;case"[object RegExp]":case"[object String]":return e==n+"";case"[object Map]":var u=s;case"[object Set]":var p=1&a;if(u||(u=l),e.size!=n.size&&!p)return!1;var g=m.get(e);if(g)return g==n;a|=2,m.set(e,n);var f=o(u(e),u(n),a,c,h,m);return m.delete(e),f;case"[object Symbol]":if(d)return d.call(e)==d.call(n)}return!1}},function(e,n,t){var a=t(16).Uint8Array;e.exports=a},function(e,n){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e,a){t[++n]=[a,e]})),t}},function(e,n,t){var a=t(250),i=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,r,o,s){var l=1&t,c=a(e),d=c.length;if(d!=a(n).length&&!l)return!1;for(var h=d;h--;){var m=c[h];if(!(l?m in n:i.call(n,m)))return!1}var u=s.get(e),p=s.get(n);if(u&&p)return u==n&&p==e;var g=!0;s.set(e,n),s.set(n,e);for(var f=l;++h<d;){var y=e[m=c[h]],b=n[m];if(r)var v=l?r(b,y,m,n,e,s):r(y,b,m,e,n,s);if(!(void 0===v?y===b||o(y,b,t,r,s):v)){g=!1;break}f||(f="constructor"==m)}if(g&&!f){var w=e.constructor,_=n.constructor;w==_||!("constructor"in e)||!("constructor"in n)||"function"==typeof w&&w instanceof w&&"function"==typeof _&&_ instanceof _||(g=!1)}return s.delete(e),s.delete(n),g}},function(e,n,t){var a=t(251),i=t(252),r=t(114);e.exports=function(e){return a(e,r,i)}},function(e,n,t){var a=t(104),i=t(14);e.exports=function(e,n,t){var r=n(e);return i(e)?r:a(r,t(e))}},function(e,n,t){var a=t(253),i=t(254),r=Object.prototype.propertyIsEnumerable,o=Object.getOwnPropertySymbols,s=o?function(e){return null==e?[]:(e=Object(e),a(o(e),(function(n){return r.call(e,n)})))}:i;e.exports=s},function(e,n){e.exports=function(e,n){for(var t=-1,a=null==e?0:e.length,i=0,r=[];++t<a;){var o=e[t];n(o,t,e)&&(r[i++]=o)}return r}},function(e,n){e.exports=function(){return[]}},function(e,n,t){var a=t(256),i=t(61),r=t(14),o=t(115),s=t(116),l=t(117),c=Object.prototype.hasOwnProperty;e.exports=function(e,n){var t=r(e),d=!t&&i(e),h=!t&&!d&&o(e),m=!t&&!d&&!h&&l(e),u=t||d||h||m,p=u?a(e.length,String):[],g=p.length;for(var f in e)!n&&!c.call(e,f)||u&&("length"==f||h&&("offset"==f||"parent"==f)||m&&("buffer"==f||"byteLength"==f||"byteOffset"==f)||s(f,g))||p.push(f);return p}},function(e,n){e.exports=function(e,n){for(var t=-1,a=Array(e);++t<e;)a[t]=n(t);return a}},function(e,n){e.exports=function(){return!1}},function(e,n,t){var a=t(30),i=t(66),r=t(24),o={};o["[object Float32Array]"]=o["[object Float64Array]"]=o["[object Int8Array]"]=o["[object Int16Array]"]=o["[object Int32Array]"]=o["[object Uint8Array]"]=o["[object Uint8ClampedArray]"]=o["[object Uint16Array]"]=o["[object Uint32Array]"]=!0,o["[object Arguments]"]=o["[object Array]"]=o["[object ArrayBuffer]"]=o["[object Boolean]"]=o["[object DataView]"]=o["[object Date]"]=o["[object Error]"]=o["[object Function]"]=o["[object Map]"]=o["[object Number]"]=o["[object Object]"]=o["[object RegExp]"]=o["[object Set]"]=o["[object String]"]=o["[object WeakMap]"]=!1,e.exports=function(e){return r(e)&&i(e.length)&&!!o[a(e)]}},function(e,n){e.exports=function(e){return function(n){return e(n)}}},function(e,n,t){(function(e){var a=t(105),i=n&&!n.nodeType&&n,r=i&&"object"==typeof e&&e&&!e.nodeType&&e,o=r&&r.exports===i&&a.process,s=function(){try{var e=r&&r.require&&r.require("util").types;return e||o&&o.binding&&o.binding("util")}catch(e){}}();e.exports=s}).call(this,t(77)(e))},function(e,n,t){var a=t(262),i=t(263),r=Object.prototype.hasOwnProperty;e.exports=function(e){if(!a(e))return i(e);var n=[];for(var t in Object(e))r.call(e,t)&&"constructor"!=t&&n.push(t);return n}},function(e,n){var t=Object.prototype;e.exports=function(e){var n=e&&e.constructor;return e===("function"==typeof n&&n.prototype||t)}},function(e,n,t){var a=t(264)(Object.keys,Object);e.exports=a},function(e,n){e.exports=function(e,n){return function(t){return e(n(t))}}},function(e,n,t){var a=t(266),i=t(62),r=t(267),o=t(119),s=t(268),l=t(30),c=t(109),d=c(a),h=c(i),m=c(r),u=c(o),p=c(s),g=l;(a&&"[object DataView]"!=g(new a(new ArrayBuffer(1)))||i&&"[object Map]"!=g(new i)||r&&"[object Promise]"!=g(r.resolve())||o&&"[object Set]"!=g(new o)||s&&"[object WeakMap]"!=g(new s))&&(g=function(e){var n=l(e),t="[object Object]"==n?e.constructor:void 0,a=t?c(t):"";if(a)switch(a){case d:return"[object DataView]";case h:return"[object Map]";case m:return"[object Promise]";case u:return"[object Set]";case p:return"[object WeakMap]"}return n}),e.exports=g},function(e,n,t){var a=t(19)(t(16),"DataView");e.exports=a},function(e,n,t){var a=t(19)(t(16),"Promise");e.exports=a},function(e,n,t){var a=t(19)(t(16),"WeakMap");e.exports=a},function(e,n,t){var a=t(120),i=t(114);e.exports=function(e){for(var n=i(e),t=n.length;t--;){var r=n[t],o=e[r];n[t]=[r,o,a(o)]}return n}},function(e,n,t){var a=t(110),i=t(271),r=t(278),o=t(67),s=t(120),l=t(121),c=t(45);e.exports=function(e,n){return o(e)&&s(n)?l(c(e),n):function(t){var o=i(t,e);return void 0===o&&o===n?r(t,e):a(n,o,3)}}},function(e,n,t){var a=t(122);e.exports=function(e,n,t){var i=null==e?void 0:a(e,n);return void 0===i?t:i}},function(e,n,t){var a=t(273),i=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,r=/\\(\\)?/g,o=a((function(e){var n=[];return 46===e.charCodeAt(0)&&n.push(""),e.replace(i,(function(e,t,a,i){n.push(a?i.replace(r,"$1"):t||e)})),n}));e.exports=o},function(e,n,t){var a=t(274);e.exports=function(e){var n=a(e,(function(e){return 500===t.size&&t.clear(),e})),t=n.cache;return n}},function(e,n,t){var a=t(64);function i(e,n){if("function"!=typeof e||null!=n&&"function"!=typeof n)throw new TypeError("Expected a function");var t=function(){var a=arguments,i=n?n.apply(this,a):a[0],r=t.cache;if(r.has(i))return r.get(i);var o=e.apply(this,a);return t.cache=r.set(i,o)||r,o};return t.cache=new(i.Cache||a),t}i.Cache=a,e.exports=i},function(e,n,t){var a=t(276);e.exports=function(e){return null==e?"":a(e)}},function(e,n,t){var a=t(32),i=t(277),r=t(14),o=t(68),s=a?a.prototype:void 0,l=s?s.toString:void 0;e.exports=function e(n){if("string"==typeof n)return n;if(r(n))return i(n,e)+"";if(o(n))return l?l.call(n):"";var t=n+"";return"0"==t&&1/n==-1/0?"-0":t}},function(e,n){e.exports=function(e,n){for(var t=-1,a=null==e?0:e.length,i=Array(a);++t<a;)i[t]=n(e[t],t,e);return i}},function(e,n,t){var a=t(279),i=t(280);e.exports=function(e,n){return null!=e&&i(e,n,a)}},function(e,n){e.exports=function(e,n){return null!=e&&n in Object(e)}},function(e,n,t){var a=t(123),i=t(61),r=t(14),o=t(116),s=t(66),l=t(45);e.exports=function(e,n,t){for(var c=-1,d=(n=a(n,e)).length,h=!1;++c<d;){var m=l(n[c]);if(!(h=null!=e&&t(e,m)))break;e=e[m]}return h||++c!=d?h:!!(d=null==e?0:e.length)&&s(d)&&o(m,d)&&(r(e)||i(e))}},function(e,n,t){var a=t(282),i=t(283),r=t(67),o=t(45);e.exports=function(e){return r(e)?a(o(e)):i(e)}},function(e,n){e.exports=function(e){return function(n){return null==n?void 0:n[e]}}},function(e,n,t){var a=t(122);e.exports=function(e){return function(n){return a(n,e)}}},function(e,n,t){var a=t(69),i=t(285),r=t(287);e.exports=function(e,n){return r(i(e,n,a),e+"")}},function(e,n,t){var a=t(286),i=Math.max;e.exports=function(e,n,t){return n=i(void 0===n?e.length-1:n,0),function(){for(var r=arguments,o=-1,s=i(r.length-n,0),l=Array(s);++o<s;)l[o]=r[n+o];o=-1;for(var c=Array(n+1);++o<n;)c[o]=r[o];return c[n]=t(l),a(e,this,c)}}},function(e,n){e.exports=function(e,n,t){switch(t.length){case 0:return e.call(n);case 1:return e.call(n,t[0]);case 2:return e.call(n,t[0],t[1]);case 3:return e.call(n,t[0],t[1],t[2])}return e.apply(n,t)}},function(e,n,t){var a=t(288),i=t(291)(a);e.exports=i},function(e,n,t){var a=t(289),i=t(290),r=t(69),o=i?function(e,n){return i(e,"toString",{configurable:!0,enumerable:!1,value:a(n),writable:!0})}:r;e.exports=o},function(e,n){e.exports=function(e){return function(){return e}}},function(e,n,t){var a=t(19),i=function(){try{var e=a(Object,"defineProperty");return e({},"",{}),e}catch(e){}}();e.exports=i},function(e,n){var t=Date.now;e.exports=function(e){var n=0,a=0;return function(){var i=t(),r=16-(i-a);if(a=i,r>0){if(++n>=800)return arguments[0]}else n=0;return e.apply(void 0,arguments)}}},function(e,n,t){var a=t(112),i=t(293),r=t(298),o=t(113),s=t(299),l=t(65);e.exports=function(e,n,t){var c=-1,d=i,h=e.length,m=!0,u=[],p=u;if(t)m=!1,d=r;else if(h>=200){var g=n?null:s(e);if(g)return l(g);m=!1,d=o,p=new a}else p=n?[]:u;e:for(;++c<h;){var f=e[c],y=n?n(f):f;if(f=t||0!==f?f:0,m&&y==y){for(var b=p.length;b--;)if(p[b]===y)continue e;n&&p.push(y),u.push(f)}else d(p,y,t)||(p!==u&&p.push(y),u.push(f))}return u}},function(e,n,t){var a=t(294);e.exports=function(e,n){return!!(null==e?0:e.length)&&a(e,n,0)>-1}},function(e,n,t){var a=t(295),i=t(296),r=t(297);e.exports=function(e,n,t){return n==n?r(e,n,t):a(e,i,t)}},function(e,n){e.exports=function(e,n,t,a){for(var i=e.length,r=t+(a?1:-1);a?r--:++r<i;)if(n(e[r],r,e))return r;return-1}},function(e,n){e.exports=function(e){return e!=e}},function(e,n){e.exports=function(e,n,t){for(var a=t-1,i=e.length;++a<i;)if(e[a]===n)return a;return-1}},function(e,n){e.exports=function(e,n,t){for(var a=-1,i=null==e?0:e.length;++a<i;)if(t(n,e[a]))return!0;return!1}},function(e,n,t){var a=t(119),i=t(300),r=t(65),o=a&&1/r(new a([,-0]))[1]==1/0?function(e){return new a(e)}:i;e.exports=o},function(e,n){e.exports=function(){}},function(e,n,t){var a=t(118),i=t(24);e.exports=function(e){return i(e)&&a(e)}},function(e,n,t){},function(e,n,t){},function(e,n,t){"use strict";t(124)},function(e,n,t){"use strict";t(125)},function(e,n,t){},function(e,n,t){},function(e,n,t){"use strict";var a=t(309),i=t(134),r=t(72),o=Object.prototype.hasOwnProperty,s={brackets:function(e){return e+"[]"},comma:"comma",indices:function(e,n){return e+"["+n+"]"},repeat:function(e){return e}},l=Array.isArray,c=Array.prototype.push,d=function(e,n){c.apply(e,l(n)?n:[n])},h=Date.prototype.toISOString,m=r.default,u={addQueryPrefix:!1,allowDots:!1,allowEmptyArrays:!1,arrayFormat:"indices",charset:"utf-8",charsetSentinel:!1,commaRoundTrip:!1,delimiter:"&",encode:!0,encodeDotInKeys:!1,encoder:i.encode,encodeValuesOnly:!1,filter:void 0,format:m,formatter:r.formatters[m],indices:!1,serializeDate:function(e){return h.call(e)},skipNulls:!1,strictNullHandling:!1},p={},g=function e(n,t,r,o,s,c,h,m,g,f,y,b,v,w,_,k,x,T){for(var z,M=n,P=T,C=0,A=!1;void 0!==(P=P.get(p))&&!A;){var I=P.get(n);if(C+=1,void 0!==I){if(I===C)throw new RangeError("Cyclic object value");A=!0}void 0===P.get(p)&&(C=0)}if("function"==typeof f?M=f(t,M):M instanceof Date?M=v(M):"comma"===r&&l(M)&&(M=i.maybeMap(M,(function(e){return e instanceof Date?v(e):e}))),null===M){if(c)return g&&!k?g(t,u.encoder,x,"key",w):t;M=""}if("string"==typeof(z=M)||"number"==typeof z||"boolean"==typeof z||"symbol"==typeof z||"bigint"==typeof z||i.isBuffer(M))return g?[_(k?t:g(t,u.encoder,x,"key",w))+"="+_(g(M,u.encoder,x,"value",w))]:[_(t)+"="+_(String(M))];var S,L=[];if(void 0===M)return L;if("comma"===r&&l(M))k&&g&&(M=i.maybeMap(M,g)),S=[{value:M.length>0?M.join(",")||null:void 0}];else if(l(f))S=f;else{var U=Object.keys(M);S=y?U.sort(y):U}var q=m?String(t).replace(/\./g,"%2E"):String(t),E=o&&l(M)&&1===M.length?q+"[]":q;if(s&&l(M)&&0===M.length)return E+"[]";for(var R=0;R<S.length;++R){var D=S[R],O="object"==typeof D&&D&&void 0!==D.value?D.value:M[D];if(!h||null!==O){var G=b&&m?String(D).replace(/\./g,"%2E"):String(D),F=l(M)?"function"==typeof r?r(E,G):E:E+(b?"."+G:"["+G+"]");T.set(n,C);var B=a();B.set(p,T),d(L,e(O,F,r,o,s,c,h,m,"comma"===r&&k&&l(M)?null:g,f,y,b,v,w,_,k,x,B))}}return L};e.exports=function(e,n){var t,i=e,c=function(e){if(!e)return u;if(void 0!==e.allowEmptyArrays&&"boolean"!=typeof e.allowEmptyArrays)throw new TypeError("`allowEmptyArrays` option can only be `true` or `false`, when provided");if(void 0!==e.encodeDotInKeys&&"boolean"!=typeof e.encodeDotInKeys)throw new TypeError("`encodeDotInKeys` option can only be `true` or `false`, when provided");if(null!==e.encoder&&void 0!==e.encoder&&"function"!=typeof e.encoder)throw new TypeError("Encoder has to be a function.");var n=e.charset||u.charset;if(void 0!==e.charset&&"utf-8"!==e.charset&&"iso-8859-1"!==e.charset)throw new TypeError("The charset option must be either utf-8, iso-8859-1, or undefined");var t=r.default;if(void 0!==e.format){if(!o.call(r.formatters,e.format))throw new TypeError("Unknown format option provided.");t=e.format}var a,i=r.formatters[t],c=u.filter;if(("function"==typeof e.filter||l(e.filter))&&(c=e.filter),a=e.arrayFormat in s?e.arrayFormat:"indices"in e?e.indices?"indices":"repeat":u.arrayFormat,"commaRoundTrip"in e&&"boolean"!=typeof e.commaRoundTrip)throw new TypeError("`commaRoundTrip` must be a boolean, or absent");var d=void 0===e.allowDots?!0===e.encodeDotInKeys||u.allowDots:!!e.allowDots;return{addQueryPrefix:"boolean"==typeof e.addQueryPrefix?e.addQueryPrefix:u.addQueryPrefix,allowDots:d,allowEmptyArrays:"boolean"==typeof e.allowEmptyArrays?!!e.allowEmptyArrays:u.allowEmptyArrays,arrayFormat:a,charset:n,charsetSentinel:"boolean"==typeof e.charsetSentinel?e.charsetSentinel:u.charsetSentinel,commaRoundTrip:!!e.commaRoundTrip,delimiter:void 0===e.delimiter?u.delimiter:e.delimiter,encode:"boolean"==typeof e.encode?e.encode:u.encode,encodeDotInKeys:"boolean"==typeof e.encodeDotInKeys?e.encodeDotInKeys:u.encodeDotInKeys,encoder:"function"==typeof e.encoder?e.encoder:u.encoder,encodeValuesOnly:"boolean"==typeof e.encodeValuesOnly?e.encodeValuesOnly:u.encodeValuesOnly,filter:c,format:t,formatter:i,serializeDate:"function"==typeof e.serializeDate?e.serializeDate:u.serializeDate,skipNulls:"boolean"==typeof e.skipNulls?e.skipNulls:u.skipNulls,sort:"function"==typeof e.sort?e.sort:null,strictNullHandling:"boolean"==typeof e.strictNullHandling?e.strictNullHandling:u.strictNullHandling}}(n);"function"==typeof c.filter?i=(0,c.filter)("",i):l(c.filter)&&(t=c.filter);var h=[];if("object"!=typeof i||null===i)return"";var m=s[c.arrayFormat],p="comma"===m&&c.commaRoundTrip;t||(t=Object.keys(i)),c.sort&&t.sort(c.sort);for(var f=a(),y=0;y<t.length;++y){var b=t[y],v=i[b];c.skipNulls&&null===v||d(h,g(v,b,m,p,c.allowEmptyArrays,c.strictNullHandling,c.skipNulls,c.encodeDotInKeys,c.encode?c.encoder:null,c.filter,c.sort,c.allowDots,c.serializeDate,c.format,c.formatter,c.encodeValuesOnly,c.charset,f))}var w=h.join(c.delimiter),_=!0===c.addQueryPrefix?"?":"";return c.charsetSentinel&&("iso-8859-1"===c.charset?_+="utf8=%26%2310003%3B&":_+="utf8=%E2%9C%93&"),w.length>0?_+w:""}},function(e,n,t){"use strict";var a=t(27),i=t(46),r=t(311),o=t(126),s=t(336)||o||r;e.exports=function(){var e,n={assert:function(e){if(!n.has(e))throw new a("Side channel does not contain "+i(e))},delete:function(n){return!!e&&e.delete(n)},get:function(n){return e&&e.get(n)},has:function(n){return!!e&&e.has(n)},set:function(n,t){e||(e=s()),e.set(n,t)}};return n}},function(e,n){},function(e,n,t){"use strict";var a=t(46),i=t(27),r=function(e,n,t){for(var a,i=e;null!=(a=i.next);i=a)if(a.key===n)return i.next=a.next,t||(a.next=e.next,e.next=a),a};e.exports=function(){var e,n={assert:function(e){if(!n.has(e))throw new i("Side channel does not contain "+a(e))},delete:function(n){var t=e&&e.next,a=function(e,n){if(e)return r(e,n,!0)}(e,n);return a&&t&&t===a&&(e=void 0),!!a},get:function(n){return function(e,n){if(e){var t=r(e,n);return t&&t.value}}(e,n)},has:function(n){return function(e,n){return!!e&&!!r(e,n)}(e,n)},set:function(n,t){e||(e={next:void 0}),function(e,n,t){var a=r(e,n);a?a.value=t:e.next={key:n,next:e.next,value:t}}(e,n,t)}};return n}},function(e,n,t){"use strict";e.exports=Error},function(e,n,t){"use strict";e.exports=EvalError},function(e,n,t){"use strict";e.exports=RangeError},function(e,n,t){"use strict";e.exports=ReferenceError},function(e,n,t){"use strict";e.exports=SyntaxError},function(e,n,t){"use strict";e.exports=URIError},function(e,n,t){"use strict";e.exports=Math.abs},function(e,n,t){"use strict";e.exports=Math.floor},function(e,n,t){"use strict";e.exports=Math.max},function(e,n,t){"use strict";e.exports=Math.min},function(e,n,t){"use strict";e.exports=Math.pow},function(e,n,t){"use strict";e.exports=Math.round},function(e,n,t){"use strict";var a=t(325);e.exports=function(e){return a(e)||0===e?e:e<0?-1:1}},function(e,n,t){"use strict";e.exports=Number.isNaN||function(e){return e!=e}},function(e,n,t){"use strict";e.exports=Object.getOwnPropertyDescriptor},function(e,n,t){"use strict";var a=Object.defineProperty||!1;if(a)try{a({},"a",{value:1})}catch(e){a=!1}e.exports=a},function(e,n,t){"use strict";var a="undefined"!=typeof Symbol&&Symbol,i=t(329);e.exports=function(){return"function"==typeof a&&("function"==typeof Symbol&&("symbol"==typeof a("foo")&&("symbol"==typeof Symbol("bar")&&i())))}},function(e,n,t){"use strict";e.exports=function(){if("function"!=typeof Symbol||"function"!=typeof Object.getOwnPropertySymbols)return!1;if("symbol"==typeof Symbol.iterator)return!0;var e={},n=Symbol("test"),t=Object(n);if("string"==typeof n)return!1;if("[object Symbol]"!==Object.prototype.toString.call(n))return!1;if("[object Symbol]"!==Object.prototype.toString.call(t))return!1;for(var a in e[n]=42,e)return!1;if("function"==typeof Object.keys&&0!==Object.keys(e).length)return!1;if("function"==typeof Object.getOwnPropertyNames&&0!==Object.getOwnPropertyNames(e).length)return!1;var i=Object.getOwnPropertySymbols(e);if(1!==i.length||i[0]!==n)return!1;if(!Object.prototype.propertyIsEnumerable.call(e,n))return!1;if("function"==typeof Object.getOwnPropertyDescriptor){var r=Object.getOwnPropertyDescriptor(e,n);if(42!==r.value||!0!==r.enumerable)return!1}return!0}},function(e,n,t){"use strict";var a=t(129),i=t(130),r=t(331);e.exports=a?function(e){return a(e)}:i?function(e){if(!e||"object"!=typeof e&&"function"!=typeof e)throw new TypeError("getProto: not an object");return i(e)}:r?function(e){return r(e)}:null},function(e,n,t){"use strict";var a,i=t(131),r=t(128);try{a=[].__proto__===Array.prototype}catch(e){if(!e||"object"!=typeof e||!("code"in e)||"ERR_PROTO_ACCESS"!==e.code)throw e}var o=!!a&&r&&r(Object.prototype,"__proto__"),s=Object,l=s.getPrototypeOf;e.exports=o&&"function"==typeof o.get?i([o.get]):"function"==typeof l&&function(e){return l(null==e?e:s(e))}},function(e,n,t){"use strict";var a="Function.prototype.bind called on incompatible ",i=Object.prototype.toString,r=Math.max,o=function(e,n){for(var t=[],a=0;a<e.length;a+=1)t[a]=e[a];for(var i=0;i<n.length;i+=1)t[i+e.length]=n[i];return t},s=function(e,n){for(var t=[],a=n||0,i=0;a<e.length;a+=1,i+=1)t[i]=e[a];return t},l=function(e,n){for(var t="",a=0;a<e.length;a+=1)t+=e[a],a+1<e.length&&(t+=n);return t};e.exports=function(e){var n=this;if("function"!=typeof n||"[object Function]"!==i.apply(n))throw new TypeError(a+n);for(var t,c=s(arguments,1),d=function(){if(this instanceof t){var a=n.apply(this,o(c,arguments));return Object(a)===a?a:this}return n.apply(e,o(c,arguments))},h=r(0,n.length-c.length),m=[],u=0;u<h;u++)m[u]="$"+u;if(t=Function("binder","return function ("+l(m,",")+"){ return binder.apply(this,arguments); }")(d),n.prototype){var p=function(){};p.prototype=n.prototype,t.prototype=new p,p.prototype=null}return t}},function(e,n,t){"use strict";var a=t(47),i=t(132),r=t(71),o=t(334);e.exports=o||a.call(r,i)},function(e,n,t){"use strict";e.exports="undefined"!=typeof Reflect&&Reflect&&Reflect.apply},function(e,n,t){"use strict";var a=Function.prototype.call,i=Object.prototype.hasOwnProperty,r=t(47);e.exports=r.call(a,i)},function(e,n,t){"use strict";var a=t(70),i=t(133),r=t(46),o=t(126),s=t(27),l=a("%WeakMap%",!0),c=i("WeakMap.prototype.get",!0),d=i("WeakMap.prototype.set",!0),h=i("WeakMap.prototype.has",!0),m=i("WeakMap.prototype.delete",!0);e.exports=l?function(){var e,n,t={assert:function(e){if(!t.has(e))throw new s("Side channel does not contain "+r(e))},delete:function(t){if(l&&t&&("object"==typeof t||"function"==typeof t)){if(e)return m(e,t)}else if(o&&n)return n.delete(t);return!1},get:function(t){return l&&t&&("object"==typeof t||"function"==typeof t)&&e?c(e,t):n&&n.get(t)},has:function(t){return l&&t&&("object"==typeof t||"function"==typeof t)&&e?h(e,t):!!n&&n.has(t)},set:function(t,a){l&&t&&("object"==typeof t||"function"==typeof t)?(e||(e=new l),d(e,t,a)):o&&(n||(n=o()),n.set(t,a))}};return t}:o},function(e,n,t){"use strict";var a=t(134),i=Object.prototype.hasOwnProperty,r=Array.isArray,o={allowDots:!1,allowEmptyArrays:!1,allowPrototypes:!1,allowSparse:!1,arrayLimit:20,charset:"utf-8",charsetSentinel:!1,comma:!1,decodeDotInKeys:!1,decoder:a.decode,delimiter:"&",depth:5,duplicates:"combine",ignoreQueryPrefix:!1,interpretNumericEntities:!1,parameterLimit:1e3,parseArrays:!0,plainObjects:!1,strictDepth:!1,strictNullHandling:!1,throwOnLimitExceeded:!1},s=function(e){return e.replace(/&#(\d+);/g,(function(e,n){return String.fromCharCode(parseInt(n,10))}))},l=function(e,n,t){if(e&&"string"==typeof e&&n.comma&&e.indexOf(",")>-1)return e.split(",");if(n.throwOnLimitExceeded&&t>=n.arrayLimit)throw new RangeError("Array limit exceeded. Only "+n.arrayLimit+" element"+(1===n.arrayLimit?"":"s")+" allowed in an array.");return e},c=function(e,n,t,r){if(e){var o=t.allowDots?e.replace(/\.([^.[]+)/g,"[$1]"):e,s=/(\[[^[\]]*])/g,c=t.depth>0&&/(\[[^[\]]*])/.exec(o),d=c?o.slice(0,c.index):o,h=[];if(d){if(!t.plainObjects&&i.call(Object.prototype,d)&&!t.allowPrototypes)return;h.push(d)}for(var m=0;t.depth>0&&null!==(c=s.exec(o))&&m<t.depth;){if(m+=1,!t.plainObjects&&i.call(Object.prototype,c[1].slice(1,-1))&&!t.allowPrototypes)return;h.push(c[1])}if(c){if(!0===t.strictDepth)throw new RangeError("Input depth exceeded depth option of "+t.depth+" and strictDepth is true");h.push("["+o.slice(c.index)+"]")}return function(e,n,t,i){var r=0;if(e.length>0&&"[]"===e[e.length-1]){var o=e.slice(0,-1).join("");r=Array.isArray(n)&&n[o]?n[o].length:0}for(var s=i?n:l(n,t,r),c=e.length-1;c>=0;--c){var d,h=e[c];if("[]"===h&&t.parseArrays)d=t.allowEmptyArrays&&(""===s||t.strictNullHandling&&null===s)?[]:a.combine([],s);else{d=t.plainObjects?{__proto__:null}:{};var m="["===h.charAt(0)&&"]"===h.charAt(h.length-1)?h.slice(1,-1):h,u=t.decodeDotInKeys?m.replace(/%2E/g,"."):m,p=parseInt(u,10);t.parseArrays||""!==u?!isNaN(p)&&h!==u&&String(p)===u&&p>=0&&t.parseArrays&&p<=t.arrayLimit?(d=[])[p]=s:"__proto__"!==u&&(d[u]=s):d={0:s}}s=d}return s}(h,n,t,r)}};e.exports=function(e,n){var t=function(e){if(!e)return o;if(void 0!==e.allowEmptyArrays&&"boolean"!=typeof e.allowEmptyArrays)throw new TypeError("`allowEmptyArrays` option can only be `true` or `false`, when provided");if(void 0!==e.decodeDotInKeys&&"boolean"!=typeof e.decodeDotInKeys)throw new TypeError("`decodeDotInKeys` option can only be `true` or `false`, when provided");if(null!==e.decoder&&void 0!==e.decoder&&"function"!=typeof e.decoder)throw new TypeError("Decoder has to be a function.");if(void 0!==e.charset&&"utf-8"!==e.charset&&"iso-8859-1"!==e.charset)throw new TypeError("The charset option must be either utf-8, iso-8859-1, or undefined");if(void 0!==e.throwOnLimitExceeded&&"boolean"!=typeof e.throwOnLimitExceeded)throw new TypeError("`throwOnLimitExceeded` option must be a boolean");var n=void 0===e.charset?o.charset:e.charset,t=void 0===e.duplicates?o.duplicates:e.duplicates;if("combine"!==t&&"first"!==t&&"last"!==t)throw new TypeError("The duplicates option must be either combine, first, or last");return{allowDots:void 0===e.allowDots?!0===e.decodeDotInKeys||o.allowDots:!!e.allowDots,allowEmptyArrays:"boolean"==typeof e.allowEmptyArrays?!!e.allowEmptyArrays:o.allowEmptyArrays,allowPrototypes:"boolean"==typeof e.allowPrototypes?e.allowPrototypes:o.allowPrototypes,allowSparse:"boolean"==typeof e.allowSparse?e.allowSparse:o.allowSparse,arrayLimit:"number"==typeof e.arrayLimit?e.arrayLimit:o.arrayLimit,charset:n,charsetSentinel:"boolean"==typeof e.charsetSentinel?e.charsetSentinel:o.charsetSentinel,comma:"boolean"==typeof e.comma?e.comma:o.comma,decodeDotInKeys:"boolean"==typeof e.decodeDotInKeys?e.decodeDotInKeys:o.decodeDotInKeys,decoder:"function"==typeof e.decoder?e.decoder:o.decoder,delimiter:"string"==typeof e.delimiter||a.isRegExp(e.delimiter)?e.delimiter:o.delimiter,depth:"number"==typeof e.depth||!1===e.depth?+e.depth:o.depth,duplicates:t,ignoreQueryPrefix:!0===e.ignoreQueryPrefix,interpretNumericEntities:"boolean"==typeof e.interpretNumericEntities?e.interpretNumericEntities:o.interpretNumericEntities,parameterLimit:"number"==typeof e.parameterLimit?e.parameterLimit:o.parameterLimit,parseArrays:!1!==e.parseArrays,plainObjects:"boolean"==typeof e.plainObjects?e.plainObjects:o.plainObjects,strictDepth:"boolean"==typeof e.strictDepth?!!e.strictDepth:o.strictDepth,strictNullHandling:"boolean"==typeof e.strictNullHandling?e.strictNullHandling:o.strictNullHandling,throwOnLimitExceeded:"boolean"==typeof e.throwOnLimitExceeded&&e.throwOnLimitExceeded}}(n);if(""===e||null==e)return t.plainObjects?{__proto__:null}:{};for(var d="string"==typeof e?function(e,n){var t={__proto__:null},c=n.ignoreQueryPrefix?e.replace(/^\?/,""):e;c=c.replace(/%5B/gi,"[").replace(/%5D/gi,"]");var d=n.parameterLimit===1/0?void 0:n.parameterLimit,h=c.split(n.delimiter,n.throwOnLimitExceeded?d+1:d);if(n.throwOnLimitExceeded&&h.length>d)throw new RangeError("Parameter limit exceeded. Only "+d+" parameter"+(1===d?"":"s")+" allowed.");var m,u=-1,p=n.charset;if(n.charsetSentinel)for(m=0;m<h.length;++m)0===h[m].indexOf("utf8=")&&("utf8=%E2%9C%93"===h[m]?p="utf-8":"utf8=%26%2310003%3B"===h[m]&&(p="iso-8859-1"),u=m,m=h.length);for(m=0;m<h.length;++m)if(m!==u){var g,f,y=h[m],b=y.indexOf("]="),v=-1===b?y.indexOf("="):b+1;-1===v?(g=n.decoder(y,o.decoder,p,"key"),f=n.strictNullHandling?null:""):(g=n.decoder(y.slice(0,v),o.decoder,p,"key"),f=a.maybeMap(l(y.slice(v+1),n,r(t[g])?t[g].length:0),(function(e){return n.decoder(e,o.decoder,p,"value")}))),f&&n.interpretNumericEntities&&"iso-8859-1"===p&&(f=s(String(f))),y.indexOf("[]=")>-1&&(f=r(f)?[f]:f);var w=i.call(t,g);w&&"combine"===n.duplicates?t[g]=a.combine(t[g],f):w&&"last"!==n.duplicates||(t[g]=f)}return t}(e,t):e,h=t.plainObjects?{__proto__:null}:{},m=Object.keys(d),u=0;u<m.length;++u){var p=m[u],g=c(p,d[p],t,"string"==typeof e);h=a.merge(h,g,t)}return!0===t.allowSparse?h:a.compact(h)}},function(e,n,t){var a=t(20),i=t(340),r=t(341);e.exports=function(e){var n=a(e);return r(n,i(n))+1}},function(e,n){e.exports=function(e){var n=new Date(e.getTime()),t=n.getTimezoneOffset();return n.setSeconds(0,0),6e4*t+n.getTime()%6e4}},function(e,n,t){var a=t(20);e.exports=function(e){var n=a(e),t=new Date(0);return t.setFullYear(n.getFullYear(),0,1),t.setHours(0,0,0,0),t}},function(e,n,t){var a=t(342);e.exports=function(e,n){var t=a(e),i=a(n),r=t.getTime()-6e4*t.getTimezoneOffset(),o=i.getTime()-6e4*i.getTimezoneOffset();return Math.round((r-o)/864e5)}},function(e,n,t){var a=t(20);e.exports=function(e){var n=a(e);return n.setHours(0,0,0,0),n}},function(e,n,t){var a=t(20),i=t(73),r=t(345);e.exports=function(e){var n=a(e),t=i(n).getTime()-r(n).getTime();return Math.round(t/6048e5)+1}},function(e,n,t){var a=t(20);e.exports=function(e,n){var t=n&&Number(n.weekStartsOn)||0,i=a(e),r=i.getDay(),o=(r<t?7:0)+r-t;return i.setDate(i.getDate()-o),i.setHours(0,0,0,0),i}},function(e,n,t){var a=t(136),i=t(73);e.exports=function(e){var n=a(e),t=new Date(0);return t.setFullYear(n,0,4),t.setHours(0,0,0,0),i(t)}},function(e,n,t){var a=t(135);e.exports=function(e){if(a(e))return!isNaN(e);throw new TypeError(toString.call(e)+" is not an instance of Date")}},function(e,n,t){var a=t(348),i=t(349);e.exports={distanceInWords:a(),format:i()}},function(e,n){e.exports=function(){var e={lessThanXSeconds:{one:"less than a second",other:"less than {{count}} seconds"},xSeconds:{one:"1 second",other:"{{count}} seconds"},halfAMinute:"half a minute",lessThanXMinutes:{one:"less than a minute",other:"less than {{count}} minutes"},xMinutes:{one:"1 minute",other:"{{count}} minutes"},aboutXHours:{one:"about 1 hour",other:"about {{count}} hours"},xHours:{one:"1 hour",other:"{{count}} hours"},xDays:{one:"1 day",other:"{{count}} days"},aboutXMonths:{one:"about 1 month",other:"about {{count}} months"},xMonths:{one:"1 month",other:"{{count}} months"},aboutXYears:{one:"about 1 year",other:"about {{count}} years"},xYears:{one:"1 year",other:"{{count}} years"},overXYears:{one:"over 1 year",other:"over {{count}} years"},almostXYears:{one:"almost 1 year",other:"almost {{count}} years"}};return{localize:function(n,t,a){var i;return a=a||{},i="string"==typeof e[n]?e[n]:1===t?e[n].one:e[n].other.replace("{{count}}",t),a.addSuffix?a.comparison>0?"in "+i:i+" ago":i}}}},function(e,n,t){var a=t(350);e.exports=function(){var e=["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],n=["January","February","March","April","May","June","July","August","September","October","November","December"],t=["Su","Mo","Tu","We","Th","Fr","Sa"],i=["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],r=["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],o=["AM","PM"],s=["am","pm"],l=["a.m.","p.m."],c={MMM:function(n){return e[n.getMonth()]},MMMM:function(e){return n[e.getMonth()]},dd:function(e){return t[e.getDay()]},ddd:function(e){return i[e.getDay()]},dddd:function(e){return r[e.getDay()]},A:function(e){return e.getHours()/12>=1?o[1]:o[0]},a:function(e){return e.getHours()/12>=1?s[1]:s[0]},aa:function(e){return e.getHours()/12>=1?l[1]:l[0]}};return["M","D","DDD","d","Q","W"].forEach((function(e){c[e+"o"]=function(n,t){return function(e){var n=e%100;if(n>20||n<10)switch(n%10){case 1:return e+"st";case 2:return e+"nd";case 3:return e+"rd"}return e+"th"}(t[e](n))}})),{formatters:c,formattingTokensRegExp:a(c)}}},function(e,n){var t=["M","MM","Q","D","DD","DDD","DDDD","d","E","W","WW","YY","YYYY","GG","GGGG","H","HH","h","hh","m","mm","s","ss","S","SS","SSS","Z","ZZ","X","x"];e.exports=function(e){var n=[];for(var a in e)e.hasOwnProperty(a)&&n.push(a);var i=t.concat(n).sort().reverse();return new RegExp("(\\[[^\\[]*\\])|(\\\\)?("+i.join("|")+"|.)","g")}},function(e,n,t){"use strict";var a=t(4),i=t(137),r=t(352),o=t(143);function s(e){var n=new r(e),t=i(r.prototype.request,n);return a.extend(t,r.prototype,n),a.extend(t,n),t}var l=s(t(74));l.Axios=r,l.create=function(e){return s(o(l.defaults,e))},l.Cancel=t(144),l.CancelToken=t(366),l.isCancel=t(142),l.all=function(e){return Promise.all(e)},l.spread=t(367),l.isAxiosError=t(368),e.exports=l,e.exports.default=l},function(e,n,t){"use strict";var a=t(4),i=t(138),r=t(353),o=t(354),s=t(143),l=t(364),c=l.validators;function d(e){this.defaults=e,this.interceptors={request:new r,response:new r}}d.prototype.request=function(e){"string"==typeof e?(e=arguments[1]||{}).url=arguments[0]:e=e||{},(e=s(this.defaults,e)).method?e.method=e.method.toLowerCase():this.defaults.method?e.method=this.defaults.method.toLowerCase():e.method="get";var n=e.transitional;void 0!==n&&l.assertOptions(n,{silentJSONParsing:c.transitional(c.boolean,"1.0.0"),forcedJSONParsing:c.transitional(c.boolean,"1.0.0"),clarifyTimeoutError:c.transitional(c.boolean,"1.0.0")},!1);var t=[],a=!0;this.interceptors.request.forEach((function(n){"function"==typeof n.runWhen&&!1===n.runWhen(e)||(a=a&&n.synchronous,t.unshift(n.fulfilled,n.rejected))}));var i,r=[];if(this.interceptors.response.forEach((function(e){r.push(e.fulfilled,e.rejected)})),!a){var d=[o,void 0];for(Array.prototype.unshift.apply(d,t),d=d.concat(r),i=Promise.resolve(e);d.length;)i=i.then(d.shift(),d.shift());return i}for(var h=e;t.length;){var m=t.shift(),u=t.shift();try{h=m(h)}catch(e){u(e);break}}try{i=o(h)}catch(e){return Promise.reject(e)}for(;r.length;)i=i.then(r.shift(),r.shift());return i},d.prototype.getUri=function(e){return e=s(this.defaults,e),i(e.url,e.params,e.paramsSerializer).replace(/^\?/,"")},a.forEach(["delete","get","head","options"],(function(e){d.prototype[e]=function(n,t){return this.request(s(t||{},{method:e,url:n,data:(t||{}).data}))}})),a.forEach(["post","put","patch"],(function(e){d.prototype[e]=function(n,t,a){return this.request(s(a||{},{method:e,url:n,data:t}))}})),e.exports=d},function(e,n,t){"use strict";var a=t(4);function i(){this.handlers=[]}i.prototype.use=function(e,n,t){return this.handlers.push({fulfilled:e,rejected:n,synchronous:!!t&&t.synchronous,runWhen:t?t.runWhen:null}),this.handlers.length-1},i.prototype.eject=function(e){this.handlers[e]&&(this.handlers[e]=null)},i.prototype.forEach=function(e){a.forEach(this.handlers,(function(n){null!==n&&e(n)}))},e.exports=i},function(e,n,t){"use strict";var a=t(4),i=t(355),r=t(142),o=t(74);function s(e){e.cancelToken&&e.cancelToken.throwIfRequested()}e.exports=function(e){return s(e),e.headers=e.headers||{},e.data=i.call(e,e.data,e.headers,e.transformRequest),e.headers=a.merge(e.headers.common||{},e.headers[e.method]||{},e.headers),a.forEach(["delete","get","head","post","put","patch","common"],(function(n){delete e.headers[n]})),(e.adapter||o.adapter)(e).then((function(n){return s(e),n.data=i.call(e,n.data,n.headers,e.transformResponse),n}),(function(n){return r(n)||(s(e),n&&n.response&&(n.response.data=i.call(e,n.response.data,n.response.headers,e.transformResponse))),Promise.reject(n)}))}},function(e,n,t){"use strict";var a=t(4),i=t(74);e.exports=function(e,n,t){var r=this||i;return a.forEach(t,(function(t){e=t.call(r,e,n)})),e}},function(e,n,t){"use strict";var a=t(4);e.exports=function(e,n){a.forEach(e,(function(t,a){a!==n&&a.toUpperCase()===n.toUpperCase()&&(e[n]=t,delete e[a])}))}},function(e,n,t){"use strict";var a=t(141);e.exports=function(e,n,t){var i=t.config.validateStatus;t.status&&i&&!i(t.status)?n(a("Request failed with status code "+t.status,t.config,null,t.request,t)):e(t)}},function(e,n,t){"use strict";var a=t(4);e.exports=a.isStandardBrowserEnv()?{write:function(e,n,t,i,r,o){var s=[];s.push(e+"="+encodeURIComponent(n)),a.isNumber(t)&&s.push("expires="+new Date(t).toGMTString()),a.isString(i)&&s.push("path="+i),a.isString(r)&&s.push("domain="+r),!0===o&&s.push("secure"),document.cookie=s.join("; ")},read:function(e){var n=document.cookie.match(new RegExp("(^|;\\s*)("+e+")=([^;]*)"));return n?decodeURIComponent(n[3]):null},remove:function(e){this.write(e,"",Date.now()-864e5)}}:{write:function(){},read:function(){return null},remove:function(){}}},function(e,n,t){"use strict";var a=t(360),i=t(361);e.exports=function(e,n){return e&&!a(n)?i(e,n):n}},function(e,n,t){"use strict";e.exports=function(e){return/^([a-z][a-z\d\+\-\.]*:)?\/\//i.test(e)}},function(e,n,t){"use strict";e.exports=function(e,n){return n?e.replace(/\/+$/,"")+"/"+n.replace(/^\/+/,""):e}},function(e,n,t){"use strict";var a=t(4),i=["age","authorization","content-length","content-type","etag","expires","from","host","if-modified-since","if-unmodified-since","last-modified","location","max-forwards","proxy-authorization","referer","retry-after","user-agent"];e.exports=function(e){var n,t,r,o={};return e?(a.forEach(e.split("\n"),(function(e){if(r=e.indexOf(":"),n=a.trim(e.substr(0,r)).toLowerCase(),t=a.trim(e.substr(r+1)),n){if(o[n]&&i.indexOf(n)>=0)return;o[n]="set-cookie"===n?(o[n]?o[n]:[]).concat([t]):o[n]?o[n]+", "+t:t}})),o):o}},function(e,n,t){"use strict";var a=t(4);e.exports=a.isStandardBrowserEnv()?function(){var e,n=/(msie|trident)/i.test(navigator.userAgent),t=document.createElement("a");function i(e){var a=e;return n&&(t.setAttribute("href",a),a=t.href),t.setAttribute("href",a),{href:t.href,protocol:t.protocol?t.protocol.replace(/:$/,""):"",host:t.host,search:t.search?t.search.replace(/^\?/,""):"",hash:t.hash?t.hash.replace(/^#/,""):"",hostname:t.hostname,port:t.port,pathname:"/"===t.pathname.charAt(0)?t.pathname:"/"+t.pathname}}return e=i(window.location.href),function(n){var t=a.isString(n)?i(n):n;return t.protocol===e.protocol&&t.host===e.host}}():function(){return!0}},function(e,n,t){"use strict";var a=t(365),i={};["object","boolean","number","function","string","symbol"].forEach((function(e,n){i[e]=function(t){return typeof t===e||"a"+(n<1?"n ":" ")+e}}));var r={},o=a.version.split(".");function s(e,n){for(var t=n?n.split("."):o,a=e.split("."),i=0;i<3;i++){if(t[i]>a[i])return!0;if(t[i]<a[i])return!1}return!1}i.transitional=function(e,n,t){var i=n&&s(n);function o(e,n){return"[Axios v"+a.version+"] Transitional option '"+e+"'"+n+(t?". "+t:"")}return function(t,a,s){if(!1===e)throw new Error(o(a," has been removed in "+n));return i&&!r[a]&&(r[a]=!0,console.warn(o(a," has been deprecated since v"+n+" and will be removed in the near future"))),!e||e(t,a,s)}},e.exports={isOlderVersion:s,assertOptions:function(e,n,t){if("object"!=typeof e)throw new TypeError("options must be an object");for(var a=Object.keys(e),i=a.length;i-- >0;){var r=a[i],o=n[r];if(o){var s=e[r],l=void 0===s||o(s,r,e);if(!0!==l)throw new TypeError("option "+r+" must be "+l)}else if(!0!==t)throw Error("Unknown option "+r)}},validators:i}},function(e){e.exports=JSON.parse('{"name":"axios","version":"0.21.4","description":"Promise based HTTP client for the browser and node.js","main":"index.js","scripts":{"test":"grunt test","start":"node ./sandbox/server.js","build":"NODE_ENV=production grunt build","preversion":"npm test","version":"npm run build && grunt version && git add -A dist && git add CHANGELOG.md bower.json package.json","postversion":"git push && git push --tags","examples":"node ./examples/server.js","coveralls":"cat coverage/lcov.info | ./node_modules/coveralls/bin/coveralls.js","fix":"eslint --fix lib/**/*.js"},"repository":{"type":"git","url":"https://github.com/axios/axios.git"},"keywords":["xhr","http","ajax","promise","node"],"author":"Matt Zabriskie","license":"MIT","bugs":{"url":"https://github.com/axios/axios/issues"},"homepage":"https://axios-http.com","devDependencies":{"coveralls":"^3.0.0","es6-promise":"^4.2.4","grunt":"^1.3.0","grunt-banner":"^0.6.0","grunt-cli":"^1.2.0","grunt-contrib-clean":"^1.1.0","grunt-contrib-watch":"^1.0.0","grunt-eslint":"^23.0.0","grunt-karma":"^4.0.0","grunt-mocha-test":"^0.13.3","grunt-ts":"^6.0.0-beta.19","grunt-webpack":"^4.0.2","istanbul-instrumenter-loader":"^1.0.0","jasmine-core":"^2.4.1","karma":"^6.3.2","karma-chrome-launcher":"^3.1.0","karma-firefox-launcher":"^2.1.0","karma-jasmine":"^1.1.1","karma-jasmine-ajax":"^0.1.13","karma-safari-launcher":"^1.0.0","karma-sauce-launcher":"^4.3.6","karma-sinon":"^1.0.5","karma-sourcemap-loader":"^0.3.8","karma-webpack":"^4.0.2","load-grunt-tasks":"^3.5.2","minimist":"^1.2.0","mocha":"^8.2.1","sinon":"^4.5.0","terser-webpack-plugin":"^4.2.3","typescript":"^4.0.5","url-search-params":"^0.10.0","webpack":"^4.44.2","webpack-dev-server":"^3.11.0"},"browser":{"./lib/adapters/http.js":"./lib/adapters/xhr.js"},"jsdelivr":"dist/axios.min.js","unpkg":"dist/axios.min.js","typings":"./index.d.ts","dependencies":{"follow-redirects":"^1.14.0"},"bundlesize":[{"path":"./dist/axios.min.js","threshold":"5kB"}]}')},function(e,n,t){"use strict";var a=t(144);function i(e){if("function"!=typeof e)throw new TypeError("executor must be a function.");var n;this.promise=new Promise((function(e){n=e}));var t=this;e((function(e){t.reason||(t.reason=new a(e),n(t.reason))}))}i.prototype.throwIfRequested=function(){if(this.reason)throw this.reason},i.source=function(){var e;return{token:new i((function(n){e=n})),cancel:e}},e.exports=i},function(e,n,t){"use strict";e.exports=function(e){return function(n){return e.apply(null,n)}}},function(e,n,t){"use strict";e.exports=function(e){return"object"==typeof e&&!0===e.isAxiosError}},function(e,n){},function(e,n){function t(e,n){for(var t=0,a=e.length-1;a>=0;a--){var i=e[a];"."===i?e.splice(a,1):".."===i?(e.splice(a,1),t++):t&&(e.splice(a,1),t--)}if(n)for(;t--;t)e.unshift("..");return e}function a(e,n){if(e.filter)return e.filter(n);for(var t=[],a=0;a<e.length;a++)n(e[a],a,e)&&t.push(e[a]);return t}n.resolve=function(){for(var e="",n=!1,i=arguments.length-1;i>=-1&&!n;i--){var r=i>=0?arguments[i]:process.cwd();if("string"!=typeof r)throw new TypeError("Arguments to path.resolve must be strings");r&&(e=r+"/"+e,n="/"===r.charAt(0))}return(n?"/":"")+(e=t(a(e.split("/"),(function(e){return!!e})),!n).join("/"))||"."},n.normalize=function(e){var r=n.isAbsolute(e),o="/"===i(e,-1);return(e=t(a(e.split("/"),(function(e){return!!e})),!r).join("/"))||r||(e="."),e&&o&&(e+="/"),(r?"/":"")+e},n.isAbsolute=function(e){return"/"===e.charAt(0)},n.join=function(){var e=Array.prototype.slice.call(arguments,0);return n.normalize(a(e,(function(e,n){if("string"!=typeof e)throw new TypeError("Arguments to path.join must be strings");return e})).join("/"))},n.relative=function(e,t){function a(e){for(var n=0;n<e.length&&""===e[n];n++);for(var t=e.length-1;t>=0&&""===e[t];t--);return n>t?[]:e.slice(n,t-n+1)}e=n.resolve(e).substr(1),t=n.resolve(t).substr(1);for(var i=a(e.split("/")),r=a(t.split("/")),o=Math.min(i.length,r.length),s=o,l=0;l<o;l++)if(i[l]!==r[l]){s=l;break}var c=[];for(l=s;l<i.length;l++)c.push("..");return(c=c.concat(r.slice(s))).join("/")},n.sep="/",n.delimiter=":",n.dirname=function(e){if("string"!=typeof e&&(e+=""),0===e.length)return".";for(var n=e.charCodeAt(0),t=47===n,a=-1,i=!0,r=e.length-1;r>=1;--r)if(47===(n=e.charCodeAt(r))){if(!i){a=r;break}}else i=!1;return-1===a?t?"/":".":t&&1===a?"/":e.slice(0,a)},n.basename=function(e,n){var t=function(e){"string"!=typeof e&&(e+="");var n,t=0,a=-1,i=!0;for(n=e.length-1;n>=0;--n)if(47===e.charCodeAt(n)){if(!i){t=n+1;break}}else-1===a&&(i=!1,a=n+1);return-1===a?"":e.slice(t,a)}(e);return n&&t.substr(-1*n.length)===n&&(t=t.substr(0,t.length-n.length)),t},n.extname=function(e){"string"!=typeof e&&(e+="");for(var n=-1,t=0,a=-1,i=!0,r=0,o=e.length-1;o>=0;--o){var s=e.charCodeAt(o);if(47!==s)-1===a&&(i=!1,a=o+1),46===s?-1===n?n=o:1!==r&&(r=1):-1!==n&&(r=-1);else if(!i){t=o+1;break}}return-1===n||-1===a||0===r||1===r&&n===a-1&&n===t+1?"":e.slice(n,a)};var i="b"==="ab".substr(-1)?function(e,n,t){return e.substr(n,t)}:function(e,n,t){return n<0&&(n=e.length+n),e.substr(n,t)}},function(e,n,t){"use strict";var a=/[|\\{}()[\]^$+*?.]/g,i=Object.prototype.hasOwnProperty,r=function(e,n){return i.apply(e,[n])};n.escapeRegExpChars=function(e){return e?String(e).replace(a,"\\$&"):""};var o={"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&#34;","'":"&#39;"},s=/[&<>'"]/g;function l(e){return o[e]||e}function c(){return Function.prototype.toString.call(this)+';\nvar _ENCODE_HTML_RULES = {\n      "&": "&amp;"\n    , "<": "&lt;"\n    , ">": "&gt;"\n    , \'"\': "&#34;"\n    , "\'": "&#39;"\n    }\n  , _MATCH_HTML = /[&<>\'"]/g;\nfunction encode_char(c) {\n  return _ENCODE_HTML_RULES[c] || c;\n};\n'}n.escapeXML=function(e){return null==e?"":String(e).replace(s,l)};try{"function"==typeof Object.defineProperty?Object.defineProperty(n.escapeXML,"toString",{value:c}):n.escapeXML.toString=c}catch(e){console.warn("Unable to set escapeXML.toString (is the Function prototype frozen?)")}n.shallowCopy=function(e,n){if(n=n||{},null!=e)for(var t in n)r(n,t)&&"__proto__"!==t&&"constructor"!==t&&(e[t]=n[t]);return e},n.shallowCopyFromList=function(e,n,t){if(t=t||[],n=n||{},null!=e)for(var a=0;a<t.length;a++){var i=t[a];if(void 0!==n[i]){if(!r(n,i))continue;if("__proto__"===i||"constructor"===i)continue;e[i]=n[i]}}return e},n.cache={_data:{},set:function(e,n){this._data[e]=n},get:function(e){return this._data[e]},remove:function(e){delete this._data[e]},reset:function(){this._data={}}},n.hyphenToCamel=function(e){return e.replace(/-[a-z]/g,(function(e){return e[1].toUpperCase()}))},n.createNullProtoObjWherePossible="function"==typeof Object.create?function(){return Object.create(null)}:{__proto__:null}instanceof Object?function(){return{}}:function(){return{__proto__:null}},n.hasOwnOnlyObject=function(e){var t=n.createNullProtoObjWherePossible();for(var a in e)r(e,a)&&(t[a]=e[a]);return t}},function(e){e.exports=JSON.parse('{"name":"ejs","description":"Embedded JavaScript templates","keywords":["template","engine","ejs"],"version":"3.1.10","author":"Matthew Eernisse <mde@fleegix.org> (http://fleegix.org)","license":"Apache-2.0","bin":{"ejs":"./bin/cli.js"},"main":"./lib/ejs.js","jsdelivr":"ejs.min.js","unpkg":"ejs.min.js","repository":{"type":"git","url":"git://github.com/mde/ejs.git"},"bugs":"https://github.com/mde/ejs/issues","homepage":"https://github.com/mde/ejs","dependencies":{"jake":"^10.8.5"},"devDependencies":{"browserify":"^16.5.1","eslint":"^6.8.0","git-directory-deploy":"^1.5.1","jsdoc":"^4.0.2","lru-cache":"^4.0.1","mocha":"^10.2.0","uglify-js":"^3.3.16"},"engines":{"node":">=0.10.0"},"scripts":{"test":"npx jake test"}}')},function(e,n,t){},function(e,n,t){"use strict";t(145)},function(e,n,t){"use strict";t(146)},function(e,n,t){"use strict";t.r(n);t(5),t(13),t(22);var a=Object.freeze({}),i=Array.isArray;function r(e){return null==e}function o(e){return null!=e}function s(e){return!0===e}function l(e){return"string"==typeof e||"number"==typeof e||"symbol"==typeof e||"boolean"==typeof e}function c(e){return"function"==typeof e}function d(e){return null!==e&&"object"==typeof e}var h=Object.prototype.toString;function m(e){return"[object Object]"===h.call(e)}function u(e){return"[object RegExp]"===h.call(e)}function p(e){var n=parseFloat(String(e));return n>=0&&Math.floor(n)===n&&isFinite(e)}function g(e){return o(e)&&"function"==typeof e.then&&"function"==typeof e.catch}function f(e){return null==e?"":Array.isArray(e)||m(e)&&e.toString===h?JSON.stringify(e,y,2):String(e)}function y(e,n){return n&&n.__v_isRef?n.value:n}function b(e){var n=parseFloat(e);return isNaN(n)?e:n}function v(e,n){for(var t=Object.create(null),a=e.split(","),i=0;i<a.length;i++)t[a[i]]=!0;return n?function(e){return t[e.toLowerCase()]}:function(e){return t[e]}}v("slot,component",!0);var w=v("key,ref,slot,slot-scope,is");function _(e,n){var t=e.length;if(t){if(n===e[t-1])return void(e.length=t-1);var a=e.indexOf(n);if(a>-1)return e.splice(a,1)}}var k=Object.prototype.hasOwnProperty;function x(e,n){return k.call(e,n)}function T(e){var n=Object.create(null);return function(t){return n[t]||(n[t]=e(t))}}var z=/-(\w)/g,M=T((function(e){return e.replace(z,(function(e,n){return n?n.toUpperCase():""}))})),P=T((function(e){return e.charAt(0).toUpperCase()+e.slice(1)})),C=/\B([A-Z])/g,A=T((function(e){return e.replace(C,"-$1").toLowerCase()}));var I=Function.prototype.bind?function(e,n){return e.bind(n)}:function(e,n){function t(t){var a=arguments.length;return a?a>1?e.apply(n,arguments):e.call(n,t):e.call(n)}return t._length=e.length,t};function S(e,n){n=n||0;for(var t=e.length-n,a=new Array(t);t--;)a[t]=e[t+n];return a}function L(e,n){for(var t in n)e[t]=n[t];return e}function U(e){for(var n={},t=0;t<e.length;t++)e[t]&&L(n,e[t]);return n}function q(e,n,t){}var E=function(e,n,t){return!1},R=function(e){return e};function D(e,n){if(e===n)return!0;var t=d(e),a=d(n);if(!t||!a)return!t&&!a&&String(e)===String(n);try{var i=Array.isArray(e),r=Array.isArray(n);if(i&&r)return e.length===n.length&&e.every((function(e,t){return D(e,n[t])}));if(e instanceof Date&&n instanceof Date)return e.getTime()===n.getTime();if(i||r)return!1;var o=Object.keys(e),s=Object.keys(n);return o.length===s.length&&o.every((function(t){return D(e[t],n[t])}))}catch(e){return!1}}function O(e,n){for(var t=0;t<e.length;t++)if(D(e[t],n))return t;return-1}function G(e){var n=!1;return function(){n||(n=!0,e.apply(this,arguments))}}function F(e,n){return e===n?0===e&&1/e!=1/n:e==e||n==n}var B=["component","directive","filter"],N=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch","renderTracked","renderTriggered"],j={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:E,isReservedAttr:E,isUnknownElement:E,getTagNamespace:q,parsePlatformTagName:R,mustUseProp:E,async:!0,_lifecycleHooks:N},V=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function H(e){var n=(e+"").charCodeAt(0);return 36===n||95===n}function $(e,n,t,a){Object.defineProperty(e,n,{value:t,enumerable:!!a,writable:!0,configurable:!0})}var W=new RegExp("[^".concat(V.source,".$_\\d]"));var K="__proto__"in{},Q="undefined"!=typeof window,X=Q&&window.navigator.userAgent.toLowerCase(),Y=X&&/msie|trident/.test(X),Z=X&&X.indexOf("msie 9.0")>0,J=X&&X.indexOf("edge/")>0;X&&X.indexOf("android");var ee=X&&/iphone|ipad|ipod|ios/.test(X);X&&/chrome\/\d+/.test(X),X&&/phantomjs/.test(X);var ne,te=X&&X.match(/firefox\/(\d+)/),ae={}.watch,ie=!1;if(Q)try{var re={};Object.defineProperty(re,"passive",{get:function(){ie=!0}}),window.addEventListener("test-passive",null,re)}catch(e){}var oe=function(){return void 0===ne&&(ne=!Q&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),ne},se=Q&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function le(e){return"function"==typeof e&&/native code/.test(e.toString())}var ce,de="undefined"!=typeof Symbol&&le(Symbol)&&"undefined"!=typeof Reflect&&le(Reflect.ownKeys);ce="undefined"!=typeof Set&&le(Set)?Set:function(){function e(){this.set=Object.create(null)}return e.prototype.has=function(e){return!0===this.set[e]},e.prototype.add=function(e){this.set[e]=!0},e.prototype.clear=function(){this.set=Object.create(null)},e}();var he=null;function me(e){void 0===e&&(e=null),e||he&&he._scope.off(),he=e,e&&e._scope.on()}var ue=function(){function e(e,n,t,a,i,r,o,s){this.tag=e,this.data=n,this.children=t,this.text=a,this.elm=i,this.ns=void 0,this.context=r,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=n&&n.key,this.componentOptions=o,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1}return Object.defineProperty(e.prototype,"child",{get:function(){return this.componentInstance},enumerable:!1,configurable:!0}),e}(),pe=function(e){void 0===e&&(e="");var n=new ue;return n.text=e,n.isComment=!0,n};function ge(e){return new ue(void 0,void 0,void 0,String(e))}function fe(e){var n=new ue(e.tag,e.data,e.children&&e.children.slice(),e.text,e.elm,e.context,e.componentOptions,e.asyncFactory);return n.ns=e.ns,n.isStatic=e.isStatic,n.key=e.key,n.isComment=e.isComment,n.fnContext=e.fnContext,n.fnOptions=e.fnOptions,n.fnScopeId=e.fnScopeId,n.asyncMeta=e.asyncMeta,n.isCloned=!0,n}"function"==typeof SuppressedError&&SuppressedError;var ye=0,be=[],ve=function(){function e(){this._pending=!1,this.id=ye++,this.subs=[]}return e.prototype.addSub=function(e){this.subs.push(e)},e.prototype.removeSub=function(e){this.subs[this.subs.indexOf(e)]=null,this._pending||(this._pending=!0,be.push(this))},e.prototype.depend=function(n){e.target&&e.target.addDep(this)},e.prototype.notify=function(e){var n=this.subs.filter((function(e){return e}));for(var t=0,a=n.length;t<a;t++){0,n[t].update()}},e}();ve.target=null;var we=[];function _e(e){we.push(e),ve.target=e}function ke(){we.pop(),ve.target=we[we.length-1]}var xe=Array.prototype,Te=Object.create(xe);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(e){var n=xe[e];$(Te,e,(function(){for(var t=[],a=0;a<arguments.length;a++)t[a]=arguments[a];var i,r=n.apply(this,t),o=this.__ob__;switch(e){case"push":case"unshift":i=t;break;case"splice":i=t.slice(2)}return i&&o.observeArray(i),o.dep.notify(),r}))}));var ze=Object.getOwnPropertyNames(Te),Me={},Pe=!0;function Ce(e){Pe=e}var Ae={notify:q,depend:q,addSub:q,removeSub:q},Ie=function(){function e(e,n,t){if(void 0===n&&(n=!1),void 0===t&&(t=!1),this.value=e,this.shallow=n,this.mock=t,this.dep=t?Ae:new ve,this.vmCount=0,$(e,"__ob__",this),i(e)){if(!t)if(K)e.__proto__=Te;else for(var a=0,r=ze.length;a<r;a++){$(e,s=ze[a],Te[s])}n||this.observeArray(e)}else{var o=Object.keys(e);for(a=0;a<o.length;a++){var s;Le(e,s=o[a],Me,void 0,n,t)}}}return e.prototype.observeArray=function(e){for(var n=0,t=e.length;n<t;n++)Se(e[n],!1,this.mock)},e}();function Se(e,n,t){return e&&x(e,"__ob__")&&e.__ob__ instanceof Ie?e.__ob__:!Pe||!t&&oe()||!i(e)&&!m(e)||!Object.isExtensible(e)||e.__v_skip||Ge(e)||e instanceof ue?void 0:new Ie(e,n,t)}function Le(e,n,t,a,r,o,s){void 0===s&&(s=!1);var l=new ve,c=Object.getOwnPropertyDescriptor(e,n);if(!c||!1!==c.configurable){var d=c&&c.get,h=c&&c.set;d&&!h||t!==Me&&2!==arguments.length||(t=e[n]);var m=r?t&&t.__ob__:Se(t,!1,o);return Object.defineProperty(e,n,{enumerable:!0,configurable:!0,get:function(){var n=d?d.call(e):t;return ve.target&&(l.depend(),m&&(m.dep.depend(),i(n)&&Ee(n))),Ge(n)&&!r?n.value:n},set:function(n){var a=d?d.call(e):t;if(F(a,n)){if(h)h.call(e,n);else{if(d)return;if(!r&&Ge(a)&&!Ge(n))return void(a.value=n);t=n}m=r?n&&n.__ob__:Se(n,!1,o),l.notify()}}}),l}}function Ue(e,n,t){if(!Oe(e)){var a=e.__ob__;return i(e)&&p(n)?(e.length=Math.max(e.length,n),e.splice(n,1,t),a&&!a.shallow&&a.mock&&Se(t,!1,!0),t):n in e&&!(n in Object.prototype)?(e[n]=t,t):e._isVue||a&&a.vmCount?t:a?(Le(a.value,n,t,void 0,a.shallow,a.mock),a.dep.notify(),t):(e[n]=t,t)}}function qe(e,n){if(i(e)&&p(n))e.splice(n,1);else{var t=e.__ob__;e._isVue||t&&t.vmCount||Oe(e)||x(e,n)&&(delete e[n],t&&t.dep.notify())}}function Ee(e){for(var n=void 0,t=0,a=e.length;t<a;t++)(n=e[t])&&n.__ob__&&n.__ob__.dep.depend(),i(n)&&Ee(n)}function Re(e){return De(e,!0),$(e,"__v_isShallow",!0),e}function De(e,n){if(!Oe(e)){Se(e,n,oe());0}}function Oe(e){return!(!e||!e.__v_isReadonly)}function Ge(e){return!(!e||!0!==e.__v_isRef)}function Fe(e,n,t){Object.defineProperty(e,t,{enumerable:!0,configurable:!0,get:function(){var e=n[t];if(Ge(e))return e.value;var a=e&&e.__ob__;return a&&a.dep.depend(),e},set:function(e){var a=n[t];Ge(a)&&!Ge(e)?a.value=e:n[t]=e}})}"".concat("watcher"," callback"),"".concat("watcher"," getter"),"".concat("watcher"," cleanup");var Be;var Ne=function(){function e(e){void 0===e&&(e=!1),this.detached=e,this.active=!0,this.effects=[],this.cleanups=[],this.parent=Be,!e&&Be&&(this.index=(Be.scopes||(Be.scopes=[])).push(this)-1)}return e.prototype.run=function(e){if(this.active){var n=Be;try{return Be=this,e()}finally{Be=n}}else 0},e.prototype.on=function(){Be=this},e.prototype.off=function(){Be=this.parent},e.prototype.stop=function(e){if(this.active){var n=void 0,t=void 0;for(n=0,t=this.effects.length;n<t;n++)this.effects[n].teardown();for(n=0,t=this.cleanups.length;n<t;n++)this.cleanups[n]();if(this.scopes)for(n=0,t=this.scopes.length;n<t;n++)this.scopes[n].stop(!0);if(!this.detached&&this.parent&&!e){var a=this.parent.scopes.pop();a&&a!==this&&(this.parent.scopes[this.index]=a,a.index=this.index)}this.parent=void 0,this.active=!1}},e}();function je(e){var n=e._provided,t=e.$parent&&e.$parent._provided;return t===n?e._provided=Object.create(t):n}var Ve=T((function(e){var n="&"===e.charAt(0),t="~"===(e=n?e.slice(1):e).charAt(0),a="!"===(e=t?e.slice(1):e).charAt(0);return{name:e=a?e.slice(1):e,once:t,capture:a,passive:n}}));function He(e,n){function t(){var e=t.fns;if(!i(e))return Cn(e,null,arguments,n,"v-on handler");for(var a=e.slice(),r=0;r<a.length;r++)Cn(a[r],null,arguments,n,"v-on handler")}return t.fns=e,t}function $e(e,n,t,a,i,o){var l,c,d,h;for(l in e)c=e[l],d=n[l],h=Ve(l),r(c)||(r(d)?(r(c.fns)&&(c=e[l]=He(c,o)),s(h.once)&&(c=e[l]=i(h.name,c,h.capture)),t(h.name,c,h.capture,h.passive,h.params)):c!==d&&(d.fns=c,e[l]=d));for(l in n)r(e[l])&&a((h=Ve(l)).name,n[l],h.capture)}function We(e,n,t){var a;e instanceof ue&&(e=e.data.hook||(e.data.hook={}));var i=e[n];function l(){t.apply(this,arguments),_(a.fns,l)}r(i)?a=He([l]):o(i.fns)&&s(i.merged)?(a=i).fns.push(l):a=He([i,l]),a.merged=!0,e[n]=a}function Ke(e,n,t,a,i){if(o(n)){if(x(n,t))return e[t]=n[t],i||delete n[t],!0;if(x(n,a))return e[t]=n[a],i||delete n[a],!0}return!1}function Qe(e){return l(e)?[ge(e)]:i(e)?function e(n,t){var a,c,d,h,m=[];for(a=0;a<n.length;a++)r(c=n[a])||"boolean"==typeof c||(d=m.length-1,h=m[d],i(c)?c.length>0&&(Xe((c=e(c,"".concat(t||"","_").concat(a)))[0])&&Xe(h)&&(m[d]=ge(h.text+c[0].text),c.shift()),m.push.apply(m,c)):l(c)?Xe(h)?m[d]=ge(h.text+c):""!==c&&m.push(ge(c)):Xe(c)&&Xe(h)?m[d]=ge(h.text+c.text):(s(n._isVList)&&o(c.tag)&&r(c.key)&&o(t)&&(c.key="__vlist".concat(t,"_").concat(a,"__")),m.push(c)));return m}(e):void 0}function Xe(e){return o(e)&&o(e.text)&&!1===e.isComment}function Ye(e,n){var t,a,r,s,l=null;if(i(e)||"string"==typeof e)for(l=new Array(e.length),t=0,a=e.length;t<a;t++)l[t]=n(e[t],t);else if("number"==typeof e)for(l=new Array(e),t=0;t<e;t++)l[t]=n(t+1,t);else if(d(e))if(de&&e[Symbol.iterator]){l=[];for(var c=e[Symbol.iterator](),h=c.next();!h.done;)l.push(n(h.value,l.length)),h=c.next()}else for(r=Object.keys(e),l=new Array(r.length),t=0,a=r.length;t<a;t++)s=r[t],l[t]=n(e[s],s,t);return o(l)||(l=[]),l._isVList=!0,l}function Ze(e,n,t,a){var i,r=this.$scopedSlots[e];r?(t=t||{},a&&(t=L(L({},a),t)),i=r(t)||(c(n)?n():n)):i=this.$slots[e]||(c(n)?n():n);var o=t&&t.slot;return o?this.$createElement("template",{slot:o},i):i}function Je(e){return St(this.$options,"filters",e,!0)||R}function en(e,n){return i(e)?-1===e.indexOf(n):e!==n}function nn(e,n,t,a,i){var r=j.keyCodes[n]||t;return i&&a&&!j.keyCodes[n]?en(i,a):r?en(r,e):a?A(a)!==n:void 0===e}function tn(e,n,t,a,r){if(t)if(d(t)){i(t)&&(t=U(t));var o=void 0,s=function(i){if("class"===i||"style"===i||w(i))o=e;else{var s=e.attrs&&e.attrs.type;o=a||j.mustUseProp(n,s,i)?e.domProps||(e.domProps={}):e.attrs||(e.attrs={})}var l=M(i),c=A(i);l in o||c in o||(o[i]=t[i],r&&((e.on||(e.on={}))["update:".concat(i)]=function(e){t[i]=e}))};for(var l in t)s(l)}else;return e}function an(e,n){var t=this._staticTrees||(this._staticTrees=[]),a=t[e];return a&&!n||on(a=t[e]=this.$options.staticRenderFns[e].call(this._renderProxy,this._c,this),"__static__".concat(e),!1),a}function rn(e,n,t){return on(e,"__once__".concat(n).concat(t?"_".concat(t):""),!0),e}function on(e,n,t){if(i(e))for(var a=0;a<e.length;a++)e[a]&&"string"!=typeof e[a]&&sn(e[a],"".concat(n,"_").concat(a),t);else sn(e,n,t)}function sn(e,n,t){e.isStatic=!0,e.key=n,e.isOnce=t}function ln(e,n){if(n)if(m(n)){var t=e.on=e.on?L({},e.on):{};for(var a in n){var i=t[a],r=n[a];t[a]=i?[].concat(i,r):r}}else;return e}function cn(e,n,t,a){n=n||{$stable:!t};for(var r=0;r<e.length;r++){var o=e[r];i(o)?cn(o,n,t):o&&(o.proxy&&(o.fn.proxy=!0),n[o.key]=o.fn)}return a&&(n.$key=a),n}function dn(e,n){for(var t=0;t<n.length;t+=2){var a=n[t];"string"==typeof a&&a&&(e[n[t]]=n[t+1])}return e}function hn(e,n){return"string"==typeof e?n+e:e}function mn(e){e._o=rn,e._n=b,e._s=f,e._l=Ye,e._t=Ze,e._q=D,e._i=O,e._m=an,e._f=Je,e._k=nn,e._b=tn,e._v=ge,e._e=pe,e._u=cn,e._g=ln,e._d=dn,e._p=hn}function un(e,n){if(!e||!e.length)return{};for(var t={},a=0,i=e.length;a<i;a++){var r=e[a],o=r.data;if(o&&o.attrs&&o.attrs.slot&&delete o.attrs.slot,r.context!==n&&r.fnContext!==n||!o||null==o.slot)(t.default||(t.default=[])).push(r);else{var s=o.slot,l=t[s]||(t[s]=[]);"template"===r.tag?l.push.apply(l,r.children||[]):l.push(r)}}for(var c in t)t[c].every(pn)&&delete t[c];return t}function pn(e){return e.isComment&&!e.asyncFactory||" "===e.text}function gn(e){return e.isComment&&e.asyncFactory}function fn(e,n,t,i){var r,o=Object.keys(t).length>0,s=n?!!n.$stable:!o,l=n&&n.$key;if(n){if(n._normalized)return n._normalized;if(s&&i&&i!==a&&l===i.$key&&!o&&!i.$hasNormal)return i;for(var c in r={},n)n[c]&&"$"!==c[0]&&(r[c]=yn(e,t,c,n[c]))}else r={};for(var d in t)d in r||(r[d]=bn(t,d));return n&&Object.isExtensible(n)&&(n._normalized=r),$(r,"$stable",s),$(r,"$key",l),$(r,"$hasNormal",o),r}function yn(e,n,t,a){var r=function(){var n=he;me(e);var t=arguments.length?a.apply(null,arguments):a({}),r=(t=t&&"object"==typeof t&&!i(t)?[t]:Qe(t))&&t[0];return me(n),t&&(!r||1===t.length&&r.isComment&&!gn(r))?void 0:t};return a.proxy&&Object.defineProperty(n,t,{get:r,enumerable:!0,configurable:!0}),r}function bn(e,n){return function(){return e[n]}}function vn(e){return{get attrs(){if(!e._attrsProxy){var n=e._attrsProxy={};$(n,"_v_attr_proxy",!0),wn(n,e.$attrs,a,e,"$attrs")}return e._attrsProxy},get listeners(){e._listenersProxy||wn(e._listenersProxy={},e.$listeners,a,e,"$listeners");return e._listenersProxy},get slots(){return function(e){e._slotsProxy||kn(e._slotsProxy={},e.$scopedSlots);return e._slotsProxy}(e)},emit:I(e.$emit,e),expose:function(n){n&&Object.keys(n).forEach((function(t){return Fe(e,n,t)}))}}}function wn(e,n,t,a,i){var r=!1;for(var o in n)o in e?n[o]!==t[o]&&(r=!0):(r=!0,_n(e,o,a,i));for(var o in e)o in n||(r=!0,delete e[o]);return r}function _n(e,n,t,a){Object.defineProperty(e,n,{enumerable:!0,configurable:!0,get:function(){return t[a][n]}})}function kn(e,n){for(var t in n)e[t]=n[t];for(var t in e)t in n||delete e[t]}var xn=null;function Tn(e,n){return(e.__esModule||de&&"Module"===e[Symbol.toStringTag])&&(e=e.default),d(e)?n.extend(e):e}function zn(e){if(i(e))for(var n=0;n<e.length;n++){var t=e[n];if(o(t)&&(o(t.componentOptions)||gn(t)))return t}}function Mn(e,n,t,a,h,m){return(i(t)||l(t))&&(h=a,a=t,t=void 0),s(m)&&(h=2),function(e,n,t,a,l){if(o(t)&&o(t.__ob__))return pe();o(t)&&o(t.is)&&(n=t.is);if(!n)return pe();0;i(a)&&c(a[0])&&((t=t||{}).scopedSlots={default:a[0]},a.length=0);2===l?a=Qe(a):1===l&&(a=function(e){for(var n=0;n<e.length;n++)if(i(e[n]))return Array.prototype.concat.apply([],e);return e}(a));var h,m;if("string"==typeof n){var u=void 0;m=e.$vnode&&e.$vnode.ns||j.getTagNamespace(n),h=j.isReservedTag(n)?new ue(j.parsePlatformTagName(n),t,a,void 0,void 0,e):t&&t.pre||!o(u=St(e.$options,"components",n))?new ue(n,t,a,void 0,void 0,e):_t(u,t,e,a,n)}else h=_t(n,t,e,a);return i(h)?h:o(h)?(o(m)&&function e(n,t,a){n.ns=t,"foreignObject"===n.tag&&(t=void 0,a=!0);if(o(n.children))for(var i=0,l=n.children.length;i<l;i++){var c=n.children[i];o(c.tag)&&(r(c.ns)||s(a)&&"svg"!==c.tag)&&e(c,t,a)}}(h,m),o(t)&&function(e){d(e.style)&&jn(e.style);d(e.class)&&jn(e.class)}(t),h):pe()}(e,n,t,a,h)}function Pn(e,n,t){_e();try{if(n)for(var a=n;a=a.$parent;){var i=a.$options.errorCaptured;if(i)for(var r=0;r<i.length;r++)try{if(!1===i[r].call(a,e,n,t))return}catch(e){An(e,a,"errorCaptured hook")}}An(e,n,t)}finally{ke()}}function Cn(e,n,t,a,i){var r;try{(r=t?e.apply(n,t):e.call(n))&&!r._isVue&&g(r)&&!r._handled&&(r.catch((function(e){return Pn(e,a,i+" (Promise/async)")})),r._handled=!0)}catch(e){Pn(e,a,i)}return r}function An(e,n,t){if(j.errorHandler)try{return j.errorHandler.call(null,e,n,t)}catch(n){n!==e&&In(n,null,"config.errorHandler")}In(e,n,t)}function In(e,n,t){if(!Q||"undefined"==typeof console)throw e;console.error(e)}var Sn,Ln=!1,Un=[],qn=!1;function En(){qn=!1;var e=Un.slice(0);Un.length=0;for(var n=0;n<e.length;n++)e[n]()}if("undefined"!=typeof Promise&&le(Promise)){var Rn=Promise.resolve();Sn=function(){Rn.then(En),ee&&setTimeout(q)},Ln=!0}else if(Y||"undefined"==typeof MutationObserver||!le(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Sn="undefined"!=typeof setImmediate&&le(setImmediate)?function(){setImmediate(En)}:function(){setTimeout(En,0)};else{var Dn=1,On=new MutationObserver(En),Gn=document.createTextNode(String(Dn));On.observe(Gn,{characterData:!0}),Sn=function(){Dn=(Dn+1)%2,Gn.data=String(Dn)},Ln=!0}function Fn(e,n){var t;if(Un.push((function(){if(e)try{e.call(n)}catch(e){Pn(e,n,"nextTick")}else t&&t(n)})),qn||(qn=!0,Sn()),!e&&"undefined"!=typeof Promise)return new Promise((function(e){t=e}))}function Bn(e){return function(n,t){if(void 0===t&&(t=he),t)return function(e,n,t){var a=e.$options;a[n]=Pt(a[n],t)}(t,e,n)}}Bn("beforeMount"),Bn("mounted"),Bn("beforeUpdate"),Bn("updated"),Bn("beforeDestroy"),Bn("destroyed"),Bn("activated"),Bn("deactivated"),Bn("serverPrefetch"),Bn("renderTracked"),Bn("renderTriggered"),Bn("errorCaptured");var Nn=new ce;function jn(e){return function e(n,t){var a,r,o=i(n);if(!o&&!d(n)||n.__v_skip||Object.isFrozen(n)||n instanceof ue)return;if(n.__ob__){var s=n.__ob__.dep.id;if(t.has(s))return;t.add(s)}if(o)for(a=n.length;a--;)e(n[a],t);else if(Ge(n))e(n.value,t);else for(r=Object.keys(n),a=r.length;a--;)e(n[r[a]],t)}(e,Nn),Nn.clear(),e}var Vn,Hn=0,$n=function(){function e(e,n,t,a,i){var r,o;r=this,void 0===(o=Be&&!Be._vm?Be:e?e._scope:void 0)&&(o=Be),o&&o.active&&o.effects.push(r),(this.vm=e)&&i&&(e._watcher=this),a?(this.deep=!!a.deep,this.user=!!a.user,this.lazy=!!a.lazy,this.sync=!!a.sync,this.before=a.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++Hn,this.active=!0,this.post=!1,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new ce,this.newDepIds=new ce,this.expression="",c(n)?this.getter=n:(this.getter=function(e){if(!W.test(e)){var n=e.split(".");return function(e){for(var t=0;t<n.length;t++){if(!e)return;e=e[n[t]]}return e}}}(n),this.getter||(this.getter=q)),this.value=this.lazy?void 0:this.get()}return e.prototype.get=function(){var e;_e(this);var n=this.vm;try{e=this.getter.call(n,n)}catch(e){if(!this.user)throw e;Pn(e,n,'getter for watcher "'.concat(this.expression,'"'))}finally{this.deep&&jn(e),ke(),this.cleanupDeps()}return e},e.prototype.addDep=function(e){var n=e.id;this.newDepIds.has(n)||(this.newDepIds.add(n),this.newDeps.push(e),this.depIds.has(n)||e.addSub(this))},e.prototype.cleanupDeps=function(){for(var e=this.deps.length;e--;){var n=this.deps[e];this.newDepIds.has(n.id)||n.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},e.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():ut(this)},e.prototype.run=function(){if(this.active){var e=this.get();if(e!==this.value||d(e)||this.deep){var n=this.value;if(this.value=e,this.user){var t='callback for watcher "'.concat(this.expression,'"');Cn(this.cb,this.vm,[e,n],this.vm,t)}else this.cb.call(this.vm,e,n)}}},e.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},e.prototype.depend=function(){for(var e=this.deps.length;e--;)this.deps[e].depend()},e.prototype.teardown=function(){if(this.vm&&!this.vm._isBeingDestroyed&&_(this.vm._scope.effects,this),this.active){for(var e=this.deps.length;e--;)this.deps[e].removeSub(this);this.active=!1,this.onStop&&this.onStop()}},e}();function Wn(e,n){Vn.$on(e,n)}function Kn(e,n){Vn.$off(e,n)}function Qn(e,n){var t=Vn;return function a(){var i=n.apply(null,arguments);null!==i&&t.$off(e,a)}}function Xn(e,n,t){Vn=e,$e(n,t||{},Wn,Kn,Qn,e),Vn=void 0}var Yn=null;function Zn(e){var n=Yn;return Yn=e,function(){Yn=n}}function Jn(e){for(;e&&(e=e.$parent);)if(e._inactive)return!0;return!1}function et(e,n){if(n){if(e._directInactive=!1,Jn(e))return}else if(e._directInactive)return;if(e._inactive||null===e._inactive){e._inactive=!1;for(var t=0;t<e.$children.length;t++)et(e.$children[t]);nt(e,"activated")}}function nt(e,n,t,a){void 0===a&&(a=!0),_e();var i=he,r=Be;a&&me(e);var o=e.$options[n],s="".concat(n," hook");if(o)for(var l=0,c=o.length;l<c;l++)Cn(o[l],e,t||null,e,s);e._hasHookEvent&&e.$emit("hook:"+n),a&&(me(i),r&&r.on()),ke()}var tt=[],at=[],it={},rt=!1,ot=!1,st=0;var lt=0,ct=Date.now;if(Q&&!Y){var dt=window.performance;dt&&"function"==typeof dt.now&&ct()>document.createEvent("Event").timeStamp&&(ct=function(){return dt.now()})}var ht=function(e,n){if(e.post){if(!n.post)return 1}else if(n.post)return-1;return e.id-n.id};function mt(){var e,n;for(lt=ct(),ot=!0,tt.sort(ht),st=0;st<tt.length;st++)(e=tt[st]).before&&e.before(),n=e.id,it[n]=null,e.run();var t=at.slice(),a=tt.slice();st=tt.length=at.length=0,it={},rt=ot=!1,function(e){for(var n=0;n<e.length;n++)e[n]._inactive=!0,et(e[n],!0)}(t),function(e){var n=e.length;for(;n--;){var t=e[n],a=t.vm;a&&a._watcher===t&&a._isMounted&&!a._isDestroyed&&nt(a,"updated")}}(a),function(){for(var e=0;e<be.length;e++){var n=be[e];n.subs=n.subs.filter((function(e){return e})),n._pending=!1}be.length=0}(),se&&j.devtools&&se.emit("flush")}function ut(e){var n=e.id;if(null==it[n]&&(e!==ve.target||!e.noRecurse)){if(it[n]=!0,ot){for(var t=tt.length-1;t>st&&tt[t].id>e.id;)t--;tt.splice(t+1,0,e)}else tt.push(e);rt||(rt=!0,Fn(mt))}}function pt(e,n){if(e){for(var t=Object.create(null),a=de?Reflect.ownKeys(e):Object.keys(e),i=0;i<a.length;i++){var r=a[i];if("__ob__"!==r){var o=e[r].from;if(o in n._provided)t[r]=n._provided[o];else if("default"in e[r]){var s=e[r].default;t[r]=c(s)?s.call(n):s}else 0}}return t}}function gt(e,n,t,r,o){var l,c=this,d=o.options;x(r,"_uid")?(l=Object.create(r))._original=r:(l=r,r=r._original);var h=s(d._compiled),m=!h;this.data=e,this.props=n,this.children=t,this.parent=r,this.listeners=e.on||a,this.injections=pt(d.inject,r),this.slots=function(){return c.$slots||fn(r,e.scopedSlots,c.$slots=un(t,r)),c.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return fn(r,e.scopedSlots,this.slots())}}),h&&(this.$options=d,this.$slots=this.slots(),this.$scopedSlots=fn(r,e.scopedSlots,this.$slots)),d._scopeId?this._c=function(e,n,t,a){var o=Mn(l,e,n,t,a,m);return o&&!i(o)&&(o.fnScopeId=d._scopeId,o.fnContext=r),o}:this._c=function(e,n,t,a){return Mn(l,e,n,t,a,m)}}function ft(e,n,t,a,i){var r=fe(e);return r.fnContext=t,r.fnOptions=a,n.slot&&((r.data||(r.data={})).slot=n.slot),r}function yt(e,n){for(var t in n)e[M(t)]=n[t]}function bt(e){return e.name||e.__name||e._componentTag}mn(gt.prototype);var vt={init:function(e,n){if(e.componentInstance&&!e.componentInstance._isDestroyed&&e.data.keepAlive){var t=e;vt.prepatch(t,t)}else{(e.componentInstance=function(e,n){var t={_isComponent:!0,_parentVnode:e,parent:n},a=e.data.inlineTemplate;o(a)&&(t.render=a.render,t.staticRenderFns=a.staticRenderFns);return new e.componentOptions.Ctor(t)}(e,Yn)).$mount(n?e.elm:void 0,n)}},prepatch:function(e,n){var t=n.componentOptions;!function(e,n,t,i,r){var o=i.data.scopedSlots,s=e.$scopedSlots,l=!!(o&&!o.$stable||s!==a&&!s.$stable||o&&e.$scopedSlots.$key!==o.$key||!o&&e.$scopedSlots.$key),c=!!(r||e.$options._renderChildren||l),d=e.$vnode;e.$options._parentVnode=i,e.$vnode=i,e._vnode&&(e._vnode.parent=i),e.$options._renderChildren=r;var h=i.data.attrs||a;e._attrsProxy&&wn(e._attrsProxy,h,d.data&&d.data.attrs||a,e,"$attrs")&&(c=!0),e.$attrs=h,t=t||a;var m=e.$options._parentListeners;if(e._listenersProxy&&wn(e._listenersProxy,t,m||a,e,"$listeners"),e.$listeners=e.$options._parentListeners=t,Xn(e,t,m),n&&e.$options.props){Ce(!1);for(var u=e._props,p=e.$options._propKeys||[],g=0;g<p.length;g++){var f=p[g],y=e.$options.props;u[f]=Lt(f,y,n,e)}Ce(!0),e.$options.propsData=n}c&&(e.$slots=un(r,i.context),e.$forceUpdate())}(n.componentInstance=e.componentInstance,t.propsData,t.listeners,n,t.children)},insert:function(e){var n,t=e.context,a=e.componentInstance;a._isMounted||(a._isMounted=!0,nt(a,"mounted")),e.data.keepAlive&&(t._isMounted?((n=a)._inactive=!1,at.push(n)):et(a,!0))},destroy:function(e){var n=e.componentInstance;n._isDestroyed||(e.data.keepAlive?function e(n,t){if(!(t&&(n._directInactive=!0,Jn(n))||n._inactive)){n._inactive=!0;for(var a=0;a<n.$children.length;a++)e(n.$children[a]);nt(n,"deactivated")}}(n,!0):n.$destroy())}},wt=Object.keys(vt);function _t(e,n,t,l,c){if(!r(e)){var h=t.$options._base;if(d(e)&&(e=h.extend(e)),"function"==typeof e){var m;if(r(e.cid)&&void 0===(e=function(e,n){if(s(e.error)&&o(e.errorComp))return e.errorComp;if(o(e.resolved))return e.resolved;var t=xn;if(t&&o(e.owners)&&-1===e.owners.indexOf(t)&&e.owners.push(t),s(e.loading)&&o(e.loadingComp))return e.loadingComp;if(t&&!o(e.owners)){var a=e.owners=[t],i=!0,l=null,c=null;t.$on("hook:destroyed",(function(){return _(a,t)}));var h=function(e){for(var n=0,t=a.length;n<t;n++)a[n].$forceUpdate();e&&(a.length=0,null!==l&&(clearTimeout(l),l=null),null!==c&&(clearTimeout(c),c=null))},m=G((function(t){e.resolved=Tn(t,n),i?a.length=0:h(!0)})),u=G((function(n){o(e.errorComp)&&(e.error=!0,h(!0))})),p=e(m,u);return d(p)&&(g(p)?r(e.resolved)&&p.then(m,u):g(p.component)&&(p.component.then(m,u),o(p.error)&&(e.errorComp=Tn(p.error,n)),o(p.loading)&&(e.loadingComp=Tn(p.loading,n),0===p.delay?e.loading=!0:l=setTimeout((function(){l=null,r(e.resolved)&&r(e.error)&&(e.loading=!0,h(!1))}),p.delay||200)),o(p.timeout)&&(c=setTimeout((function(){c=null,r(e.resolved)&&u(null)}),p.timeout)))),i=!1,e.loading?e.loadingComp:e.resolved}}(m=e,h)))return function(e,n,t,a,i){var r=pe();return r.asyncFactory=e,r.asyncMeta={data:n,context:t,children:a,tag:i},r}(m,n,t,l,c);n=n||{},$t(e),o(n.model)&&function(e,n){var t=e.model&&e.model.prop||"value",a=e.model&&e.model.event||"input";(n.attrs||(n.attrs={}))[t]=n.model.value;var r=n.on||(n.on={}),s=r[a],l=n.model.callback;o(s)?(i(s)?-1===s.indexOf(l):s!==l)&&(r[a]=[l].concat(s)):r[a]=l}(e.options,n);var u=function(e,n,t){var a=n.options.props;if(!r(a)){var i={},s=e.attrs,l=e.props;if(o(s)||o(l))for(var c in a){var d=A(c);Ke(i,l,c,d,!0)||Ke(i,s,c,d,!1)}return i}}(n,e);if(s(e.options.functional))return function(e,n,t,r,s){var l=e.options,c={},d=l.props;if(o(d))for(var h in d)c[h]=Lt(h,d,n||a);else o(t.attrs)&&yt(c,t.attrs),o(t.props)&&yt(c,t.props);var m=new gt(t,c,s,r,e),u=l.render.call(null,m._c,m);if(u instanceof ue)return ft(u,t,m.parent,l,m);if(i(u)){for(var p=Qe(u)||[],g=new Array(p.length),f=0;f<p.length;f++)g[f]=ft(p[f],t,m.parent,l,m);return g}}(e,u,n,t,l);var p=n.on;if(n.on=n.nativeOn,s(e.options.abstract)){var f=n.slot;n={},f&&(n.slot=f)}!function(e){for(var n=e.hook||(e.hook={}),t=0;t<wt.length;t++){var a=wt[t],i=n[a],r=vt[a];i===r||i&&i._merged||(n[a]=i?kt(r,i):r)}}(n);var y=bt(e.options)||c;return new ue("vue-component-".concat(e.cid).concat(y?"-".concat(y):""),n,void 0,void 0,void 0,t,{Ctor:e,propsData:u,listeners:p,tag:c,children:l},m)}}}function kt(e,n){var t=function(t,a){e(t,a),n(t,a)};return t._merged=!0,t}var xt=q,Tt=j.optionMergeStrategies;function zt(e,n,t){if(void 0===t&&(t=!0),!n)return e;for(var a,i,r,o=de?Reflect.ownKeys(n):Object.keys(n),s=0;s<o.length;s++)"__ob__"!==(a=o[s])&&(i=e[a],r=n[a],t&&x(e,a)?i!==r&&m(i)&&m(r)&&zt(i,r):Ue(e,a,r));return e}function Mt(e,n,t){return t?function(){var a=c(n)?n.call(t,t):n,i=c(e)?e.call(t,t):e;return a?zt(a,i):i}:n?e?function(){return zt(c(n)?n.call(this,this):n,c(e)?e.call(this,this):e)}:n:e}function Pt(e,n){var t=n?e?e.concat(n):i(n)?n:[n]:e;return t?function(e){for(var n=[],t=0;t<e.length;t++)-1===n.indexOf(e[t])&&n.push(e[t]);return n}(t):t}function Ct(e,n,t,a){var i=Object.create(e||null);return n?L(i,n):i}Tt.data=function(e,n,t){return t?Mt(e,n,t):n&&"function"!=typeof n?e:Mt(e,n)},N.forEach((function(e){Tt[e]=Pt})),B.forEach((function(e){Tt[e+"s"]=Ct})),Tt.watch=function(e,n,t,a){if(e===ae&&(e=void 0),n===ae&&(n=void 0),!n)return Object.create(e||null);if(!e)return n;var r={};for(var o in L(r,e),n){var s=r[o],l=n[o];s&&!i(s)&&(s=[s]),r[o]=s?s.concat(l):i(l)?l:[l]}return r},Tt.props=Tt.methods=Tt.inject=Tt.computed=function(e,n,t,a){if(!e)return n;var i=Object.create(null);return L(i,e),n&&L(i,n),i},Tt.provide=function(e,n){return e?function(){var t=Object.create(null);return zt(t,c(e)?e.call(this):e),n&&zt(t,c(n)?n.call(this):n,!1),t}:n};var At=function(e,n){return void 0===n?e:n};function It(e,n,t){if(c(n)&&(n=n.options),function(e,n){var t=e.props;if(t){var a,r,o={};if(i(t))for(a=t.length;a--;)"string"==typeof(r=t[a])&&(o[M(r)]={type:null});else if(m(t))for(var s in t)r=t[s],o[M(s)]=m(r)?r:{type:r};else 0;e.props=o}}(n),function(e,n){var t=e.inject;if(t){var a=e.inject={};if(i(t))for(var r=0;r<t.length;r++)a[t[r]]={from:t[r]};else if(m(t))for(var o in t){var s=t[o];a[o]=m(s)?L({from:o},s):{from:s}}else 0}}(n),function(e){var n=e.directives;if(n)for(var t in n){var a=n[t];c(a)&&(n[t]={bind:a,update:a})}}(n),!n._base&&(n.extends&&(e=It(e,n.extends,t)),n.mixins))for(var a=0,r=n.mixins.length;a<r;a++)e=It(e,n.mixins[a],t);var o,s={};for(o in e)l(o);for(o in n)x(e,o)||l(o);function l(a){var i=Tt[a]||At;s[a]=i(e[a],n[a],t,a)}return s}function St(e,n,t,a){if("string"==typeof t){var i=e[n];if(x(i,t))return i[t];var r=M(t);if(x(i,r))return i[r];var o=P(r);return x(i,o)?i[o]:i[t]||i[r]||i[o]}}function Lt(e,n,t,a){var i=n[e],r=!x(t,e),o=t[e],s=Rt(Boolean,i.type);if(s>-1)if(r&&!x(i,"default"))o=!1;else if(""===o||o===A(e)){var l=Rt(String,i.type);(l<0||s<l)&&(o=!0)}if(void 0===o){o=function(e,n,t){if(!x(n,"default"))return;var a=n.default;0;if(e&&e.$options.propsData&&void 0===e.$options.propsData[t]&&void 0!==e._props[t])return e._props[t];return c(a)&&"Function"!==qt(n.type)?a.call(e):a}(a,i,e);var d=Pe;Ce(!0),Se(o),Ce(d)}return o}var Ut=/^\s*function (\w+)/;function qt(e){var n=e&&e.toString().match(Ut);return n?n[1]:""}function Et(e,n){return qt(e)===qt(n)}function Rt(e,n){if(!i(n))return Et(n,e)?0:-1;for(var t=0,a=n.length;t<a;t++)if(Et(n[t],e))return t;return-1}var Dt={enumerable:!0,configurable:!0,get:q,set:q};function Ot(e,n,t){Dt.get=function(){return this[n][t]},Dt.set=function(e){this[n][t]=e},Object.defineProperty(e,t,Dt)}function Gt(e){var n=e.$options;if(n.props&&function(e,n){var t=e.$options.propsData||{},a=e._props=Re({}),i=e.$options._propKeys=[];e.$parent&&Ce(!1);var r=function(r){i.push(r);var o=Lt(r,n,t,e);Le(a,r,o,void 0,!0),r in e||Ot(e,"_props",r)};for(var o in n)r(o);Ce(!0)}(e,n.props),function(e){var n=e.$options,t=n.setup;if(t){var a=e._setupContext=vn(e);me(e),_e();var i=Cn(t,null,[e._props||Re({}),a],e,"setup");if(ke(),me(),c(i))n.render=i;else if(d(i))if(e._setupState=i,i.__sfc){var r=e._setupProxy={};for(var o in i)"__sfc"!==o&&Fe(r,i,o)}else for(var o in i)H(o)||Fe(e,i,o);else 0}}(e),n.methods&&function(e,n){e.$options.props;for(var t in n)e[t]="function"!=typeof n[t]?q:I(n[t],e)}(e,n.methods),n.data)!function(e){var n=e.$options.data;m(n=e._data=c(n)?function(e,n){_e();try{return e.call(n,n)}catch(e){return Pn(e,n,"data()"),{}}finally{ke()}}(n,e):n||{})||(n={});var t=Object.keys(n),a=e.$options.props,i=(e.$options.methods,t.length);for(;i--;){var r=t[i];0,a&&x(a,r)||H(r)||Ot(e,"_data",r)}var o=Se(n);o&&o.vmCount++}(e);else{var t=Se(e._data={});t&&t.vmCount++}n.computed&&function(e,n){var t=e._computedWatchers=Object.create(null),a=oe();for(var i in n){var r=n[i],o=c(r)?r:r.get;0,a||(t[i]=new $n(e,o||q,q,Ft)),i in e||Bt(e,i,r)}}(e,n.computed),n.watch&&n.watch!==ae&&function(e,n){for(var t in n){var a=n[t];if(i(a))for(var r=0;r<a.length;r++)Vt(e,t,a[r]);else Vt(e,t,a)}}(e,n.watch)}var Ft={lazy:!0};function Bt(e,n,t){var a=!oe();c(t)?(Dt.get=a?Nt(n):jt(t),Dt.set=q):(Dt.get=t.get?a&&!1!==t.cache?Nt(n):jt(t.get):q,Dt.set=t.set||q),Object.defineProperty(e,n,Dt)}function Nt(e){return function(){var n=this._computedWatchers&&this._computedWatchers[e];if(n)return n.dirty&&n.evaluate(),ve.target&&n.depend(),n.value}}function jt(e){return function(){return e.call(this,this)}}function Vt(e,n,t,a){return m(t)&&(a=t,t=t.handler),"string"==typeof t&&(t=e[t]),e.$watch(n,t,a)}var Ht=0;function $t(e){var n=e.options;if(e.super){var t=$t(e.super);if(t!==e.superOptions){e.superOptions=t;var a=function(e){var n,t=e.options,a=e.sealedOptions;for(var i in t)t[i]!==a[i]&&(n||(n={}),n[i]=t[i]);return n}(e);a&&L(e.extendOptions,a),(n=e.options=It(t,e.extendOptions)).name&&(n.components[n.name]=e)}}return n}function Wt(e){this._init(e)}function Kt(e){e.cid=0;var n=1;e.extend=function(e){e=e||{};var t=this,a=t.cid,i=e._Ctor||(e._Ctor={});if(i[a])return i[a];var r=bt(e)||bt(t.options);var o=function(e){this._init(e)};return(o.prototype=Object.create(t.prototype)).constructor=o,o.cid=n++,o.options=It(t.options,e),o.super=t,o.options.props&&function(e){var n=e.options.props;for(var t in n)Ot(e.prototype,"_props",t)}(o),o.options.computed&&function(e){var n=e.options.computed;for(var t in n)Bt(e.prototype,t,n[t])}(o),o.extend=t.extend,o.mixin=t.mixin,o.use=t.use,B.forEach((function(e){o[e]=t[e]})),r&&(o.options.components[r]=o),o.superOptions=t.options,o.extendOptions=e,o.sealedOptions=L({},o.options),i[a]=o,o}}function Qt(e){return e&&(bt(e.Ctor.options)||e.tag)}function Xt(e,n){return i(e)?e.indexOf(n)>-1:"string"==typeof e?e.split(",").indexOf(n)>-1:!!u(e)&&e.test(n)}function Yt(e,n){var t=e.cache,a=e.keys,i=e._vnode,r=e.$vnode;for(var o in t){var s=t[o];if(s){var l=s.name;l&&!n(l)&&Zt(t,o,a,i)}}r.componentOptions.children=void 0}function Zt(e,n,t,a){var i=e[n];!i||a&&i.tag===a.tag||i.componentInstance.$destroy(),e[n]=null,_(t,n)}Wt.prototype._init=function(e){var n=this;n._uid=Ht++,n._isVue=!0,n.__v_skip=!0,n._scope=new Ne(!0),n._scope.parent=void 0,n._scope._vm=!0,e&&e._isComponent?function(e,n){var t=e.$options=Object.create(e.constructor.options),a=n._parentVnode;t.parent=n.parent,t._parentVnode=a;var i=a.componentOptions;t.propsData=i.propsData,t._parentListeners=i.listeners,t._renderChildren=i.children,t._componentTag=i.tag,n.render&&(t.render=n.render,t.staticRenderFns=n.staticRenderFns)}(n,e):n.$options=It($t(n.constructor),e||{},n),n._renderProxy=n,n._self=n,function(e){var n=e.$options,t=n.parent;if(t&&!n.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(e)}e.$parent=t,e.$root=t?t.$root:e,e.$children=[],e.$refs={},e._provided=t?t._provided:Object.create(null),e._watcher=null,e._inactive=null,e._directInactive=!1,e._isMounted=!1,e._isDestroyed=!1,e._isBeingDestroyed=!1}(n),function(e){e._events=Object.create(null),e._hasHookEvent=!1;var n=e.$options._parentListeners;n&&Xn(e,n)}(n),function(e){e._vnode=null,e._staticTrees=null;var n=e.$options,t=e.$vnode=n._parentVnode,i=t&&t.context;e.$slots=un(n._renderChildren,i),e.$scopedSlots=t?fn(e.$parent,t.data.scopedSlots,e.$slots):a,e._c=function(n,t,a,i){return Mn(e,n,t,a,i,!1)},e.$createElement=function(n,t,a,i){return Mn(e,n,t,a,i,!0)};var r=t&&t.data;Le(e,"$attrs",r&&r.attrs||a,null,!0),Le(e,"$listeners",n._parentListeners||a,null,!0)}(n),nt(n,"beforeCreate",void 0,!1),function(e){var n=pt(e.$options.inject,e);n&&(Ce(!1),Object.keys(n).forEach((function(t){Le(e,t,n[t])})),Ce(!0))}(n),Gt(n),function(e){var n=e.$options.provide;if(n){var t=c(n)?n.call(e):n;if(!d(t))return;for(var a=je(e),i=de?Reflect.ownKeys(t):Object.keys(t),r=0;r<i.length;r++){var o=i[r];Object.defineProperty(a,o,Object.getOwnPropertyDescriptor(t,o))}}}(n),nt(n,"created"),n.$options.el&&n.$mount(n.$options.el)},function(e){var n={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(e.prototype,"$data",n),Object.defineProperty(e.prototype,"$props",t),e.prototype.$set=Ue,e.prototype.$delete=qe,e.prototype.$watch=function(e,n,t){if(m(n))return Vt(this,e,n,t);(t=t||{}).user=!0;var a=new $n(this,e,n,t);if(t.immediate){var i='callback for immediate watcher "'.concat(a.expression,'"');_e(),Cn(n,this,[a.value],this,i),ke()}return function(){a.teardown()}}}(Wt),function(e){var n=/^hook:/;e.prototype.$on=function(e,t){var a=this;if(i(e))for(var r=0,o=e.length;r<o;r++)a.$on(e[r],t);else(a._events[e]||(a._events[e]=[])).push(t),n.test(e)&&(a._hasHookEvent=!0);return a},e.prototype.$once=function(e,n){var t=this;function a(){t.$off(e,a),n.apply(t,arguments)}return a.fn=n,t.$on(e,a),t},e.prototype.$off=function(e,n){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(i(e)){for(var a=0,r=e.length;a<r;a++)t.$off(e[a],n);return t}var o,s=t._events[e];if(!s)return t;if(!n)return t._events[e]=null,t;for(var l=s.length;l--;)if((o=s[l])===n||o.fn===n){s.splice(l,1);break}return t},e.prototype.$emit=function(e){var n=this,t=n._events[e];if(t){t=t.length>1?S(t):t;for(var a=S(arguments,1),i='event handler for "'.concat(e,'"'),r=0,o=t.length;r<o;r++)Cn(t[r],n,a,n,i)}return n}}(Wt),function(e){e.prototype._update=function(e,n){var t=this,a=t.$el,i=t._vnode,r=Zn(t);t._vnode=e,t.$el=i?t.__patch__(i,e):t.__patch__(t.$el,e,n,!1),r(),a&&(a.__vue__=null),t.$el&&(t.$el.__vue__=t);for(var o=t;o&&o.$vnode&&o.$parent&&o.$vnode===o.$parent._vnode;)o.$parent.$el=o.$el,o=o.$parent},e.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},e.prototype.$destroy=function(){var e=this;if(!e._isBeingDestroyed){nt(e,"beforeDestroy"),e._isBeingDestroyed=!0;var n=e.$parent;!n||n._isBeingDestroyed||e.$options.abstract||_(n.$children,e),e._scope.stop(),e._data.__ob__&&e._data.__ob__.vmCount--,e._isDestroyed=!0,e.__patch__(e._vnode,null),nt(e,"destroyed"),e.$off(),e.$el&&(e.$el.__vue__=null),e.$vnode&&(e.$vnode.parent=null)}}}(Wt),function(e){mn(e.prototype),e.prototype.$nextTick=function(e){return Fn(e,this)},e.prototype._render=function(){var e=this,n=e.$options,t=n.render,a=n._parentVnode;a&&e._isMounted&&(e.$scopedSlots=fn(e.$parent,a.data.scopedSlots,e.$slots,e.$scopedSlots),e._slotsProxy&&kn(e._slotsProxy,e.$scopedSlots)),e.$vnode=a;var r,o=he,s=xn;try{me(e),xn=e,r=t.call(e._renderProxy,e.$createElement)}catch(n){Pn(n,e,"render"),r=e._vnode}finally{xn=s,me(o)}return i(r)&&1===r.length&&(r=r[0]),r instanceof ue||(r=pe()),r.parent=a,r}}(Wt);var Jt=[String,RegExp,Array],ea={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:Jt,exclude:Jt,max:[String,Number]},methods:{cacheVNode:function(){var e=this.cache,n=this.keys,t=this.vnodeToCache,a=this.keyToCache;if(t){var i=t.tag,r=t.componentInstance,o=t.componentOptions;e[a]={name:Qt(o),tag:i,componentInstance:r},n.push(a),this.max&&n.length>parseInt(this.max)&&Zt(e,n[0],n,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var e in this.cache)Zt(this.cache,e,this.keys)},mounted:function(){var e=this;this.cacheVNode(),this.$watch("include",(function(n){Yt(e,(function(e){return Xt(n,e)}))})),this.$watch("exclude",(function(n){Yt(e,(function(e){return!Xt(n,e)}))}))},updated:function(){this.cacheVNode()},render:function(){var e=this.$slots.default,n=zn(e),t=n&&n.componentOptions;if(t){var a=Qt(t),i=this.include,r=this.exclude;if(i&&(!a||!Xt(i,a))||r&&a&&Xt(r,a))return n;var o=this.cache,s=this.keys,l=null==n.key?t.Ctor.cid+(t.tag?"::".concat(t.tag):""):n.key;o[l]?(n.componentInstance=o[l].componentInstance,_(s,l),s.push(l)):(this.vnodeToCache=n,this.keyToCache=l),n.data.keepAlive=!0}return n||e&&e[0]}}};!function(e){var n={get:function(){return j}};Object.defineProperty(e,"config",n),e.util={warn:xt,extend:L,mergeOptions:It,defineReactive:Le},e.set=Ue,e.delete=qe,e.nextTick=Fn,e.observable=function(e){return Se(e),e},e.options=Object.create(null),B.forEach((function(n){e.options[n+"s"]=Object.create(null)})),e.options._base=e,L(e.options.components,ea),function(e){e.use=function(e){var n=this._installedPlugins||(this._installedPlugins=[]);if(n.indexOf(e)>-1)return this;var t=S(arguments,1);return t.unshift(this),c(e.install)?e.install.apply(e,t):c(e)&&e.apply(null,t),n.push(e),this}}(e),function(e){e.mixin=function(e){return this.options=It(this.options,e),this}}(e),Kt(e),function(e){B.forEach((function(n){e[n]=function(e,t){return t?("component"===n&&m(t)&&(t.name=t.name||e,t=this.options._base.extend(t)),"directive"===n&&c(t)&&(t={bind:t,update:t}),this.options[n+"s"][e]=t,t):this.options[n+"s"][e]}}))}(e)}(Wt),Object.defineProperty(Wt.prototype,"$isServer",{get:oe}),Object.defineProperty(Wt.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Wt,"FunctionalRenderContext",{value:gt}),Wt.version="2.7.16";var na=v("style,class"),ta=v("input,textarea,option,select,progress"),aa=v("contenteditable,draggable,spellcheck"),ia=v("events,caret,typing,plaintext-only"),ra=v("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),oa="http://www.w3.org/1999/xlink",sa=function(e){return":"===e.charAt(5)&&"xlink"===e.slice(0,5)},la=function(e){return sa(e)?e.slice(6,e.length):""},ca=function(e){return null==e||!1===e};function da(e){for(var n=e.data,t=e,a=e;o(a.componentInstance);)(a=a.componentInstance._vnode)&&a.data&&(n=ha(a.data,n));for(;o(t=t.parent);)t&&t.data&&(n=ha(n,t.data));return function(e,n){if(o(e)||o(n))return ma(e,ua(n));return""}(n.staticClass,n.class)}function ha(e,n){return{staticClass:ma(e.staticClass,n.staticClass),class:o(e.class)?[e.class,n.class]:n.class}}function ma(e,n){return e?n?e+" "+n:e:n||""}function ua(e){return Array.isArray(e)?function(e){for(var n,t="",a=0,i=e.length;a<i;a++)o(n=ua(e[a]))&&""!==n&&(t&&(t+=" "),t+=n);return t}(e):d(e)?function(e){var n="";for(var t in e)e[t]&&(n&&(n+=" "),n+=t);return n}(e):"string"==typeof e?e:""}var pa={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},ga=v("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),fa=v("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),ya=function(e){return ga(e)||fa(e)};var ba=Object.create(null);var va=v("text,number,password,search,email,tel,url");var wa=Object.freeze({__proto__:null,createElement:function(e,n){var t=document.createElement(e);return"select"!==e||n.data&&n.data.attrs&&void 0!==n.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(e,n){return document.createElementNS(pa[e],n)},createTextNode:function(e){return document.createTextNode(e)},createComment:function(e){return document.createComment(e)},insertBefore:function(e,n,t){e.insertBefore(n,t)},removeChild:function(e,n){e.removeChild(n)},appendChild:function(e,n){e.appendChild(n)},parentNode:function(e){return e.parentNode},nextSibling:function(e){return e.nextSibling},tagName:function(e){return e.tagName},setTextContent:function(e,n){e.textContent=n},setStyleScope:function(e,n){e.setAttribute(n,"")}}),_a={create:function(e,n){ka(n)},update:function(e,n){e.data.ref!==n.data.ref&&(ka(e,!0),ka(n))},destroy:function(e){ka(e,!0)}};function ka(e,n){var t=e.data.ref;if(o(t)){var a=e.context,r=e.componentInstance||e.elm,s=n?null:r,l=n?void 0:r;if(c(t))Cn(t,a,[s],a,"template ref function");else{var d=e.data.refInFor,h="string"==typeof t||"number"==typeof t,m=Ge(t),u=a.$refs;if(h||m)if(d){var p=h?u[t]:t.value;n?i(p)&&_(p,r):i(p)?p.includes(r)||p.push(r):h?(u[t]=[r],xa(a,t,u[t])):t.value=[r]}else if(h){if(n&&u[t]!==r)return;u[t]=l,xa(a,t,s)}else if(m){if(n&&t.value!==r)return;t.value=s}else 0}}}function xa(e,n,t){var a=e._setupState;a&&x(a,n)&&(Ge(a[n])?a[n].value=t:a[n]=t)}var Ta=new ue("",{},[]),za=["create","activate","update","remove","destroy"];function Ma(e,n){return e.key===n.key&&e.asyncFactory===n.asyncFactory&&(e.tag===n.tag&&e.isComment===n.isComment&&o(e.data)===o(n.data)&&function(e,n){if("input"!==e.tag)return!0;var t,a=o(t=e.data)&&o(t=t.attrs)&&t.type,i=o(t=n.data)&&o(t=t.attrs)&&t.type;return a===i||va(a)&&va(i)}(e,n)||s(e.isAsyncPlaceholder)&&r(n.asyncFactory.error))}function Pa(e,n,t){var a,i,r={};for(a=n;a<=t;++a)o(i=e[a].key)&&(r[i]=a);return r}var Ca={create:Aa,update:Aa,destroy:function(e){Aa(e,Ta)}};function Aa(e,n){(e.data.directives||n.data.directives)&&function(e,n){var t,a,i,r=e===Ta,o=n===Ta,s=Sa(e.data.directives,e.context),l=Sa(n.data.directives,n.context),c=[],d=[];for(t in l)a=s[t],i=l[t],a?(i.oldValue=a.value,i.oldArg=a.arg,Ua(i,"update",n,e),i.def&&i.def.componentUpdated&&d.push(i)):(Ua(i,"bind",n,e),i.def&&i.def.inserted&&c.push(i));if(c.length){var h=function(){for(var t=0;t<c.length;t++)Ua(c[t],"inserted",n,e)};r?We(n,"insert",h):h()}d.length&&We(n,"postpatch",(function(){for(var t=0;t<d.length;t++)Ua(d[t],"componentUpdated",n,e)}));if(!r)for(t in s)l[t]||Ua(s[t],"unbind",e,e,o)}(e,n)}var Ia=Object.create(null);function Sa(e,n){var t,a,i=Object.create(null);if(!e)return i;for(t=0;t<e.length;t++){if((a=e[t]).modifiers||(a.modifiers=Ia),i[La(a)]=a,n._setupState&&n._setupState.__sfc){var r=a.def||St(n,"_setupState","v-"+a.name);a.def="function"==typeof r?{bind:r,update:r}:r}a.def=a.def||St(n.$options,"directives",a.name)}return i}function La(e){return e.rawName||"".concat(e.name,".").concat(Object.keys(e.modifiers||{}).join("."))}function Ua(e,n,t,a,i){var r=e.def&&e.def[n];if(r)try{r(t.elm,e,t,a,i)}catch(a){Pn(a,t.context,"directive ".concat(e.name," ").concat(n," hook"))}}var qa=[_a,Ca];function Ea(e,n){var t=n.componentOptions;if(!(o(t)&&!1===t.Ctor.options.inheritAttrs||r(e.data.attrs)&&r(n.data.attrs))){var a,i,l=n.elm,c=e.data.attrs||{},d=n.data.attrs||{};for(a in(o(d.__ob__)||s(d._v_attr_proxy))&&(d=n.data.attrs=L({},d)),d)i=d[a],c[a]!==i&&Ra(l,a,i,n.data.pre);for(a in(Y||J)&&d.value!==c.value&&Ra(l,"value",d.value),c)r(d[a])&&(sa(a)?l.removeAttributeNS(oa,la(a)):aa(a)||l.removeAttribute(a))}}function Ra(e,n,t,a){a||e.tagName.indexOf("-")>-1?Da(e,n,t):ra(n)?ca(t)?e.removeAttribute(n):(t="allowfullscreen"===n&&"EMBED"===e.tagName?"true":n,e.setAttribute(n,t)):aa(n)?e.setAttribute(n,function(e,n){return ca(n)||"false"===n?"false":"contenteditable"===e&&ia(n)?n:"true"}(n,t)):sa(n)?ca(t)?e.removeAttributeNS(oa,la(n)):e.setAttributeNS(oa,n,t):Da(e,n,t)}function Da(e,n,t){if(ca(t))e.removeAttribute(n);else{if(Y&&!Z&&"TEXTAREA"===e.tagName&&"placeholder"===n&&""!==t&&!e.__ieph){var a=function(n){n.stopImmediatePropagation(),e.removeEventListener("input",a)};e.addEventListener("input",a),e.__ieph=!0}e.setAttribute(n,t)}}var Oa={create:Ea,update:Ea};function Ga(e,n){var t=n.elm,a=n.data,i=e.data;if(!(r(a.staticClass)&&r(a.class)&&(r(i)||r(i.staticClass)&&r(i.class)))){var s=da(n),l=t._transitionClasses;o(l)&&(s=ma(s,ua(l))),s!==t._prevClass&&(t.setAttribute("class",s),t._prevClass=s)}}var Fa,Ba={create:Ga,update:Ga};function Na(e,n,t){var a=Fa;return function i(){var r=n.apply(null,arguments);null!==r&&Ha(e,i,t,a)}}var ja=Ln&&!(te&&Number(te[1])<=53);function Va(e,n,t,a){if(ja){var i=lt,r=n;n=r._wrapper=function(e){if(e.target===e.currentTarget||e.timeStamp>=i||e.timeStamp<=0||e.target.ownerDocument!==document)return r.apply(this,arguments)}}Fa.addEventListener(e,n,ie?{capture:t,passive:a}:t)}function Ha(e,n,t,a){(a||Fa).removeEventListener(e,n._wrapper||n,t)}function $a(e,n){if(!r(e.data.on)||!r(n.data.on)){var t=n.data.on||{},a=e.data.on||{};Fa=n.elm||e.elm,function(e){if(o(e.__r)){var n=Y?"change":"input";e[n]=[].concat(e.__r,e[n]||[]),delete e.__r}o(e.__c)&&(e.change=[].concat(e.__c,e.change||[]),delete e.__c)}(t),$e(t,a,Va,Ha,Na,n.context),Fa=void 0}}var Wa,Ka={create:$a,update:$a,destroy:function(e){return $a(e,Ta)}};function Qa(e,n){if(!r(e.data.domProps)||!r(n.data.domProps)){var t,a,i=n.elm,l=e.data.domProps||{},c=n.data.domProps||{};for(t in(o(c.__ob__)||s(c._v_attr_proxy))&&(c=n.data.domProps=L({},c)),l)t in c||(i[t]="");for(t in c){if(a=c[t],"textContent"===t||"innerHTML"===t){if(n.children&&(n.children.length=0),a===l[t])continue;1===i.childNodes.length&&i.removeChild(i.childNodes[0])}if("value"===t&&"PROGRESS"!==i.tagName){i._value=a;var d=r(a)?"":String(a);Xa(i,d)&&(i.value=d)}else if("innerHTML"===t&&fa(i.tagName)&&r(i.innerHTML)){(Wa=Wa||document.createElement("div")).innerHTML="<svg>".concat(a,"</svg>");for(var h=Wa.firstChild;i.firstChild;)i.removeChild(i.firstChild);for(;h.firstChild;)i.appendChild(h.firstChild)}else if(a!==l[t])try{i[t]=a}catch(e){}}}}function Xa(e,n){return!e.composing&&("OPTION"===e.tagName||function(e,n){var t=!0;try{t=document.activeElement!==e}catch(e){}return t&&e.value!==n}(e,n)||function(e,n){var t=e.value,a=e._vModifiers;if(o(a)){if(a.number)return b(t)!==b(n);if(a.trim)return t.trim()!==n.trim()}return t!==n}(e,n))}var Ya={create:Qa,update:Qa},Za=T((function(e){var n={},t=/:(.+)/;return e.split(/;(?![^(]*\))/g).forEach((function(e){if(e){var a=e.split(t);a.length>1&&(n[a[0].trim()]=a[1].trim())}})),n}));function Ja(e){var n=ei(e.style);return e.staticStyle?L(e.staticStyle,n):n}function ei(e){return Array.isArray(e)?U(e):"string"==typeof e?Za(e):e}var ni,ti=/^--/,ai=/\s*!important$/,ii=function(e,n,t){if(ti.test(n))e.style.setProperty(n,t);else if(ai.test(t))e.style.setProperty(A(n),t.replace(ai,""),"important");else{var a=oi(n);if(Array.isArray(t))for(var i=0,r=t.length;i<r;i++)e.style[a]=t[i];else e.style[a]=t}},ri=["Webkit","Moz","ms"],oi=T((function(e){if(ni=ni||document.createElement("div").style,"filter"!==(e=M(e))&&e in ni)return e;for(var n=e.charAt(0).toUpperCase()+e.slice(1),t=0;t<ri.length;t++){var a=ri[t]+n;if(a in ni)return a}}));function si(e,n){var t=n.data,a=e.data;if(!(r(t.staticStyle)&&r(t.style)&&r(a.staticStyle)&&r(a.style))){var i,s,l=n.elm,c=a.staticStyle,d=a.normalizedStyle||a.style||{},h=c||d,m=ei(n.data.style)||{};n.data.normalizedStyle=o(m.__ob__)?L({},m):m;var u=function(e,n){var t,a={};if(n)for(var i=e;i.componentInstance;)(i=i.componentInstance._vnode)&&i.data&&(t=Ja(i.data))&&L(a,t);(t=Ja(e.data))&&L(a,t);for(var r=e;r=r.parent;)r.data&&(t=Ja(r.data))&&L(a,t);return a}(n,!0);for(s in h)r(u[s])&&ii(l,s,"");for(s in u)i=u[s],ii(l,s,null==i?"":i)}}var li={create:si,update:si},ci=/\s+/;function di(e,n){if(n&&(n=n.trim()))if(e.classList)n.indexOf(" ")>-1?n.split(ci).forEach((function(n){return e.classList.add(n)})):e.classList.add(n);else{var t=" ".concat(e.getAttribute("class")||""," ");t.indexOf(" "+n+" ")<0&&e.setAttribute("class",(t+n).trim())}}function hi(e,n){if(n&&(n=n.trim()))if(e.classList)n.indexOf(" ")>-1?n.split(ci).forEach((function(n){return e.classList.remove(n)})):e.classList.remove(n),e.classList.length||e.removeAttribute("class");else{for(var t=" ".concat(e.getAttribute("class")||""," "),a=" "+n+" ";t.indexOf(a)>=0;)t=t.replace(a," ");(t=t.trim())?e.setAttribute("class",t):e.removeAttribute("class")}}function mi(e){if(e){if("object"==typeof e){var n={};return!1!==e.css&&L(n,ui(e.name||"v")),L(n,e),n}return"string"==typeof e?ui(e):void 0}}var ui=T((function(e){return{enterClass:"".concat(e,"-enter"),enterToClass:"".concat(e,"-enter-to"),enterActiveClass:"".concat(e,"-enter-active"),leaveClass:"".concat(e,"-leave"),leaveToClass:"".concat(e,"-leave-to"),leaveActiveClass:"".concat(e,"-leave-active")}})),pi=Q&&!Z,gi="transition",fi="transitionend",yi="animation",bi="animationend";pi&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(gi="WebkitTransition",fi="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(yi="WebkitAnimation",bi="webkitAnimationEnd"));var vi=Q?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(e){return e()};function wi(e){vi((function(){vi(e)}))}function _i(e,n){var t=e._transitionClasses||(e._transitionClasses=[]);t.indexOf(n)<0&&(t.push(n),di(e,n))}function ki(e,n){e._transitionClasses&&_(e._transitionClasses,n),hi(e,n)}function xi(e,n,t){var a=zi(e,n),i=a.type,r=a.timeout,o=a.propCount;if(!i)return t();var s="transition"===i?fi:bi,l=0,c=function(){e.removeEventListener(s,d),t()},d=function(n){n.target===e&&++l>=o&&c()};setTimeout((function(){l<o&&c()}),r+1),e.addEventListener(s,d)}var Ti=/\b(transform|all)(,|$)/;function zi(e,n){var t,a=window.getComputedStyle(e),i=(a[gi+"Delay"]||"").split(", "),r=(a[gi+"Duration"]||"").split(", "),o=Mi(i,r),s=(a[yi+"Delay"]||"").split(", "),l=(a[yi+"Duration"]||"").split(", "),c=Mi(s,l),d=0,h=0;return"transition"===n?o>0&&(t="transition",d=o,h=r.length):"animation"===n?c>0&&(t="animation",d=c,h=l.length):h=(t=(d=Math.max(o,c))>0?o>c?"transition":"animation":null)?"transition"===t?r.length:l.length:0,{type:t,timeout:d,propCount:h,hasTransform:"transition"===t&&Ti.test(a[gi+"Property"])}}function Mi(e,n){for(;e.length<n.length;)e=e.concat(e);return Math.max.apply(null,n.map((function(n,t){return Pi(n)+Pi(e[t])})))}function Pi(e){return 1e3*Number(e.slice(0,-1).replace(",","."))}function Ci(e,n){var t=e.elm;o(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var a=mi(e.data.transition);if(!r(a)&&!o(t._enterCb)&&1===t.nodeType){for(var i=a.css,s=a.type,l=a.enterClass,h=a.enterToClass,m=a.enterActiveClass,u=a.appearClass,p=a.appearToClass,g=a.appearActiveClass,f=a.beforeEnter,y=a.enter,v=a.afterEnter,w=a.enterCancelled,_=a.beforeAppear,k=a.appear,x=a.afterAppear,T=a.appearCancelled,z=a.duration,M=Yn,P=Yn.$vnode;P&&P.parent;)M=P.context,P=P.parent;var C=!M._isMounted||!e.isRootInsert;if(!C||k||""===k){var A=C&&u?u:l,I=C&&g?g:m,S=C&&p?p:h,L=C&&_||f,U=C&&c(k)?k:y,q=C&&x||v,E=C&&T||w,R=b(d(z)?z.enter:z);0;var D=!1!==i&&!Z,O=Si(U),F=t._enterCb=G((function(){D&&(ki(t,S),ki(t,I)),F.cancelled?(D&&ki(t,A),E&&E(t)):q&&q(t),t._enterCb=null}));e.data.show||We(e,"insert",(function(){var n=t.parentNode,a=n&&n._pending&&n._pending[e.key];a&&a.tag===e.tag&&a.elm._leaveCb&&a.elm._leaveCb(),U&&U(t,F)})),L&&L(t),D&&(_i(t,A),_i(t,I),wi((function(){ki(t,A),F.cancelled||(_i(t,S),O||(Ii(R)?setTimeout(F,R):xi(t,s,F)))}))),e.data.show&&(n&&n(),U&&U(t,F)),D||O||F()}}}function Ai(e,n){var t=e.elm;o(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var a=mi(e.data.transition);if(r(a)||1!==t.nodeType)return n();if(!o(t._leaveCb)){var i=a.css,s=a.type,l=a.leaveClass,c=a.leaveToClass,h=a.leaveActiveClass,m=a.beforeLeave,u=a.leave,p=a.afterLeave,g=a.leaveCancelled,f=a.delayLeave,y=a.duration,v=!1!==i&&!Z,w=Si(u),_=b(d(y)?y.leave:y);0;var k=t._leaveCb=G((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[e.key]=null),v&&(ki(t,c),ki(t,h)),k.cancelled?(v&&ki(t,l),g&&g(t)):(n(),p&&p(t)),t._leaveCb=null}));f?f(x):x()}function x(){k.cancelled||(!e.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[e.key]=e),m&&m(t),v&&(_i(t,l),_i(t,h),wi((function(){ki(t,l),k.cancelled||(_i(t,c),w||(Ii(_)?setTimeout(k,_):xi(t,s,k)))}))),u&&u(t,k),v||w||k())}}function Ii(e){return"number"==typeof e&&!isNaN(e)}function Si(e){if(r(e))return!1;var n=e.fns;return o(n)?Si(Array.isArray(n)?n[0]:n):(e._length||e.length)>1}function Li(e,n){!0!==n.data.show&&Ci(n)}var Ui=function(e){var n,t,a={},c=e.modules,d=e.nodeOps;for(n=0;n<za.length;++n)for(a[za[n]]=[],t=0;t<c.length;++t)o(c[t][za[n]])&&a[za[n]].push(c[t][za[n]]);function h(e){var n=d.parentNode(e);o(n)&&d.removeChild(n,e)}function m(e,n,t,i,r,l,c){if(o(e.elm)&&o(l)&&(e=l[c]=fe(e)),e.isRootInsert=!r,!function(e,n,t,i){var r=e.data;if(o(r)){var l=o(e.componentInstance)&&r.keepAlive;if(o(r=r.hook)&&o(r=r.init)&&r(e,!1),o(e.componentInstance))return u(e,n),p(t,e.elm,i),s(l)&&function(e,n,t,i){var r,s=e;for(;s.componentInstance;)if(s=s.componentInstance._vnode,o(r=s.data)&&o(r=r.transition)){for(r=0;r<a.activate.length;++r)a.activate[r](Ta,s);n.push(s);break}p(t,e.elm,i)}(e,n,t,i),!0}}(e,n,t,i)){var h=e.data,m=e.children,f=e.tag;o(f)?(e.elm=e.ns?d.createElementNS(e.ns,f):d.createElement(f,e),b(e),g(e,m,n),o(h)&&y(e,n),p(t,e.elm,i)):s(e.isComment)?(e.elm=d.createComment(e.text),p(t,e.elm,i)):(e.elm=d.createTextNode(e.text),p(t,e.elm,i))}}function u(e,n){o(e.data.pendingInsert)&&(n.push.apply(n,e.data.pendingInsert),e.data.pendingInsert=null),e.elm=e.componentInstance.$el,f(e)?(y(e,n),b(e)):(ka(e),n.push(e))}function p(e,n,t){o(e)&&(o(t)?d.parentNode(t)===e&&d.insertBefore(e,n,t):d.appendChild(e,n))}function g(e,n,t){if(i(n)){0;for(var a=0;a<n.length;++a)m(n[a],t,e.elm,null,!0,n,a)}else l(e.text)&&d.appendChild(e.elm,d.createTextNode(String(e.text)))}function f(e){for(;e.componentInstance;)e=e.componentInstance._vnode;return o(e.tag)}function y(e,t){for(var i=0;i<a.create.length;++i)a.create[i](Ta,e);o(n=e.data.hook)&&(o(n.create)&&n.create(Ta,e),o(n.insert)&&t.push(e))}function b(e){var n;if(o(n=e.fnScopeId))d.setStyleScope(e.elm,n);else for(var t=e;t;)o(n=t.context)&&o(n=n.$options._scopeId)&&d.setStyleScope(e.elm,n),t=t.parent;o(n=Yn)&&n!==e.context&&n!==e.fnContext&&o(n=n.$options._scopeId)&&d.setStyleScope(e.elm,n)}function w(e,n,t,a,i,r){for(;a<=i;++a)m(t[a],r,e,n,!1,t,a)}function _(e){var n,t,i=e.data;if(o(i))for(o(n=i.hook)&&o(n=n.destroy)&&n(e),n=0;n<a.destroy.length;++n)a.destroy[n](e);if(o(n=e.children))for(t=0;t<e.children.length;++t)_(e.children[t])}function k(e,n,t){for(;n<=t;++n){var a=e[n];o(a)&&(o(a.tag)?(x(a),_(a)):h(a.elm))}}function x(e,n){if(o(n)||o(e.data)){var t,i=a.remove.length+1;for(o(n)?n.listeners+=i:n=function(e,n){function t(){0==--t.listeners&&h(e)}return t.listeners=n,t}(e.elm,i),o(t=e.componentInstance)&&o(t=t._vnode)&&o(t.data)&&x(t,n),t=0;t<a.remove.length;++t)a.remove[t](e,n);o(t=e.data.hook)&&o(t=t.remove)?t(e,n):n()}else h(e.elm)}function T(e,n,t,a){for(var i=t;i<a;i++){var r=n[i];if(o(r)&&Ma(e,r))return i}}function z(e,n,t,i,l,c){if(e!==n){o(n.elm)&&o(i)&&(n=i[l]=fe(n));var h=n.elm=e.elm;if(s(e.isAsyncPlaceholder))o(n.asyncFactory.resolved)?C(e.elm,n,t):n.isAsyncPlaceholder=!0;else if(s(n.isStatic)&&s(e.isStatic)&&n.key===e.key&&(s(n.isCloned)||s(n.isOnce)))n.componentInstance=e.componentInstance;else{var u,p=n.data;o(p)&&o(u=p.hook)&&o(u=u.prepatch)&&u(e,n);var g=e.children,y=n.children;if(o(p)&&f(n)){for(u=0;u<a.update.length;++u)a.update[u](e,n);o(u=p.hook)&&o(u=u.update)&&u(e,n)}r(n.text)?o(g)&&o(y)?g!==y&&function(e,n,t,a,i){var s,l,c,h=0,u=0,p=n.length-1,g=n[0],f=n[p],y=t.length-1,b=t[0],v=t[y],_=!i;for(0;h<=p&&u<=y;)r(g)?g=n[++h]:r(f)?f=n[--p]:Ma(g,b)?(z(g,b,a,t,u),g=n[++h],b=t[++u]):Ma(f,v)?(z(f,v,a,t,y),f=n[--p],v=t[--y]):Ma(g,v)?(z(g,v,a,t,y),_&&d.insertBefore(e,g.elm,d.nextSibling(f.elm)),g=n[++h],v=t[--y]):Ma(f,b)?(z(f,b,a,t,u),_&&d.insertBefore(e,f.elm,g.elm),f=n[--p],b=t[++u]):(r(s)&&(s=Pa(n,h,p)),r(l=o(b.key)?s[b.key]:T(b,n,h,p))?m(b,a,e,g.elm,!1,t,u):Ma(c=n[l],b)?(z(c,b,a,t,u),n[l]=void 0,_&&d.insertBefore(e,c.elm,g.elm)):m(b,a,e,g.elm,!1,t,u),b=t[++u]);h>p?w(e,r(t[y+1])?null:t[y+1].elm,t,u,y,a):u>y&&k(n,h,p)}(h,g,y,t,c):o(y)?(o(e.text)&&d.setTextContent(h,""),w(h,null,y,0,y.length-1,t)):o(g)?k(g,0,g.length-1):o(e.text)&&d.setTextContent(h,""):e.text!==n.text&&d.setTextContent(h,n.text),o(p)&&o(u=p.hook)&&o(u=u.postpatch)&&u(e,n)}}}function M(e,n,t){if(s(t)&&o(e.parent))e.parent.data.pendingInsert=n;else for(var a=0;a<n.length;++a)n[a].data.hook.insert(n[a])}var P=v("attrs,class,staticClass,staticStyle,key");function C(e,n,t,a){var i,r=n.tag,l=n.data,c=n.children;if(a=a||l&&l.pre,n.elm=e,s(n.isComment)&&o(n.asyncFactory))return n.isAsyncPlaceholder=!0,!0;if(o(l)&&(o(i=l.hook)&&o(i=i.init)&&i(n,!0),o(i=n.componentInstance)))return u(n,t),!0;if(o(r)){if(o(c))if(e.hasChildNodes())if(o(i=l)&&o(i=i.domProps)&&o(i=i.innerHTML)){if(i!==e.innerHTML)return!1}else{for(var d=!0,h=e.firstChild,m=0;m<c.length;m++){if(!h||!C(h,c[m],t,a)){d=!1;break}h=h.nextSibling}if(!d||h)return!1}else g(n,c,t);if(o(l)){var p=!1;for(var f in l)if(!P(f)){p=!0,y(n,t);break}!p&&l.class&&jn(l.class)}}else e.data!==n.text&&(e.data=n.text);return!0}return function(e,n,t,i){if(!r(n)){var l,c=!1,h=[];if(r(e))c=!0,m(n,h);else{var u=o(e.nodeType);if(!u&&Ma(e,n))z(e,n,h,null,null,i);else{if(u){if(1===e.nodeType&&e.hasAttribute("data-server-rendered")&&(e.removeAttribute("data-server-rendered"),t=!0),s(t)&&C(e,n,h))return M(n,h,!0),e;l=e,e=new ue(d.tagName(l).toLowerCase(),{},[],void 0,l)}var p=e.elm,g=d.parentNode(p);if(m(n,h,p._leaveCb?null:g,d.nextSibling(p)),o(n.parent))for(var y=n.parent,b=f(n);y;){for(var v=0;v<a.destroy.length;++v)a.destroy[v](y);if(y.elm=n.elm,b){for(var w=0;w<a.create.length;++w)a.create[w](Ta,y);var x=y.data.hook.insert;if(x.merged)for(var T=x.fns.slice(1),P=0;P<T.length;P++)T[P]()}else ka(y);y=y.parent}o(g)?k([e],0,0):o(e.tag)&&_(e)}}return M(n,h,c),n.elm}o(e)&&_(e)}}({nodeOps:wa,modules:[Oa,Ba,Ka,Ya,li,Q?{create:Li,activate:Li,remove:function(e,n){!0!==e.data.show?Ai(e,n):n()}}:{}].concat(qa)});Z&&document.addEventListener("selectionchange",(function(){var e=document.activeElement;e&&e.vmodel&&Bi(e,"input")}));var qi={inserted:function(e,n,t,a){"select"===t.tag?(a.elm&&!a.elm._vOptions?We(t,"postpatch",(function(){qi.componentUpdated(e,n,t)})):Ei(e,n,t.context),e._vOptions=[].map.call(e.options,Oi)):("textarea"===t.tag||va(e.type))&&(e._vModifiers=n.modifiers,n.modifiers.lazy||(e.addEventListener("compositionstart",Gi),e.addEventListener("compositionend",Fi),e.addEventListener("change",Fi),Z&&(e.vmodel=!0)))},componentUpdated:function(e,n,t){if("select"===t.tag){Ei(e,n,t.context);var a=e._vOptions,i=e._vOptions=[].map.call(e.options,Oi);if(i.some((function(e,n){return!D(e,a[n])})))(e.multiple?n.value.some((function(e){return Di(e,i)})):n.value!==n.oldValue&&Di(n.value,i))&&Bi(e,"change")}}};function Ei(e,n,t){Ri(e,n,t),(Y||J)&&setTimeout((function(){Ri(e,n,t)}),0)}function Ri(e,n,t){var a=n.value,i=e.multiple;if(!i||Array.isArray(a)){for(var r,o,s=0,l=e.options.length;s<l;s++)if(o=e.options[s],i)r=O(a,Oi(o))>-1,o.selected!==r&&(o.selected=r);else if(D(Oi(o),a))return void(e.selectedIndex!==s&&(e.selectedIndex=s));i||(e.selectedIndex=-1)}}function Di(e,n){return n.every((function(n){return!D(n,e)}))}function Oi(e){return"_value"in e?e._value:e.value}function Gi(e){e.target.composing=!0}function Fi(e){e.target.composing&&(e.target.composing=!1,Bi(e.target,"input"))}function Bi(e,n){var t=document.createEvent("HTMLEvents");t.initEvent(n,!0,!0),e.dispatchEvent(t)}function Ni(e){return!e.componentInstance||e.data&&e.data.transition?e:Ni(e.componentInstance._vnode)}var ji={model:qi,show:{bind:function(e,n,t){var a=n.value,i=(t=Ni(t)).data&&t.data.transition,r=e.__vOriginalDisplay="none"===e.style.display?"":e.style.display;a&&i?(t.data.show=!0,Ci(t,(function(){e.style.display=r}))):e.style.display=a?r:"none"},update:function(e,n,t){var a=n.value;!a!=!n.oldValue&&((t=Ni(t)).data&&t.data.transition?(t.data.show=!0,a?Ci(t,(function(){e.style.display=e.__vOriginalDisplay})):Ai(t,(function(){e.style.display="none"}))):e.style.display=a?e.__vOriginalDisplay:"none")},unbind:function(e,n,t,a,i){i||(e.style.display=e.__vOriginalDisplay)}}},Vi={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function Hi(e){var n=e&&e.componentOptions;return n&&n.Ctor.options.abstract?Hi(zn(n.children)):e}function $i(e){var n={},t=e.$options;for(var a in t.propsData)n[a]=e[a];var i=t._parentListeners;for(var a in i)n[M(a)]=i[a];return n}function Wi(e,n){if(/\d-keep-alive$/.test(n.tag))return e("keep-alive",{props:n.componentOptions.propsData})}var Ki=function(e){return e.tag||gn(e)},Qi=function(e){return"show"===e.name},Xi={name:"transition",props:Vi,abstract:!0,render:function(e){var n=this,t=this.$slots.default;if(t&&(t=t.filter(Ki)).length){0;var a=this.mode;0;var i=t[0];if(function(e){for(;e=e.parent;)if(e.data.transition)return!0}(this.$vnode))return i;var r=Hi(i);if(!r)return i;if(this._leaving)return Wi(e,i);var o="__transition-".concat(this._uid,"-");r.key=null==r.key?r.isComment?o+"comment":o+r.tag:l(r.key)?0===String(r.key).indexOf(o)?r.key:o+r.key:r.key;var s=(r.data||(r.data={})).transition=$i(this),c=this._vnode,d=Hi(c);if(r.data.directives&&r.data.directives.some(Qi)&&(r.data.show=!0),d&&d.data&&!function(e,n){return n.key===e.key&&n.tag===e.tag}(r,d)&&!gn(d)&&(!d.componentInstance||!d.componentInstance._vnode.isComment)){var h=d.data.transition=L({},s);if("out-in"===a)return this._leaving=!0,We(h,"afterLeave",(function(){n._leaving=!1,n.$forceUpdate()})),Wi(e,i);if("in-out"===a){if(gn(r))return c;var m,u=function(){m()};We(s,"afterEnter",u),We(s,"enterCancelled",u),We(h,"delayLeave",(function(e){m=e}))}}return i}}},Yi=L({tag:String,moveClass:String},Vi);function Zi(e){e.elm._moveCb&&e.elm._moveCb(),e.elm._enterCb&&e.elm._enterCb()}function Ji(e){e.data.newPos=e.elm.getBoundingClientRect()}function er(e){var n=e.data.pos,t=e.data.newPos,a=n.left-t.left,i=n.top-t.top;if(a||i){e.data.moved=!0;var r=e.elm.style;r.transform=r.WebkitTransform="translate(".concat(a,"px,").concat(i,"px)"),r.transitionDuration="0s"}}delete Yi.mode;var nr={Transition:Xi,TransitionGroup:{props:Yi,beforeMount:function(){var e=this,n=this._update;this._update=function(t,a){var i=Zn(e);e.__patch__(e._vnode,e.kept,!1,!0),e._vnode=e.kept,i(),n.call(e,t,a)}},render:function(e){for(var n=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),a=this.prevChildren=this.children,i=this.$slots.default||[],r=this.children=[],o=$i(this),s=0;s<i.length;s++){if((d=i[s]).tag)if(null!=d.key&&0!==String(d.key).indexOf("__vlist"))r.push(d),t[d.key]=d,(d.data||(d.data={})).transition=o;else;}if(a){var l=[],c=[];for(s=0;s<a.length;s++){var d;(d=a[s]).data.transition=o,d.data.pos=d.elm.getBoundingClientRect(),t[d.key]?l.push(d):c.push(d)}this.kept=e(n,null,l),this.removed=c}return e(n,null,r)},updated:function(){var e=this.prevChildren,n=this.moveClass||(this.name||"v")+"-move";e.length&&this.hasMove(e[0].elm,n)&&(e.forEach(Zi),e.forEach(Ji),e.forEach(er),this._reflow=document.body.offsetHeight,e.forEach((function(e){if(e.data.moved){var t=e.elm,a=t.style;_i(t,n),a.transform=a.WebkitTransform=a.transitionDuration="",t.addEventListener(fi,t._moveCb=function e(a){a&&a.target!==t||a&&!/transform$/.test(a.propertyName)||(t.removeEventListener(fi,e),t._moveCb=null,ki(t,n))})}})))},methods:{hasMove:function(e,n){if(!pi)return!1;if(this._hasMove)return this._hasMove;var t=e.cloneNode();e._transitionClasses&&e._transitionClasses.forEach((function(e){hi(t,e)})),di(t,n),t.style.display="none",this.$el.appendChild(t);var a=zi(t);return this.$el.removeChild(t),this._hasMove=a.hasTransform}}}};function tr(e,n){for(var t in n)e[t]=n[t];return e}Wt.config.mustUseProp=function(e,n,t){return"value"===t&&ta(e)&&"button"!==n||"selected"===t&&"option"===e||"checked"===t&&"input"===e||"muted"===t&&"video"===e},Wt.config.isReservedTag=ya,Wt.config.isReservedAttr=na,Wt.config.getTagNamespace=function(e){return fa(e)?"svg":"math"===e?"math":void 0},Wt.config.isUnknownElement=function(e){if(!Q)return!0;if(ya(e))return!1;if(e=e.toLowerCase(),null!=ba[e])return ba[e];var n=document.createElement(e);return e.indexOf("-")>-1?ba[e]=n.constructor===window.HTMLUnknownElement||n.constructor===window.HTMLElement:ba[e]=/HTMLUnknownElement/.test(n.toString())},L(Wt.options.directives,ji),L(Wt.options.components,nr),Wt.prototype.__patch__=Q?Ui:q,Wt.prototype.$mount=function(e,n){return function(e,n,t){var a;e.$el=n,e.$options.render||(e.$options.render=pe),nt(e,"beforeMount"),a=function(){e._update(e._render(),t)},new $n(e,a,q,{before:function(){e._isMounted&&!e._isDestroyed&&nt(e,"beforeUpdate")}},!0),t=!1;var i=e._preWatchers;if(i)for(var r=0;r<i.length;r++)i[r].run();return null==e.$vnode&&(e._isMounted=!0,nt(e,"mounted")),e}(this,e=e&&Q?function(e){if("string"==typeof e){var n=document.querySelector(e);return n||document.createElement("div")}return e}(e):void 0,n)},Q&&setTimeout((function(){j.devtools&&se&&se.emit("init",Wt)}),0);var ar=/[!'()*]/g,ir=function(e){return"%"+e.charCodeAt(0).toString(16)},rr=/%2C/g,or=function(e){return encodeURIComponent(e).replace(ar,ir).replace(rr,",")};function sr(e){try{return decodeURIComponent(e)}catch(e){0}return e}var lr=function(e){return null==e||"object"==typeof e?e:String(e)};function cr(e){var n={};return(e=e.trim().replace(/^(\?|#|&)/,""))?(e.split("&").forEach((function(e){var t=e.replace(/\+/g," ").split("="),a=sr(t.shift()),i=t.length>0?sr(t.join("=")):null;void 0===n[a]?n[a]=i:Array.isArray(n[a])?n[a].push(i):n[a]=[n[a],i]})),n):n}function dr(e){var n=e?Object.keys(e).map((function(n){var t=e[n];if(void 0===t)return"";if(null===t)return or(n);if(Array.isArray(t)){var a=[];return t.forEach((function(e){void 0!==e&&(null===e?a.push(or(n)):a.push(or(n)+"="+or(e)))})),a.join("&")}return or(n)+"="+or(t)})).filter((function(e){return e.length>0})).join("&"):null;return n?"?"+n:""}var hr=/\/?$/;function mr(e,n,t,a){var i=a&&a.options.stringifyQuery,r=n.query||{};try{r=ur(r)}catch(e){}var o={name:n.name||e&&e.name,meta:e&&e.meta||{},path:n.path||"/",hash:n.hash||"",query:r,params:n.params||{},fullPath:fr(n,i),matched:e?gr(e):[]};return t&&(o.redirectedFrom=fr(t,i)),Object.freeze(o)}function ur(e){if(Array.isArray(e))return e.map(ur);if(e&&"object"==typeof e){var n={};for(var t in e)n[t]=ur(e[t]);return n}return e}var pr=mr(null,{path:"/"});function gr(e){for(var n=[];e;)n.unshift(e),e=e.parent;return n}function fr(e,n){var t=e.path,a=e.query;void 0===a&&(a={});var i=e.hash;return void 0===i&&(i=""),(t||"/")+(n||dr)(a)+i}function yr(e,n,t){return n===pr?e===n:!!n&&(e.path&&n.path?e.path.replace(hr,"")===n.path.replace(hr,"")&&(t||e.hash===n.hash&&br(e.query,n.query)):!(!e.name||!n.name)&&(e.name===n.name&&(t||e.hash===n.hash&&br(e.query,n.query)&&br(e.params,n.params))))}function br(e,n){if(void 0===e&&(e={}),void 0===n&&(n={}),!e||!n)return e===n;var t=Object.keys(e).sort(),a=Object.keys(n).sort();return t.length===a.length&&t.every((function(t,i){var r=e[t];if(a[i]!==t)return!1;var o=n[t];return null==r||null==o?r===o:"object"==typeof r&&"object"==typeof o?br(r,o):String(r)===String(o)}))}function vr(e){for(var n=0;n<e.matched.length;n++){var t=e.matched[n];for(var a in t.instances){var i=t.instances[a],r=t.enteredCbs[a];if(i&&r){delete t.enteredCbs[a];for(var o=0;o<r.length;o++)i._isBeingDestroyed||r[o](i)}}}}var wr={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(e,n){var t=n.props,a=n.children,i=n.parent,r=n.data;r.routerView=!0;for(var o=i.$createElement,s=t.name,l=i.$route,c=i._routerViewCache||(i._routerViewCache={}),d=0,h=!1;i&&i._routerRoot!==i;){var m=i.$vnode?i.$vnode.data:{};m.routerView&&d++,m.keepAlive&&i._directInactive&&i._inactive&&(h=!0),i=i.$parent}if(r.routerViewDepth=d,h){var u=c[s],p=u&&u.component;return p?(u.configProps&&_r(p,r,u.route,u.configProps),o(p,r,a)):o()}var g=l.matched[d],f=g&&g.components[s];if(!g||!f)return c[s]=null,o();c[s]={component:f},r.registerRouteInstance=function(e,n){var t=g.instances[s];(n&&t!==e||!n&&t===e)&&(g.instances[s]=n)},(r.hook||(r.hook={})).prepatch=function(e,n){g.instances[s]=n.componentInstance},r.hook.init=function(e){e.data.keepAlive&&e.componentInstance&&e.componentInstance!==g.instances[s]&&(g.instances[s]=e.componentInstance),vr(l)};var y=g.props&&g.props[s];return y&&(tr(c[s],{route:l,configProps:y}),_r(f,r,l,y)),o(f,r,a)}};function _r(e,n,t,a){var i=n.props=function(e,n){switch(typeof n){case"undefined":return;case"object":return n;case"function":return n(e);case"boolean":return n?e.params:void 0;default:0}}(t,a);if(i){i=n.props=tr({},i);var r=n.attrs=n.attrs||{};for(var o in i)e.props&&o in e.props||(r[o]=i[o],delete i[o])}}function kr(e,n,t){var a=e.charAt(0);if("/"===a)return e;if("?"===a||"#"===a)return n+e;var i=n.split("/");t&&i[i.length-1]||i.pop();for(var r=e.replace(/^\//,"").split("/"),o=0;o<r.length;o++){var s=r[o];".."===s?i.pop():"."!==s&&i.push(s)}return""!==i[0]&&i.unshift(""),i.join("/")}function xr(e){return e.replace(/\/(?:\s*\/)+/g,"/")}var Tr=Array.isArray||function(e){return"[object Array]"==Object.prototype.toString.call(e)},zr=Gr,Mr=Sr,Pr=function(e,n){return Ur(Sr(e,n),n)},Cr=Ur,Ar=Or,Ir=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function Sr(e,n){for(var t,a=[],i=0,r=0,o="",s=n&&n.delimiter||"/";null!=(t=Ir.exec(e));){var l=t[0],c=t[1],d=t.index;if(o+=e.slice(r,d),r=d+l.length,c)o+=c[1];else{var h=e[r],m=t[2],u=t[3],p=t[4],g=t[5],f=t[6],y=t[7];o&&(a.push(o),o="");var b=null!=m&&null!=h&&h!==m,v="+"===f||"*"===f,w="?"===f||"*"===f,_=t[2]||s,k=p||g;a.push({name:u||i++,prefix:m||"",delimiter:_,optional:w,repeat:v,partial:b,asterisk:!!y,pattern:k?Er(k):y?".*":"[^"+qr(_)+"]+?"})}}return r<e.length&&(o+=e.substr(r)),o&&a.push(o),a}function Lr(e){return encodeURI(e).replace(/[\/?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()}))}function Ur(e,n){for(var t=new Array(e.length),a=0;a<e.length;a++)"object"==typeof e[a]&&(t[a]=new RegExp("^(?:"+e[a].pattern+")$",Dr(n)));return function(n,a){for(var i="",r=n||{},o=(a||{}).pretty?Lr:encodeURIComponent,s=0;s<e.length;s++){var l=e[s];if("string"!=typeof l){var c,d=r[l.name];if(null==d){if(l.optional){l.partial&&(i+=l.prefix);continue}throw new TypeError('Expected "'+l.name+'" to be defined')}if(Tr(d)){if(!l.repeat)throw new TypeError('Expected "'+l.name+'" to not repeat, but received `'+JSON.stringify(d)+"`");if(0===d.length){if(l.optional)continue;throw new TypeError('Expected "'+l.name+'" to not be empty')}for(var h=0;h<d.length;h++){if(c=o(d[h]),!t[s].test(c))throw new TypeError('Expected all "'+l.name+'" to match "'+l.pattern+'", but received `'+JSON.stringify(c)+"`");i+=(0===h?l.prefix:l.delimiter)+c}}else{if(c=l.asterisk?encodeURI(d).replace(/[?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()})):o(d),!t[s].test(c))throw new TypeError('Expected "'+l.name+'" to match "'+l.pattern+'", but received "'+c+'"');i+=l.prefix+c}}else i+=l}return i}}function qr(e){return e.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function Er(e){return e.replace(/([=!:$\/()])/g,"\\$1")}function Rr(e,n){return e.keys=n,e}function Dr(e){return e&&e.sensitive?"":"i"}function Or(e,n,t){Tr(n)||(t=n||t,n=[]);for(var a=(t=t||{}).strict,i=!1!==t.end,r="",o=0;o<e.length;o++){var s=e[o];if("string"==typeof s)r+=qr(s);else{var l=qr(s.prefix),c="(?:"+s.pattern+")";n.push(s),s.repeat&&(c+="(?:"+l+c+")*"),r+=c=s.optional?s.partial?l+"("+c+")?":"(?:"+l+"("+c+"))?":l+"("+c+")"}}var d=qr(t.delimiter||"/"),h=r.slice(-d.length)===d;return a||(r=(h?r.slice(0,-d.length):r)+"(?:"+d+"(?=$))?"),r+=i?"$":a&&h?"":"(?="+d+"|$)",Rr(new RegExp("^"+r,Dr(t)),n)}function Gr(e,n,t){return Tr(n)||(t=n||t,n=[]),t=t||{},e instanceof RegExp?function(e,n){var t=e.source.match(/\((?!\?)/g);if(t)for(var a=0;a<t.length;a++)n.push({name:a,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return Rr(e,n)}(e,n):Tr(e)?function(e,n,t){for(var a=[],i=0;i<e.length;i++)a.push(Gr(e[i],n,t).source);return Rr(new RegExp("(?:"+a.join("|")+")",Dr(t)),n)}(e,n,t):function(e,n,t){return Or(Sr(e,t),n,t)}(e,n,t)}zr.parse=Mr,zr.compile=Pr,zr.tokensToFunction=Cr,zr.tokensToRegExp=Ar;var Fr=Object.create(null);function Br(e,n,t){n=n||{};try{var a=Fr[e]||(Fr[e]=zr.compile(e));return"string"==typeof n.pathMatch&&(n[0]=n.pathMatch),a(n,{pretty:!0})}catch(e){return""}finally{delete n[0]}}function Nr(e,n,t,a){var i="string"==typeof e?{path:e}:e;if(i._normalized)return i;if(i.name){var r=(i=tr({},e)).params;return r&&"object"==typeof r&&(i.params=tr({},r)),i}if(!i.path&&i.params&&n){(i=tr({},i))._normalized=!0;var o=tr(tr({},n.params),i.params);if(n.name)i.name=n.name,i.params=o;else if(n.matched.length){var s=n.matched[n.matched.length-1].path;i.path=Br(s,o,n.path)}else 0;return i}var l=function(e){var n="",t="",a=e.indexOf("#");a>=0&&(n=e.slice(a),e=e.slice(0,a));var i=e.indexOf("?");return i>=0&&(t=e.slice(i+1),e=e.slice(0,i)),{path:e,query:t,hash:n}}(i.path||""),c=n&&n.path||"/",d=l.path?kr(l.path,c,t||i.append):c,h=function(e,n,t){void 0===n&&(n={});var a,i=t||cr;try{a=i(e||"")}catch(e){a={}}for(var r in n){var o=n[r];a[r]=Array.isArray(o)?o.map(lr):lr(o)}return a}(l.query,i.query,a&&a.options.parseQuery),m=i.hash||l.hash;return m&&"#"!==m.charAt(0)&&(m="#"+m),{_normalized:!0,path:d,query:h,hash:m}}var jr,Vr=function(){},Hr={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(e){var n=this,t=this.$router,a=this.$route,i=t.resolve(this.to,a,this.append),r=i.location,o=i.route,s=i.href,l={},c=t.options.linkActiveClass,d=t.options.linkExactActiveClass,h=null==c?"router-link-active":c,m=null==d?"router-link-exact-active":d,u=null==this.activeClass?h:this.activeClass,p=null==this.exactActiveClass?m:this.exactActiveClass,g=o.redirectedFrom?mr(null,Nr(o.redirectedFrom),null,t):o;l[p]=yr(a,g,this.exactPath),l[u]=this.exact||this.exactPath?l[p]:function(e,n){return 0===e.path.replace(hr,"/").indexOf(n.path.replace(hr,"/"))&&(!n.hash||e.hash===n.hash)&&function(e,n){for(var t in n)if(!(t in e))return!1;return!0}(e.query,n.query)}(a,g);var f=l[p]?this.ariaCurrentValue:null,y=function(e){$r(e)&&(n.replace?t.replace(r,Vr):t.push(r,Vr))},b={click:$r};Array.isArray(this.event)?this.event.forEach((function(e){b[e]=y})):b[this.event]=y;var v={class:l},w=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:s,route:o,navigate:y,isActive:l[u],isExactActive:l[p]});if(w){if(1===w.length)return w[0];if(w.length>1||!w.length)return 0===w.length?e():e("span",{},w)}if("a"===this.tag)v.on=b,v.attrs={href:s,"aria-current":f};else{var _=function e(n){var t;if(n)for(var a=0;a<n.length;a++){if("a"===(t=n[a]).tag)return t;if(t.children&&(t=e(t.children)))return t}}(this.$slots.default);if(_){_.isStatic=!1;var k=_.data=tr({},_.data);for(var x in k.on=k.on||{},k.on){var T=k.on[x];x in b&&(k.on[x]=Array.isArray(T)?T:[T])}for(var z in b)z in k.on?k.on[z].push(b[z]):k.on[z]=y;var M=_.data.attrs=tr({},_.data.attrs);M.href=s,M["aria-current"]=f}else v.on=b}return e(this.tag,v,this.$slots.default)}};function $r(e){if(!(e.metaKey||e.altKey||e.ctrlKey||e.shiftKey||e.defaultPrevented||void 0!==e.button&&0!==e.button)){if(e.currentTarget&&e.currentTarget.getAttribute){var n=e.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(n))return}return e.preventDefault&&e.preventDefault(),!0}}var Wr="undefined"!=typeof window;function Kr(e,n,t,a,i){var r=n||[],o=t||Object.create(null),s=a||Object.create(null);e.forEach((function(e){!function e(n,t,a,i,r,o){var s=i.path,l=i.name;0;var c=i.pathToRegexpOptions||{},d=function(e,n,t){t||(e=e.replace(/\/$/,""));if("/"===e[0])return e;if(null==n)return e;return xr(n.path+"/"+e)}(s,r,c.strict);"boolean"==typeof i.caseSensitive&&(c.sensitive=i.caseSensitive);var h={path:d,regex:Qr(d,c),components:i.components||{default:i.component},alias:i.alias?"string"==typeof i.alias?[i.alias]:i.alias:[],instances:{},enteredCbs:{},name:l,parent:r,matchAs:o,redirect:i.redirect,beforeEnter:i.beforeEnter,meta:i.meta||{},props:null==i.props?{}:i.components?i.props:{default:i.props}};i.children&&i.children.forEach((function(i){var r=o?xr(o+"/"+i.path):void 0;e(n,t,a,i,h,r)}));t[h.path]||(n.push(h.path),t[h.path]=h);if(void 0!==i.alias)for(var m=Array.isArray(i.alias)?i.alias:[i.alias],u=0;u<m.length;++u){0;var p={path:m[u],children:i.children};e(n,t,a,p,r,h.path||"/")}l&&(a[l]||(a[l]=h))}(r,o,s,e,i)}));for(var l=0,c=r.length;l<c;l++)"*"===r[l]&&(r.push(r.splice(l,1)[0]),c--,l--);return{pathList:r,pathMap:o,nameMap:s}}function Qr(e,n){return zr(e,[],n)}function Xr(e,n){var t=Kr(e),a=t.pathList,i=t.pathMap,r=t.nameMap;function o(e,t,o){var s=Nr(e,t,!1,n),c=s.name;if(c){var d=r[c];if(!d)return l(null,s);var h=d.regex.keys.filter((function(e){return!e.optional})).map((function(e){return e.name}));if("object"!=typeof s.params&&(s.params={}),t&&"object"==typeof t.params)for(var m in t.params)!(m in s.params)&&h.indexOf(m)>-1&&(s.params[m]=t.params[m]);return s.path=Br(d.path,s.params),l(d,s,o)}if(s.path){s.params={};for(var u=0;u<a.length;u++){var p=a[u],g=i[p];if(Yr(g.regex,s.path,s.params))return l(g,s,o)}}return l(null,s)}function s(e,t){var a=e.redirect,i="function"==typeof a?a(mr(e,t,null,n)):a;if("string"==typeof i&&(i={path:i}),!i||"object"!=typeof i)return l(null,t);var s=i,c=s.name,d=s.path,h=t.query,m=t.hash,u=t.params;if(h=s.hasOwnProperty("query")?s.query:h,m=s.hasOwnProperty("hash")?s.hash:m,u=s.hasOwnProperty("params")?s.params:u,c){r[c];return o({_normalized:!0,name:c,query:h,hash:m,params:u},void 0,t)}if(d){var p=function(e,n){return kr(e,n.parent?n.parent.path:"/",!0)}(d,e);return o({_normalized:!0,path:Br(p,u),query:h,hash:m},void 0,t)}return l(null,t)}function l(e,t,a){return e&&e.redirect?s(e,a||t):e&&e.matchAs?function(e,n,t){var a=o({_normalized:!0,path:Br(t,n.params)});if(a){var i=a.matched,r=i[i.length-1];return n.params=a.params,l(r,n)}return l(null,n)}(0,t,e.matchAs):mr(e,t,a,n)}return{match:o,addRoute:function(e,n){var t="object"!=typeof e?r[e]:void 0;Kr([n||e],a,i,r,t),t&&t.alias.length&&Kr(t.alias.map((function(e){return{path:e,children:[n]}})),a,i,r,t)},getRoutes:function(){return a.map((function(e){return i[e]}))},addRoutes:function(e){Kr(e,a,i,r)}}}function Yr(e,n,t){var a=n.match(e);if(!a)return!1;if(!t)return!0;for(var i=1,r=a.length;i<r;++i){var o=e.keys[i-1];o&&(t[o.name||"pathMatch"]="string"==typeof a[i]?sr(a[i]):a[i])}return!0}var Zr=Wr&&window.performance&&window.performance.now?window.performance:Date;function Jr(){return Zr.now().toFixed(3)}var eo=Jr();function no(){return eo}function to(e){return eo=e}var ao=Object.create(null);function io(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var e=window.location.protocol+"//"+window.location.host,n=window.location.href.replace(e,""),t=tr({},window.history.state);return t.key=no(),window.history.replaceState(t,"",n),window.addEventListener("popstate",so),function(){window.removeEventListener("popstate",so)}}function ro(e,n,t,a){if(e.app){var i=e.options.scrollBehavior;i&&e.app.$nextTick((function(){var r=function(){var e=no();if(e)return ao[e]}(),o=i.call(e,n,t,a?r:null);o&&("function"==typeof o.then?o.then((function(e){uo(e,r)})).catch((function(e){0})):uo(o,r))}))}}function oo(){var e=no();e&&(ao[e]={x:window.pageXOffset,y:window.pageYOffset})}function so(e){oo(),e.state&&e.state.key&&to(e.state.key)}function lo(e){return ho(e.x)||ho(e.y)}function co(e){return{x:ho(e.x)?e.x:window.pageXOffset,y:ho(e.y)?e.y:window.pageYOffset}}function ho(e){return"number"==typeof e}var mo=/^#\d/;function uo(e,n){var t,a="object"==typeof e;if(a&&"string"==typeof e.selector){var i=mo.test(e.selector)?document.getElementById(e.selector.slice(1)):document.querySelector(e.selector);if(i){var r=e.offset&&"object"==typeof e.offset?e.offset:{};n=function(e,n){var t=document.documentElement.getBoundingClientRect(),a=e.getBoundingClientRect();return{x:a.left-t.left-n.x,y:a.top-t.top-n.y}}(i,r={x:ho((t=r).x)?t.x:0,y:ho(t.y)?t.y:0})}else lo(e)&&(n=co(e))}else a&&lo(e)&&(n=co(e));n&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:n.x,top:n.y,behavior:e.behavior}):window.scrollTo(n.x,n.y))}var po,go=Wr&&((-1===(po=window.navigator.userAgent).indexOf("Android 2.")&&-1===po.indexOf("Android 4.0")||-1===po.indexOf("Mobile Safari")||-1!==po.indexOf("Chrome")||-1!==po.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function fo(e,n){oo();var t=window.history;try{if(n){var a=tr({},t.state);a.key=no(),t.replaceState(a,"",e)}else t.pushState({key:to(Jr())},"",e)}catch(t){window.location[n?"replace":"assign"](e)}}function yo(e){fo(e,!0)}var bo={redirected:2,aborted:4,cancelled:8,duplicated:16};function vo(e,n){return _o(e,n,bo.redirected,'Redirected when going from "'+e.fullPath+'" to "'+function(e){if("string"==typeof e)return e;if("path"in e)return e.path;var n={};return ko.forEach((function(t){t in e&&(n[t]=e[t])})),JSON.stringify(n,null,2)}(n)+'" via a navigation guard.')}function wo(e,n){return _o(e,n,bo.cancelled,'Navigation cancelled from "'+e.fullPath+'" to "'+n.fullPath+'" with a new navigation.')}function _o(e,n,t,a){var i=new Error(a);return i._isRouter=!0,i.from=e,i.to=n,i.type=t,i}var ko=["params","query","hash"];function xo(e){return Object.prototype.toString.call(e).indexOf("Error")>-1}function To(e,n){return xo(e)&&e._isRouter&&(null==n||e.type===n)}function zo(e,n,t){var a=function(i){i>=e.length?t():e[i]?n(e[i],(function(){a(i+1)})):a(i+1)};a(0)}function Mo(e){return function(n,t,a){var i=!1,r=0,o=null;Po(e,(function(e,n,t,s){if("function"==typeof e&&void 0===e.cid){i=!0,r++;var l,c=Io((function(n){var i;((i=n).__esModule||Ao&&"Module"===i[Symbol.toStringTag])&&(n=n.default),e.resolved="function"==typeof n?n:jr.extend(n),t.components[s]=n,--r<=0&&a()})),d=Io((function(e){var n="Failed to resolve async component "+s+": "+e;o||(o=xo(e)?e:new Error(n),a(o))}));try{l=e(c,d)}catch(e){d(e)}if(l)if("function"==typeof l.then)l.then(c,d);else{var h=l.component;h&&"function"==typeof h.then&&h.then(c,d)}}})),i||a()}}function Po(e,n){return Co(e.map((function(e){return Object.keys(e.components).map((function(t){return n(e.components[t],e.instances[t],e,t)}))})))}function Co(e){return Array.prototype.concat.apply([],e)}var Ao="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function Io(e){var n=!1;return function(){for(var t=[],a=arguments.length;a--;)t[a]=arguments[a];if(!n)return n=!0,e.apply(this,t)}}var So=function(e,n){this.router=e,this.base=function(e){if(!e)if(Wr){var n=document.querySelector("base");e=(e=n&&n.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else e="/";"/"!==e.charAt(0)&&(e="/"+e);return e.replace(/\/$/,"")}(n),this.current=pr,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function Lo(e,n,t,a){var i=Po(e,(function(e,a,i,r){var o=function(e,n){"function"!=typeof e&&(e=jr.extend(e));return e.options[n]}(e,n);if(o)return Array.isArray(o)?o.map((function(e){return t(e,a,i,r)})):t(o,a,i,r)}));return Co(a?i.reverse():i)}function Uo(e,n){if(n)return function(){return e.apply(n,arguments)}}So.prototype.listen=function(e){this.cb=e},So.prototype.onReady=function(e,n){this.ready?e():(this.readyCbs.push(e),n&&this.readyErrorCbs.push(n))},So.prototype.onError=function(e){this.errorCbs.push(e)},So.prototype.transitionTo=function(e,n,t){var a,i=this;try{a=this.router.match(e,this.current)}catch(e){throw this.errorCbs.forEach((function(n){n(e)})),e}var r=this.current;this.confirmTransition(a,(function(){i.updateRoute(a),n&&n(a),i.ensureURL(),i.router.afterHooks.forEach((function(e){e&&e(a,r)})),i.ready||(i.ready=!0,i.readyCbs.forEach((function(e){e(a)})))}),(function(e){t&&t(e),e&&!i.ready&&(To(e,bo.redirected)&&r===pr||(i.ready=!0,i.readyErrorCbs.forEach((function(n){n(e)}))))}))},So.prototype.confirmTransition=function(e,n,t){var a=this,i=this.current;this.pending=e;var r=function(e){!To(e)&&xo(e)&&(a.errorCbs.length?a.errorCbs.forEach((function(n){n(e)})):console.error(e)),t&&t(e)},o=e.matched.length-1,s=i.matched.length-1;if(yr(e,i)&&o===s&&e.matched[o]===i.matched[s])return this.ensureURL(),e.hash&&ro(this.router,i,e,!1),r(function(e,n){var t=_o(e,n,bo.duplicated,'Avoided redundant navigation to current location: "'+e.fullPath+'".');return t.name="NavigationDuplicated",t}(i,e));var l=function(e,n){var t,a=Math.max(e.length,n.length);for(t=0;t<a&&e[t]===n[t];t++);return{updated:n.slice(0,t),activated:n.slice(t),deactivated:e.slice(t)}}(this.current.matched,e.matched),c=l.updated,d=l.deactivated,h=l.activated,m=[].concat(function(e){return Lo(e,"beforeRouteLeave",Uo,!0)}(d),this.router.beforeHooks,function(e){return Lo(e,"beforeRouteUpdate",Uo)}(c),h.map((function(e){return e.beforeEnter})),Mo(h)),u=function(n,t){if(a.pending!==e)return r(wo(i,e));try{n(e,i,(function(n){!1===n?(a.ensureURL(!0),r(function(e,n){return _o(e,n,bo.aborted,'Navigation aborted from "'+e.fullPath+'" to "'+n.fullPath+'" via a navigation guard.')}(i,e))):xo(n)?(a.ensureURL(!0),r(n)):"string"==typeof n||"object"==typeof n&&("string"==typeof n.path||"string"==typeof n.name)?(r(vo(i,e)),"object"==typeof n&&n.replace?a.replace(n):a.push(n)):t(n)}))}catch(e){r(e)}};zo(m,u,(function(){zo(function(e){return Lo(e,"beforeRouteEnter",(function(e,n,t,a){return function(e,n,t){return function(a,i,r){return e(a,i,(function(e){"function"==typeof e&&(n.enteredCbs[t]||(n.enteredCbs[t]=[]),n.enteredCbs[t].push(e)),r(e)}))}}(e,t,a)}))}(h).concat(a.router.resolveHooks),u,(function(){if(a.pending!==e)return r(wo(i,e));a.pending=null,n(e),a.router.app&&a.router.app.$nextTick((function(){vr(e)}))}))}))},So.prototype.updateRoute=function(e){this.current=e,this.cb&&this.cb(e)},So.prototype.setupListeners=function(){},So.prototype.teardown=function(){this.listeners.forEach((function(e){e()})),this.listeners=[],this.current=pr,this.pending=null};var qo=function(e){function n(n,t){e.call(this,n,t),this._startLocation=Eo(this.base)}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var n=this.router,t=n.options.scrollBehavior,a=go&&t;a&&this.listeners.push(io());var i=function(){var t=e.current,i=Eo(e.base);e.current===pr&&i===e._startLocation||e.transitionTo(i,(function(e){a&&ro(n,e,t,!0)}))};window.addEventListener("popstate",i),this.listeners.push((function(){window.removeEventListener("popstate",i)}))}},n.prototype.go=function(e){window.history.go(e)},n.prototype.push=function(e,n,t){var a=this,i=this.current;this.transitionTo(e,(function(e){fo(xr(a.base+e.fullPath)),ro(a.router,e,i,!1),n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var a=this,i=this.current;this.transitionTo(e,(function(e){yo(xr(a.base+e.fullPath)),ro(a.router,e,i,!1),n&&n(e)}),t)},n.prototype.ensureURL=function(e){if(Eo(this.base)!==this.current.fullPath){var n=xr(this.base+this.current.fullPath);e?fo(n):yo(n)}},n.prototype.getCurrentLocation=function(){return Eo(this.base)},n}(So);function Eo(e){var n=window.location.pathname,t=n.toLowerCase(),a=e.toLowerCase();return!e||t!==a&&0!==t.indexOf(xr(a+"/"))||(n=n.slice(e.length)),(n||"/")+window.location.search+window.location.hash}var Ro=function(e){function n(n,t,a){e.call(this,n,t),a&&function(e){var n=Eo(e);if(!/^\/#/.test(n))return window.location.replace(xr(e+"/#"+n)),!0}(this.base)||Do()}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var n=this.router.options.scrollBehavior,t=go&&n;t&&this.listeners.push(io());var a=function(){var n=e.current;Do()&&e.transitionTo(Oo(),(function(a){t&&ro(e.router,a,n,!0),go||Bo(a.fullPath)}))},i=go?"popstate":"hashchange";window.addEventListener(i,a),this.listeners.push((function(){window.removeEventListener(i,a)}))}},n.prototype.push=function(e,n,t){var a=this,i=this.current;this.transitionTo(e,(function(e){Fo(e.fullPath),ro(a.router,e,i,!1),n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var a=this,i=this.current;this.transitionTo(e,(function(e){Bo(e.fullPath),ro(a.router,e,i,!1),n&&n(e)}),t)},n.prototype.go=function(e){window.history.go(e)},n.prototype.ensureURL=function(e){var n=this.current.fullPath;Oo()!==n&&(e?Fo(n):Bo(n))},n.prototype.getCurrentLocation=function(){return Oo()},n}(So);function Do(){var e=Oo();return"/"===e.charAt(0)||(Bo("/"+e),!1)}function Oo(){var e=window.location.href,n=e.indexOf("#");return n<0?"":e=e.slice(n+1)}function Go(e){var n=window.location.href,t=n.indexOf("#");return(t>=0?n.slice(0,t):n)+"#"+e}function Fo(e){go?fo(Go(e)):window.location.hash=e}function Bo(e){go?yo(Go(e)):window.location.replace(Go(e))}var No=function(e){function n(n,t){e.call(this,n,t),this.stack=[],this.index=-1}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.push=function(e,n,t){var a=this;this.transitionTo(e,(function(e){a.stack=a.stack.slice(0,a.index+1).concat(e),a.index++,n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var a=this;this.transitionTo(e,(function(e){a.stack=a.stack.slice(0,a.index).concat(e),n&&n(e)}),t)},n.prototype.go=function(e){var n=this,t=this.index+e;if(!(t<0||t>=this.stack.length)){var a=this.stack[t];this.confirmTransition(a,(function(){var e=n.current;n.index=t,n.updateRoute(a),n.router.afterHooks.forEach((function(n){n&&n(a,e)}))}),(function(e){To(e,bo.duplicated)&&(n.index=t)}))}},n.prototype.getCurrentLocation=function(){var e=this.stack[this.stack.length-1];return e?e.fullPath:"/"},n.prototype.ensureURL=function(){},n}(So),jo=function(e){void 0===e&&(e={}),this.app=null,this.apps=[],this.options=e,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Xr(e.routes||[],this);var n=e.mode||"hash";switch(this.fallback="history"===n&&!go&&!1!==e.fallback,this.fallback&&(n="hash"),Wr||(n="abstract"),this.mode=n,n){case"history":this.history=new qo(this,e.base);break;case"hash":this.history=new Ro(this,e.base,this.fallback);break;case"abstract":this.history=new No(this,e.base);break;default:0}},Vo={currentRoute:{configurable:!0}};jo.prototype.match=function(e,n,t){return this.matcher.match(e,n,t)},Vo.currentRoute.get=function(){return this.history&&this.history.current},jo.prototype.init=function(e){var n=this;if(this.apps.push(e),e.$once("hook:destroyed",(function(){var t=n.apps.indexOf(e);t>-1&&n.apps.splice(t,1),n.app===e&&(n.app=n.apps[0]||null),n.app||n.history.teardown()})),!this.app){this.app=e;var t=this.history;if(t instanceof qo||t instanceof Ro){var a=function(e){t.setupListeners(),function(e){var a=t.current,i=n.options.scrollBehavior;go&&i&&"fullPath"in e&&ro(n,e,a,!1)}(e)};t.transitionTo(t.getCurrentLocation(),a,a)}t.listen((function(e){n.apps.forEach((function(n){n._route=e}))}))}},jo.prototype.beforeEach=function(e){return $o(this.beforeHooks,e)},jo.prototype.beforeResolve=function(e){return $o(this.resolveHooks,e)},jo.prototype.afterEach=function(e){return $o(this.afterHooks,e)},jo.prototype.onReady=function(e,n){this.history.onReady(e,n)},jo.prototype.onError=function(e){this.history.onError(e)},jo.prototype.push=function(e,n,t){var a=this;if(!n&&!t&&"undefined"!=typeof Promise)return new Promise((function(n,t){a.history.push(e,n,t)}));this.history.push(e,n,t)},jo.prototype.replace=function(e,n,t){var a=this;if(!n&&!t&&"undefined"!=typeof Promise)return new Promise((function(n,t){a.history.replace(e,n,t)}));this.history.replace(e,n,t)},jo.prototype.go=function(e){this.history.go(e)},jo.prototype.back=function(){this.go(-1)},jo.prototype.forward=function(){this.go(1)},jo.prototype.getMatchedComponents=function(e){var n=e?e.matched?e:this.resolve(e).route:this.currentRoute;return n?[].concat.apply([],n.matched.map((function(e){return Object.keys(e.components).map((function(n){return e.components[n]}))}))):[]},jo.prototype.resolve=function(e,n,t){var a=Nr(e,n=n||this.history.current,t,this),i=this.match(a,n),r=i.redirectedFrom||i.fullPath;return{location:a,route:i,href:function(e,n,t){var a="hash"===t?"#"+n:n;return e?xr(e+"/"+a):a}(this.history.base,r,this.mode),normalizedTo:a,resolved:i}},jo.prototype.getRoutes=function(){return this.matcher.getRoutes()},jo.prototype.addRoute=function(e,n){this.matcher.addRoute(e,n),this.history.current!==pr&&this.history.transitionTo(this.history.getCurrentLocation())},jo.prototype.addRoutes=function(e){this.matcher.addRoutes(e),this.history.current!==pr&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(jo.prototype,Vo);var Ho=jo;function $o(e,n){return e.push(n),function(){var t=e.indexOf(n);t>-1&&e.splice(t,1)}}jo.install=function e(n){if(!e.installed||jr!==n){e.installed=!0,jr=n;var t=function(e){return void 0!==e},a=function(e,n){var a=e.$options._parentVnode;t(a)&&t(a=a.data)&&t(a=a.registerRouteInstance)&&a(e,n)};n.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),n.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,a(this,this)},destroyed:function(){a(this)}}),Object.defineProperty(n.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(n.prototype,"$route",{get:function(){return this._routerRoot._route}}),n.component("RouterView",wr),n.component("RouterLink",Hr);var i=n.config.optionMergeStrategies;i.beforeRouteEnter=i.beforeRouteLeave=i.beforeRouteUpdate=i.created}},jo.version="3.6.5",jo.isNavigationFailure=To,jo.NavigationFailureType=bo,jo.START_LOCATION=pr,Wr&&window.Vue&&window.Vue.use(jo);t(182),t(23),t(191);t(192),t(49);var Wo={NotFound:()=>Promise.all([t.e(0),t.e(4)]).then(t.bind(null,457)),Layout:()=>Promise.all([t.e(0),t.e(2)]).then(t.bind(null,456))},Ko={"v-0fdda314":()=>t.e(5).then(t.bind(null,458)),"v-4553c3a8":()=>t.e(7).then(t.bind(null,459)),"v-37c5bc96":()=>t.e(6).then(t.bind(null,460)),"v-07282236":()=>t.e(8).then(t.bind(null,461)),"v-bd5cd4d4":()=>t.e(9).then(t.bind(null,462)),"v-07abb6d6":()=>t.e(10).then(t.bind(null,463)),"v-577baade":()=>t.e(11).then(t.bind(null,464)),"v-9c9c1094":()=>t.e(12).then(t.bind(null,465)),"v-abb0f8a2":()=>t.e(13).then(t.bind(null,466)),"v-b2fd417c":()=>t.e(15).then(t.bind(null,467)),"v-9842a21a":()=>t.e(16).then(t.bind(null,468)),"v-d7471e16":()=>t.e(14).then(t.bind(null,469)),"v-7f8db936":()=>t.e(17).then(t.bind(null,470)),"v-171f66aa":()=>t.e(18).then(t.bind(null,471)),"v-662ff4fd":()=>t.e(21).then(t.bind(null,472)),"v-788071c5":()=>t.e(19).then(t.bind(null,473)),"v-25995465":()=>t.e(20).then(t.bind(null,474)),"v-065eff05":()=>t.e(22).then(t.bind(null,475)),"v-0b217b45":()=>t.e(24).then(t.bind(null,476)),"v-2d0923f6":()=>t.e(25).then(t.bind(null,477)),"v-3d658257":()=>t.e(23).then(t.bind(null,478)),"v-4d042c96":()=>t.e(26).then(t.bind(null,479)),"v-49e4dae0":()=>t.e(27).then(t.bind(null,480)),"v-c73e5956":()=>t.e(28).then(t.bind(null,481)),"v-89581d5c":()=>t.e(29).then(t.bind(null,482)),"v-81151826":()=>t.e(30).then(t.bind(null,483)),"v-e9f07ff6":()=>t.e(32).then(t.bind(null,484)),"v-e0b126ce":()=>t.e(34).then(t.bind(null,485)),"v-49a96495":()=>t.e(35).then(t.bind(null,486)),"v-0d06efa2":()=>t.e(31).then(t.bind(null,487)),"v-e8215df6":()=>t.e(33).then(t.bind(null,488)),"v-7ba96ade":()=>t.e(36).then(t.bind(null,489)),"v-99a1d2ee":()=>t.e(38).then(t.bind(null,490)),"v-2bb0fc85":()=>t.e(39).then(t.bind(null,491)),"v-3aad308d":()=>t.e(37).then(t.bind(null,492)),"v-b79a3afe":()=>t.e(40).then(t.bind(null,493)),"v-5841ee0f":()=>t.e(42).then(t.bind(null,494)),"v-1cb4c87d":()=>t.e(41).then(t.bind(null,495)),"v-5e7857ea":()=>t.e(43).then(t.bind(null,496)),"v-6ffd4505":()=>t.e(44).then(t.bind(null,497)),"v-b25c491a":()=>t.e(45).then(t.bind(null,498)),"v-f96426b0":()=>t.e(46).then(t.bind(null,499)),"v-2861ec8b":()=>t.e(48).then(t.bind(null,500)),"v-04292a45":()=>t.e(49).then(t.bind(null,501)),"v-5849f946":()=>t.e(47).then(t.bind(null,502)),"v-65bd31e5":()=>t.e(51).then(t.bind(null,503)),"v-d22a70f6":()=>t.e(50).then(t.bind(null,504)),"v-e33712b6":()=>t.e(52).then(t.bind(null,505)),"v-a0731602":()=>t.e(53).then(t.bind(null,506)),"v-75a07a01":()=>t.e(54).then(t.bind(null,507)),"v-5474106a":()=>t.e(55).then(t.bind(null,508)),"v-02aef12e":()=>t.e(56).then(t.bind(null,509)),"v-4349358d":()=>t.e(57).then(t.bind(null,510)),"v-9e671962":()=>t.e(58).then(t.bind(null,511)),"v-1e4fb111":()=>t.e(59).then(t.bind(null,512)),"v-c25059fe":()=>t.e(61).then(t.bind(null,513)),"v-e85a225a":()=>t.e(60).then(t.bind(null,514)),"v-798f0c72":()=>t.e(62).then(t.bind(null,515)),"v-53bdf925":()=>t.e(63).then(t.bind(null,516)),"v-5552df36":()=>t.e(64).then(t.bind(null,517)),"v-4301e3b6":()=>t.e(65).then(t.bind(null,518)),"v-dcff77f6":()=>t.e(66).then(t.bind(null,519)),"v-3690e719":()=>t.e(69).then(t.bind(null,520)),"v-6f6994e3":()=>t.e(67).then(t.bind(null,521)),"v-eff2fe36":()=>t.e(68).then(t.bind(null,522)),"v-27253dc5":()=>t.e(70).then(t.bind(null,523)),"v-70c0f165":()=>t.e(71).then(t.bind(null,524)),"v-66582056":()=>t.e(72).then(t.bind(null,525)),"v-70c8fe25":()=>t.e(73).then(t.bind(null,526)),"v-8a6d3836":()=>t.e(74).then(t.bind(null,527)),"v-feef1d7a":()=>t.e(75).then(t.bind(null,528)),"v-072788bb":()=>t.e(76).then(t.bind(null,529)),"v-121d90a9":()=>t.e(77).then(t.bind(null,530)),"v-891a4db6":()=>t.e(79).then(t.bind(null,531)),"v-23804245":()=>t.e(78).then(t.bind(null,532)),"v-64b7ef30":()=>t.e(80).then(t.bind(null,533)),"v-90037b60":()=>t.e(81).then(t.bind(null,534)),"v-7ddc0f42":()=>t.e(82).then(t.bind(null,535)),"v-c1b2e1c6":()=>t.e(84).then(t.bind(null,536)),"v-26a9c736":()=>t.e(85).then(t.bind(null,537)),"v-bdcb41a8":()=>t.e(83).then(t.bind(null,538)),"v-b267e276":()=>t.e(86).then(t.bind(null,539)),"v-0ee7ecc5":()=>t.e(88).then(t.bind(null,540)),"v-259f44f9":()=>t.e(87).then(t.bind(null,541)),"v-c6222e76":()=>t.e(89).then(t.bind(null,542)),"v-4edbad25":()=>t.e(90).then(t.bind(null,543)),"v-e3c4ebf6":()=>t.e(91).then(t.bind(null,544)),"v-936502b6":()=>t.e(92).then(t.bind(null,545)),"v-2925b9e5":()=>t.e(93).then(t.bind(null,546)),"v-a65b4fb6":()=>t.e(95).then(t.bind(null,547)),"v-5ef480da":()=>t.e(94).then(t.bind(null,548)),"v-03195875":()=>t.e(96).then(t.bind(null,549)),"v-43caabc5":()=>t.e(97).then(t.bind(null,550)),"v-393b6085":()=>t.e(98).then(t.bind(null,551)),"v-2bf21845":()=>t.e(99).then(t.bind(null,552)),"v-041036ff":()=>t.e(100).then(t.bind(null,553)),"v-74b8ee65":()=>t.e(101).then(t.bind(null,554)),"v-3c10ac85":()=>t.e(103).then(t.bind(null,555)),"v-082f1636":()=>t.e(104).then(t.bind(null,556)),"v-5c1f14b6":()=>t.e(102).then(t.bind(null,557)),"v-915e75f6":()=>t.e(105).then(t.bind(null,558)),"v-77ef8fad":()=>t.e(108).then(t.bind(null,559)),"v-33912107":()=>t.e(106).then(t.bind(null,560)),"v-66fc8ee5":()=>t.e(109).then(t.bind(null,561)),"v-5e4b7fb6":()=>t.e(107).then(t.bind(null,562)),"v-e114a476":()=>t.e(110).then(t.bind(null,563)),"v-0d7764c5":()=>t.e(111).then(t.bind(null,564)),"v-23bbae21":()=>t.e(114).then(t.bind(null,565)),"v-bb1ed876":()=>t.e(113).then(t.bind(null,566)),"v-4a87ebca":()=>t.e(115).then(t.bind(null,567)),"v-2e715932":()=>t.e(112).then(t.bind(null,568)),"v-e225a34a":()=>t.e(116).then(t.bind(null,569)),"v-7b8d66da":()=>t.e(118).then(t.bind(null,570)),"v-ac6a65e2":()=>t.e(117).then(t.bind(null,571)),"v-29268d11":()=>t.e(119).then(t.bind(null,572)),"v-618db02d":()=>t.e(120).then(t.bind(null,573)),"v-593a8e26":()=>t.e(122).then(t.bind(null,574)),"v-18aac47d":()=>t.e(121).then(t.bind(null,575)),"v-a395c0f6":()=>t.e(123).then(t.bind(null,576)),"v-f8e02736":()=>t.e(124).then(t.bind(null,577)),"v-2f8a0a7e":()=>t.e(126).then(t.bind(null,578)),"v-01216c79":()=>t.e(127).then(t.bind(null,579)),"v-4b579c36":()=>t.e(125).then(t.bind(null,580))};function Qo(e){const n=Object.create(null);return function(t){return n[t]||(n[t]=e(t))}}const Xo=/-(\w)/g,Yo=Qo(e=>e.replace(Xo,(e,n)=>n?n.toUpperCase():"")),Zo=/\B([A-Z])/g,Jo=Qo(e=>e.replace(Zo,"-$1").toLowerCase()),es=Qo(e=>e.charAt(0).toUpperCase()+e.slice(1));function ns(e,n){if(!n)return;if(e(n))return e(n);return n.includes("-")?e(es(Yo(n))):e(es(n))||e(Jo(n))}const ts=Object.assign({},Wo,Ko),as=e=>ts[e],is=e=>Ko[e],rs=e=>Wo[e],os=e=>Wt.component(e);function ss(e){return ns(is,e)}function ls(e){return ns(rs,e)}function cs(e){return ns(as,e)}function ds(e){return ns(os,e)}function hs(...e){return Promise.all(e.filter(e=>e).map(async e=>{if(!ds(e)&&cs(e)){const n=await cs(e)();Wt.component(e,n.default)}}))}function ms(e,n){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[e]=n)}var us=t(147),ps=t.n(us),gs={created(){if(this.siteMeta=this.$site.headTags.filter(([e])=>"meta"===e).map(([e,n])=>n),this.$ssrContext){const n=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(e=n)?e.map(e=>{let n="<meta";return Object.keys(e).forEach(t=>{n+=` ${t}="${e[t]}"`}),n+">"}).join("\n    "):"",this.$ssrContext.canonicalLink=ys(this.$canonicalUrl)}var e},mounted(){this.currentMetaTags=[...document.querySelectorAll("meta")],this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta(){document.title=this.$title,document.documentElement.lang=this.$lang;const e=this.getMergedMetaTags();this.currentMetaTags=bs(e,this.currentMetaTags)},getMergedMetaTags(){const e=this.$page.frontmatter.meta||[];return ps()([{name:"description",content:this.$description}],e,this.siteMeta,vs)},updateCanonicalLink(){fs(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",ys(this.$canonicalUrl))}},watch:{$page(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy(){bs(null,this.currentMetaTags),fs()}};function fs(){const e=document.querySelector("link[rel='canonical']");e&&e.remove()}function ys(e=""){return e?`<link href="${e}" rel="canonical" />`:""}function bs(e,n){if(n&&[...n].filter(e=>e.parentNode===document.head).forEach(e=>document.head.removeChild(e)),e)return e.map(e=>{const n=document.createElement("meta");return Object.keys(e).forEach(t=>{n.setAttribute(t,e[t])}),document.head.appendChild(n),n})}function vs(e){for(const n of["name","property","itemprop"])if(e.hasOwnProperty(n))return e[n]+n;return JSON.stringify(e)}t(153);var ws=t(78),_s={mounted(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(ws)()((function(){this.setActiveHash()}),300),setActiveHash(){const e=[].slice.call(document.querySelectorAll(".sidebar-link")),n=[].slice.call(document.querySelectorAll(".header-anchor")).filter(n=>e.some(e=>e.hash===n.hash)),t=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),a=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),i=window.innerHeight+t;for(let e=0;e<n.length;e++){const r=n[e],o=n[e+1],s=0===e&&0===t||t>=r.parentElement.offsetTop+10&&(!o||t<o.parentElement.offsetTop-10),l=decodeURIComponent(this.$route.hash);if(s&&l!==decodeURIComponent(r.hash)){const t=r;if(i===a)for(let t=e+1;t<n.length;t++)if(l===decodeURIComponent(n[t].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(t.hash),()=>{this.$nextTick(()=>{this.$vuepress.$set("disableScrollBehavior",!1)})})}}}},beforeDestroy(){window.removeEventListener("scroll",this.onScroll)}},ks=t(48),xs=t.n(ks),Ts={mounted(){xs.a.configure({showSpinner:!1}),this.$router.beforeEach((e,n,t)=>{e.path===n.path||Wt.component(e.name)||xs.a.start(),t()}),this.$router.afterEach(()=>{xs.a.done(),this.isSidebarOpen=!1})}};t(302),t(303);class zs{constructor(){this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}show({text:e="",duration:n=3e3}){let t=document.createElement("div");t.className="message move-in",t.innerHTML=`\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">${e}</div>\n    `,this.containerEl.appendChild(t),n>0&&setTimeout(()=>{this.close(t)},n)}close(e){e.className=e.className.replace("move-in",""),e.className+="move-out",e.addEventListener("animationend",()=>{e.remove()})}}var Ms={mounted(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy(){setTimeout(()=>{(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach(e=>{document.querySelectorAll(e).forEach(this.generateCopyButton)})},1e3)},generateCopyButton(e){if(e.classList.contains("codecopy-enabled"))return;const n=document.createElement("i");n.className="code-copy",n.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',n.title="Copy to clipboard",n.addEventListener("click",()=>{this.copyToClipboard(e.innerText)}),e.appendChild(n),e.classList.add("codecopy-enabled")},copyToClipboard(e){const n=document.createElement("textarea");n.value=e,n.setAttribute("readonly",""),n.style.position="absolute",n.style.left="-9999px",document.body.appendChild(n);const t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);n.select(),document.execCommand("copy");(new zs).show({text:"复制成功",duration:1e3}),document.body.removeChild(n),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}},Ps="auto",Cs="zoom-in",As="zoom-out",Is="grab",Ss="move";function Ls(e,n,t){var a=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],i={passive:!1};a?e.addEventListener(n,t,i):e.removeEventListener(n,t,i)}function Us(e,n){if(e){var t=new Image;t.onload=function(){n&&n(t)},t.src=e}}function qs(e){return e.dataset.original?e.dataset.original:"A"===e.parentNode.tagName?e.parentNode.getAttribute("href"):null}function Es(e,n,t){!function(e){var n=Rs,t=Ds;if(e.transition){var a=e.transition;delete e.transition,e[n]=a}if(e.transform){var i=e.transform;delete e.transform,e[t]=i}}(n);var a=e.style,i={};for(var r in n)t&&(i[r]=a[r]||""),a[r]=n[r];return i}var Rs="transition",Ds="transform",Os="transform",Gs="transitionend";var Fs=function(){},Bs={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:Fs,onClose:Fs,onGrab:Fs,onMove:Fs,onRelease:Fs,onBeforeOpen:Fs,onBeforeClose:Fs,onBeforeGrab:Fs,onBeforeRelease:Fs,onImageLoading:Fs,onImageLoaded:Fs},Ns={init:function(e){var n,t;n=this,t=e,Object.getOwnPropertyNames(Object.getPrototypeOf(n)).forEach((function(e){n[e]=n[e].bind(t)}))},click:function(e){if(e.preventDefault(),Vs(e))return window.open(this.target.srcOriginal||e.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(e.currentTarget)},scroll:function(){var e=document.documentElement||document.body.parentNode||document.body,n=window.pageXOffset||e.scrollLeft,t=window.pageYOffset||e.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:n,y:t});var a=this.lastScrollPosition.x-n,i=this.lastScrollPosition.y-t,r=this.options.scrollThreshold;(Math.abs(i)>=r||Math.abs(a)>=r)&&(this.lastScrollPosition=null,this.close())},keydown:function(e){(function(e){return"Escape"===(e.key||e.code)||27===e.keyCode})(e)&&(this.released?this.close():this.release(this.close))},mousedown:function(e){if(js(e)&&!Vs(e)){e.preventDefault();var n=e.clientX,t=e.clientY;this.pressTimer=setTimeout(function(){this.grab(n,t)}.bind(this),200)}},mousemove:function(e){this.released||this.move(e.clientX,e.clientY)},mouseup:function(e){js(e)&&!Vs(e)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(e){e.preventDefault();var n=e.touches[0],t=n.clientX,a=n.clientY;this.pressTimer=setTimeout(function(){this.grab(t,a)}.bind(this),200)},touchmove:function(e){if(!this.released){var n=e.touches[0],t=n.clientX,a=n.clientY;this.move(t,a)}},touchend:function(e){(function(e){e.targetTouches.length})(e)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function js(e){return 0===e.button}function Vs(e){return e.metaKey||e.ctrlKey}var Hs={init:function(e){this.el=document.createElement("div"),this.instance=e,this.parent=document.body,Es(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(e.options),Ls(this.el,"click",e.handler.clickOverlay.bind(e))},updateStyle:function(e){Es(this.el,{zIndex:e.zIndex,backgroundColor:e.bgColor,transition:"opacity\n        "+e.transitionDuration+"s\n        "+e.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},$s="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e},Ws=function(){function e(e,n){for(var t=0;t<n.length;t++){var a=n[t];a.enumerable=a.enumerable||!1,a.configurable=!0,"value"in a&&(a.writable=!0),Object.defineProperty(e,a.key,a)}}return function(n,t,a){return t&&e(n.prototype,t),a&&e(n,a),n}}(),Ks=Object.assign||function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)Object.prototype.hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e},Qs={init:function(e,n){this.el=e,this.instance=n,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=qs(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var e=this.instance.options,n=e.zIndex,t=e.enableGrab,a=e.transitionDuration,i=e.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:n+1,cursor:t?Is:As,transition:Os+"\n        "+a+"s\n        "+i,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=Es(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,Es(this.el,{transform:"none"})},grab:function(e,n,t){var a=Xs(),i=a.x-e,r=a.y-n;Es(this.el,{cursor:Ss,transform:"translate3d(\n        "+(this.translate.x+i)+"px, "+(this.translate.y+r)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(e,n,t){var a=Xs(),i=a.x-e,r=a.y-n;Es(this.el,{transition:Os,transform:"translate3d(\n        "+(this.translate.x+i)+"px, "+(this.translate.y+r)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){Es(this.el,this.styleClose)},restoreOpenStyle:function(){Es(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var e=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var n=this.el.cloneNode(!1);n.setAttribute("src",this.srcOriginal),n.style.position="fixed",n.style.visibility="hidden",e.appendChild(n),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),e.removeChild(n)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var e=Xs(),n=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:e.x-n,y:e.y-t}},calculateScale:function(){var e=this.el.dataset,n=e.zoomingHeight,t=e.zoomingWidth,a=this.instance.options,i=a.customSize,r=a.scaleBase;if(!i&&n&&t)return{x:t/this.rect.width,y:n/this.rect.height};if(i&&"object"===(void 0===i?"undefined":$s(i)))return{x:i.width/this.rect.width,y:i.height/this.rect.height};var o=this.rect.width/2,s=this.rect.height/2,l=Xs(),c={x:l.x-o,y:l.y-s},d=c.x/o,h=c.y/s,m=r+Math.min(d,h);if(i&&"string"==typeof i){var u=t||this.el.naturalWidth,p=n||this.el.naturalHeight,g=parseFloat(i)*u/(100*this.rect.width),f=parseFloat(i)*p/(100*this.rect.height);if(m>g||m>f)return{x:g,y:f}}return{x:m,y:m}}};function Xs(){var e=document.documentElement;return{x:Math.min(e.clientWidth,window.innerWidth)/2,y:Math.min(e.clientHeight,window.innerHeight)/2}}function Ys(e,n,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(a){Ls(e,a,n[a],t)}))}var Zs=function(){function e(n){!function(e,n){if(!(e instanceof n))throw new TypeError("Cannot call a class as a function")}(this,e),this.target=Object.create(Qs),this.overlay=Object.create(Hs),this.handler=Object.create(Ns),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=Ks({},Bs,n),this.overlay.init(this),this.handler.init(this)}return Ws(e,[{key:"listen",value:function(e){if("string"==typeof e)for(var n=document.querySelectorAll(e),t=n.length;t--;)this.listen(n[t]);else"IMG"===e.tagName&&(e.style.cursor=Cs,Ls(e,"click",this.handler.click),this.options.preloadImage&&Us(qs(e)));return this}},{key:"config",value:function(e){return e?(Ks(this.options,e),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(e){var n=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var a="string"==typeof e?document.querySelector(e):e;if("IMG"===a.tagName){if(this.options.onBeforeOpen(a),this.target.init(a,this),!this.options.preloadImage){var i=this.target.srcOriginal;null!=i&&(this.options.onImageLoading(a),Us(i,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),Ls(document,"scroll",this.handler.scroll),Ls(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&Ls(window,"resize",this.handler.resizeWindow);var r=function e(){Ls(a,Gs,e,!1),n.lock=!1,n.target.upgradeSource(),n.options.enableGrab&&Ys(document,n.handler,!0),t(a)};return Ls(a,Gs,r),this}}}},{key:"close",value:function(){var e=this,n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=Ps,this.overlay.fadeOut(),this.target.zoomOut(),Ls(document,"scroll",this.handler.scroll,!1),Ls(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&Ls(window,"resize",this.handler.resizeWindow,!1);var a=function a(){Ls(t,Gs,a,!1),e.shown=!1,e.lock=!1,e.target.downgradeSource(),e.options.enableGrab&&Ys(document,e.handler,!1),e.target.restoreCloseStyle(),e.overlay.remove(),n(t)};return Ls(t,Gs,a),this}}},{key:"grab",value:function(e,n){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var i=this.target.el;this.options.onBeforeGrab(i),this.released=!1,this.target.grab(e,n,t);var r=function e(){Ls(i,Gs,e,!1),a(i)};return Ls(i,Gs,r),this}}},{key:"move",value:function(e,n){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=Ss,this.target.move(e,n,t);var i=this.target.el,r=function e(){Ls(i,Gs,e,!1),a(i)};return Ls(i,Gs,r),this}}},{key:"release",value:function(){var e=this,n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=Ps,this.target.restoreOpenStyle();var a=function a(){Ls(t,Gs,a,!1),e.lock=!1,e.released=!0,n(t)};return Ls(t,Gs,a),this}}}]),e}();const Js=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),el=Number("500");class nl{constructor(){this.instance=new Zs(Js)}update(e=".theme-vdoing-content img:not(.no-zoom)"){"undefined"!=typeof window&&this.instance.listen(e)}updateDelay(e=".theme-vdoing-content img:not(.no-zoom)",n=el){setTimeout(()=>this.update(e),n)}}var tl=[gs,_s,Ts,Ms,{watch:{"$page.path"(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted(){this.$vuepress.zooming=new nl,this.$vuepress.zooming.updateDelay()}}],al={name:"GlobalLayout",computed:{layout(){const e=this.getLayout();return ms("layout",e),Wt.component(e)}},methods:{getLayout(){if(this.$page.path){const e=this.$page.frontmatter.layout;return e&&(this.$vuepress.getLayoutAsyncComponent(e)||this.$vuepress.getVueComponent(e))?e:"Layout"}return"NotFound"}}},il=t(8),rl=Object(il.a)(al,(function(){return(0,this._self._c)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(e,n,t){switch(n){case"components":e[n]||(e[n]={}),Object.assign(e[n],t);break;case"mixins":e[n]||(e[n]=[]),e[n].push(...t);break;default:throw new Error("Unknown option name.")}}(rl,"mixins",tl);const ol=[{name:"v-0fdda314",path:"/pages/f27694/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-0fdda314").then(t)}},{path:"/pages/f27694/index.html",redirect:"/pages/f27694/"},{path:"/00.目录页/00.Content.html",redirect:"/pages/f27694/"},{name:"v-4553c3a8",path:"/compiler/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-4553c3a8").then(t)}},{path:"/compiler/index.html",redirect:"/compiler/"},{path:"/00.目录页/02.compiler.html",redirect:"/compiler/"},{name:"v-37c5bc96",path:"/hbm/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-37c5bc96").then(t)}},{path:"/hbm/index.html",redirect:"/hbm/"},{path:"/00.目录页/01.hbm.html",redirect:"/hbm/"},{name:"v-07282236",path:"/gpu/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-07282236").then(t)}},{path:"/gpu/index.html",redirect:"/gpu/"},{path:"/00.目录页/03.gpu.html",redirect:"/gpu/"},{name:"v-bd5cd4d4",path:"/cpu/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-bd5cd4d4").then(t)}},{path:"/cpu/index.html",redirect:"/cpu/"},{path:"/00.目录页/04.cpu.html",redirect:"/cpu/"},{name:"v-07abb6d6",path:"/llm/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-07abb6d6").then(t)}},{path:"/llm/index.html",redirect:"/llm/"},{path:"/00.目录页/05.llm.html",redirect:"/llm/"},{name:"v-577baade",path:"/unix/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-577baade").then(t)}},{path:"/unix/index.html",redirect:"/unix/"},{path:"/00.目录页/06.unix.html",redirect:"/unix/"},{name:"v-9c9c1094",path:"/mix/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-9c9c1094").then(t)}},{path:"/mix/index.html",redirect:"/mix/"},{path:"/00.目录页/07.mix.html",redirect:"/mix/"},{name:"v-abb0f8a2",path:"/pages/24769e/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-abb0f8a2").then(t)}},{path:"/pages/24769e/index.html",redirect:"/pages/24769e/"},{path:"/01.hbm/01.HBM_Paper_List.html",redirect:"/pages/24769e/"},{name:"v-b2fd417c",path:"/pages/24769f/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-b2fd417c").then(t)}},{path:"/pages/24769f/index.html",redirect:"/pages/24769f/"},{path:"/01.hbm/03.Dynamically_Adapting _Page_Migration_Policies_Based_on_Applications_Memory_Access_Behaviors.html",redirect:"/pages/24769f/"},{name:"v-9842a21a",path:"/pages/24760e/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-9842a21a").then(t)}},{path:"/pages/24760e/index.html",redirect:"/pages/24760e/"},{path:"/01.hbm/04.DRAM_PCM_NVM_Cache.html",redirect:"/pages/24760e/"},{name:"v-d7471e16",path:"/pages/2476af/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-d7471e16").then(t)}},{path:"/pages/2476af/index.html",redirect:"/pages/2476af/"},{path:"/01.hbm/02.hbm_dead_block_predictor.html",redirect:"/pages/2476af/"},{name:"v-7f8db936",path:"/pages/2476bf/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-7f8db936").then(t)}},{path:"/pages/2476bf/index.html",redirect:"/pages/2476bf/"},{path:"/01.hbm/05.cache_mem_compression.html",redirect:"/pages/2476bf/"},{name:"v-171f66aa",path:"/pages/f07695/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-171f66aa").then(t)}},{path:"/pages/f07695/index.html",redirect:"/pages/f07695/"},{path:"/01.hbm/06.memory ecc.html",redirect:"/pages/f07695/"},{name:"v-662ff4fd",path:"/pages/f07699/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-662ff4fd").then(t)}},{path:"/pages/f07699/index.html",redirect:"/pages/f07699/"},{path:"/01.hbm/09.compressibility_prediction.html",redirect:"/pages/f07699/"},{name:"v-788071c5",path:"/pages/f07696/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-788071c5").then(t)}},{path:"/pages/f07696/index.html",redirect:"/pages/f07696/"},{path:"/01.hbm/07.hbm-latency.html",redirect:"/pages/f07696/"},{name:"v-25995465",path:"/pages/f07698/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-25995465").then(t)}},{path:"/pages/f07698/index.html",redirect:"/pages/f07698/"},{path:"/01.hbm/08.compression.html",redirect:"/pages/f07698/"},{name:"v-065eff05",path:"/pages/f07692/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-065eff05").then(t)}},{path:"/pages/f07692/index.html",redirect:"/pages/f07692/"},{path:"/01.hbm/10.software_memory_paper.html",redirect:"/pages/f07692/"},{name:"v-0b217b45",path:"/pages/000002/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-0b217b45").then(t)}},{path:"/pages/000002/index.html",redirect:"/pages/000002/"},{path:"/02.compiler/02.GetStartedLLVMChap5Notes.html",redirect:"/pages/000002/"},{name:"v-2d0923f6",path:"/pages/000003/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-2d0923f6").then(t)}},{path:"/pages/000003/index.html",redirect:"/pages/000003/"},{path:"/02.compiler/03.GetStartedLLVMChap6Notes.html",redirect:"/pages/000003/"},{name:"v-3d658257",path:"/pages/000001/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-3d658257").then(t)}},{path:"/pages/000001/index.html",redirect:"/pages/000001/"},{path:"/02.compiler/01.llvm_flow.html",redirect:"/pages/000001/"},{name:"v-4d042c96",path:"/pages/000004/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-4d042c96").then(t)}},{path:"/pages/000004/index.html",redirect:"/pages/000004/"},{path:"/02.compiler/04. LearningLLVMDiary0.html",redirect:"/pages/000004/"},{name:"v-49e4dae0",path:"/pages/000005/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-49e4dae0").then(t)}},{path:"/pages/000005/index.html",redirect:"/pages/000005/"},{path:"/02.compiler/05. addInstACE.html",redirect:"/pages/000005/"},{name:"v-c73e5956",path:"/pages/000006/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-c73e5956").then(t)}},{path:"/pages/000006/index.html",redirect:"/pages/000006/"},{path:"/02.compiler/06.Value&Use.html",redirect:"/pages/000006/"},{name:"v-89581d5c",path:"/pages/000007/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-89581d5c").then(t)}},{path:"/pages/000007/index.html",redirect:"/pages/000007/"},{path:"/02.compiler/07. UnderstaningLLVMwithSourceCode.html",redirect:"/pages/000007/"},{name:"v-81151826",path:"/pages/000008/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-81151826").then(t)}},{path:"/pages/000008/index.html",redirect:"/pages/000008/"},{path:"/02.compiler/08.write_tinyriscv_backend.html",redirect:"/pages/000008/"},{name:"v-e9f07ff6",path:"/pages/000010/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-e9f07ff6").then(t)}},{path:"/pages/000010/index.html",redirect:"/pages/000010/"},{path:"/02.compiler/10.learn_tpu_mlir.html",redirect:"/pages/000010/"},{name:"v-e0b126ce",path:"/pages/000012/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-e0b126ce").then(t)}},{path:"/pages/000012/index.html",redirect:"/pages/000012/"},{path:"/02.compiler/12.auto_diff.html",redirect:"/pages/000012/"},{name:"v-49a96495",path:"/pages/000013/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-49a96495").then(t)}},{path:"/pages/000013/index.html",redirect:"/pages/000013/"},{path:"/02.compiler/13.mlir_notes_01.html",redirect:"/pages/000013/"},{name:"v-0d06efa2",path:"/pages/000009/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-0d06efa2").then(t)}},{path:"/pages/000009/index.html",redirect:"/pages/000009/"},{path:"/02.compiler/09.learn_tvm_1.html",redirect:"/pages/000009/"},{name:"v-e8215df6",path:"/pages/000011/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-e8215df6").then(t)}},{path:"/pages/000011/index.html",redirect:"/pages/000011/"},{path:"/02.compiler/11.learn_mlir_toy.html",redirect:"/pages/000011/"},{name:"v-7ba96ade",path:"/pages/000014/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-7ba96ade").then(t)}},{path:"/pages/000014/index.html",redirect:"/pages/000014/"},{path:"/02.compiler/14.mlir_notes_02.html",redirect:"/pages/000014/"},{name:"v-99a1d2ee",path:"/pages/000016/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-99a1d2ee").then(t)}},{path:"/pages/000016/index.html",redirect:"/pages/000016/"},{path:"/02.compiler/16.mlir_notes_04.html",redirect:"/pages/000016/"},{name:"v-2bb0fc85",path:"/pages/000017/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-2bb0fc85").then(t)}},{path:"/pages/000017/index.html",redirect:"/pages/000017/"},{path:"/02.compiler/17.mlir_notes_05.html",redirect:"/pages/000017/"},{name:"v-3aad308d",path:"/pages/000015/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-3aad308d").then(t)}},{path:"/pages/000015/index.html",redirect:"/pages/000015/"},{path:"/02.compiler/15.mlir_notes_03.html",redirect:"/pages/000015/"},{name:"v-b79a3afe",path:"/pages/000018/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-b79a3afe").then(t)}},{path:"/pages/000018/index.html",redirect:"/pages/000018/"},{path:"/02.compiler/18.mlir_notes_06.html",redirect:"/pages/000018/"},{name:"v-5841ee0f",path:"/pages/000020/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-5841ee0f").then(t)}},{path:"/pages/000020/index.html",redirect:"/pages/000020/"},{path:"/02.compiler/20.mlir_notes_08.html",redirect:"/pages/000020/"},{name:"v-1cb4c87d",path:"/pages/000019/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-1cb4c87d").then(t)}},{path:"/pages/000019/index.html",redirect:"/pages/000019/"},{path:"/02.compiler/19.mlir_notes_07.html",redirect:"/pages/000019/"},{name:"v-5e7857ea",path:"/pages/000021/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-5e7857ea").then(t)}},{path:"/pages/000021/index.html",redirect:"/pages/000021/"},{path:"/02.compiler/21.mlir_notes_09.html",redirect:"/pages/000021/"},{name:"v-6ffd4505",path:"/pages/cc7034/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-6ffd4505").then(t)}},{path:"/pages/cc7034/index.html",redirect:"/pages/cc7034/"},{path:"/03.gpu/01.operand_collector.html",redirect:"/pages/cc7034/"},{name:"v-b25c491a",path:"/pages/2476ae/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-b25c491a").then(t)}},{path:"/pages/2476ae/index.html",redirect:"/pages/2476ae/"},{path:"/03.gpu/02.warp_execution.html",redirect:"/pages/2476ae/"},{name:"v-f96426b0",path:"/pages/14769f/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-f96426b0").then(t)}},{path:"/pages/14769f/index.html",redirect:"/pages/14769f/"},{path:"/03.gpu/03.Precise Exception.html",redirect:"/pages/14769f/"},{name:"v-2861ec8b",path:"/pages/44871e/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-2861ec8b").then(t)}},{path:"/pages/44871e/index.html",redirect:"/pages/44871e/"},{path:"/03.gpu/05.TensorCore.html",redirect:"/pages/44871e/"},{name:"v-04292a45",path:"/pages/45871e/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-04292a45").then(t)}},{path:"/pages/45871e/index.html",redirect:"/pages/45871e/"},{path:"/03.gpu/06.MemoryBehaviour.html",redirect:"/pages/45871e/"},{name:"v-5849f946",path:"/pages/44771e/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-5849f946").then(t)}},{path:"/pages/44771e/index.html",redirect:"/pages/44771e/"},{path:"/03.gpu/04.Unified_Memory.html",redirect:"/pages/44771e/"},{name:"v-65bd31e5",path:"/pages/458720/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-65bd31e5").then(t)}},{path:"/pages/458720/index.html",redirect:"/pages/458720/"},{path:"/03.gpu/08.LLM.html",redirect:"/pages/458720/"},{name:"v-d22a70f6",path:"/pages/45871f/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-d22a70f6").then(t)}},{path:"/pages/45871f/index.html",redirect:"/pages/45871f/"},{path:"/03.gpu/07.GPUVirtualization.html",redirect:"/pages/45871f/"},{name:"v-e33712b6",path:"/pages/458721/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-e33712b6").then(t)}},{path:"/pages/458721/index.html",redirect:"/pages/458721/"},{path:"/03.gpu/09.Simulator.html",redirect:"/pages/458721/"},{name:"v-a0731602",path:"/pages/458722/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-a0731602").then(t)}},{path:"/pages/458722/index.html",redirect:"/pages/458722/"},{path:"/03.gpu/10. Architectural Survey.html",redirect:"/pages/458722/"},{name:"v-75a07a01",path:"/pages/458724/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-75a07a01").then(t)}},{path:"/pages/458724/index.html",redirect:"/pages/458724/"},{path:"/03.gpu/11.IntegratedCPUGPUMemory.html",redirect:"/pages/458724/"},{name:"v-5474106a",path:"/pages/458725/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-5474106a").then(t)}},{path:"/pages/458725/index.html",redirect:"/pages/458725/"},{path:"/03.gpu/12.gpgpusim.html",redirect:"/pages/458725/"},{name:"v-02aef12e",path:"/pages/47871e/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-02aef12e").then(t)}},{path:"/pages/47871e/index.html",redirect:"/pages/47871e/"},{path:"/03.gpu/1234.TODO.html",redirect:"/pages/47871e/"},{name:"v-4349358d",path:"/pages/458726/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-4349358d").then(t)}},{path:"/pages/458726/index.html",redirect:"/pages/458726/"},{path:"/03.gpu/13.gpgpusim.html",redirect:"/pages/458726/"},{name:"v-9e671962",path:"/pages/458727/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-9e671962").then(t)}},{path:"/pages/458727/index.html",redirect:"/pages/458727/"},{path:"/03.gpu/14.gpgpusim.html",redirect:"/pages/458727/"},{name:"v-1e4fb111",path:"/pages/45872/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-1e4fb111").then(t)}},{path:"/pages/45872/index.html",redirect:"/pages/45872/"},{path:"/03.gpu/15.gpgpusim.html",redirect:"/pages/45872/"},{name:"v-c25059fe",path:"/pages/45873/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-c25059fe").then(t)}},{path:"/pages/45873/index.html",redirect:"/pages/45873/"},{path:"/03.gpu/17.warp_mem.html",redirect:"/pages/45873/"},{name:"v-e85a225a",path:"/pages/45874/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-e85a225a").then(t)}},{path:"/pages/45874/index.html",redirect:"/pages/45874/"},{path:"/03.gpu/16.gpgpusim.html",redirect:"/pages/45874/"},{name:"v-798f0c72",path:"/pages/45875/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-798f0c72").then(t)}},{path:"/pages/45875/index.html",redirect:"/pages/45875/"},{path:"/03.gpu/18.gpucoherency.html",redirect:"/pages/45875/"},{name:"v-53bdf925",path:"/pages/45876/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-53bdf925").then(t)}},{path:"/pages/45876/index.html",redirect:"/pages/45876/"},{path:"/03.gpu/19.gpu_cache_mem.html",redirect:"/pages/45876/"},{name:"v-5552df36",path:"/pages/45877/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-5552df36").then(t)}},{path:"/pages/45877/index.html",redirect:"/pages/45877/"},{path:"/03.gpu/20.gpu_tlb.html",redirect:"/pages/45877/"},{name:"v-4301e3b6",path:"/pages/45878/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-4301e3b6").then(t)}},{path:"/pages/45878/index.html",redirect:"/pages/45878/"},{path:"/03.gpu/21.gpu_ptw.html",redirect:"/pages/45878/"},{name:"v-dcff77f6",path:"/pages/45879/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-dcff77f6").then(t)}},{path:"/pages/45879/index.html",redirect:"/pages/45879/"},{path:"/03.gpu/22.gpu_cache_paper.html",redirect:"/pages/45879/"},{name:"v-3690e719",path:"/pages/45882/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-3690e719").then(t)}},{path:"/pages/45882/index.html",redirect:"/pages/45882/"},{path:"/03.gpu/24.gpu_novel_um.html",redirect:"/pages/45882/"},{name:"v-6f6994e3",path:"/pages/45880/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-6f6994e3").then(t)}},{path:"/pages/45880/index.html",redirect:"/pages/45880/"},{path:"/03.gpu/23.gpu_warp_paper.html",redirect:"/pages/45880/"},{name:"v-eff2fe36",path:"/pages/45881/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-eff2fe36").then(t)}},{path:"/pages/45881/index.html",redirect:"/pages/45881/"},{path:"/03.gpu/24.gpu_memory_layout.html",redirect:"/pages/45881/"},{name:"v-27253dc5",path:"/pages/45883/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-27253dc5").then(t)}},{path:"/pages/45883/index.html",redirect:"/pages/45883/"},{path:"/03.gpu/25.gpu_multitask.html",redirect:"/pages/45883/"},{name:"v-70c0f165",path:"/pages/45884/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-70c0f165").then(t)}},{path:"/pages/45884/index.html",redirect:"/pages/45884/"},{path:"/03.gpu/26.gpu_cuda_training.html",redirect:"/pages/45884/"},{name:"v-66582056",path:"/pages/45885/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-66582056").then(t)}},{path:"/pages/45885/index.html",redirect:"/pages/45885/"},{path:"/03.gpu/27.gpu_paper_code.html",redirect:"/pages/45885/"},{name:"v-70c8fe25",path:"/pages/45886/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-70c8fe25").then(t)}},{path:"/pages/45886/index.html",redirect:"/pages/45886/"},{path:"/03.gpu/28.gpu_runtime.html",redirect:"/pages/45886/"},{name:"v-8a6d3836",path:"/pages/45887/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-8a6d3836").then(t)}},{path:"/pages/45887/index.html",redirect:"/pages/45887/"},{path:"/03.gpu/29.accel_sim.html",redirect:"/pages/45887/"},{name:"v-feef1d7a",path:"/pages/45889/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-feef1d7a").then(t)}},{path:"/pages/45889/index.html",redirect:"/pages/45889/"},{path:"/03.gpu/30.gpgpusim.html",redirect:"/pages/45889/"},{name:"v-072788bb",path:"/pages/45890/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-072788bb").then(t)}},{path:"/pages/45890/index.html",redirect:"/pages/45890/"},{path:"/03.gpu/31.gpu_inst.html",redirect:"/pages/45890/"},{name:"v-121d90a9",path:"/pages/cc7035/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-121d90a9").then(t)}},{path:"/pages/cc7035/index.html",redirect:"/pages/cc7035/"},{path:"/04.cpu/01.checkpoint.html",redirect:"/pages/cc7035/"},{name:"v-891a4db6",path:"/pages/cc7037/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-891a4db6").then(t)}},{path:"/pages/cc7037/index.html",redirect:"/pages/cc7037/"},{path:"/04.cpu/03.loadstore.html",redirect:"/pages/cc7037/"},{name:"v-23804245",path:"/pages/cc7036/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-23804245").then(t)}},{path:"/pages/cc7036/index.html",redirect:"/pages/cc7036/"},{path:"/04.cpu/02.trend.html",redirect:"/pages/cc7036/"},{name:"v-64b7ef30",path:"/pages/cc7038/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-64b7ef30").then(t)}},{path:"/pages/cc7038/index.html",redirect:"/pages/cc7038/"},{path:"/04.cpu/05.cache structure.html",redirect:"/pages/cc7038/"},{name:"v-90037b60",path:"/pages/cc7039/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-90037b60").then(t)}},{path:"/pages/cc7039/index.html",redirect:"/pages/cc7039/"},{path:"/04.cpu/06.cache timing.html",redirect:"/pages/cc7039/"},{name:"v-7ddc0f42",path:"/pages/cc7040/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-7ddc0f42").then(t)}},{path:"/pages/cc7040/index.html",redirect:"/pages/cc7040/"},{path:"/04.cpu/07.register file.html",redirect:"/pages/cc7040/"},{name:"v-c1b2e1c6",path:"/pages/f07697/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-c1b2e1c6").then(t)}},{path:"/pages/f07697/index.html",redirect:"/pages/f07697/"},{path:"/04.cpu/1234.markdown.html",redirect:"/pages/f07697/"},{name:"v-26a9c736",path:"/pages/dc7035/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-26a9c736").then(t)}},{path:"/pages/dc7035/index.html",redirect:"/pages/dc7035/"},{path:"/05.llm/01.How_LLM_Works.html",redirect:"/pages/dc7035/"},{name:"v-bdcb41a8",path:"/pages/cc7041/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-bdcb41a8").then(t)}},{path:"/pages/cc7041/index.html",redirect:"/pages/cc7041/"},{path:"/04.cpu/08. riscv.html",redirect:"/pages/cc7041/"},{name:"v-b267e276",path:"/pages/dc7036/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-b267e276").then(t)}},{path:"/pages/dc7036/index.html",redirect:"/pages/dc7036/"},{path:"/05.llm/02.LLM_HW_Opt.html",redirect:"/pages/dc7036/"},{name:"v-0ee7ecc5",path:"/pages/dc7038/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-0ee7ecc5").then(t)}},{path:"/pages/dc7038/index.html",redirect:"/pages/dc7038/"},{path:"/05.llm/04.mem_usage_llm.html",redirect:"/pages/dc7038/"},{name:"v-259f44f9",path:"/pages/dc7037/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-259f44f9").then(t)}},{path:"/pages/dc7037/index.html",redirect:"/pages/dc7037/"},{path:"/05.llm/03.gem5_LLAMA.html",redirect:"/pages/dc7037/"},{name:"v-c6222e76",path:"/pages/dc7039/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-c6222e76").then(t)}},{path:"/pages/dc7039/index.html",redirect:"/pages/dc7039/"},{path:"/05.llm/05.llm_optimizations.html",redirect:"/pages/dc7039/"},{name:"v-4edbad25",path:"/pages/dc7040/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-4edbad25").then(t)}},{path:"/pages/dc7040/index.html",redirect:"/pages/dc7040/"},{path:"/05.llm/06.llm_flash.html",redirect:"/pages/dc7040/"},{name:"v-e3c4ebf6",path:"/pages/dc7041/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-e3c4ebf6").then(t)}},{path:"/pages/dc7041/index.html",redirect:"/pages/dc7041/"},{path:"/05.llm/07.llm_bound.html",redirect:"/pages/dc7041/"},{name:"v-936502b6",path:"/pages/dc7042/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-936502b6").then(t)}},{path:"/pages/dc7042/index.html",redirect:"/pages/dc7042/"},{path:"/05.llm/08.llm_internals.html",redirect:"/pages/dc7042/"},{name:"v-2925b9e5",path:"/pages/dc7043/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-2925b9e5").then(t)}},{path:"/pages/dc7043/index.html",redirect:"/pages/dc7043/"},{path:"/05.llm/09.eff_llm.html",redirect:"/pages/dc7043/"},{name:"v-a65b4fb6",path:"/pages/dc7046/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-a65b4fb6").then(t)}},{path:"/pages/dc7046/index.html",redirect:"/pages/dc7046/"},{path:"/05.llm/11.inner_working.html",redirect:"/pages/dc7046/"},{name:"v-5ef480da",path:"/pages/dc7045/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-5ef480da").then(t)}},{path:"/pages/dc7045/index.html",redirect:"/pages/dc7045/"},{path:"/05.llm/10.llm_production.html",redirect:"/pages/dc7045/"},{name:"v-03195875",path:"/pages/dc7047/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-03195875").then(t)}},{path:"/pages/dc7047/index.html",redirect:"/pages/dc7047/"},{path:"/05.llm/12.llm_opt_list.html",redirect:"/pages/dc7047/"},{name:"v-43caabc5",path:"/pages/dc7048/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-43caabc5").then(t)}},{path:"/pages/dc7048/index.html",redirect:"/pages/dc7048/"},{path:"/05.llm/13.llm_mem_opt.html",redirect:"/pages/dc7048/"},{name:"v-393b6085",path:"/pages/dc7049/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-393b6085").then(t)}},{path:"/pages/dc7049/index.html",redirect:"/pages/dc7049/"},{path:"/05.llm/14.llm_reasoning.html",redirect:"/pages/dc7049/"},{name:"v-2bf21845",path:"/pages/dc7050/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-2bf21845").then(t)}},{path:"/pages/dc7050/index.html",redirect:"/pages/dc7050/"},{path:"/05.llm/15.llm_quant.html",redirect:"/pages/dc7050/"},{name:"v-041036ff",path:"/pages/dc7051/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-041036ff").then(t)}},{path:"/pages/dc7051/index.html",redirect:"/pages/dc7051/"},{path:"/05.llm/16.llm_sparsity.html",redirect:"/pages/dc7051/"},{name:"v-74b8ee65",path:"/pages/dc7052/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-74b8ee65").then(t)}},{path:"/pages/dc7052/index.html",redirect:"/pages/dc7052/"},{path:"/05.llm/17.llm_scale.html",redirect:"/pages/dc7052/"},{name:"v-3c10ac85",path:"/pages/dc7056/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-3c10ac85").then(t)}},{path:"/pages/dc7056/index.html",redirect:"/pages/dc7056/"},{path:"/05.llm/19.llm_kv_manage.html",redirect:"/pages/dc7056/"},{name:"v-082f1636",path:"/pages/dc7057/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-082f1636").then(t)}},{path:"/pages/dc7057/index.html",redirect:"/pages/dc7057/"},{path:"/05.llm/20. llm_distr.html",redirect:"/pages/dc7057/"},{name:"v-5c1f14b6",path:"/pages/dc7055/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-5c1f14b6").then(t)}},{path:"/pages/dc7055/index.html",redirect:"/pages/dc7055/"},{path:"/05.llm/18.llm_att.html",redirect:"/pages/dc7055/"},{name:"v-915e75f6",path:"/pages/dc7059/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-915e75f6").then(t)}},{path:"/pages/dc7059/index.html",redirect:"/pages/dc7059/"},{path:"/05.llm/21.llm_internals.html",redirect:"/pages/dc7059/"},{name:"v-77ef8fad",path:"/pages/dc7061/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-77ef8fad").then(t)}},{path:"/pages/dc7061/index.html",redirect:"/pages/dc7061/"},{path:"/05.llm/24.llm_comp.html",redirect:"/pages/dc7061/"},{name:"v-33912107",path:"/pages/dc7058/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-33912107").then(t)}},{path:"/pages/dc7058/index.html",redirect:"/pages/dc7058/"},{path:"/05.llm/22.llm_post.html",redirect:"/pages/dc7058/"},{name:"v-66fc8ee5",path:"/pages/dc7062/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-66fc8ee5").then(t)}},{path:"/pages/dc7062/index.html",redirect:"/pages/dc7062/"},{path:"/05.llm/25.llm_optimizer.html",redirect:"/pages/dc7062/"},{name:"v-5e4b7fb6",path:"/pages/dc7060/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-5e4b7fb6").then(t)}},{path:"/pages/dc7060/index.html",redirect:"/pages/dc7060/"},{path:"/05.llm/23.llm_moe.html",redirect:"/pages/dc7060/"},{name:"v-e114a476",path:"/pages/ec7035/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-e114a476").then(t)}},{path:"/pages/ec7035/index.html",redirect:"/pages/ec7035/"},{path:"/06.unix/01.malloc.html",redirect:"/pages/ec7035/"},{name:"v-0d7764c5",path:"/pages/ec7036/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-0d7764c5").then(t)}},{path:"/pages/ec7036/index.html",redirect:"/pages/ec7036/"},{path:"/06.unix/02.op.html",redirect:"/pages/ec7036/"},{name:"v-23bbae21",path:"/pages/f00000/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-23bbae21").then(t)}},{path:"/pages/f00000/index.html",redirect:"/pages/f00000/"},{path:"/10.mix/01.leakagecurrent.html",redirect:"/pages/f00000/"},{name:"v-bb1ed876",path:"/pages/ee5bf2/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-bb1ed876").then(t)}},{path:"/pages/ee5bf2/index.html",redirect:"/pages/ee5bf2/"},{path:"/09.nine/02.template.html",redirect:"/pages/ee5bf2/"},{name:"v-4a87ebca",path:"/pages/f00001/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-4a87ebca").then(t)}},{path:"/pages/f00001/index.html",redirect:"/pages/f00001/"},{path:"/10.mix/02.cuda_blogs.html",redirect:"/pages/f00001/"},{name:"v-2e715932",path:"/message-board/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-2e715932").then(t)}},{path:"/message-board/index.html",redirect:"/message-board/"},{path:"/09.nine/01.留言板.html",redirect:"/message-board/"},{name:"v-e225a34a",path:"/pages/f00002/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-e225a34a").then(t)}},{path:"/pages/f00002/index.html",redirect:"/pages/f00002/"},{path:"/10.mix/03.learn_pytorch_source.html",redirect:"/pages/f00002/"},{name:"v-7b8d66da",path:"/pages/f00004/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-7b8d66da").then(t)}},{path:"/pages/f00004/index.html",redirect:"/pages/f00004/"},{path:"/10.mix/05.cuda_merge.html",redirect:"/pages/f00004/"},{name:"v-ac6a65e2",path:"/pages/f00003/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-ac6a65e2").then(t)}},{path:"/pages/f00003/index.html",redirect:"/pages/f00003/"},{path:"/10.mix/04.learn_pytorch_source_aot.html",redirect:"/pages/f00003/"},{name:"v-29268d11",path:"/pages/f00006/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-29268d11").then(t)}},{path:"/pages/f00006/index.html",redirect:"/pages/f00006/"},{path:"/10.mix/06.cuda_softmax.html",redirect:"/pages/f00006/"},{name:"v-618db02d",path:"/pages/f00007/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-618db02d").then(t)}},{path:"/pages/f00007/index.html",redirect:"/pages/f00007/"},{path:"/10.mix/07.cuda_layernorm.html",redirect:"/pages/f00007/"},{name:"v-593a8e26",path:"/pages/f00009/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-593a8e26").then(t)}},{path:"/pages/f00009/index.html",redirect:"/pages/f00009/"},{path:"/10.mix/09.learn_pytorch_source_torch.html",redirect:"/pages/f00009/"},{name:"v-18aac47d",path:"/pages/f00008/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-18aac47d").then(t)}},{path:"/pages/f00008/index.html",redirect:"/pages/f00008/"},{path:"/10.mix/08.learn_pytorch_cuda.html",redirect:"/pages/f00008/"},{name:"v-a395c0f6",path:"/pages/f00010/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-a395c0f6").then(t)}},{path:"/pages/f00010/index.html",redirect:"/pages/f00010/"},{path:"/10.mix/10.cuda_pieces.html",redirect:"/pages/f00010/"},{name:"v-f8e02736",path:"/archives/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-f8e02736").then(t)}},{path:"/archives/index.html",redirect:"/archives/"},{path:"/@pages/archivesPage.html",redirect:"/archives/"},{name:"v-2f8a0a7e",path:"/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-2f8a0a7e").then(t)}},{path:"/index.html",redirect:"/"},{name:"v-01216c79",path:"/pages/dfd8e2/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-01216c79").then(t)}},{path:"/pages/dfd8e2/index.html",redirect:"/pages/dfd8e2/"},{path:"/pictures/addPictures.html",redirect:"/pages/dfd8e2/"},{name:"v-4b579c36",path:"/tags/",component:rl,beforeEnter:(e,n,t)=>{hs("Layout","v-4b579c36").then(t)}},{path:"/tags/index.html",redirect:"/tags/"},{path:"/@pages/tagsPage.html",redirect:"/tags/"},{path:"*",component:rl}],sl={title:"CPU & GPU Microarch. Qi Shao",description:"Computer System",base:"/qishao-notes/",headTags:[["link",{rel:"stylesheet",href:"custom.css"}],["meta",{name:"google-site-verification",content:"66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY"}],["meta",{name:"keywords",content:"Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin"}],["meta",{name:"theme-color",content:"#11a8cd"}],["meta",{name:"referrer",content:"no-referrer-when-downgrade"}],["script",{language:"javascript",type:"text/javascript",src:"/qishao-notes/js/pgmanor-self.js"}]],pages:[{title:"Content",frontmatter:{title:"Content",date:"2022-07-18T17:23:23.000Z",permalink:"/pages/f27694/",tags:[null]},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/00.Content.html",relativePath:"00.目录页/00.Content.md",key:"v-0fdda314",path:"/pages/f27694/",headersStr:null,content:" 1. CPU\n 2. GPU\n 3. Compiler\n 4. LLM\n 5. HBM\n 6. unix\n 7. mix",normalizedContent:" 1. cpu\n 2. gpu\n 3. compiler\n 4. llm\n 5. hbm\n 6. unix\n 7. mix",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"llvm & mlir",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"02.compiler"}},title:"llvm & mlir",date:"2023-11-21T11:05:54.000Z",permalink:"/compiler/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/02.compiler.html",relativePath:"00.目录页/02.compiler.md",key:"v-4553c3a8",path:"/compiler/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"HBM",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"01.hbm"}},title:"HBM",date:"2022-07-20T11:05:42.000Z",permalink:"/hbm/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/01.hbm.html",relativePath:"00.目录页/01.hbm.md",key:"v-37c5bc96",path:"/hbm/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"gpu",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"03.gpu"}},title:"gpu",date:"2022-07-20T11:05:54.000Z",permalink:"/gpu/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/03.gpu.html",relativePath:"00.目录页/03.gpu.md",key:"v-07282236",path:"/gpu/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"cpu",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"04.cpu"}},title:"cpu",date:"2023-11-09T15:54:15.000Z",permalink:"/cpu/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/04.cpu.html",relativePath:"00.目录页/04.cpu.md",key:"v-bd5cd4d4",path:"/cpu/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"llm",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"05.llm"}},title:"llm",date:"2024-01-02T15:54:15.000Z",permalink:"/llm/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/05.llm.html",relativePath:"00.目录页/05.llm.md",key:"v-07abb6d6",path:"/llm/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"unix",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"06.unix"}},title:"unix",date:"2024-03-03T15:54:15.000Z",permalink:"/unix/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/06.unix.html",relativePath:"00.目录页/06.unix.md",key:"v-577baade",path:"/unix/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"programing & mix",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"10.mix"}},title:"programing & mix",date:"2024-12-03T15:54:15.000Z",permalink:"/mix/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/07.mix.html",relativePath:"00.目录页/07.mix.md",key:"v-9c9c1094",path:"/mix/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"HBM Paper List",frontmatter:{title:"HBM Paper List",date:"2023-05-08T00:00:00.000Z",permalink:"/pages/24769e/",tags:[null]},regularPath:"/01.hbm/01.HBM_Paper_List.html",relativePath:"01.hbm/01.HBM_Paper_List.md",key:"v-abb0f8a2",path:"/pages/24769e/",headers:[{level:3,title:"2. CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache",slug:"_2-cameo-a-two-level-memory-organization-with-capacity-of-main-memory-and-flexibility-of-hardware-managed-cache",normalizedTitle:"2. cameo:a two-level memory organization with capacity of main memory and flexibility of hardware-managed cache",charIndex:1920},{level:3,title:"5. MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories",slug:"_5-mempod-a-clustered-architecture-for-efficient-and-scalable-migration-in-flat-address-space-multi-level-memories",normalizedTitle:"5. mempod: a clustered architecture for efficient and scalable migration in flat address space multi-level memories",charIndex:2752},{level:3,title:"6. Transparent Hardware Management of Stacked DRAM as Part of Memory",slug:"_6-transparent-hardware-management-of-stacked-dram-as-part-of-memory",normalizedTitle:"6. transparent hardware management of stacked dram as part of memory",charIndex:4917},{level:3,title:"8.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM",slug:"_8-batman-techniques-for-maximizing-system-bandwidth-of-memory-systems-with-stacked-dram",normalizedTitle:"8.batman: techniques for maximizing system bandwidth of memory systems with stacked-dram",charIndex:7541},{level:3,title:"9. Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories",slug:"_9-heterogeneous-memory-architectures-a-hw-sw-approach-for-mixing-die-stacked-and-off-package-memories",normalizedTitle:"9. heterogeneous memory architectures: a hw/sw approach for mixing die-stacked and off-package memories",charIndex:8728},{level:3,title:"10.Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems",slug:"_10-challenges-in-heterogeneous-die-stacked-and-off-chip-memory-systems",normalizedTitle:"10.challenges in heterogeneous die-stacked and off-chip memory systems",charIndex:10868},{level:3,title:"11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation",slug:"_11-banshee-bandwidth-efficient-dram-caching-via-software-hardware-cooperation",normalizedTitle:"11. banshee: bandwidth-efficient dram caching via software/hardware cooperation",charIndex:874},{level:3,title:"13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache",slug:"_13-unison-cache-a-scalable-and-effective-die-stacked-dram-cache",normalizedTitle:"13. unison cache: a scalable and effective die-stacked dram cache",charIndex:1006},{level:3,title:"14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors",slug:"_14-dynamically-adapting-page-migration-policies-based-on-applications-memory-access-behaviors",normalizedTitle:"14. dynamically adapting page migration policies based on applications memory access behaviors",charIndex:1073},{level:3,title:"15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems",slug:"_15-on-the-fly-page-migration-and-address-reconciliation-for-heterogeneous-memory-systems",normalizedTitle:"15. on-the-fly page migration and address reconciliation for heterogeneous memory systems",charIndex:1169}],headersStr:"2. CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache 5. MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories 6. Transparent Hardware Management of Stacked DRAM as Part of Memory 8.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM 9. Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories 10.Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems 11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation 13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache 14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors 15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems",content:" 1.  Baryon: Efficient Hybrid Memory Management with Compression and Sub-Blocking\n 2.  CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache\n 3.  Hybrid2: Combining Caching and Migration in Hybrid Memory Systems\n 4.  SILC-FM: Subblocked InterLeaved Cache-Like Flat Memory Organization\n 5.  MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories\n 6.  Transparent Hardware Management of Stacked DRAM as Part of Memory\n 7.  CHAMELEON: A Dynamically Reconfigurable Heterogeneous Memory System\n 8.  BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM\n 9.  Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories\n 10. Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems\n 11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation\n 12. Die-Stacked DRAM: Memory, Cache, or MemCache?\n 13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache\n 14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors\n 15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems\n 16. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems [DAC]\n 17. TicToc: Enabling Bandwidth-Efficient DRAM Caching for both Hits and Misses in Hybrid Memory Systems International Conference on Computer Design (ICCD)\n\nDRAM NVM\n\n 1. An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture\n 2. CLOCK-DWF: A Write-History-Aware Page Replacement Algorithm for Hybrid PCM and DRAM Memory Architectures\n 3. APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method\n 4. Page Placement in Hybrid Memory Systems\n\n----------------------------------------\n\n\n# 2. CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache\n\n# MemPod:\n\nCAMEO [13] proposes a cache-like flat address space memory management scheme in an attempt to close the gap between cache and flat memory organizations. CAMEO operates similarly to THM, however it does so at the granularity of cache lines (64B). Migrations are restricted within segments with one fast line location per segment. Its bookkeeping structures are entirely stored in memory, while a “Line Location Predictor” attempts to save some bookkeeping-related accesses by predicting the location of a line.\n\nCAMEO initiates a line migration upon every access to slow memory.\n\nCAMEO can incur high migration traffic as every access could induce a migration.\n\n\n\n----------------------------------------\n\n\n# 5. MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories\n\nYear: 2017\n\nMemPod uses MEA counters to track page access activity and identify hot pages. They are dramatically smaller than prior tracking mechanisms while capturing activity counts and temporal recency in a way that provides more effective prediction of future page access.\n\nWhat makes MEA most useful, though, is its failure mode – when it fails to find the most-accessed pages, it does so by favoring recency over quantity. That is, a page accessed several times near the end of an interval can easily knock out a page accessed many more times early in the interval. As a result, it combines both access counting and temporal locality, at a fraction of the cost of access counting alone.\n\n\n\nThree triggers are most commonly used whenever state must be updated based on tracking information (MC scheduling, migrations, dynamic voltage and frequency scaling etc.). Interval-based (or epoch-based) triggers occur with a set frequency, while threshold-based solutions trigger whenever a predetermined criterion is met. Finally, event-based triggers react to predefined events. Both interval-based and threshold-based approaches face the same challenge of identifying the optimal interval or threshold value.\n\nMemPod achieves the best performance (lower AMMAT) with 50us intervals and 64 counters per Pod. MemPod’s lightweight operation allows for such small intervals. For comparison purposes, HMA [14] identi-fied the best epoch length to be 100ms (2000x larger) in order to support all the lengthy processes that take place during a migration event for that method.\n\nBased on these results, we use 64 MEA 2-bit counters over 50us intervals for subsequent results in this paper. Each one of the 64 MEA entries needs 21 bits for addressing the 1.1M pages per Pod and 2 bits for its counter, leading to an area cost of only 184B per Pod and 736B total. Compared to the state of the art, MemPod’s activity tracking requirement is ∼712x smaller than THM’s (512KB) and ∼12800x smaller than HMA’s (9MB).\n\n\n\n----------------------------------------\n\n\n# 6. Transparent Hardware Management of Stacked DRAM as Part of Memory\n\n# MemPod:\n\nSim, et al. proposed a technique for transparent hardware management of a hybrid memory system [17],which we will refer to as “THM”. THM does not require OS intervention while managing migrations. In order to keep bookkeeping costs manageable, THM allows migrations only within sets of pages (called segments). Each segment includes one fast memory page and a set of slow memory pages. The slow pages of each segment can only migrate to the one fast page location, and any such migration results in the eviction of the currently-residing page. THM monitors memory accesses with one “competing counter” per segment resulting in a low cost profiling solution. Finally, THM supports caching part of its structures on chip while the rest is stored in memory.\n\nTHM’s competing counters can lead to false positives, allowing a cold page to migrate to fast memory.\n\nTHM offers significantly limited flexibility by restricting migrations withing segments, however this decision reduces bookkeeping costs significantly. Competing counters in each segment are used for activity tracking, occasionally leading to false (threshold-based) migration triggering if a cold page gets accessed at the right time. Identifying migration candidates incurs very little overhead since there is exactly one fast memory location for each slow memory page that triggers migration.\n\nCite from paper\n\n\n\nSimilar to set dueling, they adopted sample region. The locations in fast memory are grouped into 32 distinct regions in an interleaving fashion, and four regions are dedicated to sampling, while other 28 regions follow the threshold decision from sampling.\n\n• Nstatic: # of memory requests serviced from fast memory with static mapping • Ndynamic: # of memory requests expected to be serviced from fast memory when swapping with a given threshold • Nswap: # of expected swaps for a given threshold.\n\n\n\nK differs depending on the relative latency of fast and slow memory. The cost of a single fast swap is about 1200 cycles, and the difference in access latency between fast and slow memory is 72 cycles.Thus, in general, the swapped-in segment needs to get at least 17 more (future) hits than the swapped-out segment for swapping to be valuable. K is computed in hardware at boot time.\n\nMe Competing counter needs a threshold to invoke swap.\n\nSample region will have different threshold. Based on different threshold, Nswap is different. Thus they choose the the best threshold with max Bexpected as candidate threshold.\n\n----------------------------------------\n\n\n# 8.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM\n\nInsights\n\n * bandwidth distribution\n * dram and hbm similar latency\n\nAs the NM simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. We observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.\n\nWe leverage our key insight on controlling data movement and propose Bandwidth-Aware Tiered-Memory Management (BATMAN), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the NM and the FM. We define the desired access rate of the NM as the target access rate (TAR). TAR is the fraction of memory accesses serviced by the NM when memory accesses to both memories are proportional to the respective bandwidth.\n\n\n\n2X 2/3 4X 4/5 8X 8/9\n\nMe Bandwidth-Aware Tired-Memory Management tries to distritube memory according to HBM and DRAM bandwidth ratio. And also treat it as a threshold to refuse page migration.\n\n----------------------------------------\n\n\n# 9. Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories\n\n# MemPod:\n\nHMA [14] is a HW/SW mechanism that attempts to predict frequently accessed pages in memory and, at predefined intervals, migrate those pages to fast memory. HW support is required for profiling memory accesses using counters for each memory page, while the migration is handled by the OS. Due to the costly OS involvement, HMA’s intervals are kept large. Additionally, the hardware cost of its profiling counters is high. However, HMA is capable of managing migrations in a flat address space without the need of additional bookkeeping for finding migrated pages as the OS can update page tables and TLBs to reflect migrations.\n\nHMA does not require a remap table due to the OS updating the existing system’s structures. For activity tracking it uses Full Counters. The costly OS involvement and the high penalty for sorting all its counters force HMA to operate at very large intervals, weakening its adaptability to phase changes. However, HMA offers full flexibility for migrations.\n\nInterrupt and TLB shoot down assumption A fixed 5us time penalty is charged for each page fault [27] to cover the basic interrupt costs, and then another 3uspenalty is applied whenever a TLB shootdown [33] is required.\n\nCite from org paper This first-touch hot-page (FTHP) policy is effectively a generalization of both the history-based and first-touch algorithms.\n\nWe propose a dynamic feedback-directed HMA policy that can dynamically adjust the hotness threshold θ to achieve a best-of-both-worlds approach between history-based and first-touch policies.\n\n * At the start of each epoch, the size of the hot set is compared to the size of the die-stacked DRAM (N).\n * If the hot set is too small to fill the fast memory, then θ is lowered which causes more pages to be classified as hot.\n * Likewise, if the hot set is too large, θ is increased which causes fewer pages to be put in the hot set.\n * If the feedback mechanism works well, then the size of the hot set should converge to N.\n\n----------------------------------------\n\n\n# 10.Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems\n\nYear: 2012 Software OS management To prevent the mapping of pages with insufficient miss traffic, we employ a threshold θ such that any page with fewer than θ LLC misses is not considered for mapping into stacked DRAM. The application of a threshold may result in cases when the list of most frequently missed pages has only k < P items. In this case, we simply keep a random set of P − k of the existing pages from the previous epoch already in stacked DRAM to avoid consuming bandwidth to swap out the page back to the off-chip memory.\n\n----------------------------------------\n\n\n# 11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation\n\nYear: 2017 Software aovid tag look up by storing DRAM Cache presence information in the page table and tlbs.\n\nSpecifically, Banshee uses a hardwaremanaged frequency-based replacement (FBR) policy that only caches hot pages to reduce unnecessary data replacement traffic. To reduce the cost of accessing/updating frequency counters (which are stored in in-package DRAM), Banshee uses a new sampling approach to only read/write counters for a fraction of memory accesses.\n\nBackground\n\n * Using tags Alloy Cache | Unison Cache\n * Using address remapping Heterogeneous Memory Architecture (HMA) | Tagless DRAM Cache (TDC)\n\n\n\nThey reuse reverse maping mechanism.\n\nBanshee tracks each page’s access frequency with a counter, stored in the metadata. We store counters not only for the pages in the DRAM cache, but also for some pages not in cache, which are candidates to bring into the cache.\n\nInstead, an access in Banshee only updates a page’s frequency counter with a certain sample rate. For a sample rate of 10%, for example, the frequency counters are accessed/updated only once for every 10 DRAM accesses.\n\nFrequency-based replacement may lead to thrashing problem.\n\nBanshee solves this problem by only replacing a page when the candidate’s counter is greater than the victim’s counter by a certain threshold. This ensures that a page just evicted from the DRAM cache must be accessed for at least 2·threshold/sampling rate times before it can enter the cache again, thus preventing a page from entering and leaving frequently.\n\nBy default, the threshold is the product of the number of cachelines in a page and the sampling coefficient divided by two (threshold = page_size x sampling_coeff / 2). Intuitively, this means replacement can happen only if the benefit of swapping the pages outweighs the cost of the replacement operation.\n\nIf a counter saturates after being incremented, all counters in the metadata will be reduced by half using a shift operation in hardware.\n\n\n\n----------------------------------------\n\n\n# 13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache\n\nThe state-of-the-art block-based design, called Alloy Cache, colocates a tag with each data block (e.g., 64B) in the stacked DRAM to provide fast access to data in a single DRAM access. However, such a design suffers from low hit rates due to poor temporal locality in the DRAM cache. In contrast, the state-of-the-art page-based design, called Footprint Cache, organizes the DRAM cache at page granularity (e.g., 4KB), but fetches only the blocks that will likely be touched within a page. In doing so, the Footprint Cache achieves high hit rates with moderate on-chip tag storage and reasonable lookup latency. However, multi-gigabyte stacked DRAM caches will soon be practical and needed by server applications, thereby mandating tens of MBs of tag storage even for page-based DRAM caches.\n\nWe introduce a novel stacked-DRAM cache design, Unison Cache. Similar to Alloy Cache’s approach, Unison Cache incorporates the tag metadata directly into the stacked DRAM to enable scalability to arbitrary stacked-DRAM capacities. Then, leveraging the insights from the Footprint Cache design, Unison Cache employs large, page-sized cache allocation units to achieve high hit rates and reduction in tag overheads, while predicting and fetching only the useful blocks within each page to minimize the off-chip traffic. Our evaluation using server workloads and caches of up to 8GB reveals that Unison cache improves performance by 14% compared to Alloy Cache due to its high hit rate, while outperforming the state-of-the art page-based designs that require impractical SRAM-based tags of around 50MB.\n\nMe Block-based high miss ratio, page-based high migration penalty when fetching useless data. If footprint meta data is adopted to collect data trace, FootCache meta table cannot scale with increasing capactiy of HBM.\n\n\n\n----------------------------------------\n\n\n# 14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors\n\nLink to notes for Dynamically Adapting...\n\n\n# 15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems\n\n“on-the-fly” migration performs better than epoch-based page migration techniques, since we migrate recent hot pages.\n\nInstead of relying completely on OS to perform AR for the evicted entries, as done in [Ramoset al. 2011], we propose a hardware-based AR, where the MigC hardware initiates TLB shootdown and cache flushing without explicitly stopping the user program.\n\nLike previous studies [Meswani et al. 2015; Prodromou et al. 2017; Su et al. 2015], we observe that not all applications benefit from page migration, since page migration incurs performance overheads due to extra data movement.\n\nThe model works with the principle that, to get performance benefit, one should migrate the smallest set of pages from slow to fast memory that yields in the largest increase in memory accesses to fast memory to amortize the migration overhead.\n\nMe Try to use 80% principle. In our study, we look at the 80-percentile accesses and identify the “set of top-accessed pages” that contribute to more than 80% of all memory accesses.\n\nIn our proposal, we migrate a page immediately when it receives sufficient number of memory accesses, unlike any epoch-based schemes. We allow full flexibility in page relocation like HMAHS [Meswani et al. 2015] and keep a remap table for address redirection. We keep this table small by periodically evicting entries and it is placed on-chip.\n\nThe particular access count, which can separate such top-accessed pages from other pages, is referred to as “filter count.” Note that, filter count indicates an upper bound for hotness threshold\n\nMe This paper explain the address reconciliation AR in detail.\n\nFlow of AR -First, all cache lines from these pages, which are currently residing in the cache hierarchies and tagged with OS-visible PA, must be invalidated (and dirty lines written back), since the current OS-visible PA will be replaced with the new PA. All future accesses to these pages will only have access to the new PA. -Next,corresponding page table entries (PTEs) for A and B need to be updated with new PAs. -The TLB entries in all cores using the old PA must also be invalidated (as well as any other OS structures that contain the physical page addresses).\n\n(i) flush_cache_page() (ii) change PTE, (iii) flush_tlb_page()\n\nWe use a Migration Benefit Quotient (MBQ), by calculating the difference between the total number of accesses to any page and the filter count used in classifying applications’ memory locality, as described. MBQ indicates how many of the future accesses of a page may go to fast memory if the page were migrated from slow to fast memory using the filter count as a hotness threshold.\n\nSacturation account the sum of access counts of all pages with 32,909 accesses or less (the bars corresponding to x-axis 0 to 3,290) accounts for 98% of all accesses. For our purposes, we use this access count as a saturation count (or assume that the maximum number of accesses any page can receive).\n\nFor each workload, we use the difference between the saturation count and the filter count to determine MBQ.\n\n\n\n 1. Low MBQ, difference is less than 1K. For example, in Figure 5(b) xalanc, pages with memory access count 609 or less (the bars corresponding to x-axis 0 to 60) provide 98% of the memory accesses. Hence, the difference between saturation count (609) and filter count (70) is 539. These workloads may not achieve significant increase in accesses to fast memory after page migration.\n 2. Medium MBQ, difference is in between 1K to K (e.g., Figure 5(c) omnetpp). These workloads may receive moderate benefits, depending the migration overheads.\n 3. High MBQ, difference is more than 8K (e.g., Figure 5(a) mcf). These workloads are likely to receive higher hits in faster memory as a result of page migration.\n\nMe In short\n\n * sactuartion account. The sum of access counts fo all pages with sactuartion account accounts for 98% of all accesses.\n * filter account. The “set of top-accessed pages” that contribute to more than 80% of all memory accesses. If they are close, it means that this is a uniform benchmark. If they have a large difference, this means that there is a subset of pages that has far more memory accesses.\n\n----------------------------------------\n\n# 17. TicToc: Enabling Bandwidth-Efficient DRAM Caching for both Hits and Misses in Hybrid Memory Systems International Conference on Computer Design (ICCD)\n\nYear: 2019",normalizedContent:" 1.  baryon: efficient hybrid memory management with compression and sub-blocking\n 2.  cameo:a two-level memory organization with capacity of main memory and flexibility of hardware-managed cache\n 3.  hybrid2: combining caching and migration in hybrid memory systems\n 4.  silc-fm: subblocked interleaved cache-like flat memory organization\n 5.  mempod: a clustered architecture for efficient and scalable migration in flat address space multi-level memories\n 6.  transparent hardware management of stacked dram as part of memory\n 7.  chameleon: a dynamically reconfigurable heterogeneous memory system\n 8.  batman: techniques for maximizing system bandwidth of memory systems with stacked-dram\n 9.  heterogeneous memory architectures: a hw/sw approach for mixing die-stacked and off-package memories\n 10. challenges in heterogeneous die-stacked and off-chip memory systems\n 11. banshee: bandwidth-efficient dram caching via software/hardware cooperation\n 12. die-stacked dram: memory, cache, or memcache?\n 13. unison cache: a scalable and effective die-stacked dram cache\n 14. dynamically adapting page migration policies based on applications memory access behaviors\n 15. on-the-fly page migration and address reconciliation for heterogeneous memory systems\n 16. bumblebee: a memcache design for die-stacked and off-chip heterogeneous memory systems [dac]\n 17. tictoc: enabling bandwidth-efficient dram caching for both hits and misses in hybrid memory systems international conference on computer design (iccd)\n\ndram nvm\n\n 1. an operating system level data migration scheme in hybrid dram-nvm memory architecture\n 2. clock-dwf: a write-history-aware page replacement algorithm for hybrid pcm and dram memory architectures\n 3. apmigration: improving performance of hybrid memory performance via an adaptive page migration method\n 4. page placement in hybrid memory systems\n\n----------------------------------------\n\n\n# 2. cameo:a two-level memory organization with capacity of main memory and flexibility of hardware-managed cache\n\n# mempod:\n\ncameo [13] proposes a cache-like flat address space memory management scheme in an attempt to close the gap between cache and flat memory organizations. cameo operates similarly to thm, however it does so at the granularity of cache lines (64b). migrations are restricted within segments with one fast line location per segment. its bookkeeping structures are entirely stored in memory, while a “line location predictor” attempts to save some bookkeeping-related accesses by predicting the location of a line.\n\ncameo initiates a line migration upon every access to slow memory.\n\ncameo can incur high migration traffic as every access could induce a migration.\n\n\n\n----------------------------------------\n\n\n# 5. mempod: a clustered architecture for efficient and scalable migration in flat address space multi-level memories\n\nyear: 2017\n\nmempod uses mea counters to track page access activity and identify hot pages. they are dramatically smaller than prior tracking mechanisms while capturing activity counts and temporal recency in a way that provides more effective prediction of future page access.\n\nwhat makes mea most useful, though, is its failure mode – when it fails to find the most-accessed pages, it does so by favoring recency over quantity. that is, a page accessed several times near the end of an interval can easily knock out a page accessed many more times early in the interval. as a result, it combines both access counting and temporal locality, at a fraction of the cost of access counting alone.\n\n\n\nthree triggers are most commonly used whenever state must be updated based on tracking information (mc scheduling, migrations, dynamic voltage and frequency scaling etc.). interval-based (or epoch-based) triggers occur with a set frequency, while threshold-based solutions trigger whenever a predetermined criterion is met. finally, event-based triggers react to predefined events. both interval-based and threshold-based approaches face the same challenge of identifying the optimal interval or threshold value.\n\nmempod achieves the best performance (lower ammat) with 50us intervals and 64 counters per pod. mempod’s lightweight operation allows for such small intervals. for comparison purposes, hma [14] identi-fied the best epoch length to be 100ms (2000x larger) in order to support all the lengthy processes that take place during a migration event for that method.\n\nbased on these results, we use 64 mea 2-bit counters over 50us intervals for subsequent results in this paper. each one of the 64 mea entries needs 21 bits for addressing the 1.1m pages per pod and 2 bits for its counter, leading to an area cost of only 184b per pod and 736b total. compared to the state of the art, mempod’s activity tracking requirement is ∼712x smaller than thm’s (512kb) and ∼12800x smaller than hma’s (9mb).\n\n\n\n----------------------------------------\n\n\n# 6. transparent hardware management of stacked dram as part of memory\n\n# mempod:\n\nsim, et al. proposed a technique for transparent hardware management of a hybrid memory system [17],which we will refer to as “thm”. thm does not require os intervention while managing migrations. in order to keep bookkeeping costs manageable, thm allows migrations only within sets of pages (called segments). each segment includes one fast memory page and a set of slow memory pages. the slow pages of each segment can only migrate to the one fast page location, and any such migration results in the eviction of the currently-residing page. thm monitors memory accesses with one “competing counter” per segment resulting in a low cost profiling solution. finally, thm supports caching part of its structures on chip while the rest is stored in memory.\n\nthm’s competing counters can lead to false positives, allowing a cold page to migrate to fast memory.\n\nthm offers significantly limited flexibility by restricting migrations withing segments, however this decision reduces bookkeeping costs significantly. competing counters in each segment are used for activity tracking, occasionally leading to false (threshold-based) migration triggering if a cold page gets accessed at the right time. identifying migration candidates incurs very little overhead since there is exactly one fast memory location for each slow memory page that triggers migration.\n\ncite from paper\n\n\n\nsimilar to set dueling, they adopted sample region. the locations in fast memory are grouped into 32 distinct regions in an interleaving fashion, and four regions are dedicated to sampling, while other 28 regions follow the threshold decision from sampling.\n\n• nstatic: # of memory requests serviced from fast memory with static mapping • ndynamic: # of memory requests expected to be serviced from fast memory when swapping with a given threshold • nswap: # of expected swaps for a given threshold.\n\n\n\nk differs depending on the relative latency of fast and slow memory. the cost of a single fast swap is about 1200 cycles, and the difference in access latency between fast and slow memory is 72 cycles.thus, in general, the swapped-in segment needs to get at least 17 more (future) hits than the swapped-out segment for swapping to be valuable. k is computed in hardware at boot time.\n\nme competing counter needs a threshold to invoke swap.\n\nsample region will have different threshold. based on different threshold, nswap is different. thus they choose the the best threshold with max bexpected as candidate threshold.\n\n----------------------------------------\n\n\n# 8.batman: techniques for maximizing system bandwidth of memory systems with stacked-dram\n\ninsights\n\n * bandwidth distribution\n * dram and hbm similar latency\n\nas the nm simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. we observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.\n\nwe leverage our key insight on controlling data movement and propose bandwidth-aware tiered-memory management (batman), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the nm and the fm. we define the desired access rate of the nm as the target access rate (tar). tar is the fraction of memory accesses serviced by the nm when memory accesses to both memories are proportional to the respective bandwidth.\n\n\n\n2x 2/3 4x 4/5 8x 8/9\n\nme bandwidth-aware tired-memory management tries to distritube memory according to hbm and dram bandwidth ratio. and also treat it as a threshold to refuse page migration.\n\n----------------------------------------\n\n\n# 9. heterogeneous memory architectures: a hw/sw approach for mixing die-stacked and off-package memories\n\n# mempod:\n\nhma [14] is a hw/sw mechanism that attempts to predict frequently accessed pages in memory and, at predefined intervals, migrate those pages to fast memory. hw support is required for profiling memory accesses using counters for each memory page, while the migration is handled by the os. due to the costly os involvement, hma’s intervals are kept large. additionally, the hardware cost of its profiling counters is high. however, hma is capable of managing migrations in a flat address space without the need of additional bookkeeping for finding migrated pages as the os can update page tables and tlbs to reflect migrations.\n\nhma does not require a remap table due to the os updating the existing system’s structures. for activity tracking it uses full counters. the costly os involvement and the high penalty for sorting all its counters force hma to operate at very large intervals, weakening its adaptability to phase changes. however, hma offers full flexibility for migrations.\n\ninterrupt and tlb shoot down assumption a fixed 5us time penalty is charged for each page fault [27] to cover the basic interrupt costs, and then another 3uspenalty is applied whenever a tlb shootdown [33] is required.\n\ncite from org paper this first-touch hot-page (fthp) policy is effectively a generalization of both the history-based and first-touch algorithms.\n\nwe propose a dynamic feedback-directed hma policy that can dynamically adjust the hotness threshold θ to achieve a best-of-both-worlds approach between history-based and first-touch policies.\n\n * at the start of each epoch, the size of the hot set is compared to the size of the die-stacked dram (n).\n * if the hot set is too small to fill the fast memory, then θ is lowered which causes more pages to be classified as hot.\n * likewise, if the hot set is too large, θ is increased which causes fewer pages to be put in the hot set.\n * if the feedback mechanism works well, then the size of the hot set should converge to n.\n\n----------------------------------------\n\n\n# 10.challenges in heterogeneous die-stacked and off-chip memory systems\n\nyear: 2012 software os management to prevent the mapping of pages with insufficient miss traffic, we employ a threshold θ such that any page with fewer than θ llc misses is not considered for mapping into stacked dram. the application of a threshold may result in cases when the list of most frequently missed pages has only k < p items. in this case, we simply keep a random set of p − k of the existing pages from the previous epoch already in stacked dram to avoid consuming bandwidth to swap out the page back to the off-chip memory.\n\n----------------------------------------\n\n\n# 11. banshee: bandwidth-efficient dram caching via software/hardware cooperation\n\nyear: 2017 software aovid tag look up by storing dram cache presence information in the page table and tlbs.\n\nspecifically, banshee uses a hardwaremanaged frequency-based replacement (fbr) policy that only caches hot pages to reduce unnecessary data replacement traffic. to reduce the cost of accessing/updating frequency counters (which are stored in in-package dram), banshee uses a new sampling approach to only read/write counters for a fraction of memory accesses.\n\nbackground\n\n * using tags alloy cache | unison cache\n * using address remapping heterogeneous memory architecture (hma) | tagless dram cache (tdc)\n\n\n\nthey reuse reverse maping mechanism.\n\nbanshee tracks each page’s access frequency with a counter, stored in the metadata. we store counters not only for the pages in the dram cache, but also for some pages not in cache, which are candidates to bring into the cache.\n\ninstead, an access in banshee only updates a page’s frequency counter with a certain sample rate. for a sample rate of 10%, for example, the frequency counters are accessed/updated only once for every 10 dram accesses.\n\nfrequency-based replacement may lead to thrashing problem.\n\nbanshee solves this problem by only replacing a page when the candidate’s counter is greater than the victim’s counter by a certain threshold. this ensures that a page just evicted from the dram cache must be accessed for at least 2·threshold/sampling rate times before it can enter the cache again, thus preventing a page from entering and leaving frequently.\n\nby default, the threshold is the product of the number of cachelines in a page and the sampling coefficient divided by two (threshold = page_size x sampling_coeff / 2). intuitively, this means replacement can happen only if the benefit of swapping the pages outweighs the cost of the replacement operation.\n\nif a counter saturates after being incremented, all counters in the metadata will be reduced by half using a shift operation in hardware.\n\n\n\n----------------------------------------\n\n\n# 13. unison cache: a scalable and effective die-stacked dram cache\n\nthe state-of-the-art block-based design, called alloy cache, colocates a tag with each data block (e.g., 64b) in the stacked dram to provide fast access to data in a single dram access. however, such a design suffers from low hit rates due to poor temporal locality in the dram cache. in contrast, the state-of-the-art page-based design, called footprint cache, organizes the dram cache at page granularity (e.g., 4kb), but fetches only the blocks that will likely be touched within a page. in doing so, the footprint cache achieves high hit rates with moderate on-chip tag storage and reasonable lookup latency. however, multi-gigabyte stacked dram caches will soon be practical and needed by server applications, thereby mandating tens of mbs of tag storage even for page-based dram caches.\n\nwe introduce a novel stacked-dram cache design, unison cache. similar to alloy cache’s approach, unison cache incorporates the tag metadata directly into the stacked dram to enable scalability to arbitrary stacked-dram capacities. then, leveraging the insights from the footprint cache design, unison cache employs large, page-sized cache allocation units to achieve high hit rates and reduction in tag overheads, while predicting and fetching only the useful blocks within each page to minimize the off-chip traffic. our evaluation using server workloads and caches of up to 8gb reveals that unison cache improves performance by 14% compared to alloy cache due to its high hit rate, while outperforming the state-of-the art page-based designs that require impractical sram-based tags of around 50mb.\n\nme block-based high miss ratio, page-based high migration penalty when fetching useless data. if footprint meta data is adopted to collect data trace, footcache meta table cannot scale with increasing capactiy of hbm.\n\n\n\n----------------------------------------\n\n\n# 14. dynamically adapting page migration policies based on applications memory access behaviors\n\nlink to notes for dynamically adapting...\n\n\n# 15. on-the-fly page migration and address reconciliation for heterogeneous memory systems\n\n“on-the-fly” migration performs better than epoch-based page migration techniques, since we migrate recent hot pages.\n\ninstead of relying completely on os to perform ar for the evicted entries, as done in [ramoset al. 2011], we propose a hardware-based ar, where the migc hardware initiates tlb shootdown and cache flushing without explicitly stopping the user program.\n\nlike previous studies [meswani et al. 2015; prodromou et al. 2017; su et al. 2015], we observe that not all applications benefit from page migration, since page migration incurs performance overheads due to extra data movement.\n\nthe model works with the principle that, to get performance benefit, one should migrate the smallest set of pages from slow to fast memory that yields in the largest increase in memory accesses to fast memory to amortize the migration overhead.\n\nme try to use 80% principle. in our study, we look at the 80-percentile accesses and identify the “set of top-accessed pages” that contribute to more than 80% of all memory accesses.\n\nin our proposal, we migrate a page immediately when it receives sufficient number of memory accesses, unlike any epoch-based schemes. we allow full flexibility in page relocation like hmahs [meswani et al. 2015] and keep a remap table for address redirection. we keep this table small by periodically evicting entries and it is placed on-chip.\n\nthe particular access count, which can separate such top-accessed pages from other pages, is referred to as “filter count.” note that, filter count indicates an upper bound for hotness threshold\n\nme this paper explain the address reconciliation ar in detail.\n\nflow of ar -first, all cache lines from these pages, which are currently residing in the cache hierarchies and tagged with os-visible pa, must be invalidated (and dirty lines written back), since the current os-visible pa will be replaced with the new pa. all future accesses to these pages will only have access to the new pa. -next,corresponding page table entries (ptes) for a and b need to be updated with new pas. -the tlb entries in all cores using the old pa must also be invalidated (as well as any other os structures that contain the physical page addresses).\n\n(i) flush_cache_page() (ii) change pte, (iii) flush_tlb_page()\n\nwe use a migration benefit quotient (mbq), by calculating the difference between the total number of accesses to any page and the filter count used in classifying applications’ memory locality, as described. mbq indicates how many of the future accesses of a page may go to fast memory if the page were migrated from slow to fast memory using the filter count as a hotness threshold.\n\nsacturation account the sum of access counts of all pages with 32,909 accesses or less (the bars corresponding to x-axis 0 to 3,290) accounts for 98% of all accesses. for our purposes, we use this access count as a saturation count (or assume that the maximum number of accesses any page can receive).\n\nfor each workload, we use the difference between the saturation count and the filter count to determine mbq.\n\n\n\n 1. low mbq, difference is less than 1k. for example, in figure 5(b) xalanc, pages with memory access count 609 or less (the bars corresponding to x-axis 0 to 60) provide 98% of the memory accesses. hence, the difference between saturation count (609) and filter count (70) is 539. these workloads may not achieve significant increase in accesses to fast memory after page migration.\n 2. medium mbq, difference is in between 1k to k (e.g., figure 5(c) omnetpp). these workloads may receive moderate benefits, depending the migration overheads.\n 3. high mbq, difference is more than 8k (e.g., figure 5(a) mcf). these workloads are likely to receive higher hits in faster memory as a result of page migration.\n\nme in short\n\n * sactuartion account. the sum of access counts fo all pages with sactuartion account accounts for 98% of all accesses.\n * filter account. the “set of top-accessed pages” that contribute to more than 80% of all memory accesses. if they are close, it means that this is a uniform benchmark. if they have a large difference, this means that there is a subset of pages that has far more memory accesses.\n\n----------------------------------------\n\n# 17. tictoc: enabling bandwidth-efficient dram caching for both hits and misses in hybrid memory systems international conference on computer design (iccd)\n\nyear: 2019",charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"Dynamically Adapting  Page Migration Policies Based on Applications Memory Access Behaviors",frontmatter:{title:"Dynamically Adapting  Page Migration Policies Based on Applications Memory Access Behaviors",date:"2023-05-11T00:00:00.000Z",permalink:"/pages/24769f/",tags:[null]},regularPath:"/01.hbm/03.Dynamically_Adapting%20_Page_Migration_Policies_Based_on_Applications_Memory_Access_Behaviors.html",relativePath:"01.hbm/03.Dynamically_Adapting _Page_Migration_Policies_Based_on_Applications_Memory_Access_Behaviors.md",key:"v-b2fd417c",path:"/pages/24769f/",headersStr:null,content:'Year: 2021 Mem : HBM & PCM\n\n * migration friendly\n * migration unfriendly\n\nBased on previous research "On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems" from the same author.\n\n 1. Adaptive migration polices Our technique increases or reduces the hotness thresholds to reduce or increase the number of pages migrated based on either the number of pages migrated over a window of observation or based on the observed benefits of page migrations (were pages accessed after the migration to faster memories).\n 2. AR overheads can defeat the benefits of page migration To eliminate AR, we explore the benefit of reverse migrating pages to their original locations, particularly when the migrated pages are no longer heavily accessed. AR: OS tables (translation look-aside buffers (TLBs), page tables) must also be updated since physical addresses (PAs) in such memory systems are based on the physical location of pages and a migration changes PAs: we call this process of changing PAs and updating system tables address reconciliation (AR).\n\nWe discovered that an exponential-shaped histogram indicates that very few pages receive most accesses and that those applications benefit by either placing those few pages in the faster (HBM) memory at the start of execution, or migrated to HBM on demand.\n\nMcf 3% of all pages cause 97% of memory accesses. Milc 65% of pages contribute to 82% of all accesses.\n\nIf most of all pages receive about the same number of accesses, implying that too many pages may be migrated if a fixed hotness threshold is used for migrating pages, and the migration overheads outweigh performance gains. Doubt about this statement.\n\nMigration of pages to faster memories results in performance gains if those pages continue to be heavily used, because these accesses will be satisfied by faster memories.\n\nLinux3 performs the following functions when the virtual to PA mapping of a page is changed.\n\n 1. flush_cache_page\n 2. change PTE\n 3. flush_tlb_page\n\n# Key Insights\n\nAdaptive migration polices: Previous page migration techniques relied on fixed hotness thresholds: a page is migrated from slow memories to faster memories when the number of times that page was accessed exceeds the hotness threshold. In contrast, we control page migration policies based on applications’ memory access behaviors. Our technique increases or reduces the hotness thresholds to reduce or increase the number of pages migrated based on either the number of pages migrated over a window of observation or based on the observed benefits of page migrations (were pages accessed after the migration to faster memories).\n\nAR overheads can defeat the benefits of page migration: To eliminate AR, we explore the benefit of reverse migrating pages to their original locations, particularly when the migrated pages are no longer heavily accessed. Reverse migration makes page migration invisible to the OS. However, reverse migrations can result in excessive data movement between slow and fast memories. In this work, we evaluate the effectiveness of the reverse migration technique.\n\n# Algorithm\n\nIf the count is high (too many pages have been migrated), we double the hotness threshold to reduce future migrations; likewise, if too few pages have been migrated in a twindow, we halve the hotness threshold to increase future Adaptive Migration Based on Number of Pages Migrated migrations. In our experiments, we used 4 million cycles as our twindow.6 We also limit the hotness threshold variations between 64 and 256.\n\nWe define the MBQ as the average number of accesses to pages that were recently migrated to HBM.\n\n\n\n * threshold adaption We increase the threshold if more than 240 pages have been migrated in a window and reduce the threshold if fewer than 160 pages have been migrated in a window.\n * pause and resume migration If the MBQ is less than a threshold (min_MBQ), then migrations are halted. migrations are resumed if the MBQ is greater than another threshold (max_MBQ).\n\nAdaptive Migration Based on the MBQ\n\n# Summary\n\nTwo Algorithm\n\n 1. Based on number of page migrated, if too many page is migrared, reduce migration by increase threshold.\n 2. Based on reference after migration, if too less after migration, reduce or stop.',normalizedContent:'year: 2021 mem : hbm & pcm\n\n * migration friendly\n * migration unfriendly\n\nbased on previous research "on-the-fly page migration and address reconciliation for heterogeneous memory systems" from the same author.\n\n 1. adaptive migration polices our technique increases or reduces the hotness thresholds to reduce or increase the number of pages migrated based on either the number of pages migrated over a window of observation or based on the observed benefits of page migrations (were pages accessed after the migration to faster memories).\n 2. ar overheads can defeat the benefits of page migration to eliminate ar, we explore the benefit of reverse migrating pages to their original locations, particularly when the migrated pages are no longer heavily accessed. ar: os tables (translation look-aside buffers (tlbs), page tables) must also be updated since physical addresses (pas) in such memory systems are based on the physical location of pages and a migration changes pas: we call this process of changing pas and updating system tables address reconciliation (ar).\n\nwe discovered that an exponential-shaped histogram indicates that very few pages receive most accesses and that those applications benefit by either placing those few pages in the faster (hbm) memory at the start of execution, or migrated to hbm on demand.\n\nmcf 3% of all pages cause 97% of memory accesses. milc 65% of pages contribute to 82% of all accesses.\n\nif most of all pages receive about the same number of accesses, implying that too many pages may be migrated if a fixed hotness threshold is used for migrating pages, and the migration overheads outweigh performance gains. doubt about this statement.\n\nmigration of pages to faster memories results in performance gains if those pages continue to be heavily used, because these accesses will be satisfied by faster memories.\n\nlinux3 performs the following functions when the virtual to pa mapping of a page is changed.\n\n 1. flush_cache_page\n 2. change pte\n 3. flush_tlb_page\n\n# key insights\n\nadaptive migration polices: previous page migration techniques relied on fixed hotness thresholds: a page is migrated from slow memories to faster memories when the number of times that page was accessed exceeds the hotness threshold. in contrast, we control page migration policies based on applications’ memory access behaviors. our technique increases or reduces the hotness thresholds to reduce or increase the number of pages migrated based on either the number of pages migrated over a window of observation or based on the observed benefits of page migrations (were pages accessed after the migration to faster memories).\n\nar overheads can defeat the benefits of page migration: to eliminate ar, we explore the benefit of reverse migrating pages to their original locations, particularly when the migrated pages are no longer heavily accessed. reverse migration makes page migration invisible to the os. however, reverse migrations can result in excessive data movement between slow and fast memories. in this work, we evaluate the effectiveness of the reverse migration technique.\n\n# algorithm\n\nif the count is high (too many pages have been migrated), we double the hotness threshold to reduce future migrations; likewise, if too few pages have been migrated in a twindow, we halve the hotness threshold to increase future adaptive migration based on number of pages migrated migrations. in our experiments, we used 4 million cycles as our twindow.6 we also limit the hotness threshold variations between 64 and 256.\n\nwe define the mbq as the average number of accesses to pages that were recently migrated to hbm.\n\n\n\n * threshold adaption we increase the threshold if more than 240 pages have been migrated in a window and reduce the threshold if fewer than 160 pages have been migrated in a window.\n * pause and resume migration if the mbq is less than a threshold (min_mbq), then migrations are halted. migrations are resumed if the mbq is greater than another threshold (max_mbq).\n\nadaptive migration based on the mbq\n\n# summary\n\ntwo algorithm\n\n 1. based on number of page migrated, if too many page is migrared, reduce migration by increase threshold.\n 2. based on reference after migration, if too less after migration, reduce or stop.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"DRAM PCM NVM Cache",frontmatter:{title:"DRAM PCM NVM Cache",date:"2023-05-12T00:00:00.000Z",permalink:"/pages/24760e/",tags:[null]},regularPath:"/01.hbm/04.DRAM_PCM_NVM_Cache.html",relativePath:"01.hbm/04.DRAM_PCM_NVM_Cache.md",key:"v-9842a21a",path:"/pages/24760e/",headers:[{level:3,title:"Me",slug:"me",normalizedTitle:"me",charIndex:88}],headersStr:"Me",content:" 1. CLOCK-DWF: A Write-History-Aware Page Replacement Algorithm for Hybrid PCM and DRAM Memory Architectures\n 2. An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture\n 3. APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method\n 4. Page Placement in Hybrid Memory Systems\n\n----------------------------------------\n\n# 1. CLOCK-DWF: A Write-History-Aware Page Replacement Algorithm for Hybrid PCM and DRAM Memory Architectures\n\nLRU: Even though it requires only constant time and space overhead, LRU has a critical weakness in virtual memory environments. On every memory hit, LRU needs to move a page to the most recently used (MRU) position in the list. This involves list manipulations that cannot be handled by the paging unit hardware. CLOCK: Specifically, on a hit to a page, the paging unit hardware sets the reference bit of the page to 1 when a read or a write reference for that page occurs, and sets the dirty bit to 1 when a write reference occurs. Then, pages are maintained in a circular list. In the course of the scan, for every page with reference bit 1, CLOCK clears it to zero, without removing the page from the list.\n\nThe reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. Even though CLOCK does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.\n\nLRU maintains the temporal locality. Frequency of Write Reference collect statistics of reference cnter.\n\nThe shape of the curves in these figures can be modeled as a monotonic decreasing function, implying that a more recently referenced page is more likely to be written in the near future.\n\nSpecifically, we can observe ranking inversion of temporal locality, i.e., a more recently used page shows a smaller fraction of writes for some ranking ranges.\n\nIn Fig. 3, x axis is the ranking by LRU. y axis represents the number of write references of the page ranking in x-axis.\n\nIn Fig. 4, the x-axis represents the ranking of pages based on their past write counts (black plot) and read/write counts (gray plot). The y-axis represents the number of writes occurring on that ranking.\n\nThe reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. Even though CLOCK does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.\n\nThis indicates that frequency based estimations are more accurate compared to temporal locality based estimations for most cases. Specifically, frequency based stimations indicate that a wide range of top ranking pages, that is, pages that have been written to frequently in the past, are likely to be written to again in the future.\n\nIn summary, write frequency is generally a better estimator than temporal locality in predicting the re-reference likelihood of write references, but the very recent past write history is also a strong indicator of future writes.\n\nMe In pic4, the axis x is also ranked by number of reference. That's why its x axis can correlates with y axis. Maybe 80% rule can also explain this.\n\n----------------------------------------\n\n# 2. An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture\n\nContrary to CLOCK-DWF that places page faults issued by read requests on NVM, the proposed scheme moves all pages from disk to DRAM area. This is motivated by the fact that moving to either NVM or DRAM will result in a page write in NVM since the DRAM is always full and moving a data page to DRAM will issue an eviction to NVM. Therefore, the cost of moving to NVM or DRAM is the same in terms of writes in NVM. The newly accessed data pages have higher probability of access compared to the older data pages and moving this new page to DRAM will result in increase in DRAM hit ratio instead of NVM hit ratio.\n\nFirst,it requires an ordering scheme in order to identify data pages that are cold but will be accessed once in a long time. These data pages will reside long enough in NVM to have a high counter values and therefore will be moved to DRAM where they cannot compete with hot data pages and will return to NVM which makes their migration to DRAM without any benefits. Second, there is no difference between pages that are frequently accessed and typically reside near the head of the NVM LRU queue for the entire time and data pages which go back and forth in the queue.\n\nThe housekeeping information will be only stored for a few percentage of top positions in the NVM LRU queue. Once a data page moves to the end of this selected percentage of LRU, the corresponding counter will be reset to zero. This will handle both ordering scheme and identifying burst data accesses.\n\nFinding the data page in DRAM will result in a normal LRU housekeeping. Otherwise, the extra housekeeping information in NVM will be updated based on the request type. The read and write counters will be stored for readperc and writeperc top data pages in the NVM, respectively. [Still confused why they have readperc and writeperc]\n\nThe values of read threshold and write threshold determine how aggressive we plan to prevent the migrations with low probability of being useful.\n\nTo this end, we use two Least Recently Used (LRU) queues (one for DRAM and one for NVM) and optimize the LRU queue for NVM to prevent nonbeneficial migrations to DRAM.\n\n# 3. APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method\n\nComments on the previous paper: CLOCK-DWF [19]. CLOCK-DWF first proposes to load the write-request pages into DRAM. For new pages, if it is for a write request, it will be swapped into DRAM. Otherwise, it will be placed in NVRAM. For pages stored in NVRAM, if one is hit by a write request, it will be migrated from NVRAM to DRAM. At this time when DRAM is full, CLOCK-DWF will select a victim page that has the lowest number of writes or that has not been accessed for the longest period of time in DRAM to be evicted. Double LRU [20]. Double LRU recognizes the high migration cost between NVRAM and DRAM, and tries to restrict the number of page migrations by setting some threshold. It uses two separate LRU linked lists to manage pages in DRAM and NVRAM. For each page in NVRAM, it maintains a read/write request count. When a page is accessed, Double LRU checks its read/write request count, and if the count reaches a certain threshold, it will be migrated from NVRAM to DRAM; otherwise, it will remain in NVRAM. In DRAM, the page at the end of the LRU list is always selected as the victim. For new pages, Double LRU stores them directly in DRAM, assuming new pages will be accessed frequently in the near future. regardless of the read or write requests.\n\nUIMigrate consists of three parts: unified hot page identification, page migration, and self-adaptive adjustment.\n\nTo consider both the number of accesses and access time,we add the attenuation factor to quantify the hotness of each page, hoping to quickly reduce the access counts for pages that are accessed a long time ago. Thus, while updating page hotness upon each access, UIMigrate also uses an attenuation coefficient to lower the page popularity of old accesses.\n\nIf all DRAM pages should be accessed in one cycle, (acc_count.global-acc.countpage)/DRAMsize denotes the number of cycles the page that has not been accessed.\n\nthe attenuation coefficient d is closely related to the value of hotold and the number of cycles the page that has not been accessed since last time.\n\nUIMigrate sets a threshold, called new page threshold, to measure the hotness of each victim page. When the quantified hotness of a selected victim is larger than the preset threshold, it means that this victim page is too hot to be evicted, and so UIMigrate will store the new page in NVRAM. Otherwise, it will be migrated to DRAM.\n\nIn order to effectively adapt to the change of access patterns, UIMigrate adjusts migration thresholds (new page threshold, hot page threshold and cold page threshold) automatically to promote or suppress the page migrations, according to real-time migration revenue.\n\nWhen they are evicted from DRAM, UIMigrate calculates the migration revenue based on Equations (3) and (4). If the migration revenue is below zero, it means that the migration cost is greater than the benefit. In this case, UIMigrate will increase hot page threshold to prevent certain pages from getting hot in NVRAM and decrease cold page threshold to prevent some pages from becoming cold in DRAM, thus retaining more pages in NVRAM. For new pages, UIMigrate will also reduce new page threshold, so that more new pages will go to NVRAM instead of DRAM. When the calculated migration benefit is larger than the migration cost, UIMigare will reduce hot page threshold and increase cold page threshold to make migrate more pages to DRAM, and increase new page threshold to keep more new pages in DRAM.\n\n# 4. Page Placement in Hybrid Memory Systems\n\nGiven the characteristics of DRAM and PCM, RaPP seeks to (1) place performance-critical pages and frequently written pages in DRAM (2) place non-critical pages and rarely written pages in PCM (3) spread writes to PCM across many physical frames.\n\nUsing this information, RaPP dynamically ranks frames based on frequency and recency of accesses, as detailed below. Frames that rank high are called “popular”, and frames that rank low are called “unpopular”.\n\n# Algorithm\n\n 1. The descriptors in queue M − 1 represent the blocks that are most frequently used. On the first access to a block, its descriptor is placed in the tail of queue 0.\n 2. In addition, the block’s expiration time ExpirationTime is set to CurrentTime + LifeTime, where both times are measured in number of accesses and LifeT ime specifies the number of consecutive accesses that n must directed to other blocks before we expire the block.\n 3. Every time the block is accessed, its reference counter is incremented, its expiration time is reset to CurrentT ime + LifeT ime, and its descriptor is moved to the tail of its current queue.\n 4. The descriptor of a frequently used block is promoted to a higher queue (saturating at queue M − 1, of course) after a certain number of accesses to the block.\n 5. Specifically, if the descriptor is currently in queue i, it will be upgraded to queue i + 1 when its reference counter reaches 2i+1.\n 6. Conversely, MQ demotes blocks that have not been accessed recently. On each access, the descriptors at the heads of all M queues (representing the LRU block of each queue) are checked for expiration (CurrentT ime > ExpirationTime).\n\nIf a block descriptor expires, it is placed at the tail of the immediately inferior queue, and has its expiration time again set to CurrentTime + LifeTime.\n\nFirst,instead of counting all accesses, we only count an access if it occurs more than a threshold time (measured in memory cycles) after the last access to the same frame. This latter threshold is called the “filter threshold”. The MC stores the time of the last access in the descriptor for the frame. Using a 2-competitive approach, we set the filter threshold to be MigrationCost/MigrationThreshold, where MigrationCost is the uncontended number of memory cycles needed to migrate a page. (MigrationCost is roughly 1.6µs in our experiments.)\n\nSecond, we modified the demotion policy in the following ways: (a) we use time, not number of accesses, as the metric for demotion to reduce space requirements (in our experiments, we set LifeT ime to 100µs, which works well for our workloads); (b) we only demote from one queue at a time (in round-robin fashion) to reduce runtime overhead; (c) a DRAM frame that is demoted twice without any intervening accesses leaves the MQ queues and becomes a candidate to receive a popular PCM page.\n\nTo select a destination DRAM frame for a page, the MC maintains an LRU list of victim DRAM frames. The victim frames are not in any of the LRU queues (the list is initialized with all DRAM frames).\n\nTo effect a page migration to DRAM, the MC (1) migrates the page stored in the selected DRAM frame to one of the unranked PCM frames, (2) migrates the content of this latter frame to the most popular PCM frame, and finally (3) migrates the content of the most popular PCM frame to the selected DRAM frame.\n\n\n# Me\n\nWhy swap 3 time? not 2?",normalizedContent:" 1. clock-dwf: a write-history-aware page replacement algorithm for hybrid pcm and dram memory architectures\n 2. an operating system level data migration scheme in hybrid dram-nvm memory architecture\n 3. apmigration: improving performance of hybrid memory performance via an adaptive page migration method\n 4. page placement in hybrid memory systems\n\n----------------------------------------\n\n# 1. clock-dwf: a write-history-aware page replacement algorithm for hybrid pcm and dram memory architectures\n\nlru: even though it requires only constant time and space overhead, lru has a critical weakness in virtual memory environments. on every memory hit, lru needs to move a page to the most recently used (mru) position in the list. this involves list manipulations that cannot be handled by the paging unit hardware. clock: specifically, on a hit to a page, the paging unit hardware sets the reference bit of the page to 1 when a read or a write reference for that page occurs, and sets the dirty bit to 1 when a write reference occurs. then, pages are maintained in a circular list. in the course of the scan, for every page with reference bit 1, clock clears it to zero, without removing the page from the list.\n\nthe reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. even though clock does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.\n\nlru maintains the temporal locality. frequency of write reference collect statistics of reference cnter.\n\nthe shape of the curves in these figures can be modeled as a monotonic decreasing function, implying that a more recently referenced page is more likely to be written in the near future.\n\nspecifically, we can observe ranking inversion of temporal locality, i.e., a more recently used page shows a smaller fraction of writes for some ranking ranges.\n\nin fig. 3, x axis is the ranking by lru. y axis represents the number of write references of the page ranking in x-axis.\n\nin fig. 4, the x-axis represents the ranking of pages based on their past write counts (black plot) and read/write counts (gray plot). the y-axis represents the number of writes occurring on that ranking.\n\nthe reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. even though clock does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.\n\nthis indicates that frequency based estimations are more accurate compared to temporal locality based estimations for most cases. specifically, frequency based stimations indicate that a wide range of top ranking pages, that is, pages that have been written to frequently in the past, are likely to be written to again in the future.\n\nin summary, write frequency is generally a better estimator than temporal locality in predicting the re-reference likelihood of write references, but the very recent past write history is also a strong indicator of future writes.\n\nme in pic4, the axis x is also ranked by number of reference. that's why its x axis can correlates with y axis. maybe 80% rule can also explain this.\n\n----------------------------------------\n\n# 2. an operating system level data migration scheme in hybrid dram-nvm memory architecture\n\ncontrary to clock-dwf that places page faults issued by read requests on nvm, the proposed scheme moves all pages from disk to dram area. this is motivated by the fact that moving to either nvm or dram will result in a page write in nvm since the dram is always full and moving a data page to dram will issue an eviction to nvm. therefore, the cost of moving to nvm or dram is the same in terms of writes in nvm. the newly accessed data pages have higher probability of access compared to the older data pages and moving this new page to dram will result in increase in dram hit ratio instead of nvm hit ratio.\n\nfirst,it requires an ordering scheme in order to identify data pages that are cold but will be accessed once in a long time. these data pages will reside long enough in nvm to have a high counter values and therefore will be moved to dram where they cannot compete with hot data pages and will return to nvm which makes their migration to dram without any benefits. second, there is no difference between pages that are frequently accessed and typically reside near the head of the nvm lru queue for the entire time and data pages which go back and forth in the queue.\n\nthe housekeeping information will be only stored for a few percentage of top positions in the nvm lru queue. once a data page moves to the end of this selected percentage of lru, the corresponding counter will be reset to zero. this will handle both ordering scheme and identifying burst data accesses.\n\nfinding the data page in dram will result in a normal lru housekeeping. otherwise, the extra housekeeping information in nvm will be updated based on the request type. the read and write counters will be stored for readperc and writeperc top data pages in the nvm, respectively. [still confused why they have readperc and writeperc]\n\nthe values of read threshold and write threshold determine how aggressive we plan to prevent the migrations with low probability of being useful.\n\nto this end, we use two least recently used (lru) queues (one for dram and one for nvm) and optimize the lru queue for nvm to prevent nonbeneficial migrations to dram.\n\n# 3. apmigration: improving performance of hybrid memory performance via an adaptive page migration method\n\ncomments on the previous paper: clock-dwf [19]. clock-dwf first proposes to load the write-request pages into dram. for new pages, if it is for a write request, it will be swapped into dram. otherwise, it will be placed in nvram. for pages stored in nvram, if one is hit by a write request, it will be migrated from nvram to dram. at this time when dram is full, clock-dwf will select a victim page that has the lowest number of writes or that has not been accessed for the longest period of time in dram to be evicted. double lru [20]. double lru recognizes the high migration cost between nvram and dram, and tries to restrict the number of page migrations by setting some threshold. it uses two separate lru linked lists to manage pages in dram and nvram. for each page in nvram, it maintains a read/write request count. when a page is accessed, double lru checks its read/write request count, and if the count reaches a certain threshold, it will be migrated from nvram to dram; otherwise, it will remain in nvram. in dram, the page at the end of the lru list is always selected as the victim. for new pages, double lru stores them directly in dram, assuming new pages will be accessed frequently in the near future. regardless of the read or write requests.\n\nuimigrate consists of three parts: unified hot page identification, page migration, and self-adaptive adjustment.\n\nto consider both the number of accesses and access time,we add the attenuation factor to quantify the hotness of each page, hoping to quickly reduce the access counts for pages that are accessed a long time ago. thus, while updating page hotness upon each access, uimigrate also uses an attenuation coefficient to lower the page popularity of old accesses.\n\nif all dram pages should be accessed in one cycle, (acc_count.global-acc.countpage)/dramsize denotes the number of cycles the page that has not been accessed.\n\nthe attenuation coefficient d is closely related to the value of hotold and the number of cycles the page that has not been accessed since last time.\n\nuimigrate sets a threshold, called new page threshold, to measure the hotness of each victim page. when the quantified hotness of a selected victim is larger than the preset threshold, it means that this victim page is too hot to be evicted, and so uimigrate will store the new page in nvram. otherwise, it will be migrated to dram.\n\nin order to effectively adapt to the change of access patterns, uimigrate adjusts migration thresholds (new page threshold, hot page threshold and cold page threshold) automatically to promote or suppress the page migrations, according to real-time migration revenue.\n\nwhen they are evicted from dram, uimigrate calculates the migration revenue based on equations (3) and (4). if the migration revenue is below zero, it means that the migration cost is greater than the benefit. in this case, uimigrate will increase hot page threshold to prevent certain pages from getting hot in nvram and decrease cold page threshold to prevent some pages from becoming cold in dram, thus retaining more pages in nvram. for new pages, uimigrate will also reduce new page threshold, so that more new pages will go to nvram instead of dram. when the calculated migration benefit is larger than the migration cost, uimigare will reduce hot page threshold and increase cold page threshold to make migrate more pages to dram, and increase new page threshold to keep more new pages in dram.\n\n# 4. page placement in hybrid memory systems\n\ngiven the characteristics of dram and pcm, rapp seeks to (1) place performance-critical pages and frequently written pages in dram (2) place non-critical pages and rarely written pages in pcm (3) spread writes to pcm across many physical frames.\n\nusing this information, rapp dynamically ranks frames based on frequency and recency of accesses, as detailed below. frames that rank high are called “popular”, and frames that rank low are called “unpopular”.\n\n# algorithm\n\n 1. the descriptors in queue m − 1 represent the blocks that are most frequently used. on the first access to a block, its descriptor is placed in the tail of queue 0.\n 2. in addition, the block’s expiration time expirationtime is set to currenttime + lifetime, where both times are measured in number of accesses and lifet ime specifies the number of consecutive accesses that n must directed to other blocks before we expire the block.\n 3. every time the block is accessed, its reference counter is incremented, its expiration time is reset to currentt ime + lifet ime, and its descriptor is moved to the tail of its current queue.\n 4. the descriptor of a frequently used block is promoted to a higher queue (saturating at queue m − 1, of course) after a certain number of accesses to the block.\n 5. specifically, if the descriptor is currently in queue i, it will be upgraded to queue i + 1 when its reference counter reaches 2i+1.\n 6. conversely, mq demotes blocks that have not been accessed recently. on each access, the descriptors at the heads of all m queues (representing the lru block of each queue) are checked for expiration (currentt ime > expirationtime).\n\nif a block descriptor expires, it is placed at the tail of the immediately inferior queue, and has its expiration time again set to currenttime + lifetime.\n\nfirst,instead of counting all accesses, we only count an access if it occurs more than a threshold time (measured in memory cycles) after the last access to the same frame. this latter threshold is called the “filter threshold”. the mc stores the time of the last access in the descriptor for the frame. using a 2-competitive approach, we set the filter threshold to be migrationcost/migrationthreshold, where migrationcost is the uncontended number of memory cycles needed to migrate a page. (migrationcost is roughly 1.6µs in our experiments.)\n\nsecond, we modified the demotion policy in the following ways: (a) we use time, not number of accesses, as the metric for demotion to reduce space requirements (in our experiments, we set lifet ime to 100µs, which works well for our workloads); (b) we only demote from one queue at a time (in round-robin fashion) to reduce runtime overhead; (c) a dram frame that is demoted twice without any intervening accesses leaves the mq queues and becomes a candidate to receive a popular pcm page.\n\nto select a destination dram frame for a page, the mc maintains an lru list of victim dram frames. the victim frames are not in any of the lru queues (the list is initialized with all dram frames).\n\nto effect a page migration to dram, the mc (1) migrates the page stored in the selected dram frame to one of the unranked pcm frames, (2) migrates the content of this latter frame to the most popular pcm frame, and finally (3) migrates the content of the most popular pcm frame to the selected dram frame.\n\n\n# me\n\nwhy swap 3 time? not 2?",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"HBM Dead Block Predictor",frontmatter:{title:"HBM Dead Block Predictor",date:"2023-05-15T00:00:00.000Z",permalink:"/pages/2476af/",tags:[null]},regularPath:"/01.hbm/02.hbm_dead_block_predictor.html",relativePath:"01.hbm/02.hbm_dead_block_predictor.md",key:"v-d7471e16",path:"/pages/2476af/",headers:[{level:3,title:"1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory",slug:"_1-data-placement-in-hpc-architectures-with-heterogeneous-off-chip-memory",normalizedTitle:"1. data placement in hpc architectures with heterogeneous off-chip memory",charIndex:1},{level:3,title:"2. Die-Stacked DRAM: Memory, Cache, or MemCache?",slug:"_2-die-stacked-dram-memory-cache-or-memcache",normalizedTitle:"2. die-stacked dram: memory, cache, or memcache?",charIndex:76},{level:3,title:"4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)",slug:"_4-bumblebee-a-memcache-design-for-die-stacked-and-off-chip-heterogeneous-memory-systems-2023",normalizedTitle:"4. bumblebee: a memcache design for die-stacked and off-chip heterogeneous memory systems (2023)",charIndex:182},{level:3,title:"5.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM",slug:"_5-batman-techniques-for-maximizing-system-bandwidth-of-memory-systems-with-stacked-dram",normalizedTitle:"5.batman: techniques for maximizing system bandwidth of memory systems with stacked-dram",charIndex:7054},{level:3,title:"6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches",slug:"_6-bear-techniques-for-mitigating-bandwidth-bloat-in-gigascale-dram-caches",normalizedTitle:"6. bear: techniques for mitigating bandwidth bloat in gigascale dram caches",charIndex:371},{level:3,title:"7.To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches",slug:"_7-to-update-or-not-to-update-bandwidth-efficient-intelligent-replacement-policies-for-dram-caches",normalizedTitle:"7.to update or not to update?: bandwidth-efficient intelligent replacement policies for dram caches",charIndex:10703},{level:3,title:"8.ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction",slug:"_8-accord-enabling-associativity-for-gigascale-dram-caches-by-coordinating-way-install-and-way-prediction",normalizedTitle:"8.accord: enabling associativity for gigascale dram caches by coordinating way-install and way-prediction",charIndex:13307},{level:3,title:"9. A Survey of Cache Bypassing Techniques",slug:"_9-a-survey-of-cache-bypassing-techniques",normalizedTitle:"9. a survey of cache bypassing techniques",charIndex:13505},{level:3,title:"10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel",slug:"_10-the-evicted-address-filter-a-unified-mechanism-to-address-both-cache-pollution-and-thrashing-not-read-yet-intel",normalizedTitle:"10. the evicted-address filter: a unified mechanism to address both cache pollution and thrashing - not read yet intel",charIndex:745},{level:3,title:"11. Bypass and Insertion Algorithms for Exclusive Last-level Caches",slug:"_11-bypass-and-insertion-algorithms-for-exclusive-last-level-caches",normalizedTitle:"11. bypass and insertion algorithms for exclusive last-level caches",charIndex:865},{level:3,title:"12. Counter-Based Cache Replacement and Bypassing Algorithms",slug:"_12-counter-based-cache-replacement-and-bypassing-algorithms",normalizedTitle:"12. counter-based cache replacement and bypassing algorithms",charIndex:934},{level:3,title:"13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)",slug:"_13-techniques-for-bandwidth-efficient-prefetching-of-linked-data-structures-in-hybrid-prefetching-systems-lds-prefetch",normalizedTitle:"13. techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems (lds prefetch)",charIndex:996}],headersStr:"1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory 2. Die-Stacked DRAM: Memory, Cache, or MemCache? 4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023) 5.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM 6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches 7.To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches 8.ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction 9. A Survey of Cache Bypassing Techniques 10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel 11. Bypass and Insertion Algorithms for Exclusive Last-level Caches 12. Counter-Based Cache Replacement and Bypassing Algorithms 13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)",content:' 1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory\n 2. Die-Stacked DRAM: Memory, Cache, or MemCache?\n 3. A Survey Of Techniques for Architecting DRAM Caches\n 4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)\n 5. BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM\n 6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches\n 7. To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches\n 8. ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction\n\n----------------------------------------\n\n 9.  A Survey of Cache Bypassing Techniques\n 10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel\n 11. Bypass and Insertion Algorithms for Exclusive Last-level Caches\n 12. Counter-Based Cache Replacement and Bypassing Algorithms\n 13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)\n\n----------------------------------------\n\n\n# 1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory\n\n * Software manage DRAM and NVM\n\n 1. First touch policy\n    Alloc all pages in DRAM\n\n 2. Static profile-based policy\n\n 3. Spill Migration\n    LRU spill policy keeps track of last access time for each page in DRAM, and in case of eviction selects one that is least recently used. Spill migration policy first allocates a page in fast memory (in our case DRAM), and later evicts it to PCM. Spill profile-based policy can either spare a page from eviction if its future traffic is high, or victimize it if it is low, regardless of its previous access count.\n\n 4. Dynamic page migration\n    \n    \n    \n    When a page is first brought to the PCM we reset its access counter, regardless of how many times it was accessed in the DRAM. At the same time we keep track of the number of accesses for every page in the DRAM, as well as the average for all the pages (nDRAMavg). When a page in PCM is accessed, we compare its access counter (naccesses) with the average number of accesses to pages in DRAM. Back migration threshold (BMT) is a value that controls the aggressiveness of migration triggering. If it is set to zero, a page is migrated as soon as it is touched in PCM, so the DRAM acts as a typical cache. In this case we expect good performance as the system tends to always move active pages to DRAM, but due to a large number of migrations, number of writes to PCM may go high. On the other hand, if BMT is set to infinity the page never gets migrated back, and then the policy is equivalent to LRU spill. In between those extremes we would like to search for values that give good performace and low number of PCM writes.\n\n----------------------------------------\n\n\n# 2. Die-Stacked DRAM: Memory, Cache, or MemCache?\n\n * Part as Memory ans Part as Cache\n * Discuss and compared with Alloy Cache, Unison Cache, Banshee Cache, HMA\n * Hot Data Sets pages in memory HBM and transient pages in cache HBM\n\nCited from org paper: In this proposal, a software procedure pre-processes the application and determines hot pages,then asks the OS to map them to the memory portion of the die-stacked DRAM. The cache portion of the die-stacked DRAM is managed by hardware, caching data allocated in the off-chip memory.\n\nTo identify hot pages, we use a static profile-based approach before the execution of an application. A software procedure, incorporated into the compiler, pre-processes the application and sorts the pages based on their access frequency. Then it picks the top pages and asks the OS to map them to the memory portion of the die-stacked DRAM.\n\nAfter detection of hot pages, their details are coded into the program binary. Whenever the program gets executed, the Loader passes the required information of hot pages to the OS. Then, the OS tries to map such hot pages to physical locations that belong to the memory portion of the die-stacked DRAM. For the OS, allocating pages in the die-stacked and off-chip memory is similar to the same operations in Non-Uniform Memory Architecture (NUMA) [50] systems.\n\nIn this paper, they raised the issue that when process switches, previous hbm space allocated to a process might left inadequate space for the following process. Other orthogonal research " Various proposals (e.g., [13, 47, 54, 62, 78]) have suggested to optimize memory management in such situations typically by gradually or periodically migrating application pages between different types of memories based on factors like programming model, application’s criticality, sharing degree, and so on".\n\nThey identify the portion of hbm memory(the hot pages that need to be allocated to HBM) for hot pages by trying to allocate the maximum number of pages into hbm memory without worsening the cacheAHF.\n\n\n\n----------------------------------------\n\n\n# 4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)\n\n# Me\n\n * Hybrid Memory\n * Blk/Page Size 2KB/64KB\n   64KB page size is due to the fact that it maps all memory in dram and hbm.\n   Every request will cam PRT and BLE.\n   In multi core simulation env,it have to support multi core read and write the SRAM.\n   \n * Distinguish Spacial Locality and Temporal Locality.\n   cacheHBM (cHBM) for temporal locality\n   memoryHBM (mHBM) for spacial locality\n   \n * Page Allocation.\n   Different from previous design that allocate all memory in HBM or DRAM. It allocate page according to its neighbour pages. But it does not mention how it interact with page table. If page is deallocate or written back to disk, the PRT should also be updated.\n * The ratio between cHBM and mHBM is flexible.\n\n\n\n * If memory footprint is high, all used by OS, all the HBM will be served as flat memory.\n\nCited from org paper: Program Statistics\n\n\n\nIn each remapping set, the hotness tracker includes a hot table and five parameters: the HBM occupied ratio (Rh), a hotness threshold (T) to decide if an off-chip DRAM page should be brought in HBM for high Rh condition, the number of cHBM pages (Nc), and the number of mHBM pages in which most blocks have/have not been accessed (Na/Nn).\n\n\n\nFor SL>0 (strong spatial locality), more hot data should be brought in mHBM to better exploit the spatial locality and utilize the memory bandwidth. For SL ≤ 0 (weak spatial locality), hot data should be cached in cHBM to reduce over-fetching.\n\nThe threshold T in the hotness tracker can alleviate this issue. If Rh is high, for SL>0, only pages whose hotness value is larger than T are permitted to be migrated to mHBM and for SL ≤ 0, only blocks in a page whose hotness value is larger than T are permitted to be cached in cHBM. Me From this aspect, SL means that number of mHBM pages that most blocks have been accessed is far larger than not been accessed. This means strong spatial locality.\n\n----------------------------------------\n\n\n# 5.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM\n\nInsights\n\n * bandwidth distribution\n * dram and hbm similar latency\n\nAs the NM simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. We observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.\n\nWe leverage our key insight on controlling data movement and propose Bandwidth-Aware Tiered-Memory Management (BATMAN), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the NM and the FM. We define the desired access rate of the NM as the target access rate (TAR). TAR is the fraction of memory accesses serviced by the NM when memory accesses to both memories are proportional to the respective bandwidth.\n\n\n\n2X 2/3 4X 4/5 8X 8/9\n\nMe Bandwidth-Aware Tired-Memory Management tries to distritube memory according to HBM and DRAM bandwidth ratio. And also treat it as a threshold to refuse page migration.\n\nThis bandwidth division is also adopted in "Design and Implementation of Bandwidth-Aware Memory Placement and Migration Policies for Heterogeneous Memory".\n\n\n\n----------------------------------------\n\n\n# 6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches\n\nYear:2015\n\nIdeally, we want the bandwidth consumed for such secondary operations to be negligible, and have almost all the bandwidth be available for transfer of useful data from the DRAM cache to the processor. BEAR integrates three components, one each for reducing the bandwidth consumed by miss detection, miss fill, and writeback probes.\n\n 1. Miss Probe (to detect a miss, we need to look up the tag store in the DRAM cache)\n 2. Miss Fill (on a cache miss the missed line is obtained from memory and filled in the cache)\n 3. Write back Probe (on a dirty eviction from the on-chip LLC identifying if that line is present in the DRAM cache)\n 4. Writeback Update (if writeback probe gives a hit, updating the content of the line in DRAM cache)\n 5. Writeback Fill (filling the writeback data in the cache, if a writeback probe gives a miss)\n\nThey define BloatFactor, the ratio of the total bandwidth consumed by the DRAM cache to the bandwidth required for transferring only the data lines to the processor chip.\n\n 1. Bandwidth Efficient Cache Fills We propose Bandwidth Aware Bypass (BAB) to reduce the bandwidth consumed by fill operations while limiting the loss in cache hit rate to a desired level.\n    \n    \n\n 2. Bandwidth Efficient Writeback Probe DRAM Cache Presence (DCP), reduces Writeback Probe by introducing state information in the on-chip Last Level Cache (LLC) to track if the line exists in the DRAM cache. Inclusive Cache\n    \n    \n\n 3. Bandwidth Efficient Miss Probe We reduce the bandwidth consumed by Miss Probe by leveraging the property of DRAM caches to streams multiple tags on each access. We buffer the tags of recently accessed adjacent cache line\'s tags in the Neighboring Tag Cache (NTC). Neighboring Tag Cache\n    \n    \n\nComment from To Update or Not to update Along the same lines, Chou et. al [6] propose a policy that bypasses the cache with 90% probability (we call this policy 90%-Bypass). Me This comment from to update or not to update is not accurate, the bear paper mentioned that "Overall, the speed up from probabilistic bypass is negligible, and we may deem PB to be ineffective at improving performance." Then it prefers set-duleling.\n\n----------------------------------------\n\n\n# 7.To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches\n\nYear: 2019\n\nMe Previous dram cache is stateless, due to the fact that maintaining state of cache would require significant bandwidth.\n\nCite from org paper We propose a stateful replacement/bypass policy called RRIP Age-On-Bypass (RRIP-AOB), that tracks reuse state for high-reuse lines, protects such lines by bypassing other lines, and Ages the state On cache Bypass.\n\nThe DRAM cache in KNL [4, 5],for example, employs an Always-Install policy. The DRAM cache places each tag information in the unused bits in the ECC space and streams out the data and tag (contained in ECC) on each access.\n\n\n\nOur goal is to increase the hit-rate of such DRAM caches. In fact, the DRAM cache only uses about 8-10 bits from the unused 28 bits in the ECC space, so we have 18-20 bits per line available for managing the DRAM cache intelligently.\n\nTo reduce significant bandwidth to update state, we propose Efficient Tracking of Reuse (ETR). ETR makes state tracking efficient by accurately tracking the state of only one line from a region, and using the state of that line to guide the replacement decisions for other lines in that region.\n\n 1. We propose a bypass version of RRIP (RRIP-AOB) suitable for caches with limited associativity. However, we find an effective replacement policy for DRAM caches must optimize not only hit-rate but also state update cost. We introduce two properties, coresidency and eviction-locality, that can be exploited to reduce state update cost for implementing intelligent replacement.\n    \n    \n    \n    Coresidency indicates that at any given time if a line is present, then several other line belonging to that 4KB region are also present in the cache. Eviction-Locality indicates that when a line gets evicted from the cache, the replacement-state of the other coresident lines belonging to that region tend to have similar replacement state as the line being evicted. Me Just a synonym for spacial locality. This granularity is 4KB. Doubt about its authenticity.\n\n 2. We propose Efficient Tracking of Reuse (ETR), a design that performs updates for only a subset of lines and uses their state to guide the replacement decisions of other lines.\n\n\n\nMe This is similar to set dueling.\n\nThe design of ETR consists of three parts: (1) Selecting a Representative-Line in the region. (2) Keeping accurate RRPV for only the Representative-Line. (3) Using the representative’s RRPV to infer coresident lines’ RRPV to make bypass decisions.\n\n----------------------------------------\n\n\n# 8.ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction\n\nA method to optimize prediction way of dram.\n\n----------------------------------------\n\n\n# 9. A Survey of Cache Bypassing Techniques\n\n----------------------------------------\n\n\n# 10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel\n\n----------------------------------------\n\n\n# 11. Bypass and Insertion Algorithms for Exclusive Last-level Caches\n\n----------------------------------------\n\n\n# 12. Counter-Based Cache Replacement and Bypassing Algorithms\n\n----------------------------------------\n\n\n# 13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)',normalizedContent:' 1. data placement in hpc architectures with heterogeneous off-chip memory\n 2. die-stacked dram: memory, cache, or memcache?\n 3. a survey of techniques for architecting dram caches\n 4. bumblebee: a memcache design for die-stacked and off-chip heterogeneous memory systems (2023)\n 5. batman: techniques for maximizing system bandwidth of memory systems with stacked-dram\n 6. bear: techniques for mitigating bandwidth bloat in gigascale dram caches\n 7. to update or not to update?: bandwidth-efficient intelligent replacement policies for dram caches\n 8. accord: enabling associativity for gigascale dram caches by coordinating way-install and way-prediction\n\n----------------------------------------\n\n 9.  a survey of cache bypassing techniques\n 10. the evicted-address filter: a unified mechanism to address both cache pollution and thrashing - not read yet intel\n 11. bypass and insertion algorithms for exclusive last-level caches\n 12. counter-based cache replacement and bypassing algorithms\n 13. techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems (lds prefetch)\n\n----------------------------------------\n\n\n# 1. data placement in hpc architectures with heterogeneous off-chip memory\n\n * software manage dram and nvm\n\n 1. first touch policy\n    alloc all pages in dram\n\n 2. static profile-based policy\n\n 3. spill migration\n    lru spill policy keeps track of last access time for each page in dram, and in case of eviction selects one that is least recently used. spill migration policy first allocates a page in fast memory (in our case dram), and later evicts it to pcm. spill profile-based policy can either spare a page from eviction if its future traffic is high, or victimize it if it is low, regardless of its previous access count.\n\n 4. dynamic page migration\n    \n    \n    \n    when a page is first brought to the pcm we reset its access counter, regardless of how many times it was accessed in the dram. at the same time we keep track of the number of accesses for every page in the dram, as well as the average for all the pages (ndramavg). when a page in pcm is accessed, we compare its access counter (naccesses) with the average number of accesses to pages in dram. back migration threshold (bmt) is a value that controls the aggressiveness of migration triggering. if it is set to zero, a page is migrated as soon as it is touched in pcm, so the dram acts as a typical cache. in this case we expect good performance as the system tends to always move active pages to dram, but due to a large number of migrations, number of writes to pcm may go high. on the other hand, if bmt is set to infinity the page never gets migrated back, and then the policy is equivalent to lru spill. in between those extremes we would like to search for values that give good performace and low number of pcm writes.\n\n----------------------------------------\n\n\n# 2. die-stacked dram: memory, cache, or memcache?\n\n * part as memory ans part as cache\n * discuss and compared with alloy cache, unison cache, banshee cache, hma\n * hot data sets pages in memory hbm and transient pages in cache hbm\n\ncited from org paper: in this proposal, a software procedure pre-processes the application and determines hot pages,then asks the os to map them to the memory portion of the die-stacked dram. the cache portion of the die-stacked dram is managed by hardware, caching data allocated in the off-chip memory.\n\nto identify hot pages, we use a static profile-based approach before the execution of an application. a software procedure, incorporated into the compiler, pre-processes the application and sorts the pages based on their access frequency. then it picks the top pages and asks the os to map them to the memory portion of the die-stacked dram.\n\nafter detection of hot pages, their details are coded into the program binary. whenever the program gets executed, the loader passes the required information of hot pages to the os. then, the os tries to map such hot pages to physical locations that belong to the memory portion of the die-stacked dram. for the os, allocating pages in the die-stacked and off-chip memory is similar to the same operations in non-uniform memory architecture (numa) [50] systems.\n\nin this paper, they raised the issue that when process switches, previous hbm space allocated to a process might left inadequate space for the following process. other orthogonal research " various proposals (e.g., [13, 47, 54, 62, 78]) have suggested to optimize memory management in such situations typically by gradually or periodically migrating application pages between different types of memories based on factors like programming model, application’s criticality, sharing degree, and so on".\n\nthey identify the portion of hbm memory(the hot pages that need to be allocated to hbm) for hot pages by trying to allocate the maximum number of pages into hbm memory without worsening the cacheahf.\n\n\n\n----------------------------------------\n\n\n# 4. bumblebee: a memcache design for die-stacked and off-chip heterogeneous memory systems (2023)\n\n# me\n\n * hybrid memory\n * blk/page size 2kb/64kb\n   64kb page size is due to the fact that it maps all memory in dram and hbm.\n   every request will cam prt and ble.\n   in multi core simulation env,it have to support multi core read and write the sram.\n   \n * distinguish spacial locality and temporal locality.\n   cachehbm (chbm) for temporal locality\n   memoryhbm (mhbm) for spacial locality\n   \n * page allocation.\n   different from previous design that allocate all memory in hbm or dram. it allocate page according to its neighbour pages. but it does not mention how it interact with page table. if page is deallocate or written back to disk, the prt should also be updated.\n * the ratio between chbm and mhbm is flexible.\n\n\n\n * if memory footprint is high, all used by os, all the hbm will be served as flat memory.\n\ncited from org paper: program statistics\n\n\n\nin each remapping set, the hotness tracker includes a hot table and five parameters: the hbm occupied ratio (rh), a hotness threshold (t) to decide if an off-chip dram page should be brought in hbm for high rh condition, the number of chbm pages (nc), and the number of mhbm pages in which most blocks have/have not been accessed (na/nn).\n\n\n\nfor sl>0 (strong spatial locality), more hot data should be brought in mhbm to better exploit the spatial locality and utilize the memory bandwidth. for sl ≤ 0 (weak spatial locality), hot data should be cached in chbm to reduce over-fetching.\n\nthe threshold t in the hotness tracker can alleviate this issue. if rh is high, for sl>0, only pages whose hotness value is larger than t are permitted to be migrated to mhbm and for sl ≤ 0, only blocks in a page whose hotness value is larger than t are permitted to be cached in chbm. me from this aspect, sl means that number of mhbm pages that most blocks have been accessed is far larger than not been accessed. this means strong spatial locality.\n\n----------------------------------------\n\n\n# 5.batman: techniques for maximizing system bandwidth of memory systems with stacked-dram\n\ninsights\n\n * bandwidth distribution\n * dram and hbm similar latency\n\nas the nm simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. we observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.\n\nwe leverage our key insight on controlling data movement and propose bandwidth-aware tiered-memory management (batman), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the nm and the fm. we define the desired access rate of the nm as the target access rate (tar). tar is the fraction of memory accesses serviced by the nm when memory accesses to both memories are proportional to the respective bandwidth.\n\n\n\n2x 2/3 4x 4/5 8x 8/9\n\nme bandwidth-aware tired-memory management tries to distritube memory according to hbm and dram bandwidth ratio. and also treat it as a threshold to refuse page migration.\n\nthis bandwidth division is also adopted in "design and implementation of bandwidth-aware memory placement and migration policies for heterogeneous memory".\n\n\n\n----------------------------------------\n\n\n# 6. bear: techniques for mitigating bandwidth bloat in gigascale dram caches\n\nyear:2015\n\nideally, we want the bandwidth consumed for such secondary operations to be negligible, and have almost all the bandwidth be available for transfer of useful data from the dram cache to the processor. bear integrates three components, one each for reducing the bandwidth consumed by miss detection, miss fill, and writeback probes.\n\n 1. miss probe (to detect a miss, we need to look up the tag store in the dram cache)\n 2. miss fill (on a cache miss the missed line is obtained from memory and filled in the cache)\n 3. write back probe (on a dirty eviction from the on-chip llc identifying if that line is present in the dram cache)\n 4. writeback update (if writeback probe gives a hit, updating the content of the line in dram cache)\n 5. writeback fill (filling the writeback data in the cache, if a writeback probe gives a miss)\n\nthey define bloatfactor, the ratio of the total bandwidth consumed by the dram cache to the bandwidth required for transferring only the data lines to the processor chip.\n\n 1. bandwidth efficient cache fills we propose bandwidth aware bypass (bab) to reduce the bandwidth consumed by fill operations while limiting the loss in cache hit rate to a desired level.\n    \n    \n\n 2. bandwidth efficient writeback probe dram cache presence (dcp), reduces writeback probe by introducing state information in the on-chip last level cache (llc) to track if the line exists in the dram cache. inclusive cache\n    \n    \n\n 3. bandwidth efficient miss probe we reduce the bandwidth consumed by miss probe by leveraging the property of dram caches to streams multiple tags on each access. we buffer the tags of recently accessed adjacent cache line\'s tags in the neighboring tag cache (ntc). neighboring tag cache\n    \n    \n\ncomment from to update or not to update along the same lines, chou et. al [6] propose a policy that bypasses the cache with 90% probability (we call this policy 90%-bypass). me this comment from to update or not to update is not accurate, the bear paper mentioned that "overall, the speed up from probabilistic bypass is negligible, and we may deem pb to be ineffective at improving performance." then it prefers set-duleling.\n\n----------------------------------------\n\n\n# 7.to update or not to update?: bandwidth-efficient intelligent replacement policies for dram caches\n\nyear: 2019\n\nme previous dram cache is stateless, due to the fact that maintaining state of cache would require significant bandwidth.\n\ncite from org paper we propose a stateful replacement/bypass policy called rrip age-on-bypass (rrip-aob), that tracks reuse state for high-reuse lines, protects such lines by bypassing other lines, and ages the state on cache bypass.\n\nthe dram cache in knl [4, 5],for example, employs an always-install policy. the dram cache places each tag information in the unused bits in the ecc space and streams out the data and tag (contained in ecc) on each access.\n\n\n\nour goal is to increase the hit-rate of such dram caches. in fact, the dram cache only uses about 8-10 bits from the unused 28 bits in the ecc space, so we have 18-20 bits per line available for managing the dram cache intelligently.\n\nto reduce significant bandwidth to update state, we propose efficient tracking of reuse (etr). etr makes state tracking efficient by accurately tracking the state of only one line from a region, and using the state of that line to guide the replacement decisions for other lines in that region.\n\n 1. we propose a bypass version of rrip (rrip-aob) suitable for caches with limited associativity. however, we find an effective replacement policy for dram caches must optimize not only hit-rate but also state update cost. we introduce two properties, coresidency and eviction-locality, that can be exploited to reduce state update cost for implementing intelligent replacement.\n    \n    \n    \n    coresidency indicates that at any given time if a line is present, then several other line belonging to that 4kb region are also present in the cache. eviction-locality indicates that when a line gets evicted from the cache, the replacement-state of the other coresident lines belonging to that region tend to have similar replacement state as the line being evicted. me just a synonym for spacial locality. this granularity is 4kb. doubt about its authenticity.\n\n 2. we propose efficient tracking of reuse (etr), a design that performs updates for only a subset of lines and uses their state to guide the replacement decisions of other lines.\n\n\n\nme this is similar to set dueling.\n\nthe design of etr consists of three parts: (1) selecting a representative-line in the region. (2) keeping accurate rrpv for only the representative-line. (3) using the representative’s rrpv to infer coresident lines’ rrpv to make bypass decisions.\n\n----------------------------------------\n\n\n# 8.accord: enabling associativity for gigascale dram caches by coordinating way-install and way-prediction\n\na method to optimize prediction way of dram.\n\n----------------------------------------\n\n\n# 9. a survey of cache bypassing techniques\n\n----------------------------------------\n\n\n# 10. the evicted-address filter: a unified mechanism to address both cache pollution and thrashing - not read yet intel\n\n----------------------------------------\n\n\n# 11. bypass and insertion algorithms for exclusive last-level caches\n\n----------------------------------------\n\n\n# 12. counter-based cache replacement and bypassing algorithms\n\n----------------------------------------\n\n\n# 13. techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems (lds prefetch)',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Cache Memory Compression",frontmatter:{title:"Cache Memory Compression",date:"2023-06-06T00:00:00.000Z",permalink:"/pages/2476bf/",tags:[null]},regularPath:"/01.hbm/05.cache_mem_compression.html",relativePath:"01.hbm/05.cache_mem_compression.md",key:"v-7f8db936",path:"/pages/2476bf/",headersStr:null,content:" 1. Compresso: Pragmatic Main Memory Compression\n 2. Translation-optimized Memory Compression for Capacity\n 3. Touche: Towards Ideal and Efficient Cache Compression By Mitigating Tag Area Overheads\n\n----------------------------------------\n\n# 1. Compresso: Pragmatic Main Memory Compression\n\nCite from the paper: We propose Compresso, with optimizations to reduce compressed data movement in a hardware compressed memory, while maintaining high compression ratio by repacking data at the right time.\n\nCompresso uses the modified BPC compression algorithm, achieving 1.85x average compression on a wide range of applications.\n\nCompresso uses the compression granularity of 64B.\n\nCompresso uses LinePack with 4 possible cache line sizes.\n\nWe compare variable-sized chunks (512B, 1KB, 2KB and 4KB) with 512B fixed-sized chunks. Compresso uses incremental allocation in 512B chunks,thereby allowing 8 page sizes (512B, 1KB, 1.5KB and so on).\n\n\n\nAdditional Data Movement:\n\n 1. split-access cachelines\n 2. changes in compressibility(overflows)\n 3. metadata access\n\n\n\nDifference in exception in LCP compression Instead, Compresso allows some number of such inflated cachelines to be stored uncompressed in the inflation room at the end of an MPA page, provided that there is space in that page (Fig. 5a). This is similar to the exception region in LCP, but is used for an entirely different reason—to reduce compression-related data movement, rather than to support a specific packing scheme.\n\n\n\nWe present the first main-memory compression architecture that is designed to run an unmodified operating system.\n\n----------------------------------------\n\n# 2. Translation-optimized Memory Compression for Capacity\n\nprior workscompress and pack/migrate data at a small - memory block-level - granularity; this introduces an additional block-level translation after the page-level virtual address translation. In general, the smaller the granularity of address translation, the higher the translation overhead.\n\nA promising solution is to only save memory from cold (i.e.,less recently accessed) pages without saving memory from hot (i.e., more recently accessed) pages (e.g., keep the hot pages uncompressed).\n\nTwo challenges:\n\n 1. after a compressed cold page becomes hot again, migrating the page to a full 4KB DRAM location still adds another level (albeit page-level, instead of block-level) of translation on top of existing virtual address translation. Solution we propose compressing page table blocks in hardware to opportunistically embed compression translations into them in a software-transparent manner to effectively prefetch compression translations during a page walk, instead of serially fetching them after the walk.\n\nFirst, CTE misses typically occur after PTE misses in TLB because CTEs, especially the page-level CTEs under an OS-inspired approach, have similar translation reach as PTEs. Second, we observe page table blocks (PTBs) are highly compressible because adjacent virtual pages often have identical status bits and the most significant bits in physical page numbers are unused. As such, to hide the latency of CTE misses, TMCC transparently compresses each PTB in hardware to free up space in the PTB to embed the CTEs of the 4KB pages (i.e., either data pages or page table pages) that the PTB points to; this enables each page walk to also prefetch the matching CTE required for fetching from DRAM either the end data or the next PTB.\n\n 2. only compressing cold data require compressing them very aggressively to achieve high overall memory savings. Solution we perform a large design space exploration across many hardware configurations and diverse workloads to derive and implement in HDL an ASIC Deflate that is specialized for memory.\n\nPrior new hardware managed translation entries as Compression Translation Entries (CTEs), as they are similar to OS page table entries (PTEs). Prior works cache CTEs in the memory controller via a dedicated CTE cache, similar to the TLBs dedicated to caching PTEs.\n\nlet hardware take on an OS-inspired approach: only save memory from cold (i.e., less recently accessed) pages without saving memory from hot (i.e., recently accessed) pages (e.g., keep the hot pages uncompressed), like OS memory compression. Solves the problem of\n\n 1. translation overheads that large and/or irregular workloads suffer from high PTE miss under hardware memory compression.\n 2. Fine-grained address translation\n\nAccesses to a compressed virtual page in ML2 incurs a page fault to wake up OS to pop a free physical page from ML1’s free list and migrate the virtual page to the page.\n\nML2 also keeps many free lists, each tracking sub-physical pages of a different size, to store any compressed virtual page in a practically ideal matching sub-physical page.\n\nML2 gracefully grows and shrinks relative to ML1 with increasing and decreasing memory usage. When everything can fit in memory uncompressed, ML2 shrinks to zero bytes in physical size so ML1 can have every physical page. Specifically, when ML2’s free list(s) get large (e.g., due to reducing memory usage), ML2 donates free physical pages from its free list(s) to ML1. OS also grows ML1 free list, when it gets small, by migrating cold virtual pages to ML2. Migrating a virtual page to ML2 shrinks one of ML2’s free lists. If a ML2 free list gets empty, ML1 gives cold victim physical pages to ML2 (i.e., track them in ML2 instead of ML1), so that ML2 can compress the virtual pages currently in the victim pages to free space in the victims to grow ML2’s free list(s).\n\n\n\nKey Idea: Based on our observations, we propose transparently compressing each PTB in hardware to free up space in the PTB to embed the CTEs of the 4KB pages (i.e., either data pages or page table pages) that the PTB points to; this enables each page walk access to also prefetch the matching CTE required either for the next page walk access (i.e., to the next PTB) or for the actual data (or instruction) access after the walk.\n\n\n\nA practical challenge is that after migrating a page (e.g., from ML1 to ML2 after the page becomes cold), the corresponding CTE embedded in the page’s PTB should be updated. However, hardware has no easy way to use the PPN of the migrating page to find/access the page’s PTB(s). TMCC addresses this challenge by lazily updating the CTE in the PTB later around when the PTB is naturally accessed by the page walker, instead of updating it at the time of migrating the page. However, this means that for the first page walker access to the PTB after migrating one of the pages that the PTB points to, the corresponding CTE is out-of-date. To ensure correctness, TMCC also accesses the correct CTE in DRAM (or in CTE cache) in parallel to verify the correctness of the DRAM access. Figure 8 compares and contrasts how TMCC serves an LLC miss that also misses in CTE cache with the baseline approach. Figure 9 provides n architectural overview of TMCC.\n\n\n\n\n\n\n\nMy Comment When os access compressed page, that page is migrated from ML2 to ML1. Hardware cannot update the PTB easily. Thus it utilizes lazily update. During the page table walk, it will buffer the piggybacked CTE into CTE buffer. And when data miss req happens, L2 extracts the PPN from the received request to lookup the CTE Buffer to obtain the CTE for MC to translate the PPN.\n\n----------------------------------------\n\n# 3. Touche: Towards Ideal and Efficient Cache Compression By Mitigating Tag Area Overheads\n\nThe first component, called the “Signature” (SIGN) engine, creates shortened signatures from the tag addresses of compressed blocks. Due to this, the SIGN engine can store multiple signatures in each tag entry. On a cache access, the physical cacheline is accessed only if there is a signature match (which has a negligible probability of false positive). The second component, called the “Tag Appended Data” (TADA) mechanism, stores the full tag addresses with data. TADA enables Touch´e to detect false positive signature matches by ensuring that the actual tag address is available for comparison. The third component, called the “Superblock Marker” (SMARK) mechanism, uses a unique marker in the tag entry to indicate the occurrence of compressed cache blocks from neighboring physical addresses in the same cacheline.\n\n\n\nOn average, 55% of the blocks can be compressed to less than 48 bytes in size. Furthermore, 17% of the lines can be compressed to be less than 16 bytes in size. Therefore, several workloads tend to have blocks with low entropy and can benefit from compression.\n\n\n\n\n\n\n\nFor instance, a cacheline cannot be marked both invalid and dirty at the same time. The tag manager uses this unused state to flag cachelines that contains compressed blocks. Thereafter, for a cacheline that stores compressed blocks, the 1st and 2nd bits of the tag address encodes its valid bit and dirty bit.\n\n\n\n\n\nThe tag manager then retrieves the 16-bit marker from the SMARK mechanism. It then informs the SIGN engine to ignore the last 2-bits (corresponding to four neighboring addresses) of the full tag address to generate a unique 9-bit signature.\n\nThis SMARK generate a random 16-bit marker and concated with signature. Since non-superblocks use 3 signature to identify blks, it should also use a tag to compare not just 0.\n\nIf this paper doest not support superblock 4 compressed blks in a super block, it can only store 3* 16B compressed block or 48B + 64B block, due to extra real tag stored in data, 43bit for each data.",normalizedContent:" 1. compresso: pragmatic main memory compression\n 2. translation-optimized memory compression for capacity\n 3. touche: towards ideal and efficient cache compression by mitigating tag area overheads\n\n----------------------------------------\n\n# 1. compresso: pragmatic main memory compression\n\ncite from the paper: we propose compresso, with optimizations to reduce compressed data movement in a hardware compressed memory, while maintaining high compression ratio by repacking data at the right time.\n\ncompresso uses the modified bpc compression algorithm, achieving 1.85x average compression on a wide range of applications.\n\ncompresso uses the compression granularity of 64b.\n\ncompresso uses linepack with 4 possible cache line sizes.\n\nwe compare variable-sized chunks (512b, 1kb, 2kb and 4kb) with 512b fixed-sized chunks. compresso uses incremental allocation in 512b chunks,thereby allowing 8 page sizes (512b, 1kb, 1.5kb and so on).\n\n\n\nadditional data movement:\n\n 1. split-access cachelines\n 2. changes in compressibility(overflows)\n 3. metadata access\n\n\n\ndifference in exception in lcp compression instead, compresso allows some number of such inflated cachelines to be stored uncompressed in the inflation room at the end of an mpa page, provided that there is space in that page (fig. 5a). this is similar to the exception region in lcp, but is used for an entirely different reason—to reduce compression-related data movement, rather than to support a specific packing scheme.\n\n\n\nwe present the first main-memory compression architecture that is designed to run an unmodified operating system.\n\n----------------------------------------\n\n# 2. translation-optimized memory compression for capacity\n\nprior workscompress and pack/migrate data at a small - memory block-level - granularity; this introduces an additional block-level translation after the page-level virtual address translation. in general, the smaller the granularity of address translation, the higher the translation overhead.\n\na promising solution is to only save memory from cold (i.e.,less recently accessed) pages without saving memory from hot (i.e., more recently accessed) pages (e.g., keep the hot pages uncompressed).\n\ntwo challenges:\n\n 1. after a compressed cold page becomes hot again, migrating the page to a full 4kb dram location still adds another level (albeit page-level, instead of block-level) of translation on top of existing virtual address translation. solution we propose compressing page table blocks in hardware to opportunistically embed compression translations into them in a software-transparent manner to effectively prefetch compression translations during a page walk, instead of serially fetching them after the walk.\n\nfirst, cte misses typically occur after pte misses in tlb because ctes, especially the page-level ctes under an os-inspired approach, have similar translation reach as ptes. second, we observe page table blocks (ptbs) are highly compressible because adjacent virtual pages often have identical status bits and the most significant bits in physical page numbers are unused. as such, to hide the latency of cte misses, tmcc transparently compresses each ptb in hardware to free up space in the ptb to embed the ctes of the 4kb pages (i.e., either data pages or page table pages) that the ptb points to; this enables each page walk to also prefetch the matching cte required for fetching from dram either the end data or the next ptb.\n\n 2. only compressing cold data require compressing them very aggressively to achieve high overall memory savings. solution we perform a large design space exploration across many hardware configurations and diverse workloads to derive and implement in hdl an asic deflate that is specialized for memory.\n\nprior new hardware managed translation entries as compression translation entries (ctes), as they are similar to os page table entries (ptes). prior works cache ctes in the memory controller via a dedicated cte cache, similar to the tlbs dedicated to caching ptes.\n\nlet hardware take on an os-inspired approach: only save memory from cold (i.e., less recently accessed) pages without saving memory from hot (i.e., recently accessed) pages (e.g., keep the hot pages uncompressed), like os memory compression. solves the problem of\n\n 1. translation overheads that large and/or irregular workloads suffer from high pte miss under hardware memory compression.\n 2. fine-grained address translation\n\naccesses to a compressed virtual page in ml2 incurs a page fault to wake up os to pop a free physical page from ml1’s free list and migrate the virtual page to the page.\n\nml2 also keeps many free lists, each tracking sub-physical pages of a different size, to store any compressed virtual page in a practically ideal matching sub-physical page.\n\nml2 gracefully grows and shrinks relative to ml1 with increasing and decreasing memory usage. when everything can fit in memory uncompressed, ml2 shrinks to zero bytes in physical size so ml1 can have every physical page. specifically, when ml2’s free list(s) get large (e.g., due to reducing memory usage), ml2 donates free physical pages from its free list(s) to ml1. os also grows ml1 free list, when it gets small, by migrating cold virtual pages to ml2. migrating a virtual page to ml2 shrinks one of ml2’s free lists. if a ml2 free list gets empty, ml1 gives cold victim physical pages to ml2 (i.e., track them in ml2 instead of ml1), so that ml2 can compress the virtual pages currently in the victim pages to free space in the victims to grow ml2’s free list(s).\n\n\n\nkey idea: based on our observations, we propose transparently compressing each ptb in hardware to free up space in the ptb to embed the ctes of the 4kb pages (i.e., either data pages or page table pages) that the ptb points to; this enables each page walk access to also prefetch the matching cte required either for the next page walk access (i.e., to the next ptb) or for the actual data (or instruction) access after the walk.\n\n\n\na practical challenge is that after migrating a page (e.g., from ml1 to ml2 after the page becomes cold), the corresponding cte embedded in the page’s ptb should be updated. however, hardware has no easy way to use the ppn of the migrating page to find/access the page’s ptb(s). tmcc addresses this challenge by lazily updating the cte in the ptb later around when the ptb is naturally accessed by the page walker, instead of updating it at the time of migrating the page. however, this means that for the first page walker access to the ptb after migrating one of the pages that the ptb points to, the corresponding cte is out-of-date. to ensure correctness, tmcc also accesses the correct cte in dram (or in cte cache) in parallel to verify the correctness of the dram access. figure 8 compares and contrasts how tmcc serves an llc miss that also misses in cte cache with the baseline approach. figure 9 provides n architectural overview of tmcc.\n\n\n\n\n\n\n\nmy comment when os access compressed page, that page is migrated from ml2 to ml1. hardware cannot update the ptb easily. thus it utilizes lazily update. during the page table walk, it will buffer the piggybacked cte into cte buffer. and when data miss req happens, l2 extracts the ppn from the received request to lookup the cte buffer to obtain the cte for mc to translate the ppn.\n\n----------------------------------------\n\n# 3. touche: towards ideal and efficient cache compression by mitigating tag area overheads\n\nthe first component, called the “signature” (sign) engine, creates shortened signatures from the tag addresses of compressed blocks. due to this, the sign engine can store multiple signatures in each tag entry. on a cache access, the physical cacheline is accessed only if there is a signature match (which has a negligible probability of false positive). the second component, called the “tag appended data” (tada) mechanism, stores the full tag addresses with data. tada enables touch´e to detect false positive signature matches by ensuring that the actual tag address is available for comparison. the third component, called the “superblock marker” (smark) mechanism, uses a unique marker in the tag entry to indicate the occurrence of compressed cache blocks from neighboring physical addresses in the same cacheline.\n\n\n\non average, 55% of the blocks can be compressed to less than 48 bytes in size. furthermore, 17% of the lines can be compressed to be less than 16 bytes in size. therefore, several workloads tend to have blocks with low entropy and can benefit from compression.\n\n\n\n\n\n\n\nfor instance, a cacheline cannot be marked both invalid and dirty at the same time. the tag manager uses this unused state to flag cachelines that contains compressed blocks. thereafter, for a cacheline that stores compressed blocks, the 1st and 2nd bits of the tag address encodes its valid bit and dirty bit.\n\n\n\n\n\nthe tag manager then retrieves the 16-bit marker from the smark mechanism. it then informs the sign engine to ignore the last 2-bits (corresponding to four neighboring addresses) of the full tag address to generate a unique 9-bit signature.\n\nthis smark generate a random 16-bit marker and concated with signature. since non-superblocks use 3 signature to identify blks, it should also use a tag to compare not just 0.\n\nif this paper doest not support superblock 4 compressed blks in a super block, it can only store 3* 16b compressed block or 48b + 64b block, due to extra real tag stored in data, 43bit for each data.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"memory-ecc",frontmatter:{title:"memory-ecc",date:"2023-05-31T17:24:24.000Z",permalink:"/pages/f07695/",tags:[null]},regularPath:"/01.hbm/06.memory%20ecc.html",relativePath:"01.hbm/06.memory ecc.md",key:"v-171f66aa",path:"/pages/f07695/",headersStr:null,content:"Ecc length for different length of data.\n\nhttps://perswww.kuleuven.be/~u0068190/Onderwijs/Extra_info/Hamming%20ecc.pdf\n\n\n\nhttp://www.sxlist.com/techref/method/error/hamming.htm\n\n 1. 4-bit data path requires 3 bits for ECC (8 entry table) (75% increase in size)\n 2. 8-bit data path requires 5 bits for ECC or 1 bit for parity.\n 3. 11-bit data path requires 4 bits for ECC (16 entry table)\n 4. 16-bit data path requires 6 bit for ECC or 2 bits for parity\n 5. 32-bit data path requires 7 bits for ECC or 4 bits for parity (21.8% increase in size)\n 6. 64-bit (8 byte) data path requires 8 bits for ECC and parity (12.5% increase in size)\n 7. 128-bit (16 bytes) data path requires 9 bits for ECC or 16 bits for parity (7% increase in size\n\nUse ECC bit for compression CRAM: Efficient Hardware-Based Memory Compression for Bandwidth Enhancement\n\nEnabling Technologies for Memory Compression: Metadata, Mapping, and Prediction\n\nUse ECC bit for DRAM Cache To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches TicToc: Enabling Bandwidth-Efficient DRAM Caching for both Hits and Misses in Hybrid Memory Systems",normalizedContent:"ecc length for different length of data.\n\nhttps://perswww.kuleuven.be/~u0068190/onderwijs/extra_info/hamming%20ecc.pdf\n\n\n\nhttp://www.sxlist.com/techref/method/error/hamming.htm\n\n 1. 4-bit data path requires 3 bits for ecc (8 entry table) (75% increase in size)\n 2. 8-bit data path requires 5 bits for ecc or 1 bit for parity.\n 3. 11-bit data path requires 4 bits for ecc (16 entry table)\n 4. 16-bit data path requires 6 bit for ecc or 2 bits for parity\n 5. 32-bit data path requires 7 bits for ecc or 4 bits for parity (21.8% increase in size)\n 6. 64-bit (8 byte) data path requires 8 bits for ecc and parity (12.5% increase in size)\n 7. 128-bit (16 bytes) data path requires 9 bits for ecc or 16 bits for parity (7% increase in size\n\nuse ecc bit for compression cram: efficient hardware-based memory compression for bandwidth enhancement\n\nenabling technologies for memory compression: metadata, mapping, and prediction\n\nuse ecc bit for dram cache to update or not to update?: bandwidth-efficient intelligent replacement policies for dram caches tictoc: enabling bandwidth-efficient dram caching for both hits and misses in hybrid memory systems",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"compressibility prediction",frontmatter:{title:"compressibility prediction",date:"2024-04-04T11:42:24.000Z",permalink:"/pages/f07699/",tags:[null]},regularPath:"/01.hbm/09.compressibility_prediction.html",relativePath:"01.hbm/09.compressibility_prediction.md",key:"v-662ff4fd",path:"/pages/f07699/",headers:[{level:3,title:"1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO]",slug:"_1-attach-e-towards-ideal-memory-compression-by-mitigating-metadata-bandwidth-overheads-micro",normalizedTitle:"1. attach´e: towards ideal memory compression by mitigating metadata bandwidth overheads [micro]",charIndex:347},{level:3,title:"2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA]",slug:"_2-cram-enabling-transparent-memory-compression-for-commodity-memory-systems-hpca",normalizedTitle:"2. cram enabling transparent memory-compression for commodity memory systems [hpca]",charIndex:1311},{level:3,title:"3. MBZip: Multiblock Data Compression [TACO]",slug:"_3-mbzip-multiblock-data-compression-taco",normalizedTitle:"3. mbzip: multiblock data compression [taco]",charIndex:2596},{level:3,title:"4. Compresso: Pragmatic Main Memory Compression",slug:"_4-compresso-pragmatic-main-memory-compression",normalizedTitle:"4. compresso: pragmatic main memory compression",charIndex:245},{level:3,title:"5. Enabling Technologies for Memory Compression:Metadata, Mapping, and Prediction",slug:"_5-enabling-technologies-for-memory-compression-metadata-mapping-and-prediction",normalizedTitle:"5. enabling technologies for memory compression:metadata, mapping, and prediction",charIndex:4436}],headersStr:"1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO] 2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA] 3. MBZip: Multiblock Data Compression [TACO] 4. Compresso: Pragmatic Main Memory Compression 5. Enabling Technologies for Memory Compression:Metadata, Mapping, and Prediction",content:" 1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO 2018]\n 2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA 2019]\n 3. MBZip: Multiblock Data Compression [TACO 2017]\n 4. Compresso: Pragmatic Main Memory Compression [MICRO]\n\n----------------------------------------\n\n\n# 1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO]\n\nYear: 2018\n\nAttach´e does not use the free space made available by compression.\n\nCompression Predictor (COPR), predicts if the memory block is compressed.\n\nGlobal Indication(GI): GI is composed of eight two-bit saturating counters, each of which keeps track of the compressibility of 18th the memory space. GI can be used as an accurate indicator for predicting the compressibility within a memory space if there is abundant similarity in compressibility.\n\nPage-Level Predictor (PaPR): By exploiting the similarity in the compressibility of cachelines within an OS page [12], [18], [37], PaPR provides compression predictions at the page granularity.\n\nLine-Level Predictor (LiPR): LiPR is a set-associative cache structure indexed by the page number. LiPR uses the two-bit values of PaPR to determine if the neighboring cachelines have the same compressibility.\n\n\n# 2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA]\n\nYear: 2019\n\nTransparent Memory-Compression (TMC) can provide bandwidth benefits of memory compression in an OS-transparent manner by trying to exploit only the increased bandwidth and not the extra capacity.\n\nLine Location Predictor (LLP) that can determine the location of the line with 98% accuracy and a dynamic solution that disables compression if the benefits of compression are smaller than the overheads.\n\nIf we use HBM, we dont need to care too much about the bandwidth and metadata.\n\nWe propose a history-based Line Location Predictor (LLP), that can identify the correct location of the line with a high accuracy (98%). The LLP is based on the observation that lines within a page tend to have similar compressibility.\n\nLLP contains the Last Compressibility Table (LCT), that tracks the last compression status seen for a given index. The LCT is indexed with the hash of the page address. So, for a given access, the index corresponding to the page address is used to predict the compressibility, then line location.\n\n\n\nEven though the LLP is quite small, it provides an accuracy of 98%, much higher than the hit-rate of the metadata cache.\n\nLCP stores metadata inline with block.\n\n\n\n\n\n\n# 3. MBZip: Multiblock Data Compression [TACO]\n\nA write to a location in memory may not change the existing data in that location and is thus a redundant write. Such write requests to cache have been termed as silent stores [25]/writes [21].\n\nWe find that, on average, across 21 benchmarks, 9.6% of the writes are silent. More than 15% of the writes are silent in benchmarks such as bwaves, GemsFDTD, lbm, leslie3d, mcf, mesa, sjeng, soplex, vortex2, and zeusmp.\n\nIn such a scenario, we essentially issue one read request (to read the existing data) and no write request. However, if the write request is not silent, we add the overhead of a read request to the existing write request.\n\nWe observe that there is a strong correlation to a write being silent or nonsilent both across writes made to the same address during the course of the program execution and across writes to consecutive addresses. To exploit this correlation, we propose using a 2b bimodal predictor (indexed using the page addresses) to predict whether a write request is silent. The accuracy of our predictor (4kB structure) is around 94.4%, on average.\n\n\n# 4. Compresso: Pragmatic Main Memory Compression\n\n\n\nWe associate a 2-bit saturating counter with each entry in the metadata cache (Fig. 5b). The counter is incremented when any writeback to the associated page results in a cache line overflow and is decremented upon cache line underflows (i.e., new data being more compressible).\n\nAnother 3-bit global predictor changes state based on page overflows in the system. We speculatively increase a page’s size to the maximum (4KB) when the local as well as global predictors have the higher bit set.\n\nHence, a page is stored uncompressed if it receives multiple streaming cache line overflows during a phase when the overall system is experiencing page overflows\n\n\n# 5. Enabling Technologies for Memory Compression:Metadata, Mapping, and Prediction\n\n\n\n\n\nMetadata is coexist with data. Thus they have to predict, even read.\n\nReads are problematic because the size of the block is encoded in the block itself. Therefore, the read has to either be performed in two phases (read the metadata from the 0th chip, then read data from the appropriate subset of chips) or the read has to conservatively read data from all 9 chips in parallel.\n\nThe first is PCbased, where the PC of the load instruction serves as the index into a predictor table. This assumes that a load tends to access the same type of data record, with relatively uniform compressibility.\n\nThe second is page-based, where the physical page number serves as the index into a predictor table. This assumes that the data records in a single page are of a similar type and have uniform compressibility.\n\nOn a look-up, the highest-valued saturating counter indicates the predicted size of the block. In case of a tie, we conservatively predict the larger block size.\n\nThey keep a counter for each block compression length, and try to keep counter for each block length. Then predict by voting, following majority wins low.",normalizedContent:" 1. attach´e: towards ideal memory compression by mitigating metadata bandwidth overheads [micro 2018]\n 2. cram enabling transparent memory-compression for commodity memory systems [hpca 2019]\n 3. mbzip: multiblock data compression [taco 2017]\n 4. compresso: pragmatic main memory compression [micro]\n\n----------------------------------------\n\n\n# 1. attach´e: towards ideal memory compression by mitigating metadata bandwidth overheads [micro]\n\nyear: 2018\n\nattach´e does not use the free space made available by compression.\n\ncompression predictor (copr), predicts if the memory block is compressed.\n\nglobal indication(gi): gi is composed of eight two-bit saturating counters, each of which keeps track of the compressibility of 18th the memory space. gi can be used as an accurate indicator for predicting the compressibility within a memory space if there is abundant similarity in compressibility.\n\npage-level predictor (papr): by exploiting the similarity in the compressibility of cachelines within an os page [12], [18], [37], papr provides compression predictions at the page granularity.\n\nline-level predictor (lipr): lipr is a set-associative cache structure indexed by the page number. lipr uses the two-bit values of papr to determine if the neighboring cachelines have the same compressibility.\n\n\n# 2. cram enabling transparent memory-compression for commodity memory systems [hpca]\n\nyear: 2019\n\ntransparent memory-compression (tmc) can provide bandwidth benefits of memory compression in an os-transparent manner by trying to exploit only the increased bandwidth and not the extra capacity.\n\nline location predictor (llp) that can determine the location of the line with 98% accuracy and a dynamic solution that disables compression if the benefits of compression are smaller than the overheads.\n\nif we use hbm, we dont need to care too much about the bandwidth and metadata.\n\nwe propose a history-based line location predictor (llp), that can identify the correct location of the line with a high accuracy (98%). the llp is based on the observation that lines within a page tend to have similar compressibility.\n\nllp contains the last compressibility table (lct), that tracks the last compression status seen for a given index. the lct is indexed with the hash of the page address. so, for a given access, the index corresponding to the page address is used to predict the compressibility, then line location.\n\n\n\neven though the llp is quite small, it provides an accuracy of 98%, much higher than the hit-rate of the metadata cache.\n\nlcp stores metadata inline with block.\n\n\n\n\n\n\n# 3. mbzip: multiblock data compression [taco]\n\na write to a location in memory may not change the existing data in that location and is thus a redundant write. such write requests to cache have been termed as silent stores [25]/writes [21].\n\nwe find that, on average, across 21 benchmarks, 9.6% of the writes are silent. more than 15% of the writes are silent in benchmarks such as bwaves, gemsfdtd, lbm, leslie3d, mcf, mesa, sjeng, soplex, vortex2, and zeusmp.\n\nin such a scenario, we essentially issue one read request (to read the existing data) and no write request. however, if the write request is not silent, we add the overhead of a read request to the existing write request.\n\nwe observe that there is a strong correlation to a write being silent or nonsilent both across writes made to the same address during the course of the program execution and across writes to consecutive addresses. to exploit this correlation, we propose using a 2b bimodal predictor (indexed using the page addresses) to predict whether a write request is silent. the accuracy of our predictor (4kb structure) is around 94.4%, on average.\n\n\n# 4. compresso: pragmatic main memory compression\n\n\n\nwe associate a 2-bit saturating counter with each entry in the metadata cache (fig. 5b). the counter is incremented when any writeback to the associated page results in a cache line overflow and is decremented upon cache line underflows (i.e., new data being more compressible).\n\nanother 3-bit global predictor changes state based on page overflows in the system. we speculatively increase a page’s size to the maximum (4kb) when the local as well as global predictors have the higher bit set.\n\nhence, a page is stored uncompressed if it receives multiple streaming cache line overflows during a phase when the overall system is experiencing page overflows\n\n\n# 5. enabling technologies for memory compression:metadata, mapping, and prediction\n\n\n\n\n\nmetadata is coexist with data. thus they have to predict, even read.\n\nreads are problematic because the size of the block is encoded in the block itself. therefore, the read has to either be performed in two phases (read the metadata from the 0th chip, then read data from the appropriate subset of chips) or the read has to conservatively read data from all 9 chips in parallel.\n\nthe first is pcbased, where the pc of the load instruction serves as the index into a predictor table. this assumes that a load tends to access the same type of data record, with relatively uniform compressibility.\n\nthe second is page-based, where the physical page number serves as the index into a predictor table. this assumes that the data records in a single page are of a similar type and have uniform compressibility.\n\non a look-up, the highest-valued saturating counter indicates the predicted size of the block. in case of a tie, we conservatively predict the larger block size.\n\nthey keep a counter for each block compression length, and try to keep counter for each block length. then predict by voting, following majority wins low.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"hbm-latency",frontmatter:{title:"hbm-latency",date:"2023-05-31T17:24:24.000Z",permalink:"/pages/f07696/",tags:[null]},regularPath:"/01.hbm/07.hbm-latency.html",relativePath:"01.hbm/07.hbm-latency.md",key:"v-788071c5",path:"/pages/f07696/",headersStr:null,content:" 1. HBM latency\n\nIn flat mode on Knight’s Landing, MCDRAM latency is around 176 ns, while a DDR4 access has a latency of 147 ns.\n\nCited from blog: Knight’s Landing: Atom with AVX-512.\n\nhttps://chipsandcheese.com/2022/12/08/knights-landing-atom-with-avx-512/\n\nWe report 154.0 ns latency for HBM and 130.4 ns for DRAM.\n\nPaper: Exploring the Performance Benefit of Hybrid Memory System on HPC Environments.\n\n 2. The latency and bandwidth comparison of HBM and DRAM Paper: [UPC Phd Thesis] Memory Bandwidth and Latency in HPC: System Requirements and Performance Impact.",normalizedContent:" 1. hbm latency\n\nin flat mode on knight’s landing, mcdram latency is around 176 ns, while a ddr4 access has a latency of 147 ns.\n\ncited from blog: knight’s landing: atom with avx-512.\n\nhttps://chipsandcheese.com/2022/12/08/knights-landing-atom-with-avx-512/\n\nwe report 154.0 ns latency for hbm and 130.4 ns for dram.\n\npaper: exploring the performance benefit of hybrid memory system on hpc environments.\n\n 2. the latency and bandwidth comparison of hbm and dram paper: [upc phd thesis] memory bandwidth and latency in hpc: system requirements and performance impact.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"compression",frontmatter:{title:"compression",date:"2023-12-20T17:24:24.000Z",permalink:"/pages/f07698/",tags:[null]},regularPath:"/01.hbm/08.compression.html",relativePath:"01.hbm/08.compression.md",key:"v-25995465",path:"/pages/f07698/",headersStr:null,content:" 1. Unified Compilation for Lossless Compression and Sparse Computing\n\ncompute in compressed data && compression on sparse matrix\n\n 2.  Memory Access Granularity Aware Lossless Compression for GPUs\n\n 3.  Linearly Compressed Pages: A Low-Complexity, Low-Latency Main Memory Compression Framework\n\n 4.  FlatPack: Flexible Compaction of Compressed Memory\n\n 5.  Compresso: Pragmatic Main Memory Compression\n\n 6.  Compacted CPU/GPU Data Compression via Modified Virtual Address Translation\n\n 7.  CMH: Compression Management for Improving Capacity in the Hybrid Memory Cube\n\n 8.  Buri: Scaling Big-Memory Computing with Hardware-Based Memory Expansion\n\n 9.  Compress Objects, Not Cache Lines: An Object-Based Compressed Memory Hierarchy\n\n 10. Base-Delta-Immediate Compression:Practical Data Compression for On-Chip Caches\n\n 11. Frequent Pattern Compression: A Significance-Based Compression Scheme for L2 Caches\n\n 12. \n\n 13. BCD Deduplication: Effective Memory Compression using Partial Cache-Line Deduplication\n\n 14. Could Compression Be of General Use? Evaluating Memory Compression across Domains\n\n 15. Linearly Compressed Pages: A Low-Complexity, Low-Latency Main Memory Compression Framework\n\n 16. Compresso: Pragmatic Main Memory Compression\n\n 17. FlatPack: Flexible Compaction of Compressed Memory",normalizedContent:" 1. unified compilation for lossless compression and sparse computing\n\ncompute in compressed data && compression on sparse matrix\n\n 2.  memory access granularity aware lossless compression for gpus\n\n 3.  linearly compressed pages: a low-complexity, low-latency main memory compression framework\n\n 4.  flatpack: flexible compaction of compressed memory\n\n 5.  compresso: pragmatic main memory compression\n\n 6.  compacted cpu/gpu data compression via modified virtual address translation\n\n 7.  cmh: compression management for improving capacity in the hybrid memory cube\n\n 8.  buri: scaling big-memory computing with hardware-based memory expansion\n\n 9.  compress objects, not cache lines: an object-based compressed memory hierarchy\n\n 10. base-delta-immediate compression:practical data compression for on-chip caches\n\n 11. frequent pattern compression: a significance-based compression scheme for l2 caches\n\n 12. \n\n 13. bcd deduplication: effective memory compression using partial cache-line deduplication\n\n 14. could compression be of general use? evaluating memory compression across domains\n\n 15. linearly compressed pages: a low-complexity, low-latency main memory compression framework\n\n 16. compresso: pragmatic main memory compression\n\n 17. flatpack: flexible compaction of compressed memory",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"memory management",frontmatter:{title:"memory management",date:"2024-05-06T11:42:24.000Z",permalink:"/pages/f07692/",tags:[null]},regularPath:"/01.hbm/10.software_memory_paper.html",relativePath:"01.hbm/10.software_memory_paper.md",key:"v-065eff05",path:"/pages/f07692/",headersStr:null,content:" 1. [11 ]Hardware Memory Management for Future Mobile Hybrid Memory Systems\n 2. [4 DAC] OpenMem: Hardware/Software Cooperative Management for Mobile Memory System\n 3. [TACO] FlexPointer: Fast Address Translation Based on Range TLB and Tagged Pointers\n 4. [26] Branch Prediction Is Not a Solved Problem: Measurements, Opportunities, and Future Directions\n 5. [12] When Storage Response Time Catches Up With Overall Context Switch Overhead, What Is Next?\n 6. [225] Dynamic tracking of page miss ratio curve for memory management\n 7. [299] LVI: Hijacking Transient Execution with Load Value Injection",normalizedContent:" 1. [11 ]hardware memory management for future mobile hybrid memory systems\n 2. [4 dac] openmem: hardware/software cooperative management for mobile memory system\n 3. [taco] flexpointer: fast address translation based on range tlb and tagged pointers\n 4. [26] branch prediction is not a solved problem: measurements, opportunities, and future directions\n 5. [12] when storage response time catches up with overall context switch overhead, what is next?\n 6. [225] dynamic tracking of page miss ratio curve for memory management\n 7. [299] lvi: hijacking transient execution with load value injection",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Getting Started with LLVM Core Libraries Chap5 IR",frontmatter:{title:"Getting Started with LLVM Core Libraries Chap5 IR",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000002/",tags:[null]},regularPath:"/02.compiler/02.GetStartedLLVMChap5Notes.html",relativePath:"02.compiler/02.GetStartedLLVMChap5Notes.md",key:"v-0b217b45",path:"/pages/000002/",headers:[{level:3,title:"Chap5. LLVM Intermediate Representation",slug:"chap5-llvm-intermediate-representation",normalizedTitle:"chap5. llvm intermediate representation",charIndex:2}],headersStr:"Chap5. LLVM Intermediate Representation",content:'# Chap5. LLVM Intermediate Representation\n\n\n\n# 1. This IR has three equivalent forms:\n\n\n• An in-memory representation (the Instruction class, among others)\n• An on-disk representation that is encoded in a space-efficient form (the bitcode files)\n• An on-disk representation in a human-readable text form (the LLVM assembly files)\n\n\n# 2. LLVM still conveys some target-specific aspects\n\n\nProgram might implicitly include target-specific headers, like bits linux header folder.\n\n# 3. commands\n\n\nclang *.c -emit-llvm -c -o *.bc\nclang *.c -emit-llvm -S -c -o *.ll\nllvm-as *.ll -o *.bc\nllvm-dis *.bc -o *.ll\n\n//extract function from IR module\nllvm-extract -func=* *.bc -o *.bc\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 4. LLVM IR Language Syntax\n\n\nmodule -> function -> block -> instruction • SSA(Static Single Assignment) Form • Thress Address Instruction • Infinite number of registers\n\ntarget datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"\n\n// type:<size>:<abi>:<preferred>\n// pointer 64bit 64bit 64 bit\n// p:64:64:64\n\n\n1\n2\n3\n4\n5\n\n\n# 5. Introducing llvm IR in-memory model\n\n\n * Module\n   \n   * Module::iterator iterates across functions in the module\n     \n   * begin(); end();\n     \n * Function\n   \n   * isDeclaration()\n     \n   * getArgumentList() or arg_begin(), arg_end()\n     \n   * Iterate through blocks: for (Function::iterator i = function.begin(), e = function.end(); i != e; ++i)\n     \n * BasicBlock\n   \n   * encapsulate all instructions\n   * iterates thorugh begin() and end()\n   * access predecessor or list through getSinglePredecessor\n * Instruction\n   \n   * Predicates: isAssociative(), isCommutative(), isIdempotent(), or isTerminator()\n   * getOpCode()\n   * access Operands() through op_begin() and op_end()\n     \n * Most powerful Value and User Interface\n   * Function and Intruction are subclasses of both Value and User.\n   * BasicBlock is a subclass of Value\n   * Value and User and be navigate through use-def and def-use chain\n   * Value defines a result can be used by others\n   * User means that this entity use one or more Value Interface.\n * Value & User\n   * Value defines use_begin() and use_end() to iterate through all Users def-use chain\n   * ReplaceAllUsesWith(Value *)\n   * User defines op_begin() and op_end() access all of the Value Interface it uses use-def chain\n   * ReplaceUsesOfWith(Value *From, Value *To)\n\n# 6. Compile-time and Link time Optimization\n\n\nopt -O3 sum.bc -o sum-O3.bc\nopt -std-compile-opts sum.bc -o sum-stdc.bc\n\nllvm-link file1.bc file2.bc file3.bc -o=all.bc\nopt -std-link-opts all.bc -o all-stdl.bc\n\nopt sum.bc -mem2reg -instcount -o sum-tmp.bc -stats\nopt sum.bc -time-passes -domtree -instcount -o sum-tmp.bc\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 7. Discovering which passes matter\n\n\nopt -O1 sum-O0.ll -S -o sum-O1.ll\n\nclang -Xclang -print-stats -emit-llvm -O1 sum.c -c -o sum-O1.bc\n\nopt sum-O0.ll -stats -mem2reg -o sum-O1.ll\n\n\n1\n2\n3\n4\n5\n\n\n# 8. Pass Dependencies\n\n\n// full list of passes used when you request just the mem2reg pass\nopt sum-O0.ll -debug-pass=Structure -mem2reg -S -o sum-O1.ll\n\n\n1\n2\n\n\n# 9. Pass API\n\n\n * ModulePass runOnModule()\n * FunctionPass runOnFuction()\n * BasicBlockPass runOnBasicBlock()\n\nIf Unchanged, return false. Or else, return true. 10.',normalizedContent:'# chap5. llvm intermediate representation\n\n\n\n# 1. this ir has three equivalent forms:\n\n\n• an in-memory representation (the instruction class, among others)\n• an on-disk representation that is encoded in a space-efficient form (the bitcode files)\n• an on-disk representation in a human-readable text form (the llvm assembly files)\n\n\n# 2. llvm still conveys some target-specific aspects\n\n\nprogram might implicitly include target-specific headers, like bits linux header folder.\n\n# 3. commands\n\n\nclang *.c -emit-llvm -c -o *.bc\nclang *.c -emit-llvm -s -c -o *.ll\nllvm-as *.ll -o *.bc\nllvm-dis *.bc -o *.ll\n\n//extract function from ir module\nllvm-extract -func=* *.bc -o *.bc\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 4. llvm ir language syntax\n\n\nmodule -> function -> block -> instruction • ssa(static single assignment) form • thress address instruction • infinite number of registers\n\ntarget datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-s128"\n\n// type:<size>:<abi>:<preferred>\n// pointer 64bit 64bit 64 bit\n// p:64:64:64\n\n\n1\n2\n3\n4\n5\n\n\n# 5. introducing llvm ir in-memory model\n\n\n * module\n   \n   * module::iterator iterates across functions in the module\n     \n   * begin(); end();\n     \n * function\n   \n   * isdeclaration()\n     \n   * getargumentlist() or arg_begin(), arg_end()\n     \n   * iterate through blocks: for (function::iterator i = function.begin(), e = function.end(); i != e; ++i)\n     \n * basicblock\n   \n   * encapsulate all instructions\n   * iterates thorugh begin() and end()\n   * access predecessor or list through getsinglepredecessor\n * instruction\n   \n   * predicates: isassociative(), iscommutative(), isidempotent(), or isterminator()\n   * getopcode()\n   * access operands() through op_begin() and op_end()\n     \n * most powerful value and user interface\n   * function and intruction are subclasses of both value and user.\n   * basicblock is a subclass of value\n   * value and user and be navigate through use-def and def-use chain\n   * value defines a result can be used by others\n   * user means that this entity use one or more value interface.\n * value & user\n   * value defines use_begin() and use_end() to iterate through all users def-use chain\n   * replacealluseswith(value *)\n   * user defines op_begin() and op_end() access all of the value interface it uses use-def chain\n   * replaceusesofwith(value *from, value *to)\n\n# 6. compile-time and link time optimization\n\n\nopt -o3 sum.bc -o sum-o3.bc\nopt -std-compile-opts sum.bc -o sum-stdc.bc\n\nllvm-link file1.bc file2.bc file3.bc -o=all.bc\nopt -std-link-opts all.bc -o all-stdl.bc\n\nopt sum.bc -mem2reg -instcount -o sum-tmp.bc -stats\nopt sum.bc -time-passes -domtree -instcount -o sum-tmp.bc\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 7. discovering which passes matter\n\n\nopt -o1 sum-o0.ll -s -o sum-o1.ll\n\nclang -xclang -print-stats -emit-llvm -o1 sum.c -c -o sum-o1.bc\n\nopt sum-o0.ll -stats -mem2reg -o sum-o1.ll\n\n\n1\n2\n3\n4\n5\n\n\n# 8. pass dependencies\n\n\n// full list of passes used when you request just the mem2reg pass\nopt sum-o0.ll -debug-pass=structure -mem2reg -s -o sum-o1.ll\n\n\n1\n2\n\n\n# 9. pass api\n\n\n * modulepass runonmodule()\n * functionpass runonfuction()\n * basicblockpass runonbasicblock()\n\nif unchanged, return false. or else, return true. 10.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Getting Started with LLVM Core Libraries Chap6 Backend",frontmatter:{title:"Getting Started with LLVM Core Libraries Chap6 Backend",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000003/",tags:[null]},regularPath:"/02.compiler/03.GetStartedLLVMChap6Notes.html",relativePath:"02.compiler/03.GetStartedLLVMChap6Notes.md",key:"v-2d0923f6",path:"/pages/000003/",headers:[{level:3,title:"Chap6. The Backend",slug:"chap6-the-backend",normalizedTitle:"chap6. the backend",charIndex:2},{level:3,title:"1. Using the backend tools",slug:"_1-using-the-backend-tools",normalizedTitle:"1. using the backend tools",charIndex:1060},{level:3,title:"2. Learning backend struture",slug:"_2-learning-backend-struture",normalizedTitle:"2. learning backend struture",charIndex:1232},{level:3,title:"3. Knowing backend libraries",slug:"_3-knowing-backend-libraries",normalizedTitle:"3. knowing backend libraries",charIndex:1627},{level:3,title:"4. Learning how to use TableGen for LLVM backends",slug:"_4-learning-how-to-use-tablegen-for-llvm-backends",normalizedTitle:"4. learning how to use tablegen for llvm backends",charIndex:2024},{level:3,title:"5. Instruction Selection Phase",slug:"_5-instruction-selection-phase",normalizedTitle:"5. instruction selection phase",charIndex:3229},{level:3,title:"6. Lowering",slug:"_6-lowering",normalizedTitle:"6. lowering",charIndex:4179},{level:3,title:"7. DAG Combine and legalization",slug:"_7-dag-combine-and-legalization",normalizedTitle:"7. dag combine and legalization",charIndex:4889},{level:3,title:"8. DAG-to-DAG instruction selection",slug:"_8-dag-to-dag-instruction-selection",normalizedTitle:"8. dag-to-dag instruction selection",charIndex:6153},{level:3,title:"9. Scheduler",slug:"_9-scheduler",normalizedTitle:"9. scheduler",charIndex:8783},{level:3,title:"10. Machine Instructions",slug:"_10-machine-instructions",normalizedTitle:"10. machine instructions",charIndex:9629},{level:3,title:"11. Register Allocation",slug:"_11-register-allocation",normalizedTitle:"11. register allocation",charIndex:10221},{level:3,title:"12. Prologue and epilogue",slug:"_12-prologue-and-epilogue",normalizedTitle:"12. prologue and epilogue",charIndex:13492},{level:3,title:"13. Frame indexs",slug:"_13-frame-indexs",normalizedTitle:"13. frame indexs",charIndex:13837},{level:3,title:"13. Understanding machine code framework",slug:"_13-understanding-machine-code-framework",normalizedTitle:"13. understanding machine code framework",charIndex:14047},{level:3,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:14509}],headersStr:"Chap6. The Backend 1. Using the backend tools 2. Learning backend struture 3. Knowing backend libraries 4. Learning how to use TableGen for LLVM backends 5. Instruction Selection Phase 6. Lowering 7. DAG Combine and legalization 8. DAG-to-DAG instruction selection 9. Scheduler 10. Machine Instructions 11. Register Allocation 12. Prologue and epilogue 13. Frame indexs 13. Understanding machine code framework Summary",content:'# Chap6. The Backend\n\n\n\nWhite box Essential Gray Block For generated code efficiency\n\n# 1. Instructon Selection\n\n\n * Convert IR to target-specific SelectionDAG(Directed Acyclic Graph)\n   * Block->DAG\n   * Instruction->Node\n   * Edge contains dataflow dependence and control dependence and glue.\n * LLVM use DAG to employ tree-based pattern-matching instruction selection.\n * IN the end of this phase, IR node are converted to target-machine(machine instructions) nodes.\n\n# 2. Pre-register Allocation(RA) scheduling,the first instruction scheduling.\n\n\n * This is to explore instruction-level parallelism\n * The instructions are converted to MachineInstr three-address representation.\n\n# 3. Reguster Allocation\n\n\n# 4. Post-register Allocation(RA) Instruction Scheduling, the second instruction scheduling\n\n * Now we have real register information, we can combine information of extra hazards and delays of real register to opmitize code.\n\n# 5. Code Emission\n\n * Convert MachineInstr to MCInst\n * Emit Assembly Code\n * Emit Binary blobs to object code format\n\n\n# 1. Using the backend tools\n\nllc *bc -o *.s\nllc *.bc -filetype=obj -o *.o\n\nllc *.bc -march=mips -filetype=obj -o *.o\n\n// how march options\nllc -version\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 2. Learning backend struture\n\n * CodeGen: Instruction selection, scheduler,register allocation\n * MC: assembly parser, disassembler\n * TableGen\n * Target/*.cpp *.h *.td\n\nNotice:\n\nIselLowering is for Selection DAG Node lowering\nIselDAGtoDAG is for instruction selection.\n\n\nTargetLowering is called first for target-specific call and ret.\nThe major instruction selection is in ISelDAGtoDAG.\n\n\n\n\n# 3. Knowing backend libraries\n\n * AsmParser.a\n * AsmPrinter.a\n * CodeGen.a\n    * majority of the target-dependent functionality of the backend, as following：\n    * specific register handling rules, instruction selection, and scheduling\n\n * Desc.a\n    * low-level MC infrastructure and is responsible for registering target-specific MC objects such as MCCodeEmitter\n\n * Info.a\n * Disassembler.a\n\n\n# 4. Learning how to use TableGen for LLVM backends\n\n * instruction formats,\n * instructions,\n * registers,\n * pattern-matching DAGs,\n * instruction selection matching order,\n * calling conventions,\n * target CPU properties (supported Instruction Set Architecture (ISA) features and processor families).\n\ninsns.td\n\n\n\nGenerate code using llvm-tblgen\n\n\n\nTarget Properties: .td\nRegisters: RegisterInfo.td\n\n\n$ cd <llvm_source>/lib/Target/X86\n$ llvm-tblgen -gen-register-info X86.td -I ../../../include\n\n\n1\n2\n\n\nInstruction format: InstrFormat.td\nInstructions: InstrInfo.td\n\n\ninclude/llvm/Target/Target.td\n\n\n1\n\n\n\n\ndag in the above picture represents selectDAG for opcodes, registers or constants during instruction selection phase.\n\n\nSparcInstrInfo.td\n\n\n1\n\n\n\n\nWe can get how the template parameters are assigned to class Instruction.\n\n * OutOperandList\n * InOperandList\n * AsmString\n * Pattern\n\ncd <llvm_sources>/lib/Target/Sparc\nllvm-tblgen -print-records Sparc.td -I ../../../include | grep XNORrr -A 10\n\n\n1\n2\n\n\nThe difference between the first and second need to be checked.\n\n * GenDAGISel.inc\n * GenInstrInfo.inc\n * GenAsmWriter.inc\n * GenCodeEmitter.inc\n * GenDisassemblerTables.inc\n * GenAsmMatcher.inc\n\n\n# 5. Instruction Selection Phase\n\nLLVM IR -> SelectionDAG(SDNode)\n\n 1. Create DAG, in which node carry IR op\n 2. Nodes go through lowering, DAG combiner, and legalization phases.\n 3. Instruction selection perform DAG-to-DAG conversion, using node pattern matching and transforms SelectionDAG node into nodes representing target instructions.\n\nMost expensive ones in backend\n\n# 5.1 SelectionDAG class\n\n * DAG for each basic block\n * SDNode for instruction or operand\n\n\n\n * The black arrows represent regular edges showing a dataflow dependence.\n * The dashed blue arrows represent non-dataflow chains that exist to enforce order between two otherwise unrelated instructions.\n * The red edge guarantees that its adjacent nodes must be glued together\n\nPlease notice:\n\n * CopyFromReg: This is for getting value out of scope.\n * CopyToReg: This node copies a value to a specific register without supplying any concrete value for other nodes to consume.\n\n\n# 6. Lowering\n\n\n\n 1. SelectionDAGBuilder in SelectionDAGIsel.cpp visits every fuction and creates SelectionDAG for each basic block\n 2. During 1), special IR such as call and ret needs TargetLowering class for the first time for info like: pass call arg and how to return.\n 3. Only a smalle subset are lowered in this way. Majority are matched and replaces at instruction selection.\n\n> For instance, in SelectionDAG from sum.bc, the X86TargetLowering::LowerReturn() method (see lib/Target/X86/X86ISelLowering.cpp) is used to lower the IR ret instruction.\n> While doing this, it generates the X86ISD::RET_FLAG node, which copies the function result to EAX a-target-specific way to handle the function return.\n\n\n# 7. DAG Combine and legalization\n\n * DAG Combine\n   * Optimization for simpler code\n   * Target Independent: lib/CodeGen/SelectionDAG/DAGCombiner.cpp\n   * Target Dependnet: lib/Target/<Target_Name>/ISelLowering.cpp setTargetDAGCombine()\n\nsetTargetDAGCombine({ISD::SDIVREM, ISD::UDIVREM, ISD::SELECT, ISD::AND,\n                       ISD::OR, ISD::ADD, ISD::SUB, ISD::AssertZext, ISD::SHL});\n\nstatic SDValue performADDCombine(SDNode *N, SelectionDAG &DAG,\n                                 TargetLowering::DAGCombinerInfo &DCI,\n                                 const MipsSubtarget &Subtarget) {\n  ...\n  // (add v0, (add v1, abs_lo(tjt))) => (add (add v0, v1), abs_lo(tjt))\n  SDValue Add = N->getOperand(1);\n\n  if (Add.getOpcode() != ISD::ADD)\n    return SDValue();\n\n  SDValue Lo = Add.getOperand(1);\n  ...\n  EVT ValTy = N->getValueType(0);\n  SDLoc DL(N);\n\n  SDValue Add1 = DAG.getNode(ISD::ADD, DL, ValTy, N->getOperand(0),\n                             Add.getOperand(0));\n  return DAG.getNode(ISD::ADD, DL, ValTy, Add1, Lo);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n * Legalization\n\n * Support legal types: scalar: promote, expand, soften. vec split, scalarized or widened\n * Also it can be customized\n\nPromote\nExpand(library call)\nCustom\n\n\n\n\n# 8. DAG-to-DAG instruction selection\n\nTransform target-independent nodes to target-specific nodes by using pattern matching.\n\nCopyToReg, CopyFromReg and Register nodes are untouched until Register Allocation.\n\n# 8.1 Pattern Matching\n\nlib/Target/Sparc/SparcISelDAGToDAG.cpp\n\nSelect()  in SelectionDAGISel subclass\n\n\n1\n2\n3\n\n\nSelect():\n\n * receive an SDNode parameter to be matched\n * return SDNnode value representing a phycical instruction\n\nSelection() will call TableGen generateed SelectCode method.\nTableGen also contains MatcherTable, mapping ISD and ISD to physical-instruction node.\nThis table is generated by InstrInfo.td\nThe table are contained in <build_dir>/lib/Target/Sparc/SparcGenDAGISel.inc.\n\n\nWe can add other customized matching code prior to selectCode().\n\nCurDAG->getMachineNode() will create a node with phsycial instruction SP::SPAri CurDAG->SelectNodeTo() will create an instruction node and changes all use of * result to point to the "Opcode" result.\n\nvoid SparcDAGToDAGISel::Select(SDNode *N) {\n  ...\n  case ISD::UDIV: {\n    // sdivx / udivx handle 64-bit divides.\n    if (N->getValueType(0) == MVT::i64)\n      break;\n    // FIXME: should use a custom expander to expose the SRA to the dag.\n    SDValue DivLHS = N->getOperand(0);\n    SDValue DivRHS = N->getOperand(1);\n\n    // Set the Y register to the high-part.\n    SDValue TopPart;\n    if (N->getOpcode() == ISD::SDIV) {\n      TopPart = SDValue(CurDAG->getMachineNode(SP::SRAri, dl, MVT::i32, DivLHS,\n                                   CurDAG->getTargetConstant(31, dl, MVT::i32)),\n                        0);\n    } else {\n      TopPart = CurDAG->getRegister(SP::G0, MVT::i32);\n    }\n    TopPart = CurDAG->getCopyToReg(CurDAG->getEntryNode(), dl, SP::Y, TopPart,\n                                   SDValue())\n                  .getValue(1);\n\n    // FIXME: Handle div by immediate.\n    unsigned Opcode = N->getOpcode() == ISD::SDIV ? SP::SDIVrr : SP::UDIVrr;\n    CurDAG->SelectNodeTo(N, Opcode, MVT::i32, DivLHS, DivRHS, TopPart);\n    return;\n  }\n  }\n\n  SelectCode(N);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n# 8.2 Visualizing the instruction selection process\n\nLLC                         PHASE\n-view-dag-combine1-dags     Before DAG combine 1\n-view-legalize-types-dags   Before legalize type\n-view-dag-combine-lt-dags   After legalize type 2 and before DAG combine\n-view-legalize-dags         Before legalization\n-view-dag-combine2-dags     Before DAG combine 2\n-view-isel-dags             Before instruction selection\n-view-sched-dags            After instruction selection and before scheduling\n\n\n# 9. Scheduler\n\nPre-register allocation works on SelectionDAG nodes(SDNodes).\n\n\n<llvm_source>/ lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp\n\nDifferent Algorithms: llc -pre-RA-sched=\n\n# 9.1 Instruction Itineraries\n\n<llvm_source>/include/llvm/Target/TargetItinerary.td\n<llvm_source>/lib/Target/ARM/ARMScheduleA8.td\n\n\nRepresent instruction latencya and hardware pipeline information.\n\n\n\n# 9.2 Hazard Detection\n\n> The ScheduleHazardRecognizer class provides an interface for hazard recognizer implementations and the ScoreboardHazardRecognizer subclass implements the scoreboard hazard recognizer (see the file <llvm_source>/lib/CodeGen/ScoreboardHazardRecognizer.cpp), which is LLVM\'s default recognizer.\n\n# 9.3 Scheduling Units\n\nThis scheduler runs before and after register allocation, which process both SDNode instruction and MachineInstr.\n\n\n# 10. Machine Instructions\n\nThe InstrEmitter pass, which runs after scheduling, transforms SDNode format into MachineInstr format.\nMI format is sequence of instructions rather than DAG.\n\n\nMI contains significant meta-information about an instruction:\n\n\n * it stores used and defined registers.\n * it distinguishes between register and memory operands (among other types).\n * it stores the instruction type (branch, return, call, and terminator, among others)\n * it stores predicates such as whether it is commutable or not, and so on.\n\n\n\nllc -print-machineinstrs\n\nllc -print-machineinstrs=\n\n\n# 11. Register Allocation\n\n * some MI code fragments might already use physical registers even before register allocation.\n   * Machine instructions that nee specific register\n   * ABI requirement\n * Destruct SSA form of IR\n\n4 register allocation algoritm\n\n-regalloc=<pbqp/greedy/basic/fast>\n\nby default, it will be basic(linear scan).\n\n\n\n# 11.1 Register Coalescer\n\nlib/CodeGen/RegisterCoalescer.cpp\n\n\n * A machine Function Pass, joinAllIntervals will iterate a work list of copy functions.\n * joinCopy creates CoalescerPair instances from copy machine instructions and coalesces copies wasy.\n\nBefore the coalescer, the phi node elimination pass runs.\nllc -print-machine-insts=phi-node-elimination will show this.\n\nMachine Instruction will be indexed with 0B, 16B, 32B(slot indexes).\n\nlive variable analysis pass runs before coalescing, thus the code is annotated with live variable information\n\n * which points each register is defined and killed\n * This is useful for ust to know which registers interfere with one other, that is are alive at the same time and need to live in distinct physical register.(Similar to graph coloring)\n\nCoalescer will also look for register copies, try to join the interval of the source register with the interval of the destination register.\nThe above is based on live interval analysis(different from live variable analysis).\n\nllc -march=sparc -debug-only=regalloc *.bc\n\n\n1\n\n\n# 11.2 Virtual Register Rewrite\n\n * Register Allocation Pass will selects the physical registers to be used for each virtual one.\n   \n * VirtRegMap contains mapping from virt to phy register.\n * VirtRegRewriter class implemented in <llvm_source>/lib/CodeGen/VirtRegMap.cpp—uses VirtRegMap and replaces virtual register references with physical ones.\n * Spill Code is also generated.\n * reg = COPY reg are deleted.\n\n> The register allocator and the instruction scheduler are sworn enemies in any compiler.\n> The job of the register allocator is to keep live ranges as short as possible, reducing the number of edges of the interference graph and thus reducing the number of necessary registers to avoid spills. To do this, the register allocator prefers to schedule instructions in a serial fashion (putting an instruction that depends on the other right next to it) because in this way the code uses less registers.\n> The job of the scheduler is the opposite: to extract instruction-level parallelism, it needs to keep alive as much unrelated and parallel computations as possible, requiring a much larger number of registers to hold intermediary values and increasing the number of interferences among live ranges.\n\n# 11.3 Target Hooks\n\n 1. TargetRegisterInfo includes if it is reserved or not, its parent register classes, and whether it is physical or virtual\n 2. InstrInfo\n     * isLoadFromStackSlot() and isStoreToStackSlot() are used during spill code generation to discover whether the machine instruction is a memory access to a stack slot.\n     * Spiller use using the storeRegToStackSlot() and loadRegFromStackSlot() methods with target-specific memory access instructions.\n     * copyPhyReg() method will also generate target-specific register copy.\n\nThe BuildMI() method is used everywhere in the code generator to generate machine instructions.\n\n\n# 12. Prologue and epilogue\n\n 1. Prologue sets up the stack frame and callee-saved registers during the beginning of a function.\n 2. Epilogue cleans up the stack frame prior to function return. They are target-specific, defined in FrameLowering::emitPrologue() and FrameLowering::emitEpilogue() at <llvm_source>/lib/Target//FrameLowering.cpp)\n\n\n# 13. Frame indexs\n\n<llvm_source>/lib/Target//RegisterInfo.cpp contains eliminateFrameIndex().\nIt will replace each frame index to real stack offset for all machine instructions that contain stack reference.\n\n\n# 13. Understanding machine code framework\n\nconvert machine instruction into machine code instructions(MC instructions).\n\n// show MC inst\nllc *.bc -march=x86-64 -show-mc-inst -o -\n\n// show assemble encoding\necho "movq 48879(,%riz), %rax" | llvm-mc -triple=x86_64 --show-encoding\n # encoding: [0x48,0x8b,0x04,0x25,0xef,0xbe,0x00,0x00]\n\n// disassemble\necho "0x8d 0x4c 0x24 0x04" | llvm-mc --disassemble -triple=x86_64\n leal 4(%rsp), %ecx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# Summary\n\n> https://jonathan2251.github.io/lbd/_images/9.png',normalizedContent:'# chap6. the backend\n\n\n\nwhite box essential gray block for generated code efficiency\n\n# 1. instructon selection\n\n\n * convert ir to target-specific selectiondag(directed acyclic graph)\n   * block->dag\n   * instruction->node\n   * edge contains dataflow dependence and control dependence and glue.\n * llvm use dag to employ tree-based pattern-matching instruction selection.\n * in the end of this phase, ir node are converted to target-machine(machine instructions) nodes.\n\n# 2. pre-register allocation(ra) scheduling,the first instruction scheduling.\n\n\n * this is to explore instruction-level parallelism\n * the instructions are converted to machineinstr three-address representation.\n\n# 3. reguster allocation\n\n\n# 4. post-register allocation(ra) instruction scheduling, the second instruction scheduling\n\n * now we have real register information, we can combine information of extra hazards and delays of real register to opmitize code.\n\n# 5. code emission\n\n * convert machineinstr to mcinst\n * emit assembly code\n * emit binary blobs to object code format\n\n\n# 1. using the backend tools\n\nllc *bc -o *.s\nllc *.bc -filetype=obj -o *.o\n\nllc *.bc -march=mips -filetype=obj -o *.o\n\n// how march options\nllc -version\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 2. learning backend struture\n\n * codegen: instruction selection, scheduler,register allocation\n * mc: assembly parser, disassembler\n * tablegen\n * target/*.cpp *.h *.td\n\nnotice:\n\nisellowering is for selection dag node lowering\niseldagtodag is for instruction selection.\n\n\ntargetlowering is called first for target-specific call and ret.\nthe major instruction selection is in iseldagtodag.\n\n\n\n\n# 3. knowing backend libraries\n\n * asmparser.a\n * asmprinter.a\n * codegen.a\n    * majority of the target-dependent functionality of the backend, as following：\n    * specific register handling rules, instruction selection, and scheduling\n\n * desc.a\n    * low-level mc infrastructure and is responsible for registering target-specific mc objects such as mccodeemitter\n\n * info.a\n * disassembler.a\n\n\n# 4. learning how to use tablegen for llvm backends\n\n * instruction formats,\n * instructions,\n * registers,\n * pattern-matching dags,\n * instruction selection matching order,\n * calling conventions,\n * target cpu properties (supported instruction set architecture (isa) features and processor families).\n\ninsns.td\n\n\n\ngenerate code using llvm-tblgen\n\n\n\ntarget properties: .td\nregisters: registerinfo.td\n\n\n$ cd <llvm_source>/lib/target/x86\n$ llvm-tblgen -gen-register-info x86.td -i ../../../include\n\n\n1\n2\n\n\ninstruction format: instrformat.td\ninstructions: instrinfo.td\n\n\ninclude/llvm/target/target.td\n\n\n1\n\n\n\n\ndag in the above picture represents selectdag for opcodes, registers or constants during instruction selection phase.\n\n\nsparcinstrinfo.td\n\n\n1\n\n\n\n\nwe can get how the template parameters are assigned to class instruction.\n\n * outoperandlist\n * inoperandlist\n * asmstring\n * pattern\n\ncd <llvm_sources>/lib/target/sparc\nllvm-tblgen -print-records sparc.td -i ../../../include | grep xnorrr -a 10\n\n\n1\n2\n\n\nthe difference between the first and second need to be checked.\n\n * gendagisel.inc\n * geninstrinfo.inc\n * genasmwriter.inc\n * gencodeemitter.inc\n * gendisassemblertables.inc\n * genasmmatcher.inc\n\n\n# 5. instruction selection phase\n\nllvm ir -> selectiondag(sdnode)\n\n 1. create dag, in which node carry ir op\n 2. nodes go through lowering, dag combiner, and legalization phases.\n 3. instruction selection perform dag-to-dag conversion, using node pattern matching and transforms selectiondag node into nodes representing target instructions.\n\nmost expensive ones in backend\n\n# 5.1 selectiondag class\n\n * dag for each basic block\n * sdnode for instruction or operand\n\n\n\n * the black arrows represent regular edges showing a dataflow dependence.\n * the dashed blue arrows represent non-dataflow chains that exist to enforce order between two otherwise unrelated instructions.\n * the red edge guarantees that its adjacent nodes must be glued together\n\nplease notice:\n\n * copyfromreg: this is for getting value out of scope.\n * copytoreg: this node copies a value to a specific register without supplying any concrete value for other nodes to consume.\n\n\n# 6. lowering\n\n\n\n 1. selectiondagbuilder in selectiondagisel.cpp visits every fuction and creates selectiondag for each basic block\n 2. during 1), special ir such as call and ret needs targetlowering class for the first time for info like: pass call arg and how to return.\n 3. only a smalle subset are lowered in this way. majority are matched and replaces at instruction selection.\n\n> for instance, in selectiondag from sum.bc, the x86targetlowering::lowerreturn() method (see lib/target/x86/x86isellowering.cpp) is used to lower the ir ret instruction.\n> while doing this, it generates the x86isd::ret_flag node, which copies the function result to eax a-target-specific way to handle the function return.\n\n\n# 7. dag combine and legalization\n\n * dag combine\n   * optimization for simpler code\n   * target independent: lib/codegen/selectiondag/dagcombiner.cpp\n   * target dependnet: lib/target/<target_name>/isellowering.cpp settargetdagcombine()\n\nsettargetdagcombine({isd::sdivrem, isd::udivrem, isd::select, isd::and,\n                       isd::or, isd::add, isd::sub, isd::assertzext, isd::shl});\n\nstatic sdvalue performaddcombine(sdnode *n, selectiondag &dag,\n                                 targetlowering::dagcombinerinfo &dci,\n                                 const mipssubtarget &subtarget) {\n  ...\n  // (add v0, (add v1, abs_lo(tjt))) => (add (add v0, v1), abs_lo(tjt))\n  sdvalue add = n->getoperand(1);\n\n  if (add.getopcode() != isd::add)\n    return sdvalue();\n\n  sdvalue lo = add.getoperand(1);\n  ...\n  evt valty = n->getvaluetype(0);\n  sdloc dl(n);\n\n  sdvalue add1 = dag.getnode(isd::add, dl, valty, n->getoperand(0),\n                             add.getoperand(0));\n  return dag.getnode(isd::add, dl, valty, add1, lo);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n * legalization\n\n * support legal types: scalar: promote, expand, soften. vec split, scalarized or widened\n * also it can be customized\n\npromote\nexpand(library call)\ncustom\n\n\n\n\n# 8. dag-to-dag instruction selection\n\ntransform target-independent nodes to target-specific nodes by using pattern matching.\n\ncopytoreg, copyfromreg and register nodes are untouched until register allocation.\n\n# 8.1 pattern matching\n\nlib/target/sparc/sparciseldagtodag.cpp\n\nselect()  in selectiondagisel subclass\n\n\n1\n2\n3\n\n\nselect():\n\n * receive an sdnode parameter to be matched\n * return sdnnode value representing a phycical instruction\n\nselection() will call tablegen generateed selectcode method.\ntablegen also contains matchertable, mapping isd and isd to physical-instruction node.\nthis table is generated by instrinfo.td\nthe table are contained in <build_dir>/lib/target/sparc/sparcgendagisel.inc.\n\n\nwe can add other customized matching code prior to selectcode().\n\ncurdag->getmachinenode() will create a node with phsycial instruction sp::spari curdag->selectnodeto() will create an instruction node and changes all use of * result to point to the "opcode" result.\n\nvoid sparcdagtodagisel::select(sdnode *n) {\n  ...\n  case isd::udiv: {\n    // sdivx / udivx handle 64-bit divides.\n    if (n->getvaluetype(0) == mvt::i64)\n      break;\n    // fixme: should use a custom expander to expose the sra to the dag.\n    sdvalue divlhs = n->getoperand(0);\n    sdvalue divrhs = n->getoperand(1);\n\n    // set the y register to the high-part.\n    sdvalue toppart;\n    if (n->getopcode() == isd::sdiv) {\n      toppart = sdvalue(curdag->getmachinenode(sp::srari, dl, mvt::i32, divlhs,\n                                   curdag->gettargetconstant(31, dl, mvt::i32)),\n                        0);\n    } else {\n      toppart = curdag->getregister(sp::g0, mvt::i32);\n    }\n    toppart = curdag->getcopytoreg(curdag->getentrynode(), dl, sp::y, toppart,\n                                   sdvalue())\n                  .getvalue(1);\n\n    // fixme: handle div by immediate.\n    unsigned opcode = n->getopcode() == isd::sdiv ? sp::sdivrr : sp::udivrr;\n    curdag->selectnodeto(n, opcode, mvt::i32, divlhs, divrhs, toppart);\n    return;\n  }\n  }\n\n  selectcode(n);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n# 8.2 visualizing the instruction selection process\n\nllc                         phase\n-view-dag-combine1-dags     before dag combine 1\n-view-legalize-types-dags   before legalize type\n-view-dag-combine-lt-dags   after legalize type 2 and before dag combine\n-view-legalize-dags         before legalization\n-view-dag-combine2-dags     before dag combine 2\n-view-isel-dags             before instruction selection\n-view-sched-dags            after instruction selection and before scheduling\n\n\n# 9. scheduler\n\npre-register allocation works on selectiondag nodes(sdnodes).\n\n\n<llvm_source>/ lib/codegen/selectiondag/scheduledagsdnodes.cpp\n\ndifferent algorithms: llc -pre-ra-sched=\n\n# 9.1 instruction itineraries\n\n<llvm_source>/include/llvm/target/targetitinerary.td\n<llvm_source>/lib/target/arm/armschedulea8.td\n\n\nrepresent instruction latencya and hardware pipeline information.\n\n\n\n# 9.2 hazard detection\n\n> the schedulehazardrecognizer class provides an interface for hazard recognizer implementations and the scoreboardhazardrecognizer subclass implements the scoreboard hazard recognizer (see the file <llvm_source>/lib/codegen/scoreboardhazardrecognizer.cpp), which is llvm\'s default recognizer.\n\n# 9.3 scheduling units\n\nthis scheduler runs before and after register allocation, which process both sdnode instruction and machineinstr.\n\n\n# 10. machine instructions\n\nthe instremitter pass, which runs after scheduling, transforms sdnode format into machineinstr format.\nmi format is sequence of instructions rather than dag.\n\n\nmi contains significant meta-information about an instruction:\n\n\n * it stores used and defined registers.\n * it distinguishes between register and memory operands (among other types).\n * it stores the instruction type (branch, return, call, and terminator, among others)\n * it stores predicates such as whether it is commutable or not, and so on.\n\n\n\nllc -print-machineinstrs\n\nllc -print-machineinstrs=\n\n\n# 11. register allocation\n\n * some mi code fragments might already use physical registers even before register allocation.\n   * machine instructions that nee specific register\n   * abi requirement\n * destruct ssa form of ir\n\n4 register allocation algoritm\n\n-regalloc=<pbqp/greedy/basic/fast>\n\nby default, it will be basic(linear scan).\n\n\n\n# 11.1 register coalescer\n\nlib/codegen/registercoalescer.cpp\n\n\n * a machine function pass, joinallintervals will iterate a work list of copy functions.\n * joincopy creates coalescerpair instances from copy machine instructions and coalesces copies wasy.\n\nbefore the coalescer, the phi node elimination pass runs.\nllc -print-machine-insts=phi-node-elimination will show this.\n\nmachine instruction will be indexed with 0b, 16b, 32b(slot indexes).\n\nlive variable analysis pass runs before coalescing, thus the code is annotated with live variable information\n\n * which points each register is defined and killed\n * this is useful for ust to know which registers interfere with one other, that is are alive at the same time and need to live in distinct physical register.(similar to graph coloring)\n\ncoalescer will also look for register copies, try to join the interval of the source register with the interval of the destination register.\nthe above is based on live interval analysis(different from live variable analysis).\n\nllc -march=sparc -debug-only=regalloc *.bc\n\n\n1\n\n\n# 11.2 virtual register rewrite\n\n * register allocation pass will selects the physical registers to be used for each virtual one.\n   \n * virtregmap contains mapping from virt to phy register.\n * virtregrewriter class implemented in <llvm_source>/lib/codegen/virtregmap.cpp—uses virtregmap and replaces virtual register references with physical ones.\n * spill code is also generated.\n * reg = copy reg are deleted.\n\n> the register allocator and the instruction scheduler are sworn enemies in any compiler.\n> the job of the register allocator is to keep live ranges as short as possible, reducing the number of edges of the interference graph and thus reducing the number of necessary registers to avoid spills. to do this, the register allocator prefers to schedule instructions in a serial fashion (putting an instruction that depends on the other right next to it) because in this way the code uses less registers.\n> the job of the scheduler is the opposite: to extract instruction-level parallelism, it needs to keep alive as much unrelated and parallel computations as possible, requiring a much larger number of registers to hold intermediary values and increasing the number of interferences among live ranges.\n\n# 11.3 target hooks\n\n 1. targetregisterinfo includes if it is reserved or not, its parent register classes, and whether it is physical or virtual\n 2. instrinfo\n     * isloadfromstackslot() and isstoretostackslot() are used during spill code generation to discover whether the machine instruction is a memory access to a stack slot.\n     * spiller use using the storeregtostackslot() and loadregfromstackslot() methods with target-specific memory access instructions.\n     * copyphyreg() method will also generate target-specific register copy.\n\nthe buildmi() method is used everywhere in the code generator to generate machine instructions.\n\n\n# 12. prologue and epilogue\n\n 1. prologue sets up the stack frame and callee-saved registers during the beginning of a function.\n 2. epilogue cleans up the stack frame prior to function return. they are target-specific, defined in framelowering::emitprologue() and framelowering::emitepilogue() at <llvm_source>/lib/target//framelowering.cpp)\n\n\n# 13. frame indexs\n\n<llvm_source>/lib/target//registerinfo.cpp contains eliminateframeindex().\nit will replace each frame index to real stack offset for all machine instructions that contain stack reference.\n\n\n# 13. understanding machine code framework\n\nconvert machine instruction into machine code instructions(mc instructions).\n\n// show mc inst\nllc *.bc -march=x86-64 -show-mc-inst -o -\n\n// show assemble encoding\necho "movq 48879(,%riz), %rax" | llvm-mc -triple=x86_64 --show-encoding\n # encoding: [0x48,0x8b,0x04,0x25,0xef,0xbe,0x00,0x00]\n\n// disassemble\necho "0x8d 0x4c 0x24 0x04" | llvm-mc --disassemble -triple=x86_64\n leal 4(%rsp), %ecx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# summary\n\n> https://jonathan2251.github.io/lbd/_images/9.png',charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"llvm flow",frontmatter:{title:"llvm flow",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000001/",tags:[null]},regularPath:"/02.compiler/01.llvm_flow.html",relativePath:"02.compiler/01.llvm_flow.md",key:"v-3d658257",path:"/pages/000001/",headers:[{level:2,title:"Programing Language Dependent",slug:"programing-language-dependent",normalizedTitle:"programing language dependent",charIndex:171},{level:3,title:"1. Clang parser will build an Abstract Syntax Tree(AST)",slug:"_1-clang-parser-will-build-an-abstract-syntax-tree-ast",normalizedTitle:"1. clang parser will build an abstract syntax tree(ast)",charIndex:205},{level:3,title:"2. Clang emit LLVM IR",slug:"_2-clang-emit-llvm-ir",normalizedTitle:"2. clang emit llvm ir",charIndex:1583},{level:2,title:"IR Target and Source Programing Language Independent",slug:"ir-target-and-source-programing-language-independent",normalizedTitle:"ir target and source programing language independent",charIndex:3153},{level:3,title:"3. LLVM Target Independent IR Optimization",slug:"_3-llvm-target-independent-ir-optimization",normalizedTitle:"3. llvm target independent ir optimization",charIndex:3210},{level:2,title:"Target Dependent Codegen",slug:"target-dependent-codegen",normalizedTitle:"target dependent codegen",charIndex:4520},{level:3,title:"4. SelectionDAG Node",slug:"_4-selectiondag-node",normalizedTitle:"4. selectiondag node",charIndex:4549},{level:3,title:"5. To emit machine instructions, LLVM will legalize the operation",slug:"_5-to-emit-machine-instructions-llvm-will-legalize-the-operation",normalizedTitle:"5. to emit machine instructions, llvm will legalize the operation",charIndex:4784},{level:3,title:"6. Instruction selection from SDNode to MachineSDNode",slug:"_6-instruction-selection-from-sdnode-to-machinesdnode",normalizedTitle:"6. instruction selection from sdnode to machinesdnode",charIndex:6570},{level:3,title:"7. Scheduling and emitting a MachineInstr",slug:"_7-scheduling-and-emitting-a-machineinstr",normalizedTitle:"7. scheduling and emitting a machineinstr",charIndex:8579},{level:3,title:"8. Register Allocation",slug:"_8-register-allocation",normalizedTitle:"8. register allocation",charIndex:10163},{level:3,title:"9. From MachineInstruction to MCinst",slug:"_9-from-machineinstruction-to-mcinst",normalizedTitle:"9. from machineinstruction to mcinst",charIndex:11676},{level:3,title:"10. Build LLVM",slug:"_10-build-llvm",normalizedTitle:"10. build llvm",charIndex:12728}],headersStr:"Programing Language Dependent 1. Clang parser will build an Abstract Syntax Tree(AST) 2. Clang emit LLVM IR IR Target and Source Programing Language Independent 3. LLVM Target Independent IR Optimization Target Dependent Codegen 4. SelectionDAG Node 5. To emit machine instructions, LLVM will legalize the operation 6. Instruction selection from SDNode to MachineSDNode 7. Scheduling and emitting a MachineInstr 8. Register Allocation 9. From MachineInstruction to MCinst 10. Build LLVM",content:'llvm front end demo Notes from Life of an instruction in LLVM https://blog.llvm.org/2012/11/life-of-instruction-in-llvm.html\n\n----------------------------------------\n\n\n# Programing Language Dependent\n\n\n# 1. Clang parser will build an Abstract Syntax Tree(AST)\n\n# Input: High-level source code (e.g., C, C++).\n\n# Output: An AST, which is a tree structure representing the syntactic structure of the source code\n\n# Tool: Clang Frontend\n\n# Transformation\n\nThe compiler’s frontend (Clang in LLVM) parses the source code and constructs an AST.\nThe AST captures the hierarchical structure of the program based on grammar rules (e.g., function definitions, statements, expressions).\n\n# Reason\n\nThe AST is closely tied to the original source code and allows easy analysis and checking of syntactic correctness.\nIt serves as a starting point for semantic analysis (e.g., type checking, variable scoping) before code generation.\n\n# Key Source File\n\nclang/lib/AST/\n\n# Key Functions/Classes\n\n# ParseAST\n\nThe function responsible for initiating the parsing of source code to generate an Abstract Syntax Tree (AST).\nFunction: This function takes the parsed tokens from the lexer and generates the AST by applying grammar rules.\nIt processes the Input: source code and organizes it into hierarchical structures like functions, expressions, and statements.\n\n# Sema\n\nThe Sema class in Clang performs semantic analysis on the AST.\nFunction: After parsing the AST, Sema checks the semantic validity of the code, ensuring things like proper type usage, function declarations, and scope resolution.\n\n\n# 2. Clang emit LLVM IR\n\n# Input: AST.\n\n# Output: LLVM Intermediate Representation (IR).\n\n# Tool: Clang’s Code Generation phase.\n\n# Transformation\n\nOnce the AST is constructed, the compiler translates it into LLVM IR.\nLLVM IR is a low-level, typed, static single assignment (SSA) form that is independent of any specific machine architecture.\nIt consists of a set of instructions operating on virtual registers, with each register assigned only once (SSA form).\n\n# Reason\n\nLLVM IR is a portable, intermediate code format that enables various machine-independent optimizations. It is also the bridge between different frontends (e.g., Clang, Rust) and the backend that handles machine-specific code generation.\n\n# Key Source File\n\nclang/lib/CodeGen/\n\n# Key Functions/Classes\n\n# CodeGenFunction: This class is responsible for generating LLVM IR from the AST.\n\nFunction: It traverses the AST and generates the corresponding LLVM IR.\nFor example, for a function definition in the AST, it creates an llvm::Function in the IR.\n\n# EmitFunctionBody: This method inside CodeGenFunction emits the body of a function in IR.\n\nFunction: It walks over statements and expressions within the function and emits LLVM IR instructions.\n\n# EmitExpr and EmitStmt: These methods handle expressions and statements within the AST and translate them into LLVM IR.\n\nFunction: EmitExpr generates LLVM IR for expressions (e.g., arithmetic operations, function calls), while EmitStmt generates IR for control-flow structures like if statements and loops.\n\n----------------------------------------\n\n\n# IR Target and Source Programing Language Independent\n\n\n# 3. LLVM Target Independent IR Optimization\n\n# Input: Unoptimized LLVM IR.\n\n# Output: Optimized LLVM IR.\n\n# Tool: LLVM Optimizer (opt).\n\n# Transformation\n\nLLVM applies a series of machine-independent optimizations to the IR, such as dead code elimination, constant propagation, inlining, and loop optimizations.\nThese transformations operate on the SSA form of the IR to improve performance and reduce unnecessary instructions.\n\n# Reason\n\nOptimizing at the IR level allows for improvements that are independent of the target architecture, making the resulting code more efficient before it reaches machine-specific stages.\n\n# Key Source Files: llvm/lib/Transforms/\n\n# Key Functions/Classes:\n\n# opt The opt Tool: runs optimization passes on the LLVM IR.\n\nFunction: opt applies a series of transformations to the IR to improve efficiency.\nThese transformations include passes like dead code elimination (DeadCodeElimination.cpp), inlining (InlineFunction.cpp), and constant propagation (ConstantPropagation.cpp).\n\n# Key Passes:\n\nDeadStoreElimination: Eliminates stores that are never used.\nSCCP (Sparse Conditional Constant Propagation): Optimizes based on constant values propagated through the program.\nGVN (Global Value Numbering): Removes redundant calculations.\n\n----------------------------------------\n\n\n# Target Dependent Codegen\n\n\n# 4. SelectionDAG Node\n\n\nSelectionDAGBuild creates SDGNode\nSelectionDAGIsel goes over all IR instructions and calls SelectionDAGBuilder::visit to Dispatch them\nWe can use -debug to llc or -view to get log or dump image of the graph\n\n\n\n# 5. To emit machine instructions, LLVM will legalize the operation\n\n\nUse target-specific hooks to convert all operations and types into ones that the target actually supports. This is done by TargetLowering.\nSelectionDAGLegalize::LegalizeOp\n\n# 4 & 5\n\n# Input: Optimized LLVM IR.\n\n# Output: SelectionDAG.\n\n# Tool: LLVM CodeGen Phase.\n\n# Transformation\n\nIn this phase, LLVM lowers the IR into a Selection Directed Acyclic Graph (SelectionDAG).\nThe SelectionDAG represents the program in terms of target-independent operations (nodes) and data dependencies (edges).\nEach node in the DAG represents an operation, and the edges show data flow between these operations.\n\n# Reason\n\nThe SelectionDAG abstracts machine-specific details while exposing data dependencies, allowing for architecture-specific instruction selection and scheduling in a clean, optimized manner.\n\n# Key Source File llvm/lib/CodeGen/SelectionDAG/\n\n# Key Functions/Classes\n\n# SelectionDAGBuilder: This class is responsible for constructing the SelectionDAG from the LLVM IR.\n\nFunction: It lowers the LLVM IR instructions into nodes in the SelectionDAG.\nEach node in the DAG represents an operation (e.g., addition, memory load) that can later be translated into machine instructions.\n\n# LowerOperation: A method that lowers a specific LLVM IR instruction (like an add operation) to a corresponding SelectionDAG node.\n\nFunction: This function breaks down LLVM IR into target-independent operations within the DAG.\n\n# LegalizeDAG: Ensures that the DAG\'s operations conform to the target machine\'s constraints (e.g., valid data types, supported instructions).\n\nFunction: Legalizes operations by splitting larger types into smaller ones or mapping operations to a series of simpler operations that the target can execute.\n\n\n# 6. Instruction selection from SDNode to MachineSDNode\n\n\nSelectionDAGISel::Select\nSelectCode\nThis step will create MachineSDNode, a subclass of SDNode which holds the information required to construct an actual machine instruction, but still in DAG node form.\n\n> LLVM provides a generic table-based instruction selection mechanism that is auto-generated with the help of TableGen. Many target backends, however, choose to write custom code in their SelectionDAGISel::Select implementations to handle some instructions manually. Other instructions are then sent to the auto-generated selector by calling SelectCode\n\n# Input: SelectionDAG.\n\n# Output: MachineDAG (target-specific instructions).\n\n# Tool: LLVM Target Lowering (Target-specific instruction selectors).\n\n# Transformation\n\nAt this stage, the SelectionDAG is converted into a MachineDAG, where the target-independent nodes of the DAG are mapped to actual machine instructions (specific to the target architecture).\nThis involves instruction selection, which translates abstract operations (like add, mul) into the corresponding machine instructions (e.g., x86 or ARM instructions).\n\n# Reason\n\nThe MachineDAG is essential because it ties the program’s logic to a specific machine\'s instruction set.\nThis allows for efficient use of architecture-specific features like registers, instruction pipelines, and specialized operations.\n\n# Key Source File llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp\n\n# Key Functions/Classes:\n\n# SelectionDAGISel: The core class responsible for instruction selection.\n\nFunction: It converts the target-independent DAG into a target-specific MachineDAG.\nThis involves mapping high-level operations like add to actual machine instructions for the target architecture.\n\n# SelectCode: This method performs pattern matching to select the most appropriate machine instruction for each DAG node.\n\nFunction: It uses target-specific information (provided in .td files) to match operations in the DAG to machine instructions.\n\n\n# 7. Scheduling and emitting a MachineInstr\n\n\nTranslate SDNode into Machine Instructions with InstrEmitter::EmitMachineNode, emmit into MachineBasicBlock. Here the instruction are in linear form (MI). No DAG any more.\n-print-machineinstrs\nStill SSA form.\n\n# Input: MachineDAG.\n\n# Output: A sequence of machine instructions (linear instruction stream).\n\n# Tool: LLVM CodeGen Scheduler.\n\n# Transformation\n\nThe MachineDAG undergoes instruction scheduling, where the instructions are ordered in a way that respects data dependencies while maximizing performance.\nThis is necessary to avoid pipeline stalls, reduce instruction latency, and exploit instruction-level parallelism (ILP) in the target CPU.\n\n# Reason\n\nModern CPUs can execute multiple instructions in parallel, so careful scheduling can significantly improve performance by maximizing resource utilization (e.g., CPU pipeline, functional units).\n\n# Key Source Files: llvm/lib/CodeGen/ScheduleDAG/, llvm/lib/CodeGen/MachineScheduler.cpp\n\n# Key Functions/Classes:\n\n# ScheduleDAGInstrs: This class represents the DAG used for scheduling machine instructions.\n\nFunction: It is responsible for reordering the instructions within the MachineDAG to avoid pipeline stalls, minimize instruction latency, and make use of instruction-level parallelism.\n\n# ScheduleMachineBasicBlock: This method schedules the instructions in a machine basic block based on data dependencies and resource constraints.\n\nFunction: It takes the MachineDAG, considers the target architecture’s constraints, and reorders instructions for optimal performance.\n\n\n# 8. Register Allocation\n\n\nFor instructions that can only support fixed registers, it is already allocated. Here the virtual registers are allocated into physical registers.\nThis assignment is done by X86DAGToDAGISel::Select.\nAfter this, another round of optimization is conducted, TargetPassConfig::addMachinePasses.\n\n# Input: Machine Instructions with virtual registers.\n\n# Output: Machine instructions with physical registers.\n\n# Tool: LLVM Register Allocator.\n\n# Transformation\n\nIn this phase, virtual registers (which are unlimited in the IR and DAG forms) are mapped to actual physical registers of the target architecture. The allocator decides which values stay in registers and which are spilled to memory if there aren’t enough registers available.\n\n# Reason\n\nPhysical registers are a limited resource, so register allocation is critical to ensure the efficient execution of the program on the target hardware.\n\n# Key Source Files: llvm/lib/CodeGen/RegAlloc*\n\n# Key Functions/Classes:\n\n# RegisterAllocator: A generic class for performing register allocation.\n\nFunction: It allocates physical registers to virtual registers used in the MachineDAG. If not enough physical registers are available, it spills some of the values to memory.\n\n# LinearScan and Greedy: These are specific register allocation algorithms provided by LLVM.\n\nFunction: Greedy attempts to assign physical registers in a way that minimizes spills, while LinearScan allocates registers in a simpler but potentially less optimal way.\n\n\n# 9. From MachineInstruction to MCinst\n\nJIT: AsmPrinter::EmitInstruction\nObj: ObjectStreamer::EmitInstruction\n\n\n# Input: Machine Instructions with physical registers.\n\n# Output: Assembly code.\n\n# Tool: LLVM AsmPrinter.\n\n# Transformation\n\nFinally, the machine instructions are converted into assembly code, which is a human-readable form of the machine code. This step may also involve additional final optimizations (e.g., peephole optimizations).\n\n# Reason\n\nAssembly is a textual representation of machine code, which can then be assembled into binary instructions by the assembler.\n\n# Key Source File llvm/lib/CodeGen/AsmPrinter/\n\n# Key Functions/Classes\n\n# AsmPrinter: This class converts machine instructions into textual assembly.\n\nFunction: It traverses the list of machine instructions and prints the corresponding assembly syntax for the target architecture.\n\n# EmitInstruction: This method in AsmPrinter prints an individual machine instruction.\n\nFunction: It translates machine-specific instructions into their corresponding assembly code.\n\n\n# 10. Build LLVM\n\ncreate build&cd build\ncmake -S llvm -B . -DCMAKE_BUILD_TYPE=Debug -DLLVM_TARGETS_TO_BUILD="MSP430;RISCV" ../llvm\nmake -j 8\n\n\n# How to build LC3\n\ncmake -S llvm -B . -DCMAKE_BUILD_TYPE=Debug -DLLVM_EXPERIMENTAL_TARGETS_TO_BUILD="LC3" ../llvm\n',normalizedContent:'llvm front end demo notes from life of an instruction in llvm https://blog.llvm.org/2012/11/life-of-instruction-in-llvm.html\n\n----------------------------------------\n\n\n# programing language dependent\n\n\n# 1. clang parser will build an abstract syntax tree(ast)\n\n# input: high-level source code (e.g., c, c++).\n\n# output: an ast, which is a tree structure representing the syntactic structure of the source code\n\n# tool: clang frontend\n\n# transformation\n\nthe compiler’s frontend (clang in llvm) parses the source code and constructs an ast.\nthe ast captures the hierarchical structure of the program based on grammar rules (e.g., function definitions, statements, expressions).\n\n# reason\n\nthe ast is closely tied to the original source code and allows easy analysis and checking of syntactic correctness.\nit serves as a starting point for semantic analysis (e.g., type checking, variable scoping) before code generation.\n\n# key source file\n\nclang/lib/ast/\n\n# key functions/classes\n\n# parseast\n\nthe function responsible for initiating the parsing of source code to generate an abstract syntax tree (ast).\nfunction: this function takes the parsed tokens from the lexer and generates the ast by applying grammar rules.\nit processes the input: source code and organizes it into hierarchical structures like functions, expressions, and statements.\n\n# sema\n\nthe sema class in clang performs semantic analysis on the ast.\nfunction: after parsing the ast, sema checks the semantic validity of the code, ensuring things like proper type usage, function declarations, and scope resolution.\n\n\n# 2. clang emit llvm ir\n\n# input: ast.\n\n# output: llvm intermediate representation (ir).\n\n# tool: clang’s code generation phase.\n\n# transformation\n\nonce the ast is constructed, the compiler translates it into llvm ir.\nllvm ir is a low-level, typed, static single assignment (ssa) form that is independent of any specific machine architecture.\nit consists of a set of instructions operating on virtual registers, with each register assigned only once (ssa form).\n\n# reason\n\nllvm ir is a portable, intermediate code format that enables various machine-independent optimizations. it is also the bridge between different frontends (e.g., clang, rust) and the backend that handles machine-specific code generation.\n\n# key source file\n\nclang/lib/codegen/\n\n# key functions/classes\n\n# codegenfunction: this class is responsible for generating llvm ir from the ast.\n\nfunction: it traverses the ast and generates the corresponding llvm ir.\nfor example, for a function definition in the ast, it creates an llvm::function in the ir.\n\n# emitfunctionbody: this method inside codegenfunction emits the body of a function in ir.\n\nfunction: it walks over statements and expressions within the function and emits llvm ir instructions.\n\n# emitexpr and emitstmt: these methods handle expressions and statements within the ast and translate them into llvm ir.\n\nfunction: emitexpr generates llvm ir for expressions (e.g., arithmetic operations, function calls), while emitstmt generates ir for control-flow structures like if statements and loops.\n\n----------------------------------------\n\n\n# ir target and source programing language independent\n\n\n# 3. llvm target independent ir optimization\n\n# input: unoptimized llvm ir.\n\n# output: optimized llvm ir.\n\n# tool: llvm optimizer (opt).\n\n# transformation\n\nllvm applies a series of machine-independent optimizations to the ir, such as dead code elimination, constant propagation, inlining, and loop optimizations.\nthese transformations operate on the ssa form of the ir to improve performance and reduce unnecessary instructions.\n\n# reason\n\noptimizing at the ir level allows for improvements that are independent of the target architecture, making the resulting code more efficient before it reaches machine-specific stages.\n\n# key source files: llvm/lib/transforms/\n\n# key functions/classes:\n\n# opt the opt tool: runs optimization passes on the llvm ir.\n\nfunction: opt applies a series of transformations to the ir to improve efficiency.\nthese transformations include passes like dead code elimination (deadcodeelimination.cpp), inlining (inlinefunction.cpp), and constant propagation (constantpropagation.cpp).\n\n# key passes:\n\ndeadstoreelimination: eliminates stores that are never used.\nsccp (sparse conditional constant propagation): optimizes based on constant values propagated through the program.\ngvn (global value numbering): removes redundant calculations.\n\n----------------------------------------\n\n\n# target dependent codegen\n\n\n# 4. selectiondag node\n\n\nselectiondagbuild creates sdgnode\nselectiondagisel goes over all ir instructions and calls selectiondagbuilder::visit to dispatch them\nwe can use -debug to llc or -view to get log or dump image of the graph\n\n\n\n# 5. to emit machine instructions, llvm will legalize the operation\n\n\nuse target-specific hooks to convert all operations and types into ones that the target actually supports. this is done by targetlowering.\nselectiondaglegalize::legalizeop\n\n# 4 & 5\n\n# input: optimized llvm ir.\n\n# output: selectiondag.\n\n# tool: llvm codegen phase.\n\n# transformation\n\nin this phase, llvm lowers the ir into a selection directed acyclic graph (selectiondag).\nthe selectiondag represents the program in terms of target-independent operations (nodes) and data dependencies (edges).\neach node in the dag represents an operation, and the edges show data flow between these operations.\n\n# reason\n\nthe selectiondag abstracts machine-specific details while exposing data dependencies, allowing for architecture-specific instruction selection and scheduling in a clean, optimized manner.\n\n# key source file llvm/lib/codegen/selectiondag/\n\n# key functions/classes\n\n# selectiondagbuilder: this class is responsible for constructing the selectiondag from the llvm ir.\n\nfunction: it lowers the llvm ir instructions into nodes in the selectiondag.\neach node in the dag represents an operation (e.g., addition, memory load) that can later be translated into machine instructions.\n\n# loweroperation: a method that lowers a specific llvm ir instruction (like an add operation) to a corresponding selectiondag node.\n\nfunction: this function breaks down llvm ir into target-independent operations within the dag.\n\n# legalizedag: ensures that the dag\'s operations conform to the target machine\'s constraints (e.g., valid data types, supported instructions).\n\nfunction: legalizes operations by splitting larger types into smaller ones or mapping operations to a series of simpler operations that the target can execute.\n\n\n# 6. instruction selection from sdnode to machinesdnode\n\n\nselectiondagisel::select\nselectcode\nthis step will create machinesdnode, a subclass of sdnode which holds the information required to construct an actual machine instruction, but still in dag node form.\n\n> llvm provides a generic table-based instruction selection mechanism that is auto-generated with the help of tablegen. many target backends, however, choose to write custom code in their selectiondagisel::select implementations to handle some instructions manually. other instructions are then sent to the auto-generated selector by calling selectcode\n\n# input: selectiondag.\n\n# output: machinedag (target-specific instructions).\n\n# tool: llvm target lowering (target-specific instruction selectors).\n\n# transformation\n\nat this stage, the selectiondag is converted into a machinedag, where the target-independent nodes of the dag are mapped to actual machine instructions (specific to the target architecture).\nthis involves instruction selection, which translates abstract operations (like add, mul) into the corresponding machine instructions (e.g., x86 or arm instructions).\n\n# reason\n\nthe machinedag is essential because it ties the program’s logic to a specific machine\'s instruction set.\nthis allows for efficient use of architecture-specific features like registers, instruction pipelines, and specialized operations.\n\n# key source file llvm/lib/codegen/selectiondag/selectiondagisel.cpp\n\n# key functions/classes:\n\n# selectiondagisel: the core class responsible for instruction selection.\n\nfunction: it converts the target-independent dag into a target-specific machinedag.\nthis involves mapping high-level operations like add to actual machine instructions for the target architecture.\n\n# selectcode: this method performs pattern matching to select the most appropriate machine instruction for each dag node.\n\nfunction: it uses target-specific information (provided in .td files) to match operations in the dag to machine instructions.\n\n\n# 7. scheduling and emitting a machineinstr\n\n\ntranslate sdnode into machine instructions with instremitter::emitmachinenode, emmit into machinebasicblock. here the instruction are in linear form (mi). no dag any more.\n-print-machineinstrs\nstill ssa form.\n\n# input: machinedag.\n\n# output: a sequence of machine instructions (linear instruction stream).\n\n# tool: llvm codegen scheduler.\n\n# transformation\n\nthe machinedag undergoes instruction scheduling, where the instructions are ordered in a way that respects data dependencies while maximizing performance.\nthis is necessary to avoid pipeline stalls, reduce instruction latency, and exploit instruction-level parallelism (ilp) in the target cpu.\n\n# reason\n\nmodern cpus can execute multiple instructions in parallel, so careful scheduling can significantly improve performance by maximizing resource utilization (e.g., cpu pipeline, functional units).\n\n# key source files: llvm/lib/codegen/scheduledag/, llvm/lib/codegen/machinescheduler.cpp\n\n# key functions/classes:\n\n# scheduledaginstrs: this class represents the dag used for scheduling machine instructions.\n\nfunction: it is responsible for reordering the instructions within the machinedag to avoid pipeline stalls, minimize instruction latency, and make use of instruction-level parallelism.\n\n# schedulemachinebasicblock: this method schedules the instructions in a machine basic block based on data dependencies and resource constraints.\n\nfunction: it takes the machinedag, considers the target architecture’s constraints, and reorders instructions for optimal performance.\n\n\n# 8. register allocation\n\n\nfor instructions that can only support fixed registers, it is already allocated. here the virtual registers are allocated into physical registers.\nthis assignment is done by x86dagtodagisel::select.\nafter this, another round of optimization is conducted, targetpassconfig::addmachinepasses.\n\n# input: machine instructions with virtual registers.\n\n# output: machine instructions with physical registers.\n\n# tool: llvm register allocator.\n\n# transformation\n\nin this phase, virtual registers (which are unlimited in the ir and dag forms) are mapped to actual physical registers of the target architecture. the allocator decides which values stay in registers and which are spilled to memory if there aren’t enough registers available.\n\n# reason\n\nphysical registers are a limited resource, so register allocation is critical to ensure the efficient execution of the program on the target hardware.\n\n# key source files: llvm/lib/codegen/regalloc*\n\n# key functions/classes:\n\n# registerallocator: a generic class for performing register allocation.\n\nfunction: it allocates physical registers to virtual registers used in the machinedag. if not enough physical registers are available, it spills some of the values to memory.\n\n# linearscan and greedy: these are specific register allocation algorithms provided by llvm.\n\nfunction: greedy attempts to assign physical registers in a way that minimizes spills, while linearscan allocates registers in a simpler but potentially less optimal way.\n\n\n# 9. from machineinstruction to mcinst\n\njit: asmprinter::emitinstruction\nobj: objectstreamer::emitinstruction\n\n\n# input: machine instructions with physical registers.\n\n# output: assembly code.\n\n# tool: llvm asmprinter.\n\n# transformation\n\nfinally, the machine instructions are converted into assembly code, which is a human-readable form of the machine code. this step may also involve additional final optimizations (e.g., peephole optimizations).\n\n# reason\n\nassembly is a textual representation of machine code, which can then be assembled into binary instructions by the assembler.\n\n# key source file llvm/lib/codegen/asmprinter/\n\n# key functions/classes\n\n# asmprinter: this class converts machine instructions into textual assembly.\n\nfunction: it traverses the list of machine instructions and prints the corresponding assembly syntax for the target architecture.\n\n# emitinstruction: this method in asmprinter prints an individual machine instruction.\n\nfunction: it translates machine-specific instructions into their corresponding assembly code.\n\n\n# 10. build llvm\n\ncreate build&cd build\ncmake -s llvm -b . -dcmake_build_type=debug -dllvm_targets_to_build="msp430;riscv" ../llvm\nmake -j 8\n\n\n# how to build lc3\n\ncmake -s llvm -b . -dcmake_build_type=debug -dllvm_experimental_targets_to_build="lc3" ../llvm\n',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Learning LLVM Notes",frontmatter:{title:"Learning LLVM Notes",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000004/",tags:[null]},regularPath:"/02.compiler/04.%20LearningLLVMDiary0.html",relativePath:"02.compiler/04. LearningLLVMDiary0.md",key:"v-4d042c96",path:"/pages/000004/",headers:[{level:3,title:"3. Knowledge Fragments",slug:"_3-knowledge-fragments",normalizedTitle:"3. knowledge fragments",charIndex:7893}],headersStr:"3. Knowledge Fragments",content:"# 1. Difference between ISelDAGToDAG and ISelLowering\n\nIn LLVM's backend, ISelLowering and ISelDAGToDAG are two important classes that play different roles in the instruction selection process. Here's the difference between them and the phases in which they are called:\n\nISelLowering\n\n * Role: ISelLowering stands for \"Instruction Selection Lowering\". This class provides target-specific information and handling for aspects of instruction selection during the Selection DAG Construction phase.\n\n * Phase: ISelLowering is called during the \"Selection DAG Construction\" phase of the compiler backend. This phase transforms the generic LLVM IR into a machine-independent representation, the Selection DAG.\n\n * Functions: In ISelLowering, you will find functions that define how LLVM IR operations are translated into target-specific instructions. This includes custom lowering of specific LLVM IR operations, defining patterns for converting LLVM IR to target nodes, and providing information about target-specific features and constraints.\n\nTypical Functions:\n\nLowerCall: Lower calls to target-specific calling conventions.\nLowerReturn: Lower return instructions to target-specific sequences.\nEmitInstrWithCustomInserter: Handle target-specific instructions with custom insertion logic.\ngetTargetNodeName: Provide human-readable names for target nodes.\n\n\n1\n2\n3\n4\n\n\nISelDAGToDAG\n\n * Role: ISelDAGToDAG stands for \"Instruction Selection DAG to DAG\". This class is responsible for translating the Selection DAG (Directed Acyclic Graph) into a sequence of target-specific instructions.\n\n * Phase: ISelDAGToDAG is called during the \"Instruction Selection\" phase of the compiler backend. This phase comes after the DAG Legalization phase and before Register Allocation.\n\n * Functions: In ISelDAGToDAG, you will find functions like Select, which is responsible for selecting target instructions for each node in the DAG.\n\nTypical Functions:\n\nSelect: Select target instructions for nodes in the DAG.\nSelectNode: Implement target-specific node selection.\nPreprocessISelDAG: Prepare for instruction selection.\nPostprocessISelDAG: Clean up after instruction selection.\n\n\n1\n2\n3\n4\n\n\nPhases in Summary\n\n * Selection DAG Construction:\n   \n   * ISelLowering is called during this phase.\n   * This phase transforms the generic LLVM IR into a machine-independent representation (the Selection DAG).\n   * Functions in ISelLowering handle how LLVM IR operations are translated into target-specific instructions.\n\n * DAG Legalization:\n   \n   * Various transformations to ensure DAG conforms to target-specific constraints.\n   * Typically, no specific user-defined classes are involved in this phase.\n\n * Instruction Selection (ISelDAGToDAG):\n   \n   * ISelDAGToDAG is called during this phase.\n   * Translates the Selection DAG into a sequence of target-specific instructions.\n   * Functions in ISelDAGToDAG handle how DAG nodes are selected and transformed into machine instructions.\n\n * Register Allocation:\n   \n   * Assigns virtual registers to physical registers.\n   * Ensures that the generated code does not use more registers than available.\n\n * Frame Lowering:\n   \n   * Manages the function's stack frame.\n   * Sets up the stack frame, manages the frame pointer, and handles stack frame operations.\n\n * Prologue and Epilogue Emission:\n   \n   * Generates the machine code for the function's entry and exit sequences.\n   * Includes setting up the stack frame, saving/restoring callee-saved registers, etc.\n\nIn summary, ISelLowering is called during the Selection DAG Construction phase to handle translation of LLVM IR to target-specific instructions.\nISelDAGToDAG is called during the Instruction Selection phase to translate the Selection DAG to a sequence of target-specific instructions.\nThese phases work together to convert the LLVM IR into machine code for the target architecture.\n\n----------------------------------------\n\n# 2. Analysis Pass and Tranform Pass\n\nAnalysis Passes in LLVM:\n\n * Purpose:\n\n * Analysis passes in LLVM are used to gather information about the program without modifying it.\n * They analyze the program's code structure, control flow, data flow, and other properties.\n * This information is used by subsequent optimization passes to make informed decisions.\n\n * Characteristics:\n   \n   * Do not modify the program.\n   * Collect information about the program.\n   * Used by other passes to guide optimizations.\n   * Typically run early in the optimization pipeline.\n\n * Major Analysis Passes:\n   \n   * DominatorTree Analysis:\n     \n     * Computes the dominator tree for a function.\n     * Helps in various optimizations such as loop optimization, control flow analysis, etc.\n   \n   * LoopInfo Analysis:\n     \n     * Provides information about loops in a function.\n     * Used by loop optimization passes for loop transformations.\n   \n   * ScalarEvolution Analysis:\n     \n     * Analyzes and characterizes scalar expressions in loops.\n     * Helps in loop transformations like loop unrolling, loop vectorization, etc.\n   \n   * MemorySSA Analysis:\n     \n     * Provides a memory SSA representation of the program.\n     * Used in optimizations related to memory access analysis, alias analysis, etc.\n   \n   * AliasAnalysis Analysis:\n     \n     * Determines the aliasing relationship between memory accesses.\n     * Helps in optimizations that depend on memory aliasing information.\n\nTransform Passes in LLVM:\n\n * Purpose: Transform passes in LLVM modify the program's IR to improve its performance or reduce its size. They apply optimizations and transformations to the code.\n\n * Characteristics:\n   \n   * Modify the program's IR.\n   * Apply optimizations and transformations.\n   * Can introduce new code or modify existing code.\n   * Typically run after analysis passes.\n\n * Major Transform Passes:\n   \n   * Instruction Combining:\n     \n     * Combines multiple instructions into simpler forms.\n     * Reduces the number of instructions and improves code readability.\n   \n   * Dead Code Elimination:\n     \n     * Removes code that is guaranteed to have no effect on program output.\n     * Improves code size and execution speed.\n   \n   * Loop Unrolling:\n     \n     * Duplicates the loop body multiple times to reduce loop overhead.\n     * Improves instruction-level parallelism.\n   \n   * Function Inlining:\n     \n     * Replaces a function call with the body of the called function.\n     * Reduces function call overhead and enables further optimizations.\n   \n   * Constant Propagation:\n     \n     * Propagates constant values through the program.\n     * Enables further optimizations by replacing variables with constants.\n   \n   * Vectorization (Loop Vectorization):\n     \n     * Converts scalar operations in loops into SIMD (Single Instruction, Multiple Data) operations.\n     * Improves performance by exploiting parallelism in hardware.\n   \n   * SROA (Scalar Replacement of Aggregates):\n     \n     * Breaks down aggregates (like structs) into individual scalar variables.\n     * Improves optimization opportunities by working on individual scalar variables.\n\nSummary:\n\n * Analysis Passes:\n   * Gather information about the program.\n   * Do not modify the program.\n   * Used by other passes for optimizations.\n   * Examples: DominatorTree, LoopInfo, ScalarEvolution, MemorySSA, AliasAnalysis.\n * Transform Passes:\n   * Modify the program's IR.\n   * Apply optimizations and transformations.\n   * Examples: Instruction Combining, Dead Code Elimination, Loop Unrolling, Function Inlining, Constant Propagation, Vectorization, SROA (Scalar Replacement of Aggregates). These are just a few examples of major analysis and transform passes in LLVM. The LLVM infrastructure provides a wide range of passes for various optimizations and analyses, allowing users to construct custom optimization pipelines tailored to their specific needs.\n\n----------------------------------------\n\n\n# 3. Knowledge Fragments\n\n 1. SROA is claimed to replace mem2reg.\n    discourse.llvm\n\n 2. Alias Analysis\n    Alias Analysis (aka Pointer Analysis) is a class of techniques which attempt to determine whether or not two pointers ever can point to the same object in memory.\n    Traditionally, alias analyses respond to a query with a Must, May, or No alias response, indicating that two pointers always point to the same object, might point to the same object, or are known to never point to the same object.\n    \n\n 3. RISCV Instruction Set Might deserves further explore.\n    Berkely RISCV Instruction Set Manual\n    \n\n 4. How LLVM Optimizes a Function\n    How LLVM Optimizes a Function\n    This blog trace through different passes in llvm IR.\n    \n\n 5. CambridgeSlides\n    Cambridge Modern Compiler Design\n    \n    * Introduction\n    * Modern intermediate representations\n    * LLVM IR and transform pipeline\n    * Modern processor architectures\n    * Dynamic dispatch and duck typing\n    * Autovectorisation\n    * Garbage collection\n    * JIT Compilation",normalizedContent:"# 1. difference between iseldagtodag and isellowering\n\nin llvm's backend, isellowering and iseldagtodag are two important classes that play different roles in the instruction selection process. here's the difference between them and the phases in which they are called:\n\nisellowering\n\n * role: isellowering stands for \"instruction selection lowering\". this class provides target-specific information and handling for aspects of instruction selection during the selection dag construction phase.\n\n * phase: isellowering is called during the \"selection dag construction\" phase of the compiler backend. this phase transforms the generic llvm ir into a machine-independent representation, the selection dag.\n\n * functions: in isellowering, you will find functions that define how llvm ir operations are translated into target-specific instructions. this includes custom lowering of specific llvm ir operations, defining patterns for converting llvm ir to target nodes, and providing information about target-specific features and constraints.\n\ntypical functions:\n\nlowercall: lower calls to target-specific calling conventions.\nlowerreturn: lower return instructions to target-specific sequences.\nemitinstrwithcustominserter: handle target-specific instructions with custom insertion logic.\ngettargetnodename: provide human-readable names for target nodes.\n\n\n1\n2\n3\n4\n\n\niseldagtodag\n\n * role: iseldagtodag stands for \"instruction selection dag to dag\". this class is responsible for translating the selection dag (directed acyclic graph) into a sequence of target-specific instructions.\n\n * phase: iseldagtodag is called during the \"instruction selection\" phase of the compiler backend. this phase comes after the dag legalization phase and before register allocation.\n\n * functions: in iseldagtodag, you will find functions like select, which is responsible for selecting target instructions for each node in the dag.\n\ntypical functions:\n\nselect: select target instructions for nodes in the dag.\nselectnode: implement target-specific node selection.\npreprocessiseldag: prepare for instruction selection.\npostprocessiseldag: clean up after instruction selection.\n\n\n1\n2\n3\n4\n\n\nphases in summary\n\n * selection dag construction:\n   \n   * isellowering is called during this phase.\n   * this phase transforms the generic llvm ir into a machine-independent representation (the selection dag).\n   * functions in isellowering handle how llvm ir operations are translated into target-specific instructions.\n\n * dag legalization:\n   \n   * various transformations to ensure dag conforms to target-specific constraints.\n   * typically, no specific user-defined classes are involved in this phase.\n\n * instruction selection (iseldagtodag):\n   \n   * iseldagtodag is called during this phase.\n   * translates the selection dag into a sequence of target-specific instructions.\n   * functions in iseldagtodag handle how dag nodes are selected and transformed into machine instructions.\n\n * register allocation:\n   \n   * assigns virtual registers to physical registers.\n   * ensures that the generated code does not use more registers than available.\n\n * frame lowering:\n   \n   * manages the function's stack frame.\n   * sets up the stack frame, manages the frame pointer, and handles stack frame operations.\n\n * prologue and epilogue emission:\n   \n   * generates the machine code for the function's entry and exit sequences.\n   * includes setting up the stack frame, saving/restoring callee-saved registers, etc.\n\nin summary, isellowering is called during the selection dag construction phase to handle translation of llvm ir to target-specific instructions.\niseldagtodag is called during the instruction selection phase to translate the selection dag to a sequence of target-specific instructions.\nthese phases work together to convert the llvm ir into machine code for the target architecture.\n\n----------------------------------------\n\n# 2. analysis pass and tranform pass\n\nanalysis passes in llvm:\n\n * purpose:\n\n * analysis passes in llvm are used to gather information about the program without modifying it.\n * they analyze the program's code structure, control flow, data flow, and other properties.\n * this information is used by subsequent optimization passes to make informed decisions.\n\n * characteristics:\n   \n   * do not modify the program.\n   * collect information about the program.\n   * used by other passes to guide optimizations.\n   * typically run early in the optimization pipeline.\n\n * major analysis passes:\n   \n   * dominatortree analysis:\n     \n     * computes the dominator tree for a function.\n     * helps in various optimizations such as loop optimization, control flow analysis, etc.\n   \n   * loopinfo analysis:\n     \n     * provides information about loops in a function.\n     * used by loop optimization passes for loop transformations.\n   \n   * scalarevolution analysis:\n     \n     * analyzes and characterizes scalar expressions in loops.\n     * helps in loop transformations like loop unrolling, loop vectorization, etc.\n   \n   * memoryssa analysis:\n     \n     * provides a memory ssa representation of the program.\n     * used in optimizations related to memory access analysis, alias analysis, etc.\n   \n   * aliasanalysis analysis:\n     \n     * determines the aliasing relationship between memory accesses.\n     * helps in optimizations that depend on memory aliasing information.\n\ntransform passes in llvm:\n\n * purpose: transform passes in llvm modify the program's ir to improve its performance or reduce its size. they apply optimizations and transformations to the code.\n\n * characteristics:\n   \n   * modify the program's ir.\n   * apply optimizations and transformations.\n   * can introduce new code or modify existing code.\n   * typically run after analysis passes.\n\n * major transform passes:\n   \n   * instruction combining:\n     \n     * combines multiple instructions into simpler forms.\n     * reduces the number of instructions and improves code readability.\n   \n   * dead code elimination:\n     \n     * removes code that is guaranteed to have no effect on program output.\n     * improves code size and execution speed.\n   \n   * loop unrolling:\n     \n     * duplicates the loop body multiple times to reduce loop overhead.\n     * improves instruction-level parallelism.\n   \n   * function inlining:\n     \n     * replaces a function call with the body of the called function.\n     * reduces function call overhead and enables further optimizations.\n   \n   * constant propagation:\n     \n     * propagates constant values through the program.\n     * enables further optimizations by replacing variables with constants.\n   \n   * vectorization (loop vectorization):\n     \n     * converts scalar operations in loops into simd (single instruction, multiple data) operations.\n     * improves performance by exploiting parallelism in hardware.\n   \n   * sroa (scalar replacement of aggregates):\n     \n     * breaks down aggregates (like structs) into individual scalar variables.\n     * improves optimization opportunities by working on individual scalar variables.\n\nsummary:\n\n * analysis passes:\n   * gather information about the program.\n   * do not modify the program.\n   * used by other passes for optimizations.\n   * examples: dominatortree, loopinfo, scalarevolution, memoryssa, aliasanalysis.\n * transform passes:\n   * modify the program's ir.\n   * apply optimizations and transformations.\n   * examples: instruction combining, dead code elimination, loop unrolling, function inlining, constant propagation, vectorization, sroa (scalar replacement of aggregates). these are just a few examples of major analysis and transform passes in llvm. the llvm infrastructure provides a wide range of passes for various optimizations and analyses, allowing users to construct custom optimization pipelines tailored to their specific needs.\n\n----------------------------------------\n\n\n# 3. knowledge fragments\n\n 1. sroa is claimed to replace mem2reg.\n    discourse.llvm\n\n 2. alias analysis\n    alias analysis (aka pointer analysis) is a class of techniques which attempt to determine whether or not two pointers ever can point to the same object in memory.\n    traditionally, alias analyses respond to a query with a must, may, or no alias response, indicating that two pointers always point to the same object, might point to the same object, or are known to never point to the same object.\n    \n\n 3. riscv instruction set might deserves further explore.\n    berkely riscv instruction set manual\n    \n\n 4. how llvm optimizes a function\n    how llvm optimizes a function\n    this blog trace through different passes in llvm ir.\n    \n\n 5. cambridgeslides\n    cambridge modern compiler design\n    \n    * introduction\n    * modern intermediate representations\n    * llvm ir and transform pipeline\n    * modern processor architectures\n    * dynamic dispatch and duck typing\n    * autovectorisation\n    * garbage collection\n    * jit compilation",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Add New DIY Instruction ACE to LLVM",frontmatter:{title:"Add New DIY Instruction ACE to LLVM",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000005/",tags:[null]},regularPath:"/02.compiler/05.%20addInstACE.html",relativePath:"02.compiler/05. addInstACE.md",key:"v-49e4dae0",path:"/pages/000005/",headers:[{level:3,title:"1. Add Register Class in RISCV",slug:"_1-add-register-class-in-riscv",normalizedTitle:"1. add register class in riscv",charIndex:2}],headersStr:"1. Add Register Class in RISCV",content:'# 1. Add Register Class in RISCV\n\n// RegACE - 4-bit register for RISC-V ACE inst\nclass RISCVRegACE<bits<16> Enc, string n, list<string> alt = []> : Register<n> {\n  let HWEncoding = Enc;\n  let AltNames = alt;\n}\n\n// Define the ACE registers\nlet RegAltNameIndices = [ABIRegAltName] in {\n  //foreach Index = !range(0, 16, 1) in {\n  //  def ACE#Index : RISCVRegACE<Index, "ace_reg_"#Index, ["ace_reg_"#Index]>, DwarfRegNum<[!add(Index, 128)]>;\n  //}\n  def ACE0  : RISCVRegACE<0, "ace_reg_0", ["ace_reg_0"]>, DwarfRegNum<[128]>;\n  def ACE1  : RISCVRegACE<1, "ace_reg_1", ["ace_reg_1"]>, DwarfRegNum<[129]>;\n  def ACE2  : RISCVRegACE<2, "ace_reg_2", ["ace_reg_2"]>, DwarfRegNum<[130]>;\n  def ACE3  : RISCVRegACE<3, "ace_reg_3", ["ace_reg_3"]>, DwarfRegNum<[131]>;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nanonymous_8306: \t(trunc:{ *:[i16] m1:[i16 i32] } GPR:{ *:[i32] m1:[i32 i64] }:$src)\nIncluded from /home/qishao/Project/llvm-project/llvm/lib/Target/RISCV/RISCV.td:30:\n/home/qishao/Project/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.td:1915:1: error: In anonymous_8306: Could not infer all types in pattern!\ndef : Pat<(trunc GPR:$src), (COPY GPR:$src)>;\n^\nanonymous_8306: \t(trunc:{ *:[i16] m1:[i16 i32] } GPR:{ *:[i32] m1:[i32 i64] }:$src)\nanonymous_8306: \t(COPY:{ *:[i16] m1:[i16 i32] } GPR:{ *:[i32] m1:[i32 i64] }:$src)\nIncluded from /home/qishao/Project/llvm-project/llvm/lib/Target/RISCV/RISCV.td:30:\n/home/qishao/Project/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.td:1915:1: error: In anonymous_8306: Could not infer all types in pattern result!\ndef : Pat<(trunc GPR:$src), (COPY GPR:$src)>;\n^\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nThe failed issues after I def ACE Register Class\n\ndef ACE : RegisterClass<"RISCV", [i16], 16, (add\n    (sequence "ACE%u", 0, 3)\n)>;\n\n\n\n1\n2\n3\n4\n',normalizedContent:'# 1. add register class in riscv\n\n// regace - 4-bit register for risc-v ace inst\nclass riscvregace<bits<16> enc, string n, list<string> alt = []> : register<n> {\n  let hwencoding = enc;\n  let altnames = alt;\n}\n\n// define the ace registers\nlet regaltnameindices = [abiregaltname] in {\n  //foreach index = !range(0, 16, 1) in {\n  //  def ace#index : riscvregace<index, "ace_reg_"#index, ["ace_reg_"#index]>, dwarfregnum<[!add(index, 128)]>;\n  //}\n  def ace0  : riscvregace<0, "ace_reg_0", ["ace_reg_0"]>, dwarfregnum<[128]>;\n  def ace1  : riscvregace<1, "ace_reg_1", ["ace_reg_1"]>, dwarfregnum<[129]>;\n  def ace2  : riscvregace<2, "ace_reg_2", ["ace_reg_2"]>, dwarfregnum<[130]>;\n  def ace3  : riscvregace<3, "ace_reg_3", ["ace_reg_3"]>, dwarfregnum<[131]>;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nanonymous_8306: \t(trunc:{ *:[i16] m1:[i16 i32] } gpr:{ *:[i32] m1:[i32 i64] }:$src)\nincluded from /home/qishao/project/llvm-project/llvm/lib/target/riscv/riscv.td:30:\n/home/qishao/project/llvm-project/llvm/lib/target/riscv/riscvinstrinfo.td:1915:1: error: in anonymous_8306: could not infer all types in pattern!\ndef : pat<(trunc gpr:$src), (copy gpr:$src)>;\n^\nanonymous_8306: \t(trunc:{ *:[i16] m1:[i16 i32] } gpr:{ *:[i32] m1:[i32 i64] }:$src)\nanonymous_8306: \t(copy:{ *:[i16] m1:[i16 i32] } gpr:{ *:[i32] m1:[i32 i64] }:$src)\nincluded from /home/qishao/project/llvm-project/llvm/lib/target/riscv/riscv.td:30:\n/home/qishao/project/llvm-project/llvm/lib/target/riscv/riscvinstrinfo.td:1915:1: error: in anonymous_8306: could not infer all types in pattern result!\ndef : pat<(trunc gpr:$src), (copy gpr:$src)>;\n^\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nthe failed issues after i def ace register class\n\ndef ace : registerclass<"riscv", [i16], 16, (add\n    (sequence "ace%u", 0, 3)\n)>;\n\n\n\n1\n2\n3\n4\n',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"How does LLVM perform instruction combine",frontmatter:{title:"How does LLVM perform instruction combine",output:{html_document:{code_folding:"hide"}},date:"2024-03-17T00:00:00.000Z",permalink:"/pages/000006/",tags:[null]},regularPath:"/02.compiler/06.Value&Use.html",relativePath:"02.compiler/06.Value&Use.md",key:"v-c73e5956",path:"/pages/000006/",headers:[{level:3,title:"1. CodeFile",slug:"_1-codefile",normalizedTitle:"1. codefile",charIndex:2},{level:3,title:"2. Values & User",slug:"_2-values-user",normalizedTitle:"2. values &amp; user",charIndex:null},{level:3,title:"3. Q & A in Stackoverflow",slug:"_3-q-a-in-stackoverflow",normalizedTitle:"3. q &amp; a in stackoverflow",charIndex:null},{level:3,title:"Reference",slug:"reference",normalizedTitle:"reference",charIndex:7957}],headersStr:"1. CodeFile 2. Values & User 3. Q & A in Stackoverflow Reference",content:"# 1. CodeFile\n\nlib/Target/Mips/MipsISelLowering.cpp\n\nAll DAG Combine is called here.\n\nSDValue  MipsTargetLowering::PerformDAGCombine(SDNode *N, DAGCombinerInfo &DCI)\n  const {\n  SelectionDAG &DAG = DCI.DAG;\n  unsigned Opc = N->getOpcode();\n\n  switch (Opc) {\n  default: break;\n  case ISD::SDIVREM:\n  case ISD::UDIVREM:\n    return performDivRemCombine(N, DAG, DCI, Subtarget);\n  case ISD::SELECT:\n    return performSELECTCombine(N, DAG, DCI, Subtarget);\n  case MipsISD::CMovFP_F:\n  case MipsISD::CMovFP_T:\n    return performCMovFPCombine(N, DAG, DCI, Subtarget);\n  case ISD::AND:\n    return performANDCombine(N, DAG, DCI, Subtarget);\n  case ISD::OR:\n    return performORCombine(N, DAG, DCI, Subtarget);\n  case ISD::ADD:\n    return performADDCombine(N, DAG, DCI, Subtarget);\n  case ISD::SHL:\n    return performSHLCombine(N, DAG, DCI, Subtarget);\n  case ISD::SUB:\n    return performSUBCombine(N, DAG, DCI, Subtarget);\n  }\n\n  return SDValue();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\nDefine of performSUBCombine\n\n * we can look into the process.\n * ISD::SUB will call performSUBCombine\n * It will combine (sub v0 (mul v1, v2)) into (msub v1, v2, v0)\n * The intresting thing is that, current SDNode Opcode is sub and if precedent node is mul, it will combine it into msub.\n\nHow to identify the precedent node?\n\n 1. SDValue Mult = ROOTNode->getOperand(1); // multi SDValue\n 2. SDValue AddOperand = ROOTNode->getOperand(0); // add SDValue\n 3. how about previous instruction?\n 4. ROOTNode->getOperand(0) will point to previous instruction\n\nstatic SDValue performSUBCombine(SDNode *N, SelectionDAG &DAG,\n                                 TargetLowering::DAGCombinerInfo &DCI,\n                                 const MipsSubtarget &Subtarget) {\n  // (sub v0 (mul v1, v2)) => (msub v1, v2, v0)\n  if (DCI.isBeforeLegalizeOps()) {\n    if (Subtarget.hasMips32() && !Subtarget.hasMips32r6() &&\n        !Subtarget.inMips16Mode() && N->getValueType(0) == MVT::i64)\n      return performMADD_MSUBCombine(N, DAG, Subtarget);\n\n    return SDValue();\n  }\n\n  return SDValue();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nstatic SDValue performMADD_MSUBCombine(SDNode *ROOTNode, SelectionDAG &CurDAG,\n                                       const MipsSubtarget &Subtarget) {\n  SDValue Mult = ROOTNode->getOperand(0).getOpcode() == ISD::MUL\n                     ? ROOTNode->getOperand(0)\n                     : ROOTNode->getOperand(1);\n\n  SDValue AddOperand = ROOTNode->getOperand(0).getOpcode() == ISD::MUL\n                     ? ROOTNode->getOperand(1)\n                     : ROOTNode->getOperand(0);\n\n  // Transform this to a MADD only if the user of this node is the add.\n  // If there are other users of the mul, this function returns here.\n  if (!Mult.hasOneUse())\n    return SDValue();\n\n  // maddu and madd are unusual instructions in that on MIPS64 bits 63..31\n  // must be in canonical form, i.e. sign extended. For MIPS32, the operands\n  // of the multiply must have 32 or more sign bits, otherwise we cannot\n  // perform this optimization. We have to check this here as we're performing\n  // this optimization pre-legalization.\n  SDValue MultLHS = Mult->getOperand(0);\n  SDValue MultRHS = Mult->getOperand(1);\n\n  bool IsSigned = MultLHS->getOpcode() == ISD::SIGN_EXTEND &&\n                  MultRHS->getOpcode() == ISD::SIGN_EXTEND;\n  bool IsUnsigned = MultLHS->getOpcode() == ISD::ZERO_EXTEND &&\n                    MultRHS->getOpcode() == ISD::ZERO_EXTEND;\n\n  if (!IsSigned && !IsUnsigned)\n    return SDValue();\n\n  // Initialize accumulator.\n  SDLoc DL(ROOTNode);\n  SDValue TopHalf;\n  SDValue BottomHalf;\n  BottomHalf = CurDAG.getNode(ISD::EXTRACT_ELEMENT, DL, MVT::i32, AddOperand,\n                              CurDAG.getIntPtrConstant(0, DL));\n\n  TopHalf = CurDAG.getNode(ISD::EXTRACT_ELEMENT, DL, MVT::i32, AddOperand,\n                           CurDAG.getIntPtrConstant(1, DL));\n  SDValue ACCIn = CurDAG.getNode(MipsISD::MTLOHI, DL, MVT::Untyped,\n                                  BottomHalf,\n                                  TopHalf);\n\n  // Create MipsMAdd(u) / MipsMSub(u) node.\n  bool IsAdd = ROOTNode->getOpcode() == ISD::ADD;\n  unsigned Opcode = IsAdd ? (IsUnsigned ? MipsISD::MAddu : MipsISD::MAdd)\n                          : (IsUnsigned ? MipsISD::MSubu : MipsISD::MSub);\n  SDValue MAddOps[3] = {\n      CurDAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Mult->getOperand(0)),\n      CurDAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Mult->getOperand(1)), ACCIn};\n  EVT VTs[2] = {MVT::i32, MVT::i32};\n  SDValue MAdd = CurDAG.getNode(Opcode, DL, VTs, MAddOps);\n\n  SDValue ResLo = CurDAG.getNode(MipsISD::MFLO, DL, MVT::i32, MAdd);\n  SDValue ResHi = CurDAG.getNode(MipsISD::MFHI, DL, MVT::i32, MAdd);\n  SDValue Combined =\n      CurDAG.getNode(ISD::BUILD_PAIR, DL, MVT::i64, ResLo, ResHi);\n  return Combined;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n\n# 2. Values & User\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 3. Q & A in Stackoverflow\n\nQ&A\nSince Instruction is derived from Value it inherits both functions users and uses. The difference is that a user of Value has the Value as one of its operands.\n\nWhen you are calling uses you get a list of all Use instances holding a reference from the Value to each of the users of the particular Value. Calling users gives you a list of User directly. The following code shows how to use users and uses.\n\nfor(auto U : V->users()){  // U is of type User*\n     if (auto I = dyn_cast<Instruction>(U)){\n        // an instruction uses V\n     }\n}\n\n\n1\n2\n3\n4\n5\n\n\nYou can see users as a shortcut because you can do the same with uses:\n\nfor(auto U : V->uses()){  // U is of type Use*\n     if (auto I = dyn_cast<Instruction>(U.getUser())){\n        // an instruction uses V\n     }\n}\n\n\n1\n2\n3\n4\n5\n\n\nCommonly it is enough to use users to get all dependencies of a Value.\n\nAll Values used by a Value are the operands. This direction of dependency is not part of a Value's use list.\n\nWe have still not presented the most powerful aspect of the LLVM IR (enabled by the SSA form): the Value and User interfaces; these allow you to easily navigate the use-def and def-use chains. In the LLVM in-memory IR, a class that inherits from Value means that it defines a result that can be used by others, whereas a subclass of User means that this entity uses one or more Value interfaces. Function and Instruction are subclasses of both Value and User, while BasicBlock is a subclass of just Value. To understand this, let's analyze these two classes in depth:\n\n• The Value class defines the use_begin() and use_end() methods to allow you to iterate through Users, offering an easy way to access its def-use chain. For every Value class, you can also access its name through the getName() method. This models the fact that any LLVM value can have a distinct identifier associated with it. For example, %add1 can identify the result of an add instruction, BB1 can identify a basic block, and myfunc can identify a function. Value also has a powerful method called replaceAllUsesWith(Value *), which navigates through all of the users of this value and replaces it with some other value. This is a good example of how the SSA form allows you to easily substitute instructions and write fast optimizations. You can view the full interface at LLVM Value Class.\n\n• The User class has the op_begin() and op_end() methods that allows you to quickly access all of the Value interfaces that it uses. Note that this represents the use-def chain. You can also use a helper method called replaceUsesOfWith(Value *From, Value *To) to replace any of its used values. You can view the full interface at LLVM User Class.\n\nFor short, use_begin() iterator points to users. and op_begin() points to operand values. but the value is the basic class of instruction. By Refer to a value, you can get the producer's instructin.\n\n\n# Reference\n\nHow to Write an LLVM Backend #4: Instruction Selection\n\nMore on the LLVM Compiler\n\nIntroduction to LLVM (II)\n\n深入浅出 LLVM之 Value 、User 、Use 源码解析",normalizedContent:"# 1. codefile\n\nlib/target/mips/mipsisellowering.cpp\n\nall dag combine is called here.\n\nsdvalue  mipstargetlowering::performdagcombine(sdnode *n, dagcombinerinfo &dci)\n  const {\n  selectiondag &dag = dci.dag;\n  unsigned opc = n->getopcode();\n\n  switch (opc) {\n  default: break;\n  case isd::sdivrem:\n  case isd::udivrem:\n    return performdivremcombine(n, dag, dci, subtarget);\n  case isd::select:\n    return performselectcombine(n, dag, dci, subtarget);\n  case mipsisd::cmovfp_f:\n  case mipsisd::cmovfp_t:\n    return performcmovfpcombine(n, dag, dci, subtarget);\n  case isd::and:\n    return performandcombine(n, dag, dci, subtarget);\n  case isd::or:\n    return performorcombine(n, dag, dci, subtarget);\n  case isd::add:\n    return performaddcombine(n, dag, dci, subtarget);\n  case isd::shl:\n    return performshlcombine(n, dag, dci, subtarget);\n  case isd::sub:\n    return performsubcombine(n, dag, dci, subtarget);\n  }\n\n  return sdvalue();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\ndefine of performsubcombine\n\n * we can look into the process.\n * isd::sub will call performsubcombine\n * it will combine (sub v0 (mul v1, v2)) into (msub v1, v2, v0)\n * the intresting thing is that, current sdnode opcode is sub and if precedent node is mul, it will combine it into msub.\n\nhow to identify the precedent node?\n\n 1. sdvalue mult = rootnode->getoperand(1); // multi sdvalue\n 2. sdvalue addoperand = rootnode->getoperand(0); // add sdvalue\n 3. how about previous instruction?\n 4. rootnode->getoperand(0) will point to previous instruction\n\nstatic sdvalue performsubcombine(sdnode *n, selectiondag &dag,\n                                 targetlowering::dagcombinerinfo &dci,\n                                 const mipssubtarget &subtarget) {\n  // (sub v0 (mul v1, v2)) => (msub v1, v2, v0)\n  if (dci.isbeforelegalizeops()) {\n    if (subtarget.hasmips32() && !subtarget.hasmips32r6() &&\n        !subtarget.inmips16mode() && n->getvaluetype(0) == mvt::i64)\n      return performmadd_msubcombine(n, dag, subtarget);\n\n    return sdvalue();\n  }\n\n  return sdvalue();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nstatic sdvalue performmadd_msubcombine(sdnode *rootnode, selectiondag &curdag,\n                                       const mipssubtarget &subtarget) {\n  sdvalue mult = rootnode->getoperand(0).getopcode() == isd::mul\n                     ? rootnode->getoperand(0)\n                     : rootnode->getoperand(1);\n\n  sdvalue addoperand = rootnode->getoperand(0).getopcode() == isd::mul\n                     ? rootnode->getoperand(1)\n                     : rootnode->getoperand(0);\n\n  // transform this to a madd only if the user of this node is the add.\n  // if there are other users of the mul, this function returns here.\n  if (!mult.hasoneuse())\n    return sdvalue();\n\n  // maddu and madd are unusual instructions in that on mips64 bits 63..31\n  // must be in canonical form, i.e. sign extended. for mips32, the operands\n  // of the multiply must have 32 or more sign bits, otherwise we cannot\n  // perform this optimization. we have to check this here as we're performing\n  // this optimization pre-legalization.\n  sdvalue multlhs = mult->getoperand(0);\n  sdvalue multrhs = mult->getoperand(1);\n\n  bool issigned = multlhs->getopcode() == isd::sign_extend &&\n                  multrhs->getopcode() == isd::sign_extend;\n  bool isunsigned = multlhs->getopcode() == isd::zero_extend &&\n                    multrhs->getopcode() == isd::zero_extend;\n\n  if (!issigned && !isunsigned)\n    return sdvalue();\n\n  // initialize accumulator.\n  sdloc dl(rootnode);\n  sdvalue tophalf;\n  sdvalue bottomhalf;\n  bottomhalf = curdag.getnode(isd::extract_element, dl, mvt::i32, addoperand,\n                              curdag.getintptrconstant(0, dl));\n\n  tophalf = curdag.getnode(isd::extract_element, dl, mvt::i32, addoperand,\n                           curdag.getintptrconstant(1, dl));\n  sdvalue accin = curdag.getnode(mipsisd::mtlohi, dl, mvt::untyped,\n                                  bottomhalf,\n                                  tophalf);\n\n  // create mipsmadd(u) / mipsmsub(u) node.\n  bool isadd = rootnode->getopcode() == isd::add;\n  unsigned opcode = isadd ? (isunsigned ? mipsisd::maddu : mipsisd::madd)\n                          : (isunsigned ? mipsisd::msubu : mipsisd::msub);\n  sdvalue maddops[3] = {\n      curdag.getnode(isd::truncate, dl, mvt::i32, mult->getoperand(0)),\n      curdag.getnode(isd::truncate, dl, mvt::i32, mult->getoperand(1)), accin};\n  evt vts[2] = {mvt::i32, mvt::i32};\n  sdvalue madd = curdag.getnode(opcode, dl, vts, maddops);\n\n  sdvalue reslo = curdag.getnode(mipsisd::mflo, dl, mvt::i32, madd);\n  sdvalue reshi = curdag.getnode(mipsisd::mfhi, dl, mvt::i32, madd);\n  sdvalue combined =\n      curdag.getnode(isd::build_pair, dl, mvt::i64, reslo, reshi);\n  return combined;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n\n# 2. values & user\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 3. q & a in stackoverflow\n\nq&a\nsince instruction is derived from value it inherits both functions users and uses. the difference is that a user of value has the value as one of its operands.\n\nwhen you are calling uses you get a list of all use instances holding a reference from the value to each of the users of the particular value. calling users gives you a list of user directly. the following code shows how to use users and uses.\n\nfor(auto u : v->users()){  // u is of type user*\n     if (auto i = dyn_cast<instruction>(u)){\n        // an instruction uses v\n     }\n}\n\n\n1\n2\n3\n4\n5\n\n\nyou can see users as a shortcut because you can do the same with uses:\n\nfor(auto u : v->uses()){  // u is of type use*\n     if (auto i = dyn_cast<instruction>(u.getuser())){\n        // an instruction uses v\n     }\n}\n\n\n1\n2\n3\n4\n5\n\n\ncommonly it is enough to use users to get all dependencies of a value.\n\nall values used by a value are the operands. this direction of dependency is not part of a value's use list.\n\nwe have still not presented the most powerful aspect of the llvm ir (enabled by the ssa form): the value and user interfaces; these allow you to easily navigate the use-def and def-use chains. in the llvm in-memory ir, a class that inherits from value means that it defines a result that can be used by others, whereas a subclass of user means that this entity uses one or more value interfaces. function and instruction are subclasses of both value and user, while basicblock is a subclass of just value. to understand this, let's analyze these two classes in depth:\n\n• the value class defines the use_begin() and use_end() methods to allow you to iterate through users, offering an easy way to access its def-use chain. for every value class, you can also access its name through the getname() method. this models the fact that any llvm value can have a distinct identifier associated with it. for example, %add1 can identify the result of an add instruction, bb1 can identify a basic block, and myfunc can identify a function. value also has a powerful method called replacealluseswith(value *), which navigates through all of the users of this value and replaces it with some other value. this is a good example of how the ssa form allows you to easily substitute instructions and write fast optimizations. you can view the full interface at llvm value class.\n\n• the user class has the op_begin() and op_end() methods that allows you to quickly access all of the value interfaces that it uses. note that this represents the use-def chain. you can also use a helper method called replaceusesofwith(value *from, value *to) to replace any of its used values. you can view the full interface at llvm user class.\n\nfor short, use_begin() iterator points to users. and op_begin() points to operand values. but the value is the basic class of instruction. by refer to a value, you can get the producer's instructin.\n\n\n# reference\n\nhow to write an llvm backend #4: instruction selection\n\nmore on the llvm compiler\n\nintroduction to llvm (ii)\n\n深入浅出 llvm之 value 、user 、use 源码解析",charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"Understand llvm with its source code",frontmatter:{title:"Understand llvm with its source code",output:{html_document:{code_folding:"hide"}},date:"2024-07-05T00:00:00.000Z",permalink:"/pages/000007/",tags:[null]},regularPath:"/02.compiler/07.%20UnderstaningLLVMwithSourceCode.html",relativePath:"02.compiler/07. UnderstaningLLVMwithSourceCode.md",key:"v-89581d5c",path:"/pages/000007/",headers:[{level:3,title:"1. Create SelectionDAG",slug:"_1-create-selectiondag",normalizedTitle:"1. create selectiondag",charIndex:2},{level:3,title:"2. Legalization",slug:"_2-legalization",normalizedTitle:"2. legalization",charIndex:1458},{level:3,title:"3. Instruction Selection",slug:"_3-instruction-selection",normalizedTitle:"3. instruction selection",charIndex:5867},{level:3,title:"4. Instruction Scheduler",slug:"_4-instruction-scheduler",normalizedTitle:"4. instruction scheduler",charIndex:10907}],headersStr:"1. Create SelectionDAG 2. Legalization 3. Instruction Selection 4. Instruction Scheduler",content:'# 1. Create SelectionDAG\n\nSelectionDAG Builder calls visit() function to build SDNode\n\n// CodeGen/SelectionDAG/SelectionDAGBuilder.cpp\nvoid SelectionDAGBuilder::visit(unsigned Opcode, const User &I) {\n  // Note: this doesn\'t use InstVisitor, because it has to work with\n  // ConstantExpr\'s in addition to instructions.\n  switch (Opcode) {\n  default: llvm_unreachable("Unknown instruction type encountered!");\n    // Build the switch statement using the Instruction.def file.\n#define HANDLE_INST(NUM, OPCODE, CLASS) \\\n    case Instruction::OPCODE: visit##OPCODE((const CLASS&)I); break;\n#include "llvm/IR/Instruction.def"\n  }\n}\n\n// include/llvm/IR/Instruction.def\nHANDLE_BINARY_INST(20, SDiv , BinaryOperator)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nvisitSDiv(const User &I) will create SDValue as operand and SDNode for each IR\n\nvoid SelectionDAGBuilder::visitSDiv(const User &I) {\n  SDValue Op1 = getValue(I.getOperand(0));\n  SDValue Op2 = getValue(I.getOperand(1));\n\n  SDNodeFlags Flags;\n  Flags.setExact(isa<PossiblyExactOperator>(&I) &&\n                 cast<PossiblyExactOperator>(&I)->isExact());\n  setValue(&I, DAG.getNode(ISD::SDIV, getCurSDLoc(), Op1.getValueType(), Op1,\n                           Op2, Flags));\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nSDNode also contains dependencies in SDValue, SDValue include following:\n\n 1. data dependency\n 2. chain dependency. For example. order load, store insturction to the same address\n 3. glue of instructions.\n\n\n# 2. Legalization\n\nLegalization will legalize SDNode operation that is unsupported target into supported Node. It includes legalization of operation and operand. As to operation, it includes 3 major operation:\n\n 1. Expansion expand one op into series of op\n 2. Promotion promote data type\n 3. Custom\n\n// CodeGen/SelectionDAG/LegalizeDAG.cpp\n\n/// Return a legal replacement for the given operation, with all legal operands.\nvoid SelectionDAGLegalize::LegalizeOp(SDNode *Node) {\n.....\n    case TargetLowering::Expand:\n      if (ExpandNode(Node))\n        return;\n      [[fallthrough]];\n    case TargetLowering::LibCall:\n      ConvertNodeToLibcall(Node);\n      return;\n    case TargetLowering::Promote:\n      PromoteNode(Node);\n      return;\n    }\n\n  switch (Node->getOpcode()) {\n  case ISD::LOAD:\n    return LegalizeLoadOps(Node);\n  case ISD::STORE:\n    return LegalizeStoreOps(Node);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\nLook closely to legalize of stop operation, it considered Expand, Custom and Promote.\n\nvoid SelectionDAGLegalize::LegalizeStoreOps(SDNode *Node) {\n    SDValue Value = ST->getValue();\n    MVT VT = Value.getSimpleValueType();\n    switch (TLI.getOperationAction(ISD::STORE, VT)) {\n    case TargetLowering::Legal: {\n      // If this is an unaligned store and the target doesn\'t support it, expand it.\n      EVT MemVT = ST->getMemoryVT();\n      const DataLayout &DL = DAG.getDataLayout();\n      if (!TLI.allowsMemoryAccessForAlignment(*DAG.getContext(), DL, MemVT,\n                                              *ST->getMemOperand())) {\n        SDValue Result = TLI.expandUnalignedStore(ST, DAG);\n        ReplaceNode(SDValue(ST, 0), Result);\n      }\n      break;\n    }\n    case TargetLowering::Custom: {\n      SDValue Res = TLI.LowerOperation(SDValue(Node, 0), DAG);\n      if (Res && Res != SDValue(Node, 0))\n        ReplaceNode(SDValue(Node, 0), Res);\n      return;\n    }\n    case TargetLowering::Promote: {\n      MVT NVT = TLI.getTypeToPromoteTo(ISD::STORE, VT);\n      Value = DAG.getNode(ISD::BITCAST, dl, NVT, Value);\n      SDValue Result = DAG.getStore(Chain, dl, Value, Ptr, ST->getPointerInfo(),\n                                    ST->getOriginalAlign(), MMOFlags, AAInfo);\n      ReplaceNode(SDValue(Node, 0), Result);\n      break;\n    }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\nIt also includes LibCall now.\n\n// include/llvm/CodeGen/TargetLowering.h\nclass TargetLoweringBase {\npublic:\n  /// This enum indicates whether operations are valid for a target, and if not,\n  /// what action should be used to make them valid.\n  enum LegalizeAction : uint8_t {\n    Legal,      // The target natively supports this operation.\n    Promote,    // This operation should be executed in a larger type.\n    Expand,     // Try to expand this to other ops, otherwise use a libcall.\n    LibCall,    // Don\'t try to expand this to other ops, always use a libcall.\n    Custom      // Use the LowerOperation hook to implement custom lowering.\n  };\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nAs to legalize operand, in ./CodeGen/SelectionDAG/LegalizeTypes.h, it shows supported functions.\n\n  //===--------------------------------------------------------------------===//\n  // Integer Promotion Support: LegalizeIntegerTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Integer Expansion Support: LegalizeIntegerTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Float to Integer Conversion Support: LegalizeFloatTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Float Expansion Support: LegalizeFloatTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Scalarization Support: LegalizeVectorTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Vector Splitting Support: LegalizeVectorTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Vector Widening Support: LegalizeVectorTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Vector Widening Utilities Support: LegalizeVectorTypes.cpp\n  //===--------------------------------------------------------------------===//\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 3. Instruction Selection\n\nCodeGen/SelectionDAG/SelectionDAGISel.cpp\nvoid SelectionDAGISel::CodeGenAndEmitDAG() {\n  // Pre-type legalization allow creation of any node types.\n  CurDAG->NewNodesMustHaveLegalTypes = false;\n  // Run the DAG combiner in pre-legalize mode.\n  CurDAG->Combine(BeforeLegalizeTypes, AA, OptLevel);\n  // Second step, hack on the DAG until it only uses operations and types that\n  // the target supports.\n  Changed = CurDAG->LegalizeTypes();\n\n  // Only allow creation of legal node types.\n  CurDAG->NewNodesMustHaveLegalTypes = true;\n   // Run the DAG combiner in post-type-legalize mode.\n  NamedRegionTimer T("combine_lt", "DAG Combining after legalize types",\n                         GroupName, GroupDescription, TimePassesIsEnabled);\n  CurDAG->Combine(AfterLegalizeTypes, AA, OptLevel);\n\n  Changed = CurDAG->LegalizeVectors();\n  // Run the DAG combiner in post-type-legalize mode.\n  NamedRegionTimer T("combine_lv", "DAG Combining after legalize vectors",\n                         GroupName, GroupDescription, TimePassesIsEnabled);\n  CurDAG->Combine(AfterLegalizeVectorOps, AA, OptLevel);\n\n  CurDAG->Legalize();\n\n  // Run the DAG combiner in post-legalize mode.\n  NamedRegionTimer T("combine2", "DAG Combining 2", GroupName,\n                       GroupDescription, TimePassesIsEnabled);\n  CurDAG->Combine(AfterLegalizeDAG, AA, OptLevel);\n  \n  ComputeLiveOutVRegInfo();\n\n  DoInstructionSelection();\n\n  // Schedule machine code.\n  ScheduleDAGSDNodes *Scheduler = CreateScheduler();\n  {\n    NamedRegionTimer T("sched", "Instruction Scheduling", GroupName,\n                       GroupDescription, TimePassesIsEnabled);\n    Scheduler->Run(CurDAG, FuncInfo->MBB);\n  }\n\n  // Emit machine code to BB.  This can change \'BB\' to the last block being\n  // inserted into.\n  MachineBasicBlock *FirstMBB = FuncInfo->MBB, *LastMBB;\n  {\n    NamedRegionTimer T("emit", "Instruction Creation", GroupName,\n                       GroupDescription, TimePassesIsEnabled);\n\n    // FuncInfo->InsertPt is passed by reference and set to the end of the\n    // scheduled instructions.\n    LastMBB = FuncInfo->MBB = Scheduler->EmitSchedule(FuncInfo->InsertPt);\n  }\n\n  // If the block was split, make sure we update any references that are used to\n  // update PHI nodes later on.\n  if (FirstMBB != LastMBB)\n    SDB->UpdateSplitBlock(FirstMBB, LastMBB);\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n//File include/llvm/CodeGen/SelectionDAGISel.h\n// Main hook for targets to transform nodes into machine nodes.\nvirtual void Select(SDNode *N) = 0;\n\n//File CodeGen/SelectionDAG/SelectionDAGISel.cpp\nvoid SelectionDAGISel::DoInstructionSelection() {\n  LLVM_DEBUG(dbgs() << "===== Instruction selection begins: "\n                    << printMBBReference(*FuncInfo->MBB) << " \'"\n                    << FuncInfo->MBB->getName() << "\'\\n");\n  PreprocessISelDAG();\n\n  // Select target instructions for the DAG.\n  // Number all nodes with a topological order and set DAGSize.\n  DAGSize = CurDAG->AssignTopologicalOrder();\n\n  // The AllNodes list is now topological-sorted. Visit the\n  // nodes by starting at the end of the list (the root of the\n  // graph) and preceding back toward the beginning (the entry\n  // node).\n  while (ISelPosition != CurDAG->allnodes_begin()) {\n    SDNode *Node = &*--ISelPosition;\n    ...\n    \n    Select(Node);\n  }\n\n  CurDAG->setRoot(Dummy.getValue());\n  \n  LLVM_DEBUG(dbgs() << "\\n===== Instruction selection ends:\\n");\n  PostprocessISelDAG();\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nSelect(SDNode *N) will be overide by target backend:\n\n//File Target/MSP430/MSP430ISelDAGToDAG.cpp\n\n  #include "MSP430GenDAGISel.inc"\nvoid MSP430DAGToDAGISel::Select(SDNode *Node) {\n  // Few custom selection stuff.\n  switch (Node->getOpcode()) {\n  default: break;\n  case ISD::LOAD:\n    if (tryIndexedLoad(Node))\n      return;\n    // Other cases are autogenerated.\n    break;\n  case ISD::ADD:\n    if (tryIndexedBinOp(Node, Node->getOperand(0), Node->getOperand(1),\n                        MSP430::ADD8rp, MSP430::ADD16rp))\n      return;\n    else if (tryIndexedBinOp(Node, Node->getOperand(1), Node->getOperand(0),\n                             MSP430::ADD8rp, MSP430::ADD16rp))\n      return;\n  // Select the default instruction\n  SelectCode(Node);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\nIn the end, it is the SelectCode function. This function will be generated in XXXGenDAGISel.inc from XXXInstrInfo.td. For example, it would be like this:\n\n  SDNode *SelectCode(SDValue N) {\n    ...\n    MVT::ValueType NVT = N.getNode()->getValueType(0);\n    switch (N.getOpcode()) {\n    case ISD::STORE: {\n      switch (NVT) {\n      default:\n        return Select_ISD_STORE(N);\n        break;\n      }\n      break;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nAfter the instruction selection, the LLVM IR will be represented in form of machineDAG.\n\n\n# 4. Instruction Scheduler\n\nIt includes 3 scheulder\n\n 1. ScheduleDAG. This is in the stage of instruction selection.\n\nScheduleDAGSDNodes.cpp\nScheduleDAGFast.cpp\nScheduleDAGRRList.cpp\nScheduleDAGVLIW.cpp\n\n\n1\n2\n3\n4\n\n\nFile ScheduleDAGRRList.cpp\n//===- ScheduleDAGRRList.cpp - Reg pressure reduction list scheduler ------===//\n// This implements bottom-up and top-down register pressure reduction list\n// schedulers, using standard algorithms.  The basic approach uses a priority\n// queue of available nodes to schedule.  One at a time, nodes are taken from\n// the priority queue (thus in priority order), checked for legality to\n// schedule, and emitted if legal.\n//\n//===----------------------------------------------------------------------===//\nburrListDAGScheduler("list-burr",\n                    "Bottom-up register reduction list scheduling",\n                    createBURRListDAGScheduler);\n\nsourceListDAGScheduler("source",\n                    "Similar to list-burr but schedules in source "\n                    "order when possible",\n                    createSourceListDAGScheduler);\n\nhybridListDAGScheduler("list-hybrid",\n                    "Bottom-up register pressure aware list scheduling "\n                    "which tries to balance latency and register pressure",\n                    createHybridListDAGScheduler);\n\nILPListDAGScheduler("list-ilp",\n                    "Bottom-up register pressure aware list scheduling "\n                    "which tries to balance ILP and register pressure",\n                    createILPListDAGScheduler);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\nIn every DAGScheduler, the flow is similar, the only difference is the construction of priority queue.\n\nScheduleDAGSDNodes *\nllvm::createHybridListDAGScheduler(SelectionDAGISel *IS,\n                                   CodeGenOptLevel OptLevel) {\n  HybridBURRPriorityQueue *PQ =\n    new HybridBURRPriorityQueue(*IS->MF, true, false, TII, TRI, TLI);\n\n  ScheduleDAGRRList *SD = new ScheduleDAGRRList(*IS->MF, true, PQ, OptLevel);\n  PQ->setScheduleDAG(SD);\n  return SD;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nfunction CodeGenAdnEmitDAG contains founction CreateScheduler, it can call target-defined scheduler and also the schedulers defined above.\n\nHere llvm utilize template, it can be seen that the template parameter is different scheduler policy, it defines them as struct.\n\nIn the "template class RegReductionPriorityQueue", SF will be instantiated as "Picker", the ick will influence "isReady" and "pop".\n\nusing BURegReductionPriorityQueue = RegReductionPriorityQueue<bu_ls_rr_sort>;\nusing SrcRegReductionPriorityQueue = RegReductionPriorityQueue<src_ls_rr_sort>;\nusing HybridBURRPriorityQueue = RegReductionPriorityQueue<hybrid_ls_rr_sort>;\nusing ILPBURRPriorityQueue = RegReductionPriorityQueue<ilp_ls_rr_sort>;\n\n// src_ls_rr_sort - Priority function for source order scheduler.\nstruct src_ls_rr_sort : public queue_sort {\n  enum {\n    IsBottomUp = true,\n    HasReadyFilter = false\n  };\n  RegReductionPQBase *SPQ;\n  src_ls_rr_sort(RegReductionPQBase *spq) : SPQ(spq) {}\n  bool operator()(SUnit* left, SUnit* right) const;\n};\n\n// hybrid_ls_rr_sort - Priority function for hybrid scheduler.\nstruct hybrid_ls_rr_sort : public queue_sort {\n  enum {\n    IsBottomUp = true,\n    HasReadyFilter = false\n  };\n  RegReductionPQBase *SPQ;\n  hybrid_ls_rr_sort(RegReductionPQBase *spq) : SPQ(spq) {}\n  bool isReady(SUnit *SU, unsigned CurCycle) const;\n  bool operator()(SUnit* left, SUnit* right) const;\n};\n\n//===----------------------------------------------------------------------===//\n//                RegReductionPriorityQueue Definition\n//===----------------------------------------------------------------------===//\n//\n// This is a SchedulingPriorityQueue that schedules using Sethi Ullman numbers\n// to reduce register pressure.\n//\ntemplate<class SF>\nclass RegReductionPriorityQueue : public RegReductionPQBase {\n  SF Picker;\npublic:\n  RegReductionPriorityQueue(MachineFunction &mf,\n                            bool tracksrp,\n                            bool srcorder,\n                            const TargetInstrInfo *tii,\n                            const TargetRegisterInfo *tri,\n                            const TargetLowering *tli)\n    : RegReductionPQBase(mf, SF::HasReadyFilter, tracksrp, srcorder,\n                         tii, tri, tli),\n      Picker(this) {}\n\n  bool isBottomUp() const override { return SF::IsBottomUp; }\n\n  bool isReady(SUnit *U) const override {\n    return Picker.HasReadyFilter && Picker.isReady(U, getCurCycle());\n  }\n\n  SUnit *pop() override {\n    if (Queue.empty()) return nullptr;\n\n    SUnit *V = popFromQueue(Queue, Picker, scheduleDAG);\n    V->NodeQueueId = 0;\n    return V;\n  }\n};\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\nIn Instruction Selection, it calls creater scheduler and Run. For simplicity, I delete all the trivial code, like reset or clear, or clear function in the code.\n\n  // Schedule machine code.\n  ScheduleDAGSDNodes *Scheduler = CreateScheduler();\n  {\n    NamedRegionTimer T("sched", "Instruction Scheduling", GroupName,\n                       GroupDescription, TimePassesIsEnabled);\n    Scheduler->Run(CurDAG, FuncInfo->MBB);\n  }\n\n// File CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp\n/// Run - perform scheduling.\nvoid ScheduleDAGSDNodes::Run(SelectionDAG *dag, MachineBasicBlock *bb) {\n   ....\n  // Invoke the target\'s selection of scheduler.\n  Schedule();\n}\n\n\n// File CodeGen/SelectionDAG/ScheduleDAGRRList.cpp\n/// Schedule - Schedule the DAG using list scheduling.\nvoid ScheduleDAGRRList::Schedule() {\n  LLVM_DEBUG(dbgs() << "********** List Scheduling " << printMBBReference(*BB)\n                    << " \'" << BB->getName() << "\' **********\\n");\n  BuildSchedGraph(nullptr);\n  Topo.MarkDirty();\n  AvailableQueue->initNodes(SUnits);\n  // Execute the actual scheduling loop.\n  ListScheduleBottomUp();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\nIn Schedule() function, it contains the following code:\n\n 1. BuildSchedGraph. This function creates SUnit Graph\n\n// File CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp\n/// BuildSchedGraph - Build the SUnit graph from the selection dag that we\n/// are input.  This SUnit graph is similar to the SelectionDAG, but\n/// excludes nodes that aren\'t interesting to scheduling, and represents\n/// glued together nodes with a single SUnit.\nvoid ScheduleDAGSDNodes::BuildSchedGraph(AAResults *AA) {\n  // Cluster certain nodes which should be scheduled together.\n  ClusterNodes();\n  // Populate the SUnits array.\n  // During scheduling, the NodeId field of SDNode is used to map SDNodes\n  // to their associated SUnits by holding SUnits table indices\n  // Multiple SDNodes might be associated to one SUnit.\n  BuildSchedUnits();\n  // Compute all the scheduling dependencies between nodes.\n  AddSchedEdges();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nBuildSchedUnits will also calculate the latency for each Sunit, using computeLatency\n\n 2. ListScheduleBottomUp()\n\n// ListScheduleBottomUp - The main loop of list scheduling for bottom-up\n// schedulers.\nvoid ScheduleDAGRRList::ListScheduleBottomUp() {\n  // Release any predecessors of the special Exit node.\n  ReleasePredecessors(&ExitSU);\n  // Add root to Available queue.\n  if (!SUnits.empty()) {\n    SUnit *RootSU = &SUnits[DAG->getRoot().getNode()->getNodeId()];\n    RootSU->isAvailable = true;\n    AvailableQueue->push(RootSU);\n  }\n\n  // While Available queue is not empty, grab the node with the highest\n  // priority. If it is not ready put it back.  Schedule the node.\n  Sequence.reserve(SUnits.size());\n  while (!AvailableQueue->empty() || !Interferences.empty()) {\n    // Pick the best node to schedule taking all constraints into\n    // consideration.\n    // Return a node that can be scheduled in this cycle. Requirements:\n    // (1) Ready: latency has been satisfied\n    // (2) No Hazards: resources are available\n    // (3) No Interferences: may unschedule to break register interferences.\n    SUnit *SU = PickNodeToScheduleBottomUp();\n    /// Move the scheduler state forward until the specified node\'s dependents are\n    /// ready and can be scheduled with no resource conflicts.\n    AdvancePastStalls(SU);\n    // ScheduleNodeBottomUp - Add the node to the schedule. Decrement the pending\n    // count of its predecessors. If a predecessor pending count is zero, add it to\n    // the Available queue.\n    ScheduleNodeBottomUp(SU);\n\n    while (AvailableQueue->empty() && !PendingQueue.empty()) {\n      AdvanceToCycle(std::max(CurCycle + 1, MinAvailableCycle));\n    }\n  }\n\n  // Reverse the order if it is bottom up.\n  std::reverse(Sequence.begin(), Sequence.end());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nAdvanceToCycle(unsigned NextCycle) would be good function to be noticed. For example, in AdvancePastStalls(), the scheduler will advance to the cycle when the chosedn SUnit is ready.',normalizedContent:'# 1. create selectiondag\n\nselectiondag builder calls visit() function to build sdnode\n\n// codegen/selectiondag/selectiondagbuilder.cpp\nvoid selectiondagbuilder::visit(unsigned opcode, const user &i) {\n  // note: this doesn\'t use instvisitor, because it has to work with\n  // constantexpr\'s in addition to instructions.\n  switch (opcode) {\n  default: llvm_unreachable("unknown instruction type encountered!");\n    // build the switch statement using the instruction.def file.\n#define handle_inst(num, opcode, class) \\\n    case instruction::opcode: visit##opcode((const class&)i); break;\n#include "llvm/ir/instruction.def"\n  }\n}\n\n// include/llvm/ir/instruction.def\nhandle_binary_inst(20, sdiv , binaryoperator)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nvisitsdiv(const user &i) will create sdvalue as operand and sdnode for each ir\n\nvoid selectiondagbuilder::visitsdiv(const user &i) {\n  sdvalue op1 = getvalue(i.getoperand(0));\n  sdvalue op2 = getvalue(i.getoperand(1));\n\n  sdnodeflags flags;\n  flags.setexact(isa<possiblyexactoperator>(&i) &&\n                 cast<possiblyexactoperator>(&i)->isexact());\n  setvalue(&i, dag.getnode(isd::sdiv, getcursdloc(), op1.getvaluetype(), op1,\n                           op2, flags));\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nsdnode also contains dependencies in sdvalue, sdvalue include following:\n\n 1. data dependency\n 2. chain dependency. for example. order load, store insturction to the same address\n 3. glue of instructions.\n\n\n# 2. legalization\n\nlegalization will legalize sdnode operation that is unsupported target into supported node. it includes legalization of operation and operand. as to operation, it includes 3 major operation:\n\n 1. expansion expand one op into series of op\n 2. promotion promote data type\n 3. custom\n\n// codegen/selectiondag/legalizedag.cpp\n\n/// return a legal replacement for the given operation, with all legal operands.\nvoid selectiondaglegalize::legalizeop(sdnode *node) {\n.....\n    case targetlowering::expand:\n      if (expandnode(node))\n        return;\n      [[fallthrough]];\n    case targetlowering::libcall:\n      convertnodetolibcall(node);\n      return;\n    case targetlowering::promote:\n      promotenode(node);\n      return;\n    }\n\n  switch (node->getopcode()) {\n  case isd::load:\n    return legalizeloadops(node);\n  case isd::store:\n    return legalizestoreops(node);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\nlook closely to legalize of stop operation, it considered expand, custom and promote.\n\nvoid selectiondaglegalize::legalizestoreops(sdnode *node) {\n    sdvalue value = st->getvalue();\n    mvt vt = value.getsimplevaluetype();\n    switch (tli.getoperationaction(isd::store, vt)) {\n    case targetlowering::legal: {\n      // if this is an unaligned store and the target doesn\'t support it, expand it.\n      evt memvt = st->getmemoryvt();\n      const datalayout &dl = dag.getdatalayout();\n      if (!tli.allowsmemoryaccessforalignment(*dag.getcontext(), dl, memvt,\n                                              *st->getmemoperand())) {\n        sdvalue result = tli.expandunalignedstore(st, dag);\n        replacenode(sdvalue(st, 0), result);\n      }\n      break;\n    }\n    case targetlowering::custom: {\n      sdvalue res = tli.loweroperation(sdvalue(node, 0), dag);\n      if (res && res != sdvalue(node, 0))\n        replacenode(sdvalue(node, 0), res);\n      return;\n    }\n    case targetlowering::promote: {\n      mvt nvt = tli.gettypetopromoteto(isd::store, vt);\n      value = dag.getnode(isd::bitcast, dl, nvt, value);\n      sdvalue result = dag.getstore(chain, dl, value, ptr, st->getpointerinfo(),\n                                    st->getoriginalalign(), mmoflags, aainfo);\n      replacenode(sdvalue(node, 0), result);\n      break;\n    }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\nit also includes libcall now.\n\n// include/llvm/codegen/targetlowering.h\nclass targetloweringbase {\npublic:\n  /// this enum indicates whether operations are valid for a target, and if not,\n  /// what action should be used to make them valid.\n  enum legalizeaction : uint8_t {\n    legal,      // the target natively supports this operation.\n    promote,    // this operation should be executed in a larger type.\n    expand,     // try to expand this to other ops, otherwise use a libcall.\n    libcall,    // don\'t try to expand this to other ops, always use a libcall.\n    custom      // use the loweroperation hook to implement custom lowering.\n  };\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nas to legalize operand, in ./codegen/selectiondag/legalizetypes.h, it shows supported functions.\n\n  //===--------------------------------------------------------------------===//\n  // integer promotion support: legalizeintegertypes.cpp\n  //===--------------------------------------------------------------------===//\n  // integer expansion support: legalizeintegertypes.cpp\n  //===--------------------------------------------------------------------===//\n  // float to integer conversion support: legalizefloattypes.cpp\n  //===--------------------------------------------------------------------===//\n  // float expansion support: legalizefloattypes.cpp\n  //===--------------------------------------------------------------------===//\n  // scalarization support: legalizevectortypes.cpp\n  //===--------------------------------------------------------------------===//\n  // vector splitting support: legalizevectortypes.cpp\n  //===--------------------------------------------------------------------===//\n  // vector widening support: legalizevectortypes.cpp\n  //===--------------------------------------------------------------------===//\n  // vector widening utilities support: legalizevectortypes.cpp\n  //===--------------------------------------------------------------------===//\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 3. instruction selection\n\ncodegen/selectiondag/selectiondagisel.cpp\nvoid selectiondagisel::codegenandemitdag() {\n  // pre-type legalization allow creation of any node types.\n  curdag->newnodesmusthavelegaltypes = false;\n  // run the dag combiner in pre-legalize mode.\n  curdag->combine(beforelegalizetypes, aa, optlevel);\n  // second step, hack on the dag until it only uses operations and types that\n  // the target supports.\n  changed = curdag->legalizetypes();\n\n  // only allow creation of legal node types.\n  curdag->newnodesmusthavelegaltypes = true;\n   // run the dag combiner in post-type-legalize mode.\n  namedregiontimer t("combine_lt", "dag combining after legalize types",\n                         groupname, groupdescription, timepassesisenabled);\n  curdag->combine(afterlegalizetypes, aa, optlevel);\n\n  changed = curdag->legalizevectors();\n  // run the dag combiner in post-type-legalize mode.\n  namedregiontimer t("combine_lv", "dag combining after legalize vectors",\n                         groupname, groupdescription, timepassesisenabled);\n  curdag->combine(afterlegalizevectorops, aa, optlevel);\n\n  curdag->legalize();\n\n  // run the dag combiner in post-legalize mode.\n  namedregiontimer t("combine2", "dag combining 2", groupname,\n                       groupdescription, timepassesisenabled);\n  curdag->combine(afterlegalizedag, aa, optlevel);\n  \n  computeliveoutvreginfo();\n\n  doinstructionselection();\n\n  // schedule machine code.\n  scheduledagsdnodes *scheduler = createscheduler();\n  {\n    namedregiontimer t("sched", "instruction scheduling", groupname,\n                       groupdescription, timepassesisenabled);\n    scheduler->run(curdag, funcinfo->mbb);\n  }\n\n  // emit machine code to bb.  this can change \'bb\' to the last block being\n  // inserted into.\n  machinebasicblock *firstmbb = funcinfo->mbb, *lastmbb;\n  {\n    namedregiontimer t("emit", "instruction creation", groupname,\n                       groupdescription, timepassesisenabled);\n\n    // funcinfo->insertpt is passed by reference and set to the end of the\n    // scheduled instructions.\n    lastmbb = funcinfo->mbb = scheduler->emitschedule(funcinfo->insertpt);\n  }\n\n  // if the block was split, make sure we update any references that are used to\n  // update phi nodes later on.\n  if (firstmbb != lastmbb)\n    sdb->updatesplitblock(firstmbb, lastmbb);\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n//file include/llvm/codegen/selectiondagisel.h\n// main hook for targets to transform nodes into machine nodes.\nvirtual void select(sdnode *n) = 0;\n\n//file codegen/selectiondag/selectiondagisel.cpp\nvoid selectiondagisel::doinstructionselection() {\n  llvm_debug(dbgs() << "===== instruction selection begins: "\n                    << printmbbreference(*funcinfo->mbb) << " \'"\n                    << funcinfo->mbb->getname() << "\'\\n");\n  preprocessiseldag();\n\n  // select target instructions for the dag.\n  // number all nodes with a topological order and set dagsize.\n  dagsize = curdag->assigntopologicalorder();\n\n  // the allnodes list is now topological-sorted. visit the\n  // nodes by starting at the end of the list (the root of the\n  // graph) and preceding back toward the beginning (the entry\n  // node).\n  while (iselposition != curdag->allnodes_begin()) {\n    sdnode *node = &*--iselposition;\n    ...\n    \n    select(node);\n  }\n\n  curdag->setroot(dummy.getvalue());\n  \n  llvm_debug(dbgs() << "\\n===== instruction selection ends:\\n");\n  postprocessiseldag();\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nselect(sdnode *n) will be overide by target backend:\n\n//file target/msp430/msp430iseldagtodag.cpp\n\n  #include "msp430gendagisel.inc"\nvoid msp430dagtodagisel::select(sdnode *node) {\n  // few custom selection stuff.\n  switch (node->getopcode()) {\n  default: break;\n  case isd::load:\n    if (tryindexedload(node))\n      return;\n    // other cases are autogenerated.\n    break;\n  case isd::add:\n    if (tryindexedbinop(node, node->getoperand(0), node->getoperand(1),\n                        msp430::add8rp, msp430::add16rp))\n      return;\n    else if (tryindexedbinop(node, node->getoperand(1), node->getoperand(0),\n                             msp430::add8rp, msp430::add16rp))\n      return;\n  // select the default instruction\n  selectcode(node);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\nin the end, it is the selectcode function. this function will be generated in xxxgendagisel.inc from xxxinstrinfo.td. for example, it would be like this:\n\n  sdnode *selectcode(sdvalue n) {\n    ...\n    mvt::valuetype nvt = n.getnode()->getvaluetype(0);\n    switch (n.getopcode()) {\n    case isd::store: {\n      switch (nvt) {\n      default:\n        return select_isd_store(n);\n        break;\n      }\n      break;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nafter the instruction selection, the llvm ir will be represented in form of machinedag.\n\n\n# 4. instruction scheduler\n\nit includes 3 scheulder\n\n 1. scheduledag. this is in the stage of instruction selection.\n\nscheduledagsdnodes.cpp\nscheduledagfast.cpp\nscheduledagrrlist.cpp\nscheduledagvliw.cpp\n\n\n1\n2\n3\n4\n\n\nfile scheduledagrrlist.cpp\n//===- scheduledagrrlist.cpp - reg pressure reduction list scheduler ------===//\n// this implements bottom-up and top-down register pressure reduction list\n// schedulers, using standard algorithms.  the basic approach uses a priority\n// queue of available nodes to schedule.  one at a time, nodes are taken from\n// the priority queue (thus in priority order), checked for legality to\n// schedule, and emitted if legal.\n//\n//===----------------------------------------------------------------------===//\nburrlistdagscheduler("list-burr",\n                    "bottom-up register reduction list scheduling",\n                    createburrlistdagscheduler);\n\nsourcelistdagscheduler("source",\n                    "similar to list-burr but schedules in source "\n                    "order when possible",\n                    createsourcelistdagscheduler);\n\nhybridlistdagscheduler("list-hybrid",\n                    "bottom-up register pressure aware list scheduling "\n                    "which tries to balance latency and register pressure",\n                    createhybridlistdagscheduler);\n\nilplistdagscheduler("list-ilp",\n                    "bottom-up register pressure aware list scheduling "\n                    "which tries to balance ilp and register pressure",\n                    createilplistdagscheduler);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\nin every dagscheduler, the flow is similar, the only difference is the construction of priority queue.\n\nscheduledagsdnodes *\nllvm::createhybridlistdagscheduler(selectiondagisel *is,\n                                   codegenoptlevel optlevel) {\n  hybridburrpriorityqueue *pq =\n    new hybridburrpriorityqueue(*is->mf, true, false, tii, tri, tli);\n\n  scheduledagrrlist *sd = new scheduledagrrlist(*is->mf, true, pq, optlevel);\n  pq->setscheduledag(sd);\n  return sd;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nfunction codegenadnemitdag contains founction createscheduler, it can call target-defined scheduler and also the schedulers defined above.\n\nhere llvm utilize template, it can be seen that the template parameter is different scheduler policy, it defines them as struct.\n\nin the "template class regreductionpriorityqueue", sf will be instantiated as "picker", the ick will influence "isready" and "pop".\n\nusing buregreductionpriorityqueue = regreductionpriorityqueue<bu_ls_rr_sort>;\nusing srcregreductionpriorityqueue = regreductionpriorityqueue<src_ls_rr_sort>;\nusing hybridburrpriorityqueue = regreductionpriorityqueue<hybrid_ls_rr_sort>;\nusing ilpburrpriorityqueue = regreductionpriorityqueue<ilp_ls_rr_sort>;\n\n// src_ls_rr_sort - priority function for source order scheduler.\nstruct src_ls_rr_sort : public queue_sort {\n  enum {\n    isbottomup = true,\n    hasreadyfilter = false\n  };\n  regreductionpqbase *spq;\n  src_ls_rr_sort(regreductionpqbase *spq) : spq(spq) {}\n  bool operator()(sunit* left, sunit* right) const;\n};\n\n// hybrid_ls_rr_sort - priority function for hybrid scheduler.\nstruct hybrid_ls_rr_sort : public queue_sort {\n  enum {\n    isbottomup = true,\n    hasreadyfilter = false\n  };\n  regreductionpqbase *spq;\n  hybrid_ls_rr_sort(regreductionpqbase *spq) : spq(spq) {}\n  bool isready(sunit *su, unsigned curcycle) const;\n  bool operator()(sunit* left, sunit* right) const;\n};\n\n//===----------------------------------------------------------------------===//\n//                regreductionpriorityqueue definition\n//===----------------------------------------------------------------------===//\n//\n// this is a schedulingpriorityqueue that schedules using sethi ullman numbers\n// to reduce register pressure.\n//\ntemplate<class sf>\nclass regreductionpriorityqueue : public regreductionpqbase {\n  sf picker;\npublic:\n  regreductionpriorityqueue(machinefunction &mf,\n                            bool tracksrp,\n                            bool srcorder,\n                            const targetinstrinfo *tii,\n                            const targetregisterinfo *tri,\n                            const targetlowering *tli)\n    : regreductionpqbase(mf, sf::hasreadyfilter, tracksrp, srcorder,\n                         tii, tri, tli),\n      picker(this) {}\n\n  bool isbottomup() const override { return sf::isbottomup; }\n\n  bool isready(sunit *u) const override {\n    return picker.hasreadyfilter && picker.isready(u, getcurcycle());\n  }\n\n  sunit *pop() override {\n    if (queue.empty()) return nullptr;\n\n    sunit *v = popfromqueue(queue, picker, scheduledag);\n    v->nodequeueid = 0;\n    return v;\n  }\n};\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\nin instruction selection, it calls creater scheduler and run. for simplicity, i delete all the trivial code, like reset or clear, or clear function in the code.\n\n  // schedule machine code.\n  scheduledagsdnodes *scheduler = createscheduler();\n  {\n    namedregiontimer t("sched", "instruction scheduling", groupname,\n                       groupdescription, timepassesisenabled);\n    scheduler->run(curdag, funcinfo->mbb);\n  }\n\n// file codegen/selectiondag/scheduledagsdnodes.cpp\n/// run - perform scheduling.\nvoid scheduledagsdnodes::run(selectiondag *dag, machinebasicblock *bb) {\n   ....\n  // invoke the target\'s selection of scheduler.\n  schedule();\n}\n\n\n// file codegen/selectiondag/scheduledagrrlist.cpp\n/// schedule - schedule the dag using list scheduling.\nvoid scheduledagrrlist::schedule() {\n  llvm_debug(dbgs() << "********** list scheduling " << printmbbreference(*bb)\n                    << " \'" << bb->getname() << "\' **********\\n");\n  buildschedgraph(nullptr);\n  topo.markdirty();\n  availablequeue->initnodes(sunits);\n  // execute the actual scheduling loop.\n  listschedulebottomup();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\nin schedule() function, it contains the following code:\n\n 1. buildschedgraph. this function creates sunit graph\n\n// file codegen/selectiondag/scheduledagsdnodes.cpp\n/// buildschedgraph - build the sunit graph from the selection dag that we\n/// are input.  this sunit graph is similar to the selectiondag, but\n/// excludes nodes that aren\'t interesting to scheduling, and represents\n/// glued together nodes with a single sunit.\nvoid scheduledagsdnodes::buildschedgraph(aaresults *aa) {\n  // cluster certain nodes which should be scheduled together.\n  clusternodes();\n  // populate the sunits array.\n  // during scheduling, the nodeid field of sdnode is used to map sdnodes\n  // to their associated sunits by holding sunits table indices\n  // multiple sdnodes might be associated to one sunit.\n  buildschedunits();\n  // compute all the scheduling dependencies between nodes.\n  addschededges();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nbuildschedunits will also calculate the latency for each sunit, using computelatency\n\n 2. listschedulebottomup()\n\n// listschedulebottomup - the main loop of list scheduling for bottom-up\n// schedulers.\nvoid scheduledagrrlist::listschedulebottomup() {\n  // release any predecessors of the special exit node.\n  releasepredecessors(&exitsu);\n  // add root to available queue.\n  if (!sunits.empty()) {\n    sunit *rootsu = &sunits[dag->getroot().getnode()->getnodeid()];\n    rootsu->isavailable = true;\n    availablequeue->push(rootsu);\n  }\n\n  // while available queue is not empty, grab the node with the highest\n  // priority. if it is not ready put it back.  schedule the node.\n  sequence.reserve(sunits.size());\n  while (!availablequeue->empty() || !interferences.empty()) {\n    // pick the best node to schedule taking all constraints into\n    // consideration.\n    // return a node that can be scheduled in this cycle. requirements:\n    // (1) ready: latency has been satisfied\n    // (2) no hazards: resources are available\n    // (3) no interferences: may unschedule to break register interferences.\n    sunit *su = picknodetoschedulebottomup();\n    /// move the scheduler state forward until the specified node\'s dependents are\n    /// ready and can be scheduled with no resource conflicts.\n    advancepaststalls(su);\n    // schedulenodebottomup - add the node to the schedule. decrement the pending\n    // count of its predecessors. if a predecessor pending count is zero, add it to\n    // the available queue.\n    schedulenodebottomup(su);\n\n    while (availablequeue->empty() && !pendingqueue.empty()) {\n      advancetocycle(std::max(curcycle + 1, minavailablecycle));\n    }\n  }\n\n  // reverse the order if it is bottom up.\n  std::reverse(sequence.begin(), sequence.end());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nadvancetocycle(unsigned nextcycle) would be good function to be noticed. for example, in advancepaststalls(), the scheduler will advance to the cycle when the chosedn sunit is ready.',charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"Writing TinyRISCV Backend",frontmatter:{title:"Writing TinyRISCV Backend",output:{html_document:{code_folding:"hide"}},date:"2024-10-09T00:00:00.000Z",permalink:"/pages/000008/",tags:[null]},regularPath:"/02.compiler/08.write_tinyriscv_backend.html",relativePath:"02.compiler/08.write_tinyriscv_backend.md",key:"v-81151826",path:"/pages/000008/",headers:[{level:3,title:"1. 2024-10-08",slug:"_1-2024-10-08",normalizedTitle:"1. 2024-10-08",charIndex:2},{level:3,title:"2. 2024-10-09",slug:"_2-2024-10-09",normalizedTitle:"2. 2024-10-09",charIndex:300},{level:3,title:"3. 2024-10-12",slug:"_3-2024-10-12",normalizedTitle:"3. 2024-10-12",charIndex:3109},{level:3,title:"4. 2024-10-13",slug:"_4-2024-10-13",normalizedTitle:"4. 2024-10-13",charIndex:3731}],headersStr:"1. 2024-10-08 2. 2024-10-09 3. 2024-10-12 4. 2024-10-13",content:"# 1. 2024-10-08\n\nCreate the Framework of Basic Backend\n\n\n\nOnly framework is added, except InstrFormats.td, InstrInfo.td and RegisterInfo.td.\n\nAbove tablegen files are copied from https://github.com/rhysd/toy-riscv-backend. These tablegen files are uniform for riscv-compatible backend compilers.\n\n\n# 2. 2024-10-09\n\nWhen I try to compile sometest.ll with above backend, error:\n\nllvm/lib/CodeGen/LLVMTargetMachine.cpp:47:\nvoid llvm::LLVMTargetMachine::initAsmInfo(): Assertion `MRI && \"Unable to create reg info\"' failed\n\n\n1\n2\n\n\n# Claude explains why MC Register Info is needed\n\nThe reason is that MCRegisterInfo is not created.\n\nThe function createTinyRISCVMCRegisterInfo is responsible for creating and initializing the MCRegisterInfo object for your TinyRISCV target.\nThis object provides essential information about the registers available in your target architecture, which is crucial for various stages of code generation and assembly.\n\nKey Functions of createTinyRISCVMCRegisterInfo\n\n 1. Register Initialization:\n    It initializes the register information for the TinyRISCV target, including the mapping of register names to register numbers and any special properties of the registers.\n 2. Callee-Saved Registers:\n    The MCRegisterInfo object typically includes information about which registers are callee-saved, which is important for function call conventions and stack management.\n 3. Register Aliases:\n    It may define any register aliases, which are alternative names or numbers for the same physical register.\n 4. Register Classes:\n    The function helps define register classes, which group registers with similar characteristics (e.g., general-purpose registers).\n 5. Integration with LLVM:\n    By registering the MCRegisterInfo with the LLVM target registry, it ensures that the rest of the LLVM infrastructure can access and use the register information for tasks like instruction selection, register allocation, and assembly printing.\n\nExample Implementation Here's a typical implementation of createTinyRISCVMCRegisterInfo:\n\nstatic MCRegisterInfo *createTinyRISCVMCRegisterInfo(const Triple &TT) {\n  MCRegisterInfo *X = new MCRegisterInfo();\n  InitTinyRISCVMCRegisterInfo(X, /* Return address register number */);\n  return X;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n\n\nInitTinyRISCVMCRegisterInfo\nThis function, usually generated by TableGen, initializes the MCRegisterInfo object with the register definitions specified in your target's TableGen files.\n\nReturn Address Register\nThe function often takes a parameter specifying the register used for the return address, which is important for call and return instructions.\n\nBy ensuring that createTinyRISCVMCRegisterInfo is correctly implemented and registered, you provide the necessary register information to the LLVM backend, enabling it to generate and manipulate machine code effectively for your TinyRISCV target.\n\n# Debug\n\nSpend lots of time on debugs namespace tinyriscv issue\n\nroot cause is that:\n\nwe need to include TinyRISCVGenRegisterInfo.inc in both TinyRISCVMCTargetDesc.h and TinyRISCVMCTargetDesc.cpp.\n\nOnce for define, another for declare.\n\n\n# 3. 2024-10-12\n\nllc  -debug -debug-pass=Structure -march=tinyriscv\ntinyriscv_simpletest.ll\n-o tinyriscv_addition.s\n\nFailed:\nllc: error: target does not support generation of this file type\n\n\n1\n2\n3\n4\n5\n6\n\n\nTry to solve this issue, might due to missing of addPasses in TargetMachine.h/cpp.\n\nwhen subtarget is not implemented:\n\nbool runOnFunction(Function &F) override {\n  auto *TM = &getAnalysis<TargetPassConfig>().getTM<TargetMachine>();\n  auto *TLI = TM->getSubtargetImpl(F)->getTargetLowering();\n  return runImpl(F, *TLI);\n}\n\nstatic bool runImpl(Function &F, const TargetLowering &TLI) {\n.....\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 4. 2024-10-13\n\nForget to add getFrameLowering in *TargetMachine.h\n\n#11 0x00007e10aebdbd7a llvm::TargetLowering::LowerCall(llvm::TargetLowering::CallLoweringInfo&, llvm::SmallVectorImpl<llvm::SDValue>&) const ...TargetLowering.h:4770:5\n#12 0x00007e10ad58b07b llvm::SelectionDAGISel::LowerArguments(llvm::Function const&) ...SelectionDAGBuilder.cpp:11673:46\n#13 0x00007e10ad679308 llvm::SelectionDAGISel::SelectAllBasicBlocks(llvm::Function const&) ...CodeGen/SelectionDAG/SelectionDAGISel.cpp:1648:19\n\n\n1\n2\n3\n\n\nThis is the bug that when called getNumRegisters() in TargetLowering.h, the NumRegisterForVT[VT.getSimpleVT().SimpleTy] returns 0.\n\nThe NumRegisterForVT is not initialized in our case.\n\nthis is initialized by calling computeRegisterProperties in TinyRISCVTargetLowering, which does not called before solving this bug.",normalizedContent:"# 1. 2024-10-08\n\ncreate the framework of basic backend\n\n\n\nonly framework is added, except instrformats.td, instrinfo.td and registerinfo.td.\n\nabove tablegen files are copied from https://github.com/rhysd/toy-riscv-backend. these tablegen files are uniform for riscv-compatible backend compilers.\n\n\n# 2. 2024-10-09\n\nwhen i try to compile sometest.ll with above backend, error:\n\nllvm/lib/codegen/llvmtargetmachine.cpp:47:\nvoid llvm::llvmtargetmachine::initasminfo(): assertion `mri && \"unable to create reg info\"' failed\n\n\n1\n2\n\n\n# claude explains why mc register info is needed\n\nthe reason is that mcregisterinfo is not created.\n\nthe function createtinyriscvmcregisterinfo is responsible for creating and initializing the mcregisterinfo object for your tinyriscv target.\nthis object provides essential information about the registers available in your target architecture, which is crucial for various stages of code generation and assembly.\n\nkey functions of createtinyriscvmcregisterinfo\n\n 1. register initialization:\n    it initializes the register information for the tinyriscv target, including the mapping of register names to register numbers and any special properties of the registers.\n 2. callee-saved registers:\n    the mcregisterinfo object typically includes information about which registers are callee-saved, which is important for function call conventions and stack management.\n 3. register aliases:\n    it may define any register aliases, which are alternative names or numbers for the same physical register.\n 4. register classes:\n    the function helps define register classes, which group registers with similar characteristics (e.g., general-purpose registers).\n 5. integration with llvm:\n    by registering the mcregisterinfo with the llvm target registry, it ensures that the rest of the llvm infrastructure can access and use the register information for tasks like instruction selection, register allocation, and assembly printing.\n\nexample implementation here's a typical implementation of createtinyriscvmcregisterinfo:\n\nstatic mcregisterinfo *createtinyriscvmcregisterinfo(const triple &tt) {\n  mcregisterinfo *x = new mcregisterinfo();\n  inittinyriscvmcregisterinfo(x, /* return address register number */);\n  return x;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n\n\ninittinyriscvmcregisterinfo\nthis function, usually generated by tablegen, initializes the mcregisterinfo object with the register definitions specified in your target's tablegen files.\n\nreturn address register\nthe function often takes a parameter specifying the register used for the return address, which is important for call and return instructions.\n\nby ensuring that createtinyriscvmcregisterinfo is correctly implemented and registered, you provide the necessary register information to the llvm backend, enabling it to generate and manipulate machine code effectively for your tinyriscv target.\n\n# debug\n\nspend lots of time on debugs namespace tinyriscv issue\n\nroot cause is that:\n\nwe need to include tinyriscvgenregisterinfo.inc in both tinyriscvmctargetdesc.h and tinyriscvmctargetdesc.cpp.\n\nonce for define, another for declare.\n\n\n# 3. 2024-10-12\n\nllc  -debug -debug-pass=structure -march=tinyriscv\ntinyriscv_simpletest.ll\n-o tinyriscv_addition.s\n\nfailed:\nllc: error: target does not support generation of this file type\n\n\n1\n2\n3\n4\n5\n6\n\n\ntry to solve this issue, might due to missing of addpasses in targetmachine.h/cpp.\n\nwhen subtarget is not implemented:\n\nbool runonfunction(function &f) override {\n  auto *tm = &getanalysis<targetpassconfig>().gettm<targetmachine>();\n  auto *tli = tm->getsubtargetimpl(f)->gettargetlowering();\n  return runimpl(f, *tli);\n}\n\nstatic bool runimpl(function &f, const targetlowering &tli) {\n.....\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 4. 2024-10-13\n\nforget to add getframelowering in *targetmachine.h\n\n#11 0x00007e10aebdbd7a llvm::targetlowering::lowercall(llvm::targetlowering::callloweringinfo&, llvm::smallvectorimpl<llvm::sdvalue>&) const ...targetlowering.h:4770:5\n#12 0x00007e10ad58b07b llvm::selectiondagisel::lowerarguments(llvm::function const&) ...selectiondagbuilder.cpp:11673:46\n#13 0x00007e10ad679308 llvm::selectiondagisel::selectallbasicblocks(llvm::function const&) ...codegen/selectiondag/selectiondagisel.cpp:1648:19\n\n\n1\n2\n3\n\n\nthis is the bug that when called getnumregisters() in targetlowering.h, the numregisterforvt[vt.getsimplevt().simplety] returns 0.\n\nthe numregisterforvt is not initialized in our case.\n\nthis is initialized by calling computeregisterproperties in tinyriscvtargetlowering, which does not called before solving this bug.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Learn TPU_MLIR",frontmatter:{title:"Learn TPU_MLIR",date:"2024-12-09T00:00:00.000Z",permalink:"/pages/000010/",tags:[null]},regularPath:"/02.compiler/10.learn_tpu_mlir.html",relativePath:"02.compiler/10.learn_tpu_mlir.md",key:"v-e9f07ff6",path:"/pages/000010/",headers:[{level:2,title:"SOPHGO TPU Compiler",slug:"sophgo-tpu-compiler",normalizedTitle:"sophgo tpu compiler",charIndex:2},{level:2,title:"Lowering",slug:"lowering",normalizedTitle:"lowering",charIndex:206},{level:3,title:"1. Declaration of oprator and conversion pass",slug:"_1-declaration-of-oprator-and-conversion-pass",normalizedTitle:"1. declaration of oprator and conversion pass",charIndex:658},{level:3,title:"2. Implementation of lowering",slug:"_2-implementation-of-lowering",normalizedTitle:"2. implementation of lowering",charIndex:2917},{level:3,title:"3. Lowering in ConvertTopToTPU",slug:"_3-lowering-in-converttoptotpu",normalizedTitle:"3. lowering in converttoptotpu",charIndex:6903}],headersStr:"SOPHGO TPU Compiler Lowering 1. Declaration of oprator and conversion pass 2. Implementation of lowering 3. Lowering in ConvertTopToTPU",content:'# SOPHGO TPU Compiler\n\nTwo level:\n\n 1. top pass\n\n * Canonicalize graph-node level optimization. kernel fusion: conv-relu, shape merge\n * Calibration: insert min and max to each operator for quantization\n * Lowering into tpu level according to operator tye\n\n 2. tpu pass\n\n * Canonicalize graph-node level optimization. merging requant\n * Weight reorder. reorder weights for specific operator according to chip characteristic\n * Subnet. divide net into subnets\n * Layergroup: divide net to maximize operator that can be computed in local mem\n * memassign: assign gloal memory for operators\n * Codegen. adopt flatbuffer type to generate model.\n\n\n# Lowering\n\n\n# 1. Declaration of oprator and conversion pass\n\nCode\n\n       \n// include/tpu_mlir/Dialect/Tpu/IR/TpuOps.td\nclass Tpu_ConvOp<string mnemonic, list<Trait> traits = []> : Tpu_Op<mnemonic,\n    !listconcat(traits, [SupportFuseRelu,\n    DeclareOpInterfaceMethods<TypeInterface>,\n    DeclareOpInterfaceMethods<LocalGenInterface, ["BackwardH"]>])> {\n  let summary = "convolution operator";\n\n  let description = [{\n  }];\n\n  let arguments = (ins\n    AnyTensor:$input,\n    AnyTensor:$filter,\n    AnyTensorOrNone:$bias,\n    I64ArrayAttr:$kernel_shape,\n    I64ArrayAttr:$strides,\n    I64ArrayAttr:$pads, // top,left,bottom,right\n    DefaultValuedAttr<I64Attr, "1">:$group,\n    OptionalAttr<I64ArrayAttr>:$dilations,\n    OptionalAttr<I64ArrayAttr>:$inserts,\n    DefaultValuedAttr<BoolAttr, "false">:$do_relu,\n    DefaultValuedAttr<F64Attr, "-1.0">:$relu_limit,\n    //new param\n    BoolAttr:$with_bias,\n    DefaultValuedAttr<BoolAttr, "false">:$coeff_merged,\n    DefaultValuedAttr<I64Attr, "0">:$use_3ic_optimize,\n    DefaultValuedAttr<I64Attr, "0">:$kernel_zp,\n    OptionalAttr<I64ArrayAttr>:$multiplier,\n    OptionalAttr<I64ArrayAttr>:$rshift,\n    DefaultValuedAttr<Tpu_RequantModeAttr, "tpu::RequantMode::Normal">:$quant_mode,\n    OptionalAttr<Tpu_LayerGroupAttr>:$ginfo\n  );\n\n  let results = (outs AnyTensor:$output);\n  let extraClassDeclaration = [{\n    conv_attr_t parseParam();\n  }];\n}\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\nCode ``` // include/tpu_mlir/Conversion/Passes.td def ConvertTopToTpu : Pass<"convert-top-to-tpu", "ModuleOp"> { let summary = "Convert top-level Top Ops to Tpu Ops"; let constructor = "tpu_mlir::createConvertTopToTpu()"; let dependentDialects = ["tpu_mlir::top::TopDialect", "tpu_mlir::tpu::TpuDialect"]; let options = [ Option<"mode", "mode", "std::string", /*default=*/"", "default quantization mode: INT8/BF16/F32">, Option<"qtable", "qtable", "std::string", /*default=*/"", "a table of Ops that quantized to specific mode">, Option<"chip", "chip", "std::string", /*default=*/"", "chip: cv183x/cv182x/bm1684/bm1684x">, Option<"isAsymmetric", "asymmetric", "bool", /*default=*/"false", "true for asymmetric quantization, or false for symmetric">, ]; } ```\n\n\n# 2. Implementation of lowering\n\nTopLowering is derived from mlir::OpRewritePattern\n\nEach OPLowering will be declared.\n\ninclude/tpu_mlir/Conversion/TopToTpu\n\nCode\n\n// TopLowering.h\ntemplate <typename OpTy> class TopLowering : public OpRewritePattern<OpTy> {\npublic:\n  using OpRewritePattern<OpTy>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(OpTy opTy,\n                                PatternRewriter &rewriter) const override {\n...\n  virtual void LoweringINT8(PatternRewriter &rewriter, OpTy opTy,\n                            bool asymmetric) const {\n    llvm_unreachable("Not Implemented");\n  }\n  virtual void LoweringINT4(PatternRewriter &rewriter, OpTy opTy,\n                            bool asymmetric) const {\n    LoweringINT8(rewriter, opTy, asymmetric);\n  }\n  virtual void LoweringBF16(PatternRewriter &rewriter, OpTy opTy) const {\n    llvm_unreachable("Not Implemented");\n  }\n  virtual void LoweringF16(PatternRewriter &rewriter, OpTy opTy) const {\n    llvm_unreachable("Not Implemented");\n  }\n  virtual void LoweringF32(PatternRewriter &rewriter, OpTy opTy) const {\n    llvm_unreachable("Not Implemented");\n  }\n  virtual void LoweringQuantized(PatternRewriter &rewriter, OpTy opTy) const {\n    llvm_unreachable("Not Implemented");\n  }\n                                \n}\n\n// LoweringBM1684X.h\n#define LOWERING_BM1684X(OP)                                                   \\\n  struct OP##Lowering : public TopLowering<top::OP##Op> {                      \\\n    OP##Lowering(MLIRContext *ctx) : TopLowering<top::OP##Op>(ctx) {}          \\\n    void LoweringINT8(PatternRewriter &rewriter, top::OP##Op op,               \\\n                      bool asymmetric) const override;                         \\\n    void LoweringINT4(PatternRewriter &rewriter, top::OP##Op op,               \\\n                      bool asymmetric) const override;                         \\\n    void LoweringBF16(PatternRewriter &rewriter,                               \\\n                      top::OP##Op op) const override;                          \\\n    void LoweringF16(PatternRewriter &rewriter,                                \\\n                     top::OP##Op op) const override;                           \\\n    void LoweringF32(PatternRewriter &rewriter,                                \\\n                     top::OP##Op op) const override;                           \\\n    void LoweringQuantized(PatternRewriter &rewriter,                          \\\n                           top::OP##Op op) const override;                     \\\n  };\n\nLOWERING_BM1684X(Abs)\nLOWERING_BM1684X(Add)\nLOWERING_BM1684X(AddConst)\nLOWERING_BM1684X(AvgPool)\nLOWERING_BM1684X(Cast)\nLOWERING_BM1684X(Concat)\nLOWERING_BM1684X(Conv)\nLOWERING_BM1684X(Deconv)\nLOWERING_BM1684X(Depth2Space)\nLOWERING_BM1684X(Div)\nLOWERING_BM1684X(Exp)\nLOWERING_BM1684X(Gather)\nLOWERING_BM1684X(GRU)\n\n...\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\nThen in ./lib/Conversion/TopToTpu/BM1684X/ each operator will include a *.cpp that implement LoweringFunction.\n\nCode\n\nvoid ConvLowering::LoweringF32(PatternRewriter &rewriter,\n                               top::ConvOp op) const {\n  rewriter.setInsertionPointAfter(op);\n  std::vector<Value> operands;\n  const int nInputs = op->getNumOperands();\n  for (auto i = 0; i < nInputs; ++i) {\n    operands.push_back(op->getOperand(i));\n  }\n  std::vector<NamedAttribute> attrs;\n  for (auto &attr : op->getAttrs()) {\n    attrs.push_back(attr);\n  }\n  bool with_bias = !op.getBias().getType().isa<mlir::NoneType>();\n  attrs.push_back(\n      rewriter.getNamedAttr("with_bias", rewriter.getBoolAttr(with_bias)));\n  auto newValue =\n      CreateConvOp(rewriter, op.getKernelShape().size(), op->getLoc(),\n                   op.getOutput().getType(), operands, attrs);\n  rewriter.replaceOp(op, {newValue});\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 3. Lowering in ConvertTopToTPU\n\nbm1684x::populateTopToTpuConversionPatterns(&patterns);\n\nReuse applyPatternsAndFoldGreedily\n\nlib/Conversion/TopToTPU/TopToTPUPass.cpp\n\nCode\n\nstruct ConvertTopToTpu : public ::impl::ConvertTopToTpuBase<ConvertTopToTpu> {\npublic:\n  void runOnOperation() override {\n    RewritePatternSet patterns(ctx_);\n    ConversionTarget target(*ctx_);\n    target.addLegalDialect<tpu::TpuDialect, func::FuncDialect>();\n    // no need to lowering:\n    // Qi Note, add legal op, only NoneOp, InputOp, weightOp\n    target.addLegalOp<top::InputOp, top::WeightOp, top::NoneOp>();\n    if (module::isBM1684XFamily()) {\n      // Qi Note: Call LoweringBM1684X.cpp\n      // AddLowering to the patterns\n      bm1684x::populateTopToTpuConversionPatterns(&patterns);\n    } else if (module::isBM1684Family()) {\n      bm1684::populateTopToTpuConversionPatterns(&patterns);\n    } else if (module::isCV18xx()) {\n      cv18xx::populateTopToTpuConversionPatterns(&patterns);\n    } else {\n      llvm_unreachable("Not Implemented");\n    }\n    auto config = GreedyRewriteConfig();\n    config.maxIterations = 0; // apply each pattern only once.\n    // Qi Node: Apply pattern into module\n    // applyPatternsAndFoldGreedily is provided by MLIR\n    applyPatternsAndFoldGreedily(module_, std::move(patterns), config);\n    patterns.clear();\n    // \n    patterns.add<ForwardTypePattern<tpu::ReshapeOp>>(ctx_);\n    applyPatternsAndFoldGreedily(module_, std::move(patterns));\n    cast_process();\n    relu_process();\n    module::updateModuleTypes();\n    module::setState(module::State::TPU_LOWERED);\n  }\n}\n\n// Qi Node: this function will be called by\n// include/tpu_mlir/Conversion/Passes.td constructor\n// it will create the pass of converting top operator to tpu operator\nstd::unique_ptr<Pass> createConvertTopToTpu() {\n  return std::make_unique<ConvertTopToTpu>();\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\nCode\n\nvoid populateTopToTpuConversionPatterns(RewritePatternSet *patterns) {\n  patterns->add<\n      // clang-format off\n      AbsLowering,\n      AddLowering,\n      AddConstLowering,\n      AvgPoolLowering,\n      CastLowering,\n      ConcatLowering,\n      ConvLowering,\n      DeconvLowering,\n      Depth2SpaceLowering,\n      DivLowering,\n      ExpLowering,\n      GatherLowering,\n...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nThis process is similar to LLVM pass.',normalizedContent:'# sophgo tpu compiler\n\ntwo level:\n\n 1. top pass\n\n * canonicalize graph-node level optimization. kernel fusion: conv-relu, shape merge\n * calibration: insert min and max to each operator for quantization\n * lowering into tpu level according to operator tye\n\n 2. tpu pass\n\n * canonicalize graph-node level optimization. merging requant\n * weight reorder. reorder weights for specific operator according to chip characteristic\n * subnet. divide net into subnets\n * layergroup: divide net to maximize operator that can be computed in local mem\n * memassign: assign gloal memory for operators\n * codegen. adopt flatbuffer type to generate model.\n\n\n# lowering\n\n\n# 1. declaration of oprator and conversion pass\n\ncode\n\n       \n// include/tpu_mlir/dialect/tpu/ir/tpuops.td\nclass tpu_convop<string mnemonic, list<trait> traits = []> : tpu_op<mnemonic,\n    !listconcat(traits, [supportfuserelu,\n    declareopinterfacemethods<typeinterface>,\n    declareopinterfacemethods<localgeninterface, ["backwardh"]>])> {\n  let summary = "convolution operator";\n\n  let description = [{\n  }];\n\n  let arguments = (ins\n    anytensor:$input,\n    anytensor:$filter,\n    anytensorornone:$bias,\n    i64arrayattr:$kernel_shape,\n    i64arrayattr:$strides,\n    i64arrayattr:$pads, // top,left,bottom,right\n    defaultvaluedattr<i64attr, "1">:$group,\n    optionalattr<i64arrayattr>:$dilations,\n    optionalattr<i64arrayattr>:$inserts,\n    defaultvaluedattr<boolattr, "false">:$do_relu,\n    defaultvaluedattr<f64attr, "-1.0">:$relu_limit,\n    //new param\n    boolattr:$with_bias,\n    defaultvaluedattr<boolattr, "false">:$coeff_merged,\n    defaultvaluedattr<i64attr, "0">:$use_3ic_optimize,\n    defaultvaluedattr<i64attr, "0">:$kernel_zp,\n    optionalattr<i64arrayattr>:$multiplier,\n    optionalattr<i64arrayattr>:$rshift,\n    defaultvaluedattr<tpu_requantmodeattr, "tpu::requantmode::normal">:$quant_mode,\n    optionalattr<tpu_layergroupattr>:$ginfo\n  );\n\n  let results = (outs anytensor:$output);\n  let extraclassdeclaration = [{\n    conv_attr_t parseparam();\n  }];\n}\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\ncode ``` // include/tpu_mlir/conversion/passes.td def converttoptotpu : pass<"convert-top-to-tpu", "moduleop"> { let summary = "convert top-level top ops to tpu ops"; let constructor = "tpu_mlir::createconverttoptotpu()"; let dependentdialects = ["tpu_mlir::top::topdialect", "tpu_mlir::tpu::tpudialect"]; let options = [ option<"mode", "mode", "std::string", /*default=*/"", "default quantization mode: int8/bf16/f32">, option<"qtable", "qtable", "std::string", /*default=*/"", "a table of ops that quantized to specific mode">, option<"chip", "chip", "std::string", /*default=*/"", "chip: cv183x/cv182x/bm1684/bm1684x">, option<"isasymmetric", "asymmetric", "bool", /*default=*/"false", "true for asymmetric quantization, or false for symmetric">, ]; } ```\n\n\n# 2. implementation of lowering\n\ntoplowering is derived from mlir::oprewritepattern\n\neach oplowering will be declared.\n\ninclude/tpu_mlir/conversion/toptotpu\n\ncode\n\n// toplowering.h\ntemplate <typename opty> class toplowering : public oprewritepattern<opty> {\npublic:\n  using oprewritepattern<opty>::oprewritepattern;\n\n  logicalresult matchandrewrite(opty opty,\n                                patternrewriter &rewriter) const override {\n...\n  virtual void loweringint8(patternrewriter &rewriter, opty opty,\n                            bool asymmetric) const {\n    llvm_unreachable("not implemented");\n  }\n  virtual void loweringint4(patternrewriter &rewriter, opty opty,\n                            bool asymmetric) const {\n    loweringint8(rewriter, opty, asymmetric);\n  }\n  virtual void loweringbf16(patternrewriter &rewriter, opty opty) const {\n    llvm_unreachable("not implemented");\n  }\n  virtual void loweringf16(patternrewriter &rewriter, opty opty) const {\n    llvm_unreachable("not implemented");\n  }\n  virtual void loweringf32(patternrewriter &rewriter, opty opty) const {\n    llvm_unreachable("not implemented");\n  }\n  virtual void loweringquantized(patternrewriter &rewriter, opty opty) const {\n    llvm_unreachable("not implemented");\n  }\n                                \n}\n\n// loweringbm1684x.h\n#define lowering_bm1684x(op)                                                   \\\n  struct op##lowering : public toplowering<top::op##op> {                      \\\n    op##lowering(mlircontext *ctx) : toplowering<top::op##op>(ctx) {}          \\\n    void loweringint8(patternrewriter &rewriter, top::op##op op,               \\\n                      bool asymmetric) const override;                         \\\n    void loweringint4(patternrewriter &rewriter, top::op##op op,               \\\n                      bool asymmetric) const override;                         \\\n    void loweringbf16(patternrewriter &rewriter,                               \\\n                      top::op##op op) const override;                          \\\n    void loweringf16(patternrewriter &rewriter,                                \\\n                     top::op##op op) const override;                           \\\n    void loweringf32(patternrewriter &rewriter,                                \\\n                     top::op##op op) const override;                           \\\n    void loweringquantized(patternrewriter &rewriter,                          \\\n                           top::op##op op) const override;                     \\\n  };\n\nlowering_bm1684x(abs)\nlowering_bm1684x(add)\nlowering_bm1684x(addconst)\nlowering_bm1684x(avgpool)\nlowering_bm1684x(cast)\nlowering_bm1684x(concat)\nlowering_bm1684x(conv)\nlowering_bm1684x(deconv)\nlowering_bm1684x(depth2space)\nlowering_bm1684x(div)\nlowering_bm1684x(exp)\nlowering_bm1684x(gather)\nlowering_bm1684x(gru)\n\n...\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n\n\nthen in ./lib/conversion/toptotpu/bm1684x/ each operator will include a *.cpp that implement loweringfunction.\n\ncode\n\nvoid convlowering::loweringf32(patternrewriter &rewriter,\n                               top::convop op) const {\n  rewriter.setinsertionpointafter(op);\n  std::vector<value> operands;\n  const int ninputs = op->getnumoperands();\n  for (auto i = 0; i < ninputs; ++i) {\n    operands.push_back(op->getoperand(i));\n  }\n  std::vector<namedattribute> attrs;\n  for (auto &attr : op->getattrs()) {\n    attrs.push_back(attr);\n  }\n  bool with_bias = !op.getbias().gettype().isa<mlir::nonetype>();\n  attrs.push_back(\n      rewriter.getnamedattr("with_bias", rewriter.getboolattr(with_bias)));\n  auto newvalue =\n      createconvop(rewriter, op.getkernelshape().size(), op->getloc(),\n                   op.getoutput().gettype(), operands, attrs);\n  rewriter.replaceop(op, {newvalue});\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 3. lowering in converttoptotpu\n\nbm1684x::populatetoptotpuconversionpatterns(&patterns);\n\nreuse applypatternsandfoldgreedily\n\nlib/conversion/toptotpu/toptotpupass.cpp\n\ncode\n\nstruct converttoptotpu : public ::impl::converttoptotpubase<converttoptotpu> {\npublic:\n  void runonoperation() override {\n    rewritepatternset patterns(ctx_);\n    conversiontarget target(*ctx_);\n    target.addlegaldialect<tpu::tpudialect, func::funcdialect>();\n    // no need to lowering:\n    // qi note, add legal op, only noneop, inputop, weightop\n    target.addlegalop<top::inputop, top::weightop, top::noneop>();\n    if (module::isbm1684xfamily()) {\n      // qi note: call loweringbm1684x.cpp\n      // addlowering to the patterns\n      bm1684x::populatetoptotpuconversionpatterns(&patterns);\n    } else if (module::isbm1684family()) {\n      bm1684::populatetoptotpuconversionpatterns(&patterns);\n    } else if (module::iscv18xx()) {\n      cv18xx::populatetoptotpuconversionpatterns(&patterns);\n    } else {\n      llvm_unreachable("not implemented");\n    }\n    auto config = greedyrewriteconfig();\n    config.maxiterations = 0; // apply each pattern only once.\n    // qi node: apply pattern into module\n    // applypatternsandfoldgreedily is provided by mlir\n    applypatternsandfoldgreedily(module_, std::move(patterns), config);\n    patterns.clear();\n    // \n    patterns.add<forwardtypepattern<tpu::reshapeop>>(ctx_);\n    applypatternsandfoldgreedily(module_, std::move(patterns));\n    cast_process();\n    relu_process();\n    module::updatemoduletypes();\n    module::setstate(module::state::tpu_lowered);\n  }\n}\n\n// qi node: this function will be called by\n// include/tpu_mlir/conversion/passes.td constructor\n// it will create the pass of converting top operator to tpu operator\nstd::unique_ptr<pass> createconverttoptotpu() {\n  return std::make_unique<converttoptotpu>();\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\ncode\n\nvoid populatetoptotpuconversionpatterns(rewritepatternset *patterns) {\n  patterns->add<\n      // clang-format off\n      abslowering,\n      addlowering,\n      addconstlowering,\n      avgpoollowering,\n      castlowering,\n      concatlowering,\n      convlowering,\n      deconvlowering,\n      depth2spacelowering,\n      divlowering,\n      explowering,\n      gatherlowering,\n...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nthis process is similar to llvm pass.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Auto Differentiation in Compiler",frontmatter:{title:"Auto Differentiation in Compiler",date:"2024-12-25T00:00:00.000Z",permalink:"/pages/000012/",tags:[null]},regularPath:"/02.compiler/12.auto_diff.html",relativePath:"02.compiler/12.auto_diff.md",key:"v-e0b126ce",path:"/pages/000012/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"MLIR Open Meeting Notes The Torch MLIR Project",frontmatter:{title:"MLIR Open Meeting Notes The Torch MLIR Project",date:"2025-03-04T00:00:00.000Z",permalink:"/pages/000013/",tags:[null]},regularPath:"/02.compiler/13.mlir_notes_01.html",relativePath:"02.compiler/13.mlir_notes_01.md",key:"v-49a96495",path:"/pages/000013/",headers:[{level:2,title:"[1] MLIR Open Meeting 2021-10-7: The Torch MLIR Project",slug:"_1-mlir-open-meeting-2021-10-7-the-torch-mlir-project",normalizedTitle:"[1] mlir open meeting 2021-10-7: the torch mlir project",charIndex:68},{level:3,title:"Torch MLIR Project and Op Autogeneration",slug:"torch-mlir-project-and-op-autogeneration",normalizedTitle:"torch mlir project and op autogeneration",charIndex:128},{level:3,title:"ODS (Operation Definition Specification) in Torch MLIR",slug:"ods-operation-definition-specification-in-torch-mlir",normalizedTitle:"ods (operation definition specification) in torch mlir",charIndex:1947},{level:3,title:"3. Torch Dialect Transformations",slug:"_3-torch-dialect-transformations",normalizedTitle:"3. torch dialect transformations",charIndex:7076}],headersStr:"[1] MLIR Open Meeting 2021-10-7: The Torch MLIR Project Torch MLIR Project and Op Autogeneration ODS (Operation Definition Specification) in Torch MLIR 3. Torch Dialect Transformations",content:'Sources\n\n * MLIR Open Meeting 2021-10-7: The Torch MLIR Project\n\n\n# [1] MLIR Open Meeting 2021-10-7: The Torch MLIR Project\n\n\n# Torch MLIR Project and Op Autogeneration\n\n\n\nThe Torch MLIR project aims to bridge PyTorch and MLIR ecosystems, automating op generation to facilitate efficient lowering of PyTorch programs into MLIR.\n\n# 1.Op Autogeneration Process\n\n * Source of Ops: Extracted from the Torch registry at runtime (rather than native_functions.yaml).\n * Example: torch.aten.relu\n * MLIR Op Name: torch.aten.relu\n * Definition Name: Torch_AtenReluOp\n * Extracted Attributes:\n   * Namespace: aten\n   * Unqualified Name: relu\n   * Is mutable: False\n   * Input argument: self (Tensor)\n   * Return type: Tensor\n\n# 2.ODS (Operation Definition Specification) Autogeneration\n\n * Uses the Torch registry information to auto-generate ODS entries.\n * Example ODS for torch.aten.relu:\n\ndef Torch_AtenReluOp : Torch_Op<"aten.relu", [\n    AllowsTypeRefinement,\n    HasValueSemantics\n]> {\n    let summary = "Generated op for `aten::relu : (Tensor) -> (Tensor)`";\n    let arguments = (ins AnyTorchTensorType:$self);\n    let results = (outs AnyTorchTensorType:$result);\n    let assemblyFormat = "$self attr-dict `:` type($self) `->` type($result)";\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 3.Key Traits:\n\n * AllowsTypeRefinement: Supports type propagation.\n * HasValueSemantics: Indicates operations that return new tensors instead of modifying in-place.\n\n# 4.Handling Mutability (_ In-Place Ops)\n\n * PyTorch has many in-place ops (relu_, add_).\n * The Torch registry contains aliasing information that marks them as mutable.\n * This allows torch.aten.add_ to be auto-generated with correct aliasing and mutability constraints.\n\n# 5.Benefits of Op Autogeneration\n\n * Consistency: All ops align with the PyTorch op registry.\n * Minimal Maintenance: Automatically updates as PyTorch evolves.\n * Correctness: Ensures accurate aliasing, mutability, and type properties.\n\n\n# ODS (Operation Definition Specification) in Torch MLIR\n\nIn the Torch MLIR project, ODS (Operation Definition Specification) is a crucial mechanism for defining MLIR operations in a structured way.\n\nIt allows automated generation of boilerplate code, ensuring that operations are correctly specified, optimized, and maintainable.\n\n# 1. Example ODS Definition for torch.aten.relu\n\nFrom the presentation, the following ODS definition was automatically generated for the torch.aten.relu operation:\n\ndef Torch_AtenReluOp : Torch_Op<"aten.relu", [\n    AllowsTypeRefinement,\n    HasValueSemantics\n]> {\n    let summary = "Generated op for `aten::relu : (Tensor) -> (Tensor)`";\n    let arguments = (ins AnyTorchTensorType:$self);\n    let results = (outs AnyTorchTensorType:$result);\n    let assemblyFormat = "$self attr-dict `:` type($self) `->` type($result)";\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 2. Breakdown of ODS Components\n\nA. Operation Definition Header\n\ndef Torch_AtenReluOp : Torch_Op<"aten.relu", [\n    AllowsTypeRefinement,\n    HasValueSemantics\n]> {\n\n\n1\n2\n3\n4\n\n * def Torch_AtenReluOp → Defines a new operation named Torch_AtenReluOp.\n * Torch_Op<"aten.relu", [...]> → This specifies:\n   * "aten.relu": The op name as it appears in MLIR.\n   * Traits ([...]): Additional properties assigned to the operation.\n\nB. Traits (Operation Properties)\n\nAllowsTypeRefinement,\nHasValueSemantics\n\n\n1\n2\n\n * AllowsTypeRefinement → Allows the operation\'s type to be refined in later transformations.\n   * Example: If the tensor initially has an unknown dtype (unk), it can later be inferred to f32 or i32 based on usage.\n * HasValueSemantics → Indicates that the operation does not modify its inputs but instead produces a new output tensor.\n   * Example: relu(x) returns a new tensor, while relu_(x) modifies x in-place.\n\nC. Summary\n\nlet summary = "Generated op for `aten::relu : (Tensor) -> (Tensor)`";\n\n\n1\n\n * Provides a description of what the operation does.\n * Helps with documentation and debugging.\n\nD. Arguments (Inputs)\n\nlet arguments = (ins AnyTorchTensorType:$self);\n\n\n1\n\n * (ins ...) → Specifies the input types.\n * AnyTorchTensorType:$self →\n   * This operation takes a single argument named $self (representing the input tensor).\n   * The tensor type is generalized (AnyTorchTensorType), meaning it can be any valid tensor in PyTorch.\n\nE. Results (Outputs)\n\nlet results = (outs AnyTorchTensorType:$result);\n\n\n1\n\n * (outs ...) → Specifies the return type.\n * AnyTorchTensorType:$result →\n   * The output is also a tensor (same type as input).\n   * $result represents the output variable.\n\nF. Assembly Format\n\nlet assemblyFormat = "$self attr-dict `:` type($self) `->` type($result)";\n\n\n1\n\n * Defines how the operation should be printed in MLIR\'s textual format.\n\nExample MLIR Representation:\n\n%result = torch.aten.relu %self : !torch.tensor<*xf32> -> !torch.tensor<*xf32>\n\n\n1\n\n * %result stores the output.\n * torch.aten.relu is the op name.\n * %self is the input tensor.\n * *!torch.tensor<xf32> is the type of the input tensor.\n * *!torch.tensor<xf32> is the type of the output tensor.\n\n# 3. ODS for In-Place Operations (e.g., relu_)\n\nFor in-place operations like torch.aten.relu_, the ODS differs slightly:\n\ndef Torch_AtenRelu_InplaceOp : Torch_Op<"aten.relu_", [\n    AllowsTypeRefinement\n]> {\n    let summary = "Generated op for `aten::relu_ : (Tensor) -> (Tensor)`";\n    let arguments = (ins AnyTorchTensorType:$self);\n    let results = (outs AnyTorchTensorType:$self);\n    let assemblyFormat = "$self attr-dict `:` type($self) `->` type($self)";\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nKey Differences\n\n * No HasValueSemantics Trait\n   * Since this op modifies self in-place, it does not have value semantics.\n * Results Alias Input\n   * Output ($self) is the same as the input ($self), meaning no new tensor is allocated.\n\nExample MLIR Representation\n\n%self = torch.aten.relu_ %self : !torch.tensor<*xf32> -> !torch.tensor<*xf32>\n\n\n1\n\n * Here, the input %self is updated in-place.\n\n# 4. Benefits of ODS-Based Op Autogeneration\n\n * Automatically Syncs with PyTorch\n   * Since the ops are derived from the Torch registry, they always stay up-to-date with PyTorch.\n * Eliminates Manual Specification\n   * Instead of manually defining hundreds of ops, a script can generate ODS files automatically.\n\n# 5. Future Enhancements\n\n * More Ops → Expanding the ODS-based system to cover more PyTorch ops.\n * More Traits → Additional MLIR traits like Pure for mathematical functions.\n * Better Type Inference → Enhancing AllowsTypeRefinement to refine tensor shapes dynamically.\n\n# 6. Summary\n\nODS COMPONENT     DESCRIPTION\nOp Name           torch.aten.relu (MLIR name)\nBase Class        Torch_Op (inherits from PyTorch MLIR ops)\nTraits            AllowsTypeRefinement, HasValueSemantics\nInputs            AnyTorchTensorType:$self\nOutputs           AnyTorchTensorType:$result\nAssembly Format   Defines textual representation\n\nExample: MLIR Representation\n\n%result = torch.aten.relu %self : !torch.tensor<*xf32> -> !torch.tensor<*xf32>\n\n\n1\n\n\nThis ODS-based framework automates MLIR integration for PyTorch ops, making it scalable, maintainable, and accurate.\n\n\n# 3. Torch Dialect Transformations\n\n\n\n# 1. ReduceOpVariants\n\nThis transformation aims to simplify the IR by reducing multiple similar operations into a smaller, canonical set of operations.\n\nIn PyTorch and MLIR, there can be multiple variants of the same operation that differ slightly in their implementation or behavior (for example, operations that do the same thing but have in-place variants, like add and add_).\n\nBy reducing these to a canonical form, it simplifies the IR, which can make subsequent optimizations and transformations easier and more effective.\n\n# 2. MaximizeValueSemantics\n\nIn programming, "value semantics" refer to handling data in such a way that operations on data do not change the original data but rather return new data based on operations.\n\nThis transformation tries to convert as much of the program as possible to use value semantics.\n\nThis is beneficial because it can help prevent side effects and makes the program easier to reason about, debug, and optimize, since data flows can be more predictable and less intertwined.\n\n# 3. RefineTypes\n\nType refinement involves propagating more precise type information throughout the program.\n\nThis can include refining data types from broader categories (like a tensor) to more specific ones (like a tensor of floats), or adding dimensionality and shape information where possible.\n\nImproved type information helps with optimizations like loop unrolling, vectorization, and specialized kernel generation, which can significantly impact performance especially on hardware that benefits from such optimizations.\n\n# 4. GlobalizeObjectGraph\n\nTorchScript, which is used to convert PyTorch programs into a form that can be optimized and executed by MLIR, organizes code as a graph of objects (similar to the objects in object-oriented programming).\n\nEach object can contain methods and properties that reference other objects.\n\nThe GlobalizeObjectGraph transformation converts this object graph into a flat list of global functions and variables\n\nThis transformation simplifies the program structure, making it more amenable to standard compiler optimizations that operate on a global or function scope rather than having to navigate complex object interdependencies.\n\n# 5. Summary and Benefits\n\nThese transformations collectively aim to streamline the processing of PyTorch models into an efficient, standardized format suitable for backend optimizations and lowering to target hardware.\n\nBy reducing operation variants, maximizing value semantics, refining types, and globalizing the object graph, the torch dialect in MLIR can better optimize and execute PyTorch programs, potentially improving performance and reducing runtime overhead.\n\nThese are crucial for deploying machine learning models in resource-constrained environments or where high performance is required.',normalizedContent:'sources\n\n * mlir open meeting 2021-10-7: the torch mlir project\n\n\n# [1] mlir open meeting 2021-10-7: the torch mlir project\n\n\n# torch mlir project and op autogeneration\n\n\n\nthe torch mlir project aims to bridge pytorch and mlir ecosystems, automating op generation to facilitate efficient lowering of pytorch programs into mlir.\n\n# 1.op autogeneration process\n\n * source of ops: extracted from the torch registry at runtime (rather than native_functions.yaml).\n * example: torch.aten.relu\n * mlir op name: torch.aten.relu\n * definition name: torch_atenreluop\n * extracted attributes:\n   * namespace: aten\n   * unqualified name: relu\n   * is mutable: false\n   * input argument: self (tensor)\n   * return type: tensor\n\n# 2.ods (operation definition specification) autogeneration\n\n * uses the torch registry information to auto-generate ods entries.\n * example ods for torch.aten.relu:\n\ndef torch_atenreluop : torch_op<"aten.relu", [\n    allowstyperefinement,\n    hasvaluesemantics\n]> {\n    let summary = "generated op for `aten::relu : (tensor) -> (tensor)`";\n    let arguments = (ins anytorchtensortype:$self);\n    let results = (outs anytorchtensortype:$result);\n    let assemblyformat = "$self attr-dict `:` type($self) `->` type($result)";\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 3.key traits:\n\n * allowstyperefinement: supports type propagation.\n * hasvaluesemantics: indicates operations that return new tensors instead of modifying in-place.\n\n# 4.handling mutability (_ in-place ops)\n\n * pytorch has many in-place ops (relu_, add_).\n * the torch registry contains aliasing information that marks them as mutable.\n * this allows torch.aten.add_ to be auto-generated with correct aliasing and mutability constraints.\n\n# 5.benefits of op autogeneration\n\n * consistency: all ops align with the pytorch op registry.\n * minimal maintenance: automatically updates as pytorch evolves.\n * correctness: ensures accurate aliasing, mutability, and type properties.\n\n\n# ods (operation definition specification) in torch mlir\n\nin the torch mlir project, ods (operation definition specification) is a crucial mechanism for defining mlir operations in a structured way.\n\nit allows automated generation of boilerplate code, ensuring that operations are correctly specified, optimized, and maintainable.\n\n# 1. example ods definition for torch.aten.relu\n\nfrom the presentation, the following ods definition was automatically generated for the torch.aten.relu operation:\n\ndef torch_atenreluop : torch_op<"aten.relu", [\n    allowstyperefinement,\n    hasvaluesemantics\n]> {\n    let summary = "generated op for `aten::relu : (tensor) -> (tensor)`";\n    let arguments = (ins anytorchtensortype:$self);\n    let results = (outs anytorchtensortype:$result);\n    let assemblyformat = "$self attr-dict `:` type($self) `->` type($result)";\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 2. breakdown of ods components\n\na. operation definition header\n\ndef torch_atenreluop : torch_op<"aten.relu", [\n    allowstyperefinement,\n    hasvaluesemantics\n]> {\n\n\n1\n2\n3\n4\n\n * def torch_atenreluop → defines a new operation named torch_atenreluop.\n * torch_op<"aten.relu", [...]> → this specifies:\n   * "aten.relu": the op name as it appears in mlir.\n   * traits ([...]): additional properties assigned to the operation.\n\nb. traits (operation properties)\n\nallowstyperefinement,\nhasvaluesemantics\n\n\n1\n2\n\n * allowstyperefinement → allows the operation\'s type to be refined in later transformations.\n   * example: if the tensor initially has an unknown dtype (unk), it can later be inferred to f32 or i32 based on usage.\n * hasvaluesemantics → indicates that the operation does not modify its inputs but instead produces a new output tensor.\n   * example: relu(x) returns a new tensor, while relu_(x) modifies x in-place.\n\nc. summary\n\nlet summary = "generated op for `aten::relu : (tensor) -> (tensor)`";\n\n\n1\n\n * provides a description of what the operation does.\n * helps with documentation and debugging.\n\nd. arguments (inputs)\n\nlet arguments = (ins anytorchtensortype:$self);\n\n\n1\n\n * (ins ...) → specifies the input types.\n * anytorchtensortype:$self →\n   * this operation takes a single argument named $self (representing the input tensor).\n   * the tensor type is generalized (anytorchtensortype), meaning it can be any valid tensor in pytorch.\n\ne. results (outputs)\n\nlet results = (outs anytorchtensortype:$result);\n\n\n1\n\n * (outs ...) → specifies the return type.\n * anytorchtensortype:$result →\n   * the output is also a tensor (same type as input).\n   * $result represents the output variable.\n\nf. assembly format\n\nlet assemblyformat = "$self attr-dict `:` type($self) `->` type($result)";\n\n\n1\n\n * defines how the operation should be printed in mlir\'s textual format.\n\nexample mlir representation:\n\n%result = torch.aten.relu %self : !torch.tensor<*xf32> -> !torch.tensor<*xf32>\n\n\n1\n\n * %result stores the output.\n * torch.aten.relu is the op name.\n * %self is the input tensor.\n * *!torch.tensor<xf32> is the type of the input tensor.\n * *!torch.tensor<xf32> is the type of the output tensor.\n\n# 3. ods for in-place operations (e.g., relu_)\n\nfor in-place operations like torch.aten.relu_, the ods differs slightly:\n\ndef torch_atenrelu_inplaceop : torch_op<"aten.relu_", [\n    allowstyperefinement\n]> {\n    let summary = "generated op for `aten::relu_ : (tensor) -> (tensor)`";\n    let arguments = (ins anytorchtensortype:$self);\n    let results = (outs anytorchtensortype:$self);\n    let assemblyformat = "$self attr-dict `:` type($self) `->` type($self)";\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nkey differences\n\n * no hasvaluesemantics trait\n   * since this op modifies self in-place, it does not have value semantics.\n * results alias input\n   * output ($self) is the same as the input ($self), meaning no new tensor is allocated.\n\nexample mlir representation\n\n%self = torch.aten.relu_ %self : !torch.tensor<*xf32> -> !torch.tensor<*xf32>\n\n\n1\n\n * here, the input %self is updated in-place.\n\n# 4. benefits of ods-based op autogeneration\n\n * automatically syncs with pytorch\n   * since the ops are derived from the torch registry, they always stay up-to-date with pytorch.\n * eliminates manual specification\n   * instead of manually defining hundreds of ops, a script can generate ods files automatically.\n\n# 5. future enhancements\n\n * more ops → expanding the ods-based system to cover more pytorch ops.\n * more traits → additional mlir traits like pure for mathematical functions.\n * better type inference → enhancing allowstyperefinement to refine tensor shapes dynamically.\n\n# 6. summary\n\nods component     description\nop name           torch.aten.relu (mlir name)\nbase class        torch_op (inherits from pytorch mlir ops)\ntraits            allowstyperefinement, hasvaluesemantics\ninputs            anytorchtensortype:$self\noutputs           anytorchtensortype:$result\nassembly format   defines textual representation\n\nexample: mlir representation\n\n%result = torch.aten.relu %self : !torch.tensor<*xf32> -> !torch.tensor<*xf32>\n\n\n1\n\n\nthis ods-based framework automates mlir integration for pytorch ops, making it scalable, maintainable, and accurate.\n\n\n# 3. torch dialect transformations\n\n\n\n# 1. reduceopvariants\n\nthis transformation aims to simplify the ir by reducing multiple similar operations into a smaller, canonical set of operations.\n\nin pytorch and mlir, there can be multiple variants of the same operation that differ slightly in their implementation or behavior (for example, operations that do the same thing but have in-place variants, like add and add_).\n\nby reducing these to a canonical form, it simplifies the ir, which can make subsequent optimizations and transformations easier and more effective.\n\n# 2. maximizevaluesemantics\n\nin programming, "value semantics" refer to handling data in such a way that operations on data do not change the original data but rather return new data based on operations.\n\nthis transformation tries to convert as much of the program as possible to use value semantics.\n\nthis is beneficial because it can help prevent side effects and makes the program easier to reason about, debug, and optimize, since data flows can be more predictable and less intertwined.\n\n# 3. refinetypes\n\ntype refinement involves propagating more precise type information throughout the program.\n\nthis can include refining data types from broader categories (like a tensor) to more specific ones (like a tensor of floats), or adding dimensionality and shape information where possible.\n\nimproved type information helps with optimizations like loop unrolling, vectorization, and specialized kernel generation, which can significantly impact performance especially on hardware that benefits from such optimizations.\n\n# 4. globalizeobjectgraph\n\ntorchscript, which is used to convert pytorch programs into a form that can be optimized and executed by mlir, organizes code as a graph of objects (similar to the objects in object-oriented programming).\n\neach object can contain methods and properties that reference other objects.\n\nthe globalizeobjectgraph transformation converts this object graph into a flat list of global functions and variables\n\nthis transformation simplifies the program structure, making it more amenable to standard compiler optimizations that operate on a global or function scope rather than having to navigate complex object interdependencies.\n\n# 5. summary and benefits\n\nthese transformations collectively aim to streamline the processing of pytorch models into an efficient, standardized format suitable for backend optimizations and lowering to target hardware.\n\nby reducing operation variants, maximizing value semantics, refining types, and globalizing the object graph, the torch dialect in mlir can better optimize and execute pytorch programs, potentially improving performance and reducing runtime overhead.\n\nthese are crucial for deploying machine learning models in resource-constrained environments or where high performance is required.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Learn TVM",frontmatter:{title:"Learn TVM",date:"2024-12-08T00:00:00.000Z",permalink:"/pages/000009/",tags:[null]},regularPath:"/02.compiler/09.learn_tvm_1.html",relativePath:"02.compiler/09.learn_tvm_1.md",key:"v-0d06efa2",path:"/pages/000009/",headers:[{level:2,title:"0. Feel the flow of TVM compilation",slug:"_0-feel-the-flow-of-tvm-compilation",normalizedTitle:"0. feel the flow of tvm compilation",charIndex:45},{level:3,title:"0.1 Model Parsing and Relay IR Construction",slug:"_0-1-model-parsing-and-relay-ir-construction",normalizedTitle:"0.1 model parsing and relay ir construction",charIndex:3525},{level:3,title:"0.2 High-Level Optimizations in Relay",slug:"_0-2-high-level-optimizations-in-relay",normalizedTitle:"0.2 high-level optimizations in relay",charIndex:3953},{level:3,title:"0.3. Lowering to Tensor Expression (TE)",slug:"_0-3-lowering-to-tensor-expression-te",normalizedTitle:"0.3. lowering to tensor expression (te)",charIndex:4499},{level:3,title:"0.4. Scheduling in TE",slug:"_0-4-scheduling-in-te",normalizedTitle:"0.4. scheduling in te",charIndex:5037},{level:3,title:"0.5. Lowering to TIR",slug:"_0-5-lowering-to-tir",normalizedTitle:"0.5. lowering to tir",charIndex:5651},{level:3,title:"0.6. Code Generation",slug:"_0-6-code-generation",normalizedTitle:"0.6. code generation",charIndex:6010},{level:3,title:"0.7. Final Compilation and Deployment",slug:"_0-7-final-compilation-and-deployment",normalizedTitle:"0.7. final compilation and deployment",charIndex:6736},{level:2,title:"1. Model Parsing and Relay IR Construction",slug:"_1-model-parsing-and-relay-ir-construction",normalizedTitle:"1. model parsing and relay ir construction",charIndex:7104},{level:3,title:"1.1 Graph-Level Optimizations",slug:"_1-1-graph-level-optimizations",normalizedTitle:"1.1 graph-level optimizations",charIndex:7541},{level:3,title:"1.2 Data Layout Transformations",slug:"_1-2-data-layout-transformations",normalizedTitle:"1.2 data layout transformations",charIndex:8668},{level:3,title:"1.3 Quantization and Precision Management",slug:"_1-3-quantization-and-precision-management",normalizedTitle:"1.3 quantization and precision management",charIndex:9271},{level:3,title:"1.4 Automatic Differentiation",slug:"_1-4-automatic-differentiation",normalizedTitle:"1.4 automatic differentiation",charIndex:9746},{level:3,title:"1.5 High-Level Hardware-Aware Optimizations",slug:"_1-5-high-level-hardware-aware-optimizations",normalizedTitle:"1.5 high-level hardware-aware optimizations",charIndex:10011},{level:3,title:"1.6 Device Placement",slug:"_1-6-device-placement",normalizedTitle:"1.6 device placement",charIndex:10494},{level:3,title:"1.7 Meta-Pass Management",slug:"_1-7-meta-pass-management",normalizedTitle:"1.7 meta-pass management",charIndex:10864},{level:2,title:"2 Lowering to Tensor Expression (TE), Scheduing in TE and Lowering into TIR",slug:"_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir",normalizedTitle:"2 lowering to tensor expression (te), scheduing in te and lowering into tir",charIndex:11263},{level:3,title:"2.1 Converting Relay IR to Tensor Expression (TE)",slug:"_2-1-converting-relay-ir-to-tensor-expression-te",normalizedTitle:"2.1 converting relay ir to tensor expression (te)",charIndex:11717},{level:3,title:"2.2 Abstraction of Computation in Tensor Expression (TE)",slug:"_2-2-abstraction-of-computation-in-tensor-expression-te",normalizedTitle:"2.2 abstraction of computation in tensor expression (te)",charIndex:12555},{level:3,title:"2.3 Scheduling in Tensor Expression",slug:"_2-3-scheduling-in-tensor-expression",normalizedTitle:"2.3 scheduling in tensor expression",charIndex:13303},{level:3,title:"2.4 Constructing Low-Level Tensor IR (TIR)",slug:"_2-4-constructing-low-level-tensor-ir-tir",normalizedTitle:"2.4 constructing low-level tensor ir (tir)",charIndex:14210},{level:3,title:"2.5 Device-Specific Optimizations",slug:"_2-5-device-specific-optimizations",normalizedTitle:"2.5 device-specific optimizations",charIndex:14981},{level:2,title:"3. Code Generation",slug:"_3-code-generation",normalizedTitle:"3. code generation",charIndex:16203},{level:3,title:"3.1 GPU Code Generation",slug:"_3-1-gpu-code-generation",normalizedTitle:"3.1 gpu code generation",charIndex:16226},{level:3,title:"3.2. Kernel Construction",slug:"_3-2-kernel-construction",normalizedTitle:"3.2. kernel construction",charIndex:17111},{level:3,title:"3.3. cuBLAS/CUTLASS Integration",slug:"_3-3-cublas-cutlass-integration",normalizedTitle:"3.3. cublas/cutlass integration",charIndex:17780},{level:3,title:"3.4. Target-Specific Optimizations",slug:"_3-4-target-specific-optimizations",normalizedTitle:"3.4. target-specific optimizations",charIndex:18652},{level:3,title:"3.5. Memory Management",slug:"_3-5-memory-management",normalizedTitle:"3.5. memory management",charIndex:19562},{level:3,title:"3.6. Overall Codegen Workflow",slug:"_3-6-overall-codegen-workflow",normalizedTitle:"3.6. overall codegen workflow",charIndex:20211}],headersStr:"0. Feel the flow of TVM compilation 0.1 Model Parsing and Relay IR Construction 0.2 High-Level Optimizations in Relay 0.3. Lowering to Tensor Expression (TE) 0.4. Scheduling in TE 0.5. Lowering to TIR 0.6. Code Generation 0.7. Final Compilation and Deployment 1. Model Parsing and Relay IR Construction 1.1 Graph-Level Optimizations 1.2 Data Layout Transformations 1.3 Quantization and Precision Management 1.4 Automatic Differentiation 1.5 High-Level Hardware-Aware Optimizations 1.6 Device Placement 1.7 Meta-Pass Management 2 Lowering to Tensor Expression (TE), Scheduing in TE and Lowering into TIR 2.1 Converting Relay IR to Tensor Expression (TE) 2.2 Abstraction of Computation in Tensor Expression (TE) 2.3 Scheduling in Tensor Expression 2.4 Constructing Low-Level Tensor IR (TIR) 2.5 Device-Specific Optimizations 3. Code Generation 3.1 GPU Code Generation 3.2. Kernel Construction 3.3. cuBLAS/CUTLASS Integration 3.4. Target-Specific Optimizations 3.5. Memory Management 3.6. Overall Codegen Workflow",content:'----------------------------------------\n\n\n# 0. Feel the flow of TVM compilation\n\nModel Definition\n\nimport tvm\nfrom tvm import relay, te\nimport numpy as np\n\n# Model parameters\nbatch_size, input_dim, output_dim = 32, 128, 64\n\n# Relay model\nx = relay.var("x", shape=(batch_size, input_dim), dtype="float32")\nw = relay.var("w", shape=(input_dim, output_dim), dtype="float32")\ny = relay.nn.dense(x, w)\nmodel = relay.Function([x, w], y)\n\n# Input data\nx_data = np.random.rand(batch_size, input_dim).astype("float32")\nw_data = np.random.rand(input_dim, output_dim).astype("float32")\nparams = {"w": w_data}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\nRelay IR The relay.Function represents the high-level computational graph.\n\nprint(model)\n\n# Simplified Relay IR:\n# fn (%x: Tensor[(32, 128), float32], %w: Tensor[(128, 64), float32]) {\n#   nn.dense(%x, %w)\n# }\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nLowering to Tensor Expression (TE)\n\nwith tvm.transform.PassContext(opt_level=3):\n    mod, params = relay.build_module.bind_params_by_name(model, params)\n    graph, lib, params = relay.build(mod, target="cuda", params=params)\n\n\n1\n2\n3\n\n\nIn Tensor Expression (TE), computations are represented using tensor operations:\n\n * Compute: C[i, j] = sum(A[i, k] * B[k, j] for k in range(input_dim))\n * Schedule: Operations like tiling, thread binding, and vectorization are applied.\n\nExample TE for matrix multiplication:\n\nA = te.placeholder((batch_size, input_dim), name="A")\nB = te.placeholder((input_dim, output_dim), name="B")\nk = te.reduce_axis((0, input_dim), name="k")\n\n# Compute definition\nC = te.compute(\n    (batch_size, output_dim),\n    lambda i, j: te.sum(A[i, k] * B[k, j], axis=k),\n    name="C"\n)\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nTIR (Tensor IR)\n\nAfter applying schedules, TE is lowered to TIR. TIR is a low-level representation focusing on loops and memory hierarchy.\n\nExample TIR (simplified):\n\n@tvm.script.ir_module\nclass MyModule:\n    @tvm.tir.prim_func\n    def main(A: tvm.tir.Buffer[(32, 128), "float32"],\n             B: tvm.tir.Buffer[(128, 64), "float32"],\n             C: tvm.tir.Buffer[(32, 64), "float32"]):\n        for i in range(32):  # Outer loop for batch\n            for j in range(64):  # Outer loop for output_dim\n                with tvm.tir.block("C"):\n                    C[i, j] = 0.0\n                    for k in range(128):  # Reduction loop for input_dim\n                        C[i, j] += A[i, k] * B[k, j]\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nCUDA Code Generation\n\nFinally, TIR is compiled into CUDA code:\n\nprint(lib.imported_modules[0].get_source())\n\n# Simplified CUDA Code:\n# __global__ void fused_dense(float* __restrict__ A, float* __restrict__ B, float* __restrict__ C) {\n#   int idx = threadIdx.x + blockIdx.x * blockDim.x;\n#   if (idx < 2048) {  // 32 * 64 = batch_size * output_dim\n#     int i = idx / 64;  // Batch index\n#     int j = idx % 64;  // Output index\n#     float result = 0.0;\n#     for (int k = 0; k < 128; ++k) {  // Reduction loop\n#       result += A[i * 128 + k] * B[k * 64 + j];\n#     }\n#     C[idx] = result;\n#   }\n# }\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nSummary of Intermediate Representations\n\n 1. Relay IR: High-level computational graph, defines operators like nn.dense.\n 2. TE: Abstracts computation using mathematical tensor operations and supports scheduling primitives.\n 3. TIR: Low-level, loop-based representation with explicit memory hierarchy.\n 4. CUDA Code: GPU kernel for matrix multiplication, including thread and block mappings.\n\n----------------------------------------\n\n\n# 0.1 Model Parsing and Relay IR Construction\n\n * Input Model in high-level frameworks like TensorFlow, PyTorch, or ONNX.\n * Process:\n   * TVM parses the input model and converts it into Relay IR, a hig-h-level intermediate representation.\n   * The Relay IR describes the computational graph with operator-level abstractions.\n * Key Functions: relay.frontend.from_pytorch(), relay.frontend.from_onnx() in src/relay/frontend/.\n\n\n# 0.2 High-Level Optimizations in Relay\n\n * Input: Relay IR.\n * Process:\n   * Optimize the Relay IR for performance and hardware compatibility through:\n   * Operator Fusion: Fuse adjacent operations.\n   * Constant Folding: Precompute static expressions.\n   * Layout Transformation: Adjust data layouts (e.g., NCHW → NCHWc).\n   * Quantization: Lower precision where applicable.\n   * Common Subexpression Elimination. Finalize the optimized Relay graph.\n * Key Functions:\n   src/relay/transforms/ for passes like fuse_ops.cc, alter_op_layout.cc.\n\n\n# 0.3. Lowering to Tensor Expression (TE)\n\n * Input: Optimized Relay IR.\n * Process:\n   * Translate high-level Relay operators into Tensor Expressions (TE).\n   * TE represents computations as mathematical tensor operations and allows for:\n     * Abstraction of computation patterns (e.g., matrix multiplication).\n     * Introduction of scheduling primitives (e.g., tiling, unrolling, vectorization).\n * Key Functions:\n   * src/relay/backend/te_compiler.cc: Bridges Relay IR and TE.\n   * src/te/tensor.cc: Constructs tensor expressions.\n\n\n# 0.4. Scheduling in TE\n\n * Input: Tensor Expressions.\n * Process:\n   * Apply scheduling primitives to improve performance:\n   * Tiling: Divide tensors into smaller chunks for parallelism.\n   * Unrolling: Optimize loops for instruction pipelining.\n   * Thread/Block Mapping: Map computations to GPU threads and blocks.\n   * Vectorization: Use SIMD instructions where applicable.\n   * Refines Tensor Expressions into Tensor Intermediate Representation (TIR).\n * Key Functions:\n   * src/te/schedule/ for scheduling functions.\n   * src/te/schedule/schedule_dataflow_rewrite.cc: Handles dataflow rewrite scheduling.\n\n\n# 0.5. Lowering to TIR\n\n * Input: Tensor Expressions with schedules.\n * Process:\n   * Convert TE into Tensor IR (TIR), a low-level IR closer to device execution.\n   * Perform device-specific optimizations for CUDA (e.g., thread hierarchy mapping).\n * Key Functions:\n   * src/tir/transform/ for device-specific passes like loop unrolling and thread binding.\n\n\n# 0.6. Code Generation\n\n * Input: TIR optimized for CUDA.\n * Process:\n   * Code Generation:Translate TIR into CUDA kernels. Use TVM\'s built-in CUDA code generator.\n   * Calling cuBLAS/cuDNN or CUTLASS:\n     * For specific operations (e.g., GEMM), call external libraries.\n     * Determine the sequence of library calls and parameters based on operator attributes.\n   * Memory Allocation: Analyze dataflow to allocate memory efficiently on GPU.\n * Key Functions:\n   * CUDA Codegen: src/target/source/codegen_cuda.cc: Generates CUDA source code.\n   * External Libraries: src/runtime/contrib/cublas.cc: Integrates with cuBLAS. src/runtime/contrib/cudnn.cc: Integrates with cuDNN. src/contrib/cutlass/: Integrates with CUTLASS.\n\n\n# 0.7. Final Compilation and Deployment\n\n * Input: CUDA source code.\n * Process:\n   * Compile CUDA source code using NVCC or the TVM runtime.\n   * Deploy the compiled kernel and runtime modules.\n * Key Functions:\n   * src/target/source/: Handles code generation.\n   * src/runtime/: Manages runtime execution and deployment.\n\n----------------------------------------\n\n\n# 1. Model Parsing and Relay IR Construction\n\nIn TVM, high-level optimization in the Relay IR phase includes several graph-level optimizations, data layout transformations, and other functional passes.\n\nThese optimizations are implemented in various source files under src/relay/transform and src/relay/op directories.\n\nBelow is a categorized list of these optimizations along with their corresponding source code files and functions:\n\n\n# 1.1 Graph-Level Optimizations\n\nGraph-level optimizations restructure or simplify the computation graph for better performance.\n\nOPTIMIZATION SOURCE                FILE                                              KEY FUNCTIONS/CLASSES\nConstant Folding                   src/relay/transform/fold_constant.cc              FoldConstant, ConstantFolder\nOperator Fusion                    src/relay/transform/fuse_ops.cc                   FuseOps, FuseMutator, PatternMatcher\nDead Code Elimination (DCE)        src/relay/transform/eliminate_common_subexpr.cc   EliminateCommonSubexpr\nCommon Subexpression Elimination   src/relay/transform/eliminate_common_subexpr.cc   EliminateCommonSubexpr\nSimplify Inference                 src/relay/transform/simplify_inference.cc         SimplifyInference, SimplifyInferenceMutator\nCall Folding                       src/relay/transform/fold_call.cc                  FoldCall\nInline Functions                   src/relay/transform/inline.cc                     Inline, InlineMutator\nPrune Unused Functions             src/relay/transform/prune_unused_functions.cc     PruneUnusedFunctions\n\n\n# 1.2 Data Layout Transformations\n\nThese optimizations adjust the layout of tensors for better memory access patterns and compatibility with target hardware.\n\nTRANSFORMATION        FILE                                      KEY FUNCTIONS/CLASSES\nAlter Layout          src/relay/transform/alter_op_layout.cc    AlterOpLayout, AlterOpLayoutRewriter\nConvert Layout        s src/relay/transform/convert_layout.cc   ConvertLayout\nFold Scale Axis       src/relay/transform/fold_scale_axis.cc    FoldScaleAxis, ScaleAxisSimplifier\nLayout Optimization   src/relay/transform/layout_rewrite.cc     LayoutRewrite\n\n\n# 1.3 Quantization and Precision Management\n\nTVM supports quantization optimizations for reduced precision operations.\n\nOPTIMIZATION       FILE                                       KEY FUNCTIONS/CLASSES\nQuantize           src/relay/quantize/quantize.cc             Quantize, CreateQuantizePass\nDequantize         src/relay/quantize/dequantize.cc           Dequantize\nSimplifyQuantize   src/relay/transform/simplify_quantize.cc   SimplifyQuantize, SimplifyQuantizeRewriter\n\n\n# 1.4 Automatic Differentiation\n\nTVM includes an autodiff system for neural networks.\n\nTRANSFORMATION          FILE                               KEY FUNCTIONS/CLASSES\nReverse Mode Autodiff   src/relay/transforms/gradient.cc   AutomaticDifferentiation, ReverseAD\n\n\n# 1.5 High-Level Hardware-Aware Optimizations\n\nThese optimizations modify operations based on the target hardware.\n\nOPTIMIZATION             FILE                                            KEY FUNCTIONS/CLASSES\nAnnotate Target          src/relay/transform/annotate_target.cc          AnnotateTarget\nPartition Graph          src/relay/transform/partition_graph.cc          PartitionGraph\nMerge Compiler Regions   src/relay/transform/merge_compiler_regions.cc   MergeCompilerRegions\n\n\n# 1.6 Device Placement\n\nThese passes assign operations to devices for heterogeneous execution.\n\nTRANSFORMATION          FILE                                           KEY FUNCTIONS/CLASSES\nRewrite Annotated Ops   src/relay/transform/rewrite_annotated_ops.cc   RewriteAnnotatedOps\nDevice Annotation       src/relay/transform/device_annotation.cc       DeviceAnnotation\n\n\n# 1.7 Meta-Pass Management\n\nRelay provides a meta-pass system to manage and sequence passes.\n\nMETA-PASS                 FILE                                KEY FUNCTIONS/CLASSES\nSequential Pass Manager   src/relay/transform/sequential.cc   Sequential, PassManager\nPass Context              src/relay/transform/pass.cc         PassContext, WithPassContext\n\n----------------------------------------\n\n\n# 2 Lowering to Tensor Expression (TE), Scheduing in TE and Lowering into TIR\n\nThe lowering process from Relay IR to Tensor Expression (TE) and Tensor IR (TIR) in TVM involves multiple phases.\n\nThese include converting Relay IR to TE, applying tensor computation abstractions, performing scheduling, and constructing device-specific low-level TIR.\n\nHere’s a detailed breakdown of the corresponding TVM source code files and functions for these stages:\n\n\n# 2.1 Converting Relay IR to Tensor Expression (TE)\n\nThis phase converts high-level Relay IR into the computation abstractions provided by TE.\n\nPROCESS                   FILE                                     KEY FUNCTIONS/CLASSES\nRelay to TE Lowering      src/relay/backend/te_compiler.cc         LowerToTE, CreateSchedule, ScheduleGetter\nOperator Strategy         src/relay/op/strategy/generic.cc         GenericFunc, OpStrategy\nRelay to TE Bridge        src/relay/backend/te_compiler_cache.cc   TECompiler, LowerTE\nShape Function Lowering   src/relay/backend/te_compiler.cc         LowerShapeFunc\n\nExplanation:\n\n * The Relay IR graph is analyzed, and for each operator, TVM retrieves a corresponding TE function using OpStrategy.\n * TE functions define high-level operations like matrix multiplication, element-wise addition, etc.\n\n\n# 2.2 Abstraction of Computation in Tensor Expression (TE)\n\nTE provides a declarative way to express computation. This includes operations like tiling, unrolling, and vectorizing.\n\nPROCESS                     FILE                                  KEY FUNCTIONS/CLASSES\nTensor Expression Build     src/te/operation/create_primfunc.cc   CreatePrimFunc, ComputeBody, ScheduleOps\nCompute Definition          src/te/operation/compute_op.cc        ComputeOpNode, ComputeOp\nTensor Compute Intrinsics   src/te/operation/tensorize.cc         Tensorize, CreateIntrinBody\n\nExplanation:\n\n * High-level computations are abstracted into a declarative format using ComputeOp.\n * Intrinsic support for tensorization is added for specialized hardware operations.\n\n\n# 2.3 Scheduling in Tensor Expression\n\nScheduling is where TVM optimizes how computations are performed on the target device.\n\nPROCESS                    FILE                                           KEY FUNCTIONS/CLASSES\nTile, Unroll, Vectorize    src/te/schedule/schedule_dataflow_rewrite.cc   ScheduleDataFlowRewrite, Tile, Unroll, Vectorize\nThread and Block Mapping   src/te/schedule/schedule_lang.cc               bind, split, reorder, fuse\nAutoScheduler Interface    src/auto_scheduler/compute_dag.cc              ComputeDAG, ApplySteps\nLowering Schedule to TIR   src/te/schedule/graph.cc                       ScheduleGraph, LowerSchedule\n\nExplanation:\n\n * This phase defines how computations should be split into smaller tiles, mapped to hardware threads and blocks, and optimized using unrolling and vectorization.\n * Tensor schedules are converted into lower-level forms through ScheduleGraph.\n\n\n# 2.4 Constructing Low-Level Tensor IR (TIR)\n\nTIR represents a low-level, device-specific IR used to generate target-specific code.\n\nPROCESS               FILE                                    KEY FUNCTIONS/CLASSES\nTIR Construction      src/tir/stmt_functor.cc                 StmtFunctor, VisitStmt, MakeStmt\nLowering to TIR       src/tir/transforms/lower_tir.cc         LowerTIR, TransformTIR\nMemory Planning       src/tir/transforms/storage_rewrite.cc   StorageRewrite, PlanMemory\nDevice-Specific TIR   src/target/codegen.cc                   Build, BuildIRModule\n\nExplanation:\n\n * TE schedules are converted into TIR, which provides explicit control over memory accesses and device-specific optimizations.\n * StorageRewrite optimizes memory allocation and reuse.\n\n\n# 2.5 Device-Specific Optimizations\n\nDevice-specific optimizations tailor the generated code for the target platform (e.g., CUDA).\n\nTRANSFORMATION         FILE                                        KEY FUNCTIONS/CLASSES\nThread/Block Mapping   src/tir/transforms/thread_storage_sync.cc   ThreadStorageSync\nLoop Partitioning      src/tir/transforms/loop_partition.cc        LoopPartition\nDevice Codegen         src/target/source/codegen_cuda.cc           CodeGenCUDA, PrintKernel\n\nHigh-Level Summary of the Workflow\n\n * Relay to TE:\n   Converts high-level operations into Tensor Expression (TE) definitions using strategies (src/relay/backend/te_compiler.cc).\n * Computation Abstraction: Defines computations in TE with ComputeOp (src/te/operation/compute_op.cc).\n * Scheduling:\n   Applies optimizations like tiling, unrolling, and mapping computations to threads/blocks (src/te/schedule/schedule_lang.cc).\n * Lowering to TIR:\n   Translates the schedule into TIR, which explicitly handles device memory and control flow (src/tir/transforms/lower_tir.cc).\n * Device-Specific Codegen:\n   Emits target-specific code (e.g., CUDA) via CodeGenCUDA (src/target/source/codegen_cuda.cc).\n\n----------------------------------------\n\n\n# 3. Code Generation\n\n\n# 3.1 GPU Code Generation\n\nThis phase translates Tensor IR (TIR) into GPU-compatible low-level code, generating CUDA kernels and API calls.\n\nPROCESS                        FILE                                        KEY FUNCTIONS/CLASSES\nTIR to CUDA Kernel             src/target/source/codegen_cuda.cc           CodeGenCUDA, GenerateKernel, PrintStmt\nCodeGen Base Class             src/target/source/codegen_c.cc              CodeGenC, PrintExpr\nShared Memory Handling         src/target/source/codegen_cuda.cc           PrintStorageScope, PrintStorageSync\nThread/Block Synchronization   src/tir/transforms/thread_storage_sync.cc   ThreadStorageSync\n\nExplanation: CodeGenCUDA translates TIR to CUDA kernels, emitting device-side code and managing constructs like thread/block mappings, shared memory, and synchronization. Synchronization points are inserted using PrintStorageSync.\n\n\n# 3.2. Kernel Construction\n\nKernel construction involves creating CUDA device kernels and host-side launcher code to invoke them.\n\nPROCESS                      FILE                                KEY FUNCTIONS/CLASSES\nKernel Emission              src/target/source/codegen_cuda.cc   PrintFuncBody, EmitFunction\nKernel Launch Code           src/runtime/cuda/cuda_module.cc     CUDAWrappedFunc, LaunchKernel\nKernel Metadata Management   src/runtime/module.cc               PackImports, ExportModule\n\nExplanation: The EmitFunction generates kernel function declarations and definitions for execution on the GPU. Host-side kernel launchers are defined in cuda_module.cc.\n\n\n# 3.3. cuBLAS/CUTLASS Integration\n\nWhen using cuBLAS or CUTLASS for tensor computations (e.g., GEMM), TVM generates calls to these libraries instead of writing explicit CUDA kernels.\n\nPROCESS                    FILE                                          KEY FUNCTIONS/CLASSES\ncuBLAS Integration         src/runtime/contrib/cublas/cublas.cc          CUBLASCall, InitCUBLASHandle, GemmOp\nCUTLASS Integration        src/contrib/cutlass/gen_cutlass_gemm.cc       GenerateCutlassGemm, EmitCutlassCode\nExternal Code Generation   src/relay/backend/contrib/cublas_codegen.cc   CUBLASFunction, CodegenCUBLAS\n\nExplanation: cublas.cc provides wrappers for cuBLAS API calls like cublasSgemm, with TVM handling data layout transformations as needed. CUTLASS integration uses template-based code generation in gen_cutlass_gemm.cc, emitting optimized kernels for matrix operations.\n\n\n# 3.4. Target-Specific Optimizations\n\nTarget-specific optimizations fine-tune the generated CUDA code based on the GPU architecture and memory hierarchy.\n\nPROCESS                   FILE                                        KEY FUNCTIONS/CLASSES\nThread/Block Mapping      src/tir/transforms/thread_storage_sync.cc   ThreadStorageSync, OptimizeThreads\nLoop Partitioning         src/tir/transforms/loop_partition.cc        LoopPartition\nMemory Planning           src/tir/transforms/storage_rewrite.cc       StorageRewrite, PlanMemory\nWarp-Level Optimization   src/tir/transforms/vectorize_loop.cc        VectorizeLoop, Vectorizer\n\nExplanation: Thread and block mapping ensures optimal utilization of GPU threads and memory. Loop partitioning and vectorization optimize data access patterns for warp-level efficiency. StorageRewrite minimizes memory usage by analyzing reuse patterns and adjusting allocation.\n\n\n# 3.5. Memory Management\n\nEfficient memory management involves optimizing shared/global memory usage and enabling memory reuse.\n\nPROCESS               FILE                                    KEY FUNCTIONS/CLASSES\nShared Memory Usage   src/target/source/codegen_cuda.cc       PrintStorageScope, EmitSharedMemory\nMemory Allocation     src/tir/transforms/storage_rewrite.cc   PlanMemory, ReuseMemory\nMemory Alignment      src/target/source/codegen_cuda.cc       PrintStorageAlloc\n\nExplanation: Shared memory scopes are explicitly emitted during CUDA codegen (EmitSharedMemory). PlanMemory optimizes allocation to minimize fragmentation and overhead.\n\n\n# 3.6. Overall Codegen Workflow\n\nKey Stages and Their Files\n\n * TIR Lowering:\n   File: src/tir/transforms/lower_tir.cc\n   Function: LowerTIR, TransformTIR\n\n * CUDA Kernel Emission:\n   File: src/target/source/codegen_cuda.cc\n   Function: EmitFunction, GenerateKernel\n\n * cuBLAS Integration:\n   File: src/runtime/contrib/cublas/cublas.cc\n   Function: CUBLASCall, InitCUBLASHandle\n\n * CUTLASS Integration:\n   File: src/contrib/cutlass/gen_cutlass_gemm.cc\n   Function: GenerateCutlassGemm, EmitCutlassCode\n\n * Target-Specific Optimizations:\n   File: src/tir/transforms/thread_storage_sync.cc\n   Function: ThreadStorageSync, OptimizeThreads',normalizedContent:'----------------------------------------\n\n\n# 0. feel the flow of tvm compilation\n\nmodel definition\n\nimport tvm\nfrom tvm import relay, te\nimport numpy as np\n\n# model parameters\nbatch_size, input_dim, output_dim = 32, 128, 64\n\n# relay model\nx = relay.var("x", shape=(batch_size, input_dim), dtype="float32")\nw = relay.var("w", shape=(input_dim, output_dim), dtype="float32")\ny = relay.nn.dense(x, w)\nmodel = relay.function([x, w], y)\n\n# input data\nx_data = np.random.rand(batch_size, input_dim).astype("float32")\nw_data = np.random.rand(input_dim, output_dim).astype("float32")\nparams = {"w": w_data}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\nrelay ir the relay.function represents the high-level computational graph.\n\nprint(model)\n\n# simplified relay ir:\n# fn (%x: tensor[(32, 128), float32], %w: tensor[(128, 64), float32]) {\n#   nn.dense(%x, %w)\n# }\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nlowering to tensor expression (te)\n\nwith tvm.transform.passcontext(opt_level=3):\n    mod, params = relay.build_module.bind_params_by_name(model, params)\n    graph, lib, params = relay.build(mod, target="cuda", params=params)\n\n\n1\n2\n3\n\n\nin tensor expression (te), computations are represented using tensor operations:\n\n * compute: c[i, j] = sum(a[i, k] * b[k, j] for k in range(input_dim))\n * schedule: operations like tiling, thread binding, and vectorization are applied.\n\nexample te for matrix multiplication:\n\na = te.placeholder((batch_size, input_dim), name="a")\nb = te.placeholder((input_dim, output_dim), name="b")\nk = te.reduce_axis((0, input_dim), name="k")\n\n# compute definition\nc = te.compute(\n    (batch_size, output_dim),\n    lambda i, j: te.sum(a[i, k] * b[k, j], axis=k),\n    name="c"\n)\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\ntir (tensor ir)\n\nafter applying schedules, te is lowered to tir. tir is a low-level representation focusing on loops and memory hierarchy.\n\nexample tir (simplified):\n\n@tvm.script.ir_module\nclass mymodule:\n    @tvm.tir.prim_func\n    def main(a: tvm.tir.buffer[(32, 128), "float32"],\n             b: tvm.tir.buffer[(128, 64), "float32"],\n             c: tvm.tir.buffer[(32, 64), "float32"]):\n        for i in range(32):  # outer loop for batch\n            for j in range(64):  # outer loop for output_dim\n                with tvm.tir.block("c"):\n                    c[i, j] = 0.0\n                    for k in range(128):  # reduction loop for input_dim\n                        c[i, j] += a[i, k] * b[k, j]\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\ncuda code generation\n\nfinally, tir is compiled into cuda code:\n\nprint(lib.imported_modules[0].get_source())\n\n# simplified cuda code:\n# __global__ void fused_dense(float* __restrict__ a, float* __restrict__ b, float* __restrict__ c) {\n#   int idx = threadidx.x + blockidx.x * blockdim.x;\n#   if (idx < 2048) {  // 32 * 64 = batch_size * output_dim\n#     int i = idx / 64;  // batch index\n#     int j = idx % 64;  // output index\n#     float result = 0.0;\n#     for (int k = 0; k < 128; ++k) {  // reduction loop\n#       result += a[i * 128 + k] * b[k * 64 + j];\n#     }\n#     c[idx] = result;\n#   }\n# }\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nsummary of intermediate representations\n\n 1. relay ir: high-level computational graph, defines operators like nn.dense.\n 2. te: abstracts computation using mathematical tensor operations and supports scheduling primitives.\n 3. tir: low-level, loop-based representation with explicit memory hierarchy.\n 4. cuda code: gpu kernel for matrix multiplication, including thread and block mappings.\n\n----------------------------------------\n\n\n# 0.1 model parsing and relay ir construction\n\n * input model in high-level frameworks like tensorflow, pytorch, or onnx.\n * process:\n   * tvm parses the input model and converts it into relay ir, a hig-h-level intermediate representation.\n   * the relay ir describes the computational graph with operator-level abstractions.\n * key functions: relay.frontend.from_pytorch(), relay.frontend.from_onnx() in src/relay/frontend/.\n\n\n# 0.2 high-level optimizations in relay\n\n * input: relay ir.\n * process:\n   * optimize the relay ir for performance and hardware compatibility through:\n   * operator fusion: fuse adjacent operations.\n   * constant folding: precompute static expressions.\n   * layout transformation: adjust data layouts (e.g., nchw → nchwc).\n   * quantization: lower precision where applicable.\n   * common subexpression elimination. finalize the optimized relay graph.\n * key functions:\n   src/relay/transforms/ for passes like fuse_ops.cc, alter_op_layout.cc.\n\n\n# 0.3. lowering to tensor expression (te)\n\n * input: optimized relay ir.\n * process:\n   * translate high-level relay operators into tensor expressions (te).\n   * te represents computations as mathematical tensor operations and allows for:\n     * abstraction of computation patterns (e.g., matrix multiplication).\n     * introduction of scheduling primitives (e.g., tiling, unrolling, vectorization).\n * key functions:\n   * src/relay/backend/te_compiler.cc: bridges relay ir and te.\n   * src/te/tensor.cc: constructs tensor expressions.\n\n\n# 0.4. scheduling in te\n\n * input: tensor expressions.\n * process:\n   * apply scheduling primitives to improve performance:\n   * tiling: divide tensors into smaller chunks for parallelism.\n   * unrolling: optimize loops for instruction pipelining.\n   * thread/block mapping: map computations to gpu threads and blocks.\n   * vectorization: use simd instructions where applicable.\n   * refines tensor expressions into tensor intermediate representation (tir).\n * key functions:\n   * src/te/schedule/ for scheduling functions.\n   * src/te/schedule/schedule_dataflow_rewrite.cc: handles dataflow rewrite scheduling.\n\n\n# 0.5. lowering to tir\n\n * input: tensor expressions with schedules.\n * process:\n   * convert te into tensor ir (tir), a low-level ir closer to device execution.\n   * perform device-specific optimizations for cuda (e.g., thread hierarchy mapping).\n * key functions:\n   * src/tir/transform/ for device-specific passes like loop unrolling and thread binding.\n\n\n# 0.6. code generation\n\n * input: tir optimized for cuda.\n * process:\n   * code generation:translate tir into cuda kernels. use tvm\'s built-in cuda code generator.\n   * calling cublas/cudnn or cutlass:\n     * for specific operations (e.g., gemm), call external libraries.\n     * determine the sequence of library calls and parameters based on operator attributes.\n   * memory allocation: analyze dataflow to allocate memory efficiently on gpu.\n * key functions:\n   * cuda codegen: src/target/source/codegen_cuda.cc: generates cuda source code.\n   * external libraries: src/runtime/contrib/cublas.cc: integrates with cublas. src/runtime/contrib/cudnn.cc: integrates with cudnn. src/contrib/cutlass/: integrates with cutlass.\n\n\n# 0.7. final compilation and deployment\n\n * input: cuda source code.\n * process:\n   * compile cuda source code using nvcc or the tvm runtime.\n   * deploy the compiled kernel and runtime modules.\n * key functions:\n   * src/target/source/: handles code generation.\n   * src/runtime/: manages runtime execution and deployment.\n\n----------------------------------------\n\n\n# 1. model parsing and relay ir construction\n\nin tvm, high-level optimization in the relay ir phase includes several graph-level optimizations, data layout transformations, and other functional passes.\n\nthese optimizations are implemented in various source files under src/relay/transform and src/relay/op directories.\n\nbelow is a categorized list of these optimizations along with their corresponding source code files and functions:\n\n\n# 1.1 graph-level optimizations\n\ngraph-level optimizations restructure or simplify the computation graph for better performance.\n\noptimization source                file                                              key functions/classes\nconstant folding                   src/relay/transform/fold_constant.cc              foldconstant, constantfolder\noperator fusion                    src/relay/transform/fuse_ops.cc                   fuseops, fusemutator, patternmatcher\ndead code elimination (dce)        src/relay/transform/eliminate_common_subexpr.cc   eliminatecommonsubexpr\ncommon subexpression elimination   src/relay/transform/eliminate_common_subexpr.cc   eliminatecommonsubexpr\nsimplify inference                 src/relay/transform/simplify_inference.cc         simplifyinference, simplifyinferencemutator\ncall folding                       src/relay/transform/fold_call.cc                  foldcall\ninline functions                   src/relay/transform/inline.cc                     inline, inlinemutator\nprune unused functions             src/relay/transform/prune_unused_functions.cc     pruneunusedfunctions\n\n\n# 1.2 data layout transformations\n\nthese optimizations adjust the layout of tensors for better memory access patterns and compatibility with target hardware.\n\ntransformation        file                                      key functions/classes\nalter layout          src/relay/transform/alter_op_layout.cc    alteroplayout, alteroplayoutrewriter\nconvert layout        s src/relay/transform/convert_layout.cc   convertlayout\nfold scale axis       src/relay/transform/fold_scale_axis.cc    foldscaleaxis, scaleaxissimplifier\nlayout optimization   src/relay/transform/layout_rewrite.cc     layoutrewrite\n\n\n# 1.3 quantization and precision management\n\ntvm supports quantization optimizations for reduced precision operations.\n\noptimization       file                                       key functions/classes\nquantize           src/relay/quantize/quantize.cc             quantize, createquantizepass\ndequantize         src/relay/quantize/dequantize.cc           dequantize\nsimplifyquantize   src/relay/transform/simplify_quantize.cc   simplifyquantize, simplifyquantizerewriter\n\n\n# 1.4 automatic differentiation\n\ntvm includes an autodiff system for neural networks.\n\ntransformation          file                               key functions/classes\nreverse mode autodiff   src/relay/transforms/gradient.cc   automaticdifferentiation, reversead\n\n\n# 1.5 high-level hardware-aware optimizations\n\nthese optimizations modify operations based on the target hardware.\n\noptimization             file                                            key functions/classes\nannotate target          src/relay/transform/annotate_target.cc          annotatetarget\npartition graph          src/relay/transform/partition_graph.cc          partitiongraph\nmerge compiler regions   src/relay/transform/merge_compiler_regions.cc   mergecompilerregions\n\n\n# 1.6 device placement\n\nthese passes assign operations to devices for heterogeneous execution.\n\ntransformation          file                                           key functions/classes\nrewrite annotated ops   src/relay/transform/rewrite_annotated_ops.cc   rewriteannotatedops\ndevice annotation       src/relay/transform/device_annotation.cc       deviceannotation\n\n\n# 1.7 meta-pass management\n\nrelay provides a meta-pass system to manage and sequence passes.\n\nmeta-pass                 file                                key functions/classes\nsequential pass manager   src/relay/transform/sequential.cc   sequential, passmanager\npass context              src/relay/transform/pass.cc         passcontext, withpasscontext\n\n----------------------------------------\n\n\n# 2 lowering to tensor expression (te), scheduing in te and lowering into tir\n\nthe lowering process from relay ir to tensor expression (te) and tensor ir (tir) in tvm involves multiple phases.\n\nthese include converting relay ir to te, applying tensor computation abstractions, performing scheduling, and constructing device-specific low-level tir.\n\nhere’s a detailed breakdown of the corresponding tvm source code files and functions for these stages:\n\n\n# 2.1 converting relay ir to tensor expression (te)\n\nthis phase converts high-level relay ir into the computation abstractions provided by te.\n\nprocess                   file                                     key functions/classes\nrelay to te lowering      src/relay/backend/te_compiler.cc         lowertote, createschedule, schedulegetter\noperator strategy         src/relay/op/strategy/generic.cc         genericfunc, opstrategy\nrelay to te bridge        src/relay/backend/te_compiler_cache.cc   tecompiler, lowerte\nshape function lowering   src/relay/backend/te_compiler.cc         lowershapefunc\n\nexplanation:\n\n * the relay ir graph is analyzed, and for each operator, tvm retrieves a corresponding te function using opstrategy.\n * te functions define high-level operations like matrix multiplication, element-wise addition, etc.\n\n\n# 2.2 abstraction of computation in tensor expression (te)\n\nte provides a declarative way to express computation. this includes operations like tiling, unrolling, and vectorizing.\n\nprocess                     file                                  key functions/classes\ntensor expression build     src/te/operation/create_primfunc.cc   createprimfunc, computebody, scheduleops\ncompute definition          src/te/operation/compute_op.cc        computeopnode, computeop\ntensor compute intrinsics   src/te/operation/tensorize.cc         tensorize, createintrinbody\n\nexplanation:\n\n * high-level computations are abstracted into a declarative format using computeop.\n * intrinsic support for tensorization is added for specialized hardware operations.\n\n\n# 2.3 scheduling in tensor expression\n\nscheduling is where tvm optimizes how computations are performed on the target device.\n\nprocess                    file                                           key functions/classes\ntile, unroll, vectorize    src/te/schedule/schedule_dataflow_rewrite.cc   scheduledataflowrewrite, tile, unroll, vectorize\nthread and block mapping   src/te/schedule/schedule_lang.cc               bind, split, reorder, fuse\nautoscheduler interface    src/auto_scheduler/compute_dag.cc              computedag, applysteps\nlowering schedule to tir   src/te/schedule/graph.cc                       schedulegraph, lowerschedule\n\nexplanation:\n\n * this phase defines how computations should be split into smaller tiles, mapped to hardware threads and blocks, and optimized using unrolling and vectorization.\n * tensor schedules are converted into lower-level forms through schedulegraph.\n\n\n# 2.4 constructing low-level tensor ir (tir)\n\ntir represents a low-level, device-specific ir used to generate target-specific code.\n\nprocess               file                                    key functions/classes\ntir construction      src/tir/stmt_functor.cc                 stmtfunctor, visitstmt, makestmt\nlowering to tir       src/tir/transforms/lower_tir.cc         lowertir, transformtir\nmemory planning       src/tir/transforms/storage_rewrite.cc   storagerewrite, planmemory\ndevice-specific tir   src/target/codegen.cc                   build, buildirmodule\n\nexplanation:\n\n * te schedules are converted into tir, which provides explicit control over memory accesses and device-specific optimizations.\n * storagerewrite optimizes memory allocation and reuse.\n\n\n# 2.5 device-specific optimizations\n\ndevice-specific optimizations tailor the generated code for the target platform (e.g., cuda).\n\ntransformation         file                                        key functions/classes\nthread/block mapping   src/tir/transforms/thread_storage_sync.cc   threadstoragesync\nloop partitioning      src/tir/transforms/loop_partition.cc        looppartition\ndevice codegen         src/target/source/codegen_cuda.cc           codegencuda, printkernel\n\nhigh-level summary of the workflow\n\n * relay to te:\n   converts high-level operations into tensor expression (te) definitions using strategies (src/relay/backend/te_compiler.cc).\n * computation abstraction: defines computations in te with computeop (src/te/operation/compute_op.cc).\n * scheduling:\n   applies optimizations like tiling, unrolling, and mapping computations to threads/blocks (src/te/schedule/schedule_lang.cc).\n * lowering to tir:\n   translates the schedule into tir, which explicitly handles device memory and control flow (src/tir/transforms/lower_tir.cc).\n * device-specific codegen:\n   emits target-specific code (e.g., cuda) via codegencuda (src/target/source/codegen_cuda.cc).\n\n----------------------------------------\n\n\n# 3. code generation\n\n\n# 3.1 gpu code generation\n\nthis phase translates tensor ir (tir) into gpu-compatible low-level code, generating cuda kernels and api calls.\n\nprocess                        file                                        key functions/classes\ntir to cuda kernel             src/target/source/codegen_cuda.cc           codegencuda, generatekernel, printstmt\ncodegen base class             src/target/source/codegen_c.cc              codegenc, printexpr\nshared memory handling         src/target/source/codegen_cuda.cc           printstoragescope, printstoragesync\nthread/block synchronization   src/tir/transforms/thread_storage_sync.cc   threadstoragesync\n\nexplanation: codegencuda translates tir to cuda kernels, emitting device-side code and managing constructs like thread/block mappings, shared memory, and synchronization. synchronization points are inserted using printstoragesync.\n\n\n# 3.2. kernel construction\n\nkernel construction involves creating cuda device kernels and host-side launcher code to invoke them.\n\nprocess                      file                                key functions/classes\nkernel emission              src/target/source/codegen_cuda.cc   printfuncbody, emitfunction\nkernel launch code           src/runtime/cuda/cuda_module.cc     cudawrappedfunc, launchkernel\nkernel metadata management   src/runtime/module.cc               packimports, exportmodule\n\nexplanation: the emitfunction generates kernel function declarations and definitions for execution on the gpu. host-side kernel launchers are defined in cuda_module.cc.\n\n\n# 3.3. cublas/cutlass integration\n\nwhen using cublas or cutlass for tensor computations (e.g., gemm), tvm generates calls to these libraries instead of writing explicit cuda kernels.\n\nprocess                    file                                          key functions/classes\ncublas integration         src/runtime/contrib/cublas/cublas.cc          cublascall, initcublashandle, gemmop\ncutlass integration        src/contrib/cutlass/gen_cutlass_gemm.cc       generatecutlassgemm, emitcutlasscode\nexternal code generation   src/relay/backend/contrib/cublas_codegen.cc   cublasfunction, codegencublas\n\nexplanation: cublas.cc provides wrappers for cublas api calls like cublassgemm, with tvm handling data layout transformations as needed. cutlass integration uses template-based code generation in gen_cutlass_gemm.cc, emitting optimized kernels for matrix operations.\n\n\n# 3.4. target-specific optimizations\n\ntarget-specific optimizations fine-tune the generated cuda code based on the gpu architecture and memory hierarchy.\n\nprocess                   file                                        key functions/classes\nthread/block mapping      src/tir/transforms/thread_storage_sync.cc   threadstoragesync, optimizethreads\nloop partitioning         src/tir/transforms/loop_partition.cc        looppartition\nmemory planning           src/tir/transforms/storage_rewrite.cc       storagerewrite, planmemory\nwarp-level optimization   src/tir/transforms/vectorize_loop.cc        vectorizeloop, vectorizer\n\nexplanation: thread and block mapping ensures optimal utilization of gpu threads and memory. loop partitioning and vectorization optimize data access patterns for warp-level efficiency. storagerewrite minimizes memory usage by analyzing reuse patterns and adjusting allocation.\n\n\n# 3.5. memory management\n\nefficient memory management involves optimizing shared/global memory usage and enabling memory reuse.\n\nprocess               file                                    key functions/classes\nshared memory usage   src/target/source/codegen_cuda.cc       printstoragescope, emitsharedmemory\nmemory allocation     src/tir/transforms/storage_rewrite.cc   planmemory, reusememory\nmemory alignment      src/target/source/codegen_cuda.cc       printstoragealloc\n\nexplanation: shared memory scopes are explicitly emitted during cuda codegen (emitsharedmemory). planmemory optimizes allocation to minimize fragmentation and overhead.\n\n\n# 3.6. overall codegen workflow\n\nkey stages and their files\n\n * tir lowering:\n   file: src/tir/transforms/lower_tir.cc\n   function: lowertir, transformtir\n\n * cuda kernel emission:\n   file: src/target/source/codegen_cuda.cc\n   function: emitfunction, generatekernel\n\n * cublas integration:\n   file: src/runtime/contrib/cublas/cublas.cc\n   function: cublascall, initcublashandle\n\n * cutlass integration:\n   file: src/contrib/cutlass/gen_cutlass_gemm.cc\n   function: generatecutlassgemm, emitcutlasscode\n\n * target-specific optimizations:\n   file: src/tir/transforms/thread_storage_sync.cc\n   function: threadstoragesync, optimizethreads',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"MLIR TOY Tutorial",frontmatter:{title:"MLIR TOY Tutorial",date:"2024-12-10T00:00:00.000Z",permalink:"/pages/000011/",tags:[null]},regularPath:"/02.compiler/11.learn_mlir_toy.html",relativePath:"02.compiler/11.learn_mlir_toy.md",key:"v-e8215df6",path:"/pages/000011/",headers:[{level:3,title:"Chapter 1. Toy Language and AST",slug:"chapter-1-toy-language-and-ast",normalizedTitle:"chapter 1. toy language and ast",charIndex:2},{level:3,title:"Chapter 2. Emit Basic MLIR",slug:"chapter-2-emit-basic-mlir",normalizedTitle:"chapter 2. emit basic mlir",charIndex:87},{level:3,title:"Chapter 3. High-level Language-Specific Analysis and Transformation",slug:"chapter-3-high-level-language-specific-analysis-and-transformation",normalizedTitle:"chapter 3. high-level language-specific analysis and transformation",charIndex:3740},{level:3,title:"Chapter 4. Enabling Generic Transformation with Interfaces",slug:"chapter-4-enabling-generic-transformation-with-interfaces",normalizedTitle:"chapter 4. enabling generic transformation with interfaces",charIndex:5051},{level:3,title:"Chapter 5. Partial Lowering to Lower-Level Dialects for Optimization",slug:"chapter-5-partial-lowering-to-lower-level-dialects-for-optimization",normalizedTitle:"chapter 5. partial lowering to lower-level dialects for optimization",charIndex:11260},{level:3,title:"Chapter 6.Lowering to LLVM and CodeGeneration",slug:"chapter-6-lowering-to-llvm-and-codegeneration",normalizedTitle:"chapter 6.lowering to llvm and codegeneration",charIndex:16147}],headersStr:"Chapter 1. Toy Language and AST Chapter 2. Emit Basic MLIR Chapter 3. High-level Language-Specific Analysis and Transformation Chapter 4. Enabling Generic Transformation with Interfaces Chapter 5. Partial Lowering to Lower-Level Dialects for Optimization Chapter 6.Lowering to LLVM and CodeGeneration",content:'# Chapter 1. Toy Language and AST\n\nLexer and recursive descent parser construt AST\n\n\n# Chapter 2. Emit Basic MLIR\n\nOperations: instructions, globals(functions), modules, in LLVM\n\nFrom link\n\nTranspose Operation\n\n%t_tensor = "toy.transpose"(%tensor) {inplace = true} : (tensor<2x3xf64>) ->\ntensor<3x2xf64> loc("example/file/path":12:1)\n\n\n1\n2\n\n\nmeaning of each part\n\nresult  = name of operation (input operands) dictionary of traits :\ntype of operations (input and output), location\n\n\n1\n2\n\n * A name for the operation.\n * A list of SSA operand values.\n * A list of attributes.\n * A list of types for result values.\n * A source location for debugging purposes.\n * A list of successors blocks (for branches, mostly).\n * A list of regions (for structural operations like functions).\n\nOpaque API\n\nDefine a Toy Dialect\n\ndialect could be defined by c++ or tablegen(declarative specification).\n\nafter the definiation, it could be loaded to MLIR Context. context.loadDialect();\n\nDefining Toy Operations\n\nclass ConstantOp : public mlir::Op<\n                     /// `mlir::Op` is a CRTP class, meaning that we provide the\n                     /// derived class as a template parameter.\n                     ConstantOp,\n                     /// The ConstantOp takes zero input operands.\n                     mlir::OpTrait::ZeroOperands,\n                     /// The ConstantOp returns a single result.\n                     mlir::OpTrait::OneResult,\n                     /// We also provide a utility `getType` accessor that\n                     /// returns the TensorType of the single result.\n                     mlir::OpTraits::OneTypedResult<TensorType>::Impl> {\n\n public:\n  /// Inherit the constructors from the base Op class.\n  using Op::Op;\n  ...\n  static void build(mlir::OpBuilder &builder, mlir::OperationState &state,\n                    mlir::Type result, mlir::DenseElementsAttr value);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\nregister operation:\n\nvoid ToyDialect::initialize() {\n  addOperations<ConstantOp>();\n}\n\n\n1\n2\n3\n\n\nOp vs Operation: Using MLIR Operations\n\nUsing Operation Definition Specification Framwork\n\nbase Toy_Op\n\nclass Toy_Op<string mnemonic, list<Trait> traits = []> :\n    Op<Toy_Dialect, mnemonic, traits>;\n\n\n1\n2\n\n\nConstantOp\n\ndef ConstantOp : Toy_Op<"constant"> {\n}\n\n\n1\n2\n\n\nAttaching build Methods\n\nIn ConstantOp, it declared a list of build. ODS will generate the first build.\nAs to other builds, we have to atttach.\n\ndef ConstantOp : Toy_Op<"constant"> {\n  ...\n\n  // Add custom build methods for the constant operation. These methods populate\n  // the `state` that MLIR uses to create operations, i.e. these are used when\n  // using `builder.create<ConstantOp>(...)`.\n  let builders = [\n    // Build a constant with a given constant tensor value.\n    OpBuilder<(ins "DenseElementsAttr":$value), [{\n      // Call into an autogenerated `build` method.\n      build(builder, result, value.getType(), value);\n    }]>,\n\n    // Build a constant with a given constant floating-point value. This builder\n    // creates a declaration for `ConstantOp::build` with the given parameters.\n    OpBuilder<(ins "double":$value)>\n  ];\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\nSpecifying a Custom Assembly Format\n\nThe printout version of IR has too much information.\n\nWe can strip out by implementing our owne oversion of print and parse function.\n\nTake PrintOp as example.\n\nvoid PrintOp::print(mlir::OpAsmPrinter &printer) {\n  printer << "toy.print " << op.input();\n  printer.printOptionalAttrDict(op.getAttrs());\n  printer << " : " << op.input().getType();\n}\n\nmlir::ParseResult PrintOp::parse(mlir::OpAsmParser &parser,\n                                 mlir::OperationState &result) {\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# Chapter 3. High-level Language-Specific Analysis and Transformation\n\n * Imperative, C++ Pattern match and Rewrite\n * Decalrative, rule-based pattern-match and rewrite using table-driven\n\nC++\n\n/// Fold transpose(transpose(x)) -> x\nstruct SimplifyRedundantTranspose : public mlir::OpRewritePattern<TransposeOp> {\n  SimplifyRedundantTranspose(mlir::MLIRContext *context)\n      : OpRewritePattern<TransposeOp>(context, /*benefit=*/1) {}\n  llvm::LogicalResult\n  matchAndRewrite(TransposeOp op,\n                  mlir::PatternRewriter &rewriter) const override {\n    // Look through the input of the current transpose.\n  ....\n  }\n};\n\n// Register our patterns for rewrite by the Canonicalization framework.\nvoid TransposeOp::getCanonicalizationPatterns(\n    RewritePatternSet &results, MLIRContext *context) {\n  results.add<SimplifyRedundantTranspose>(context);\n}\n\n// Add into Pass Manager\nmlir::PassManager pm(module->getName());\npm.addNestedPass<mlir::toy::FuncOp>(mlir::createCanonicalizerPass());\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nrule-based pattern-match and rewrite (DRR)\n\ndef TypesAreIdentical : Constraint<CPred<"$0.getType() == $1.getType()">>;\ndef RedundantReshapeOptPattern : Pat<\n  (ReshapeOp:$res $arg), (replaceWithValue $arg),\n  [(TypesAreIdentical $res, $arg)]>;\n\n\n1\n2\n3\n4\n\n\n\n# Chapter 4. Enabling Generic Transformation with Interfaces\n\nIf you still remember correctly, in last chapter, we register SimplifyRedundantTranspose into getCanonicalizationPatterns\nwhich applies transformations defined by operations in a greedy, iterative manner.\n\nvoid TransposeOp::getCanonicalizationPatterns(\n    RewritePatternSet &results, MLIRContext *context) {\n  results.add<SimplifyRedundantTranspose>(context);\n}\n\n\n1\n2\n3\n4\n\n\nThis is not scalable.\n\nShape Inference: Preparing for Code Generation\n\nThis starts from an example of inlining all function calls to perform intraprocedural shape propagation.\n\nInlining\n\nMLIR provide a generic inliner algorithm that dialects can plug into.\n\nIn toy, we need to provide interfaces so the inliner could hook into.\n\n 1. The first, we need to define constraints on inlining operations.\n\nDefine a interface, which is a class containing a set of virtual hooks which the dialects can override.\n\nstruct ToyInlinerInterface : public DialectInlinerInterface {\n  using DialectInlinerInterface::DialectInlinerInterface;\n  ...\n  // check whether the region is able to inline\n  isLegalToInline();\n  ...\n  void handleTerminator(Operation *op,\n                        MutableArrayRef<Value> valuesToRepl)\n  ...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n 2. Register dialect interface on to TOY dialect.\n\nvoid ToyDialect::initialize() {\n  addInterfaces<ToyInlinerInterface>();\n}\n\n\n1\n2\n3\n\n 3. We need a way to inform inliner that toy.generic_call is a call and toy.func is function.\n\nWe achieve this goal by operation interface, marking they are callable or callop.\n\ninclude "mlir/Interfaces/CallInterfaces.td"\n\ndef FuncOp : Toy_Op<"func",\n    [DeclareOpInterfaceMethods<CallableOpInterface>]> {\n  ...\n}\n\ndef GenericCallOp : Toy_Op<"generic_call",\n    [DeclareOpInterfaceMethods<CallOpInterface>]> {\n  ...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nThe above DeclareOpInterfaceMethods directive auto-declares all of the interface methods in the class declaration of GenericCallOp.\n\nThen we need to fill in the definitions:\n\n...\nRegion *FuncOp::getCallableRegion()\n...\nCallInterfaceCallable GenericCallOp::getCallableForCallee()\n...\nGenericCallOp::setCalleeFromCallable(CallInterfaceCallable callee)\n\n\n1\n2\n3\n4\n5\n6\n\n 4. Now the inliner has been informed about the Toy dialect.\n\nadd the inlinerpasser to pass manager\n\npm.addPass(mlir::createInlinerPass());\n\n\n1\n\n 5. Considering the input of the transpose function is different from argument.\n\nThen they define a cast Op to cast between different shapes.\n\ndef CastOp : Toy_Op<"cast", [\n    DeclareOpInterfaceMethods<CastOpInterface>,\n    Pure,\n    SameOperandsAndResultShape]\n\n > {\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nNotice that CastOp add CastOpInterface into traits lists.\n\n/// Returns true if the given set of input and result types are compatible with\n/// this cast operation. This is required by the `CastOpInterface` to verify\n/// this operation and provide other additional utilities.\nbool CastOp::areCastCompatible(TypeRange inputs, TypeRange outputs) {\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\nstruct ToyInlinerInterface : public DialectInlinerInterface {\n  ...\n\n  /// Attempts to materialize a conversion for a type mismatch between a call\n  /// from this dialect, and a callable region. This method should generate an\n  /// operation that takes \'input\' as the only operand, and produces a single\n  /// result of \'resultType\'. If a conversion can not be generated, nullptr\n  /// should be returned.\n  Operation *materializeCallConversion(OpBuilder &builder, Value input,\n                                       Type resultType,\n                                       Location conversionLoc) const final {\n    return builder.create<CastOp>(conversionLoc, resultType, input);\n  }\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nThen, the output is as expected.\n\ntoy.func @main() {\n  %0 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>\n  %1 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>\n  %2 = toy.cast %1 : tensor<2x3xf64> to tensor<*xf64>\n  %3 = toy.cast %0 : tensor<2x3xf64> to tensor<*xf64>\n  %4 = toy.transpose(%2 : tensor<*xf64>) to tensor<*xf64>\n  %5 = toy.transpose(%3 : tensor<*xf64>) to tensor<*xf64>\n  %6 = toy.mul %4, %5 : tensor<*xf64>\n  toy.print %6 : tensor<*xf64>\n  toy.return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nIntraprocedural Shape Inference\n\nCurrent state: all function has been inlined and mixed with static and dynamic shaped operations.\n\nThe shape propagation should be generic to many dialects.\n\nOperation Interface\n\nWe could define this by:\n\ndef ShapeInferenceOpInterface : OpInterface<"ShapeInference"> {\n  let description = [{\n    Interface to access a registered method to infer the return types for an\n    operation that can be used during type inference.\n  }];\n\n  let methods = [\n    InterfaceMethod<"Infer and set the output shape for the current operation.",\n                    "void", "inferShapes">\n  ];\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nThen we add interface to necessary Toy Operations.\n\nThis is simple to add CallOpInterface to GenericCallOp.\n\ndef MulOp : Toy_Op<"mul",\n    [..., DeclareOpInterfaceMethods<ShapeInferenceOpInterface>]> {\n  ...\n}\n\n\n1\n2\n3\n4\n\n\nAs to the operations has been added the interface, we need to provide definition of the method.\n\nvoid MulOp::inferShapes() { getResult().setType(getLhs().getType()); }\n\n\n1\n\n\nclass ShapeInferencePass\n    : public mlir::PassWrapper<ShapeInferencePass, OperationPass<FuncOp>> {\n  void runOnOperation() override {\n    FuncOp function = getOperation();\n    ...\n  }\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nHelper method\n\nstd::unique_ptr<mlir::Pass> mlir::toy::createShapeInferencePass() {\n  return std::make_unique<ShapeInferencePass>();\n}\n\n\n1\n2\n3\n\n\npm.addPass(mlir::createShapeInferencePass());\n\n\n1\n\n\nThen the output will be like this:\n\ntoy.func @main() {\n  %0 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>\n  %1 = toy.transpose(%0 : tensor<2x3xf64>) to tensor<3x2xf64>\n  %2 = toy.mul %1, %1 : tensor<3x2xf64>\n  toy.print %2 : tensor<3x2xf64>\n  toy.return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# Chapter 5. Partial Lowering to Lower-Level Dialects for Optimization\n\nDialect Conversions\n\n 1. Conversion Target\n\n * formal specification of what operations or dialects are legal for the conversion\n * Operations that aren’t legal will require rewrite patterns to perform legalization.\n\n 2. Rewrite Patterns\n\n * set of patterns used to convert illegal operations into a set of zero or more legal ones.\n\n 3. Type Convert\n\n# Conversion Target\n\nConvert Toy Operations into combination of operations from:\n\n * Affine\n * Arith\n * Func\n * MemRef\n\nThen we set ToyDialect to be illegal. So it will be lowered.\n\nAnd print will be dynamically legal to keep it untouched.\n\nvoid ToyToAffineLoweringPass::runOnOperation() {\n  target.addLegalDialect<affine::AffineDialect,\n                         arith::ArithDialect,\n                         func::FuncDialect,\n                         memref::MemRefDialect>();\n\n  target.addIllegalDialect<ToyDialect>();\n  target.addDynamicallyLegalOp<toy::PrintOp>([](toy::PrintOp op) {\n    return llvm::none_of(op->getOperandTypes(),\n                         [](Type type) { return type.isa<TensorType>(); });\n  });\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# Conversion Patterns\n\nFor operator that needs lowering, define matchAndRewrite.\n\n/// Lower the `toy.transpose` operation to an affine loop nest.\nstruct TransposeOpLowering : public mlir::ConversionPattern {\n  TransposeOpLowering(mlir::MLIRContext *ctx)\n  llvm::LogicalResult\n  matchAndRewrite(mlir::Operation *op, ArrayRef<mlir::Value> operands,\n                  mlir::ConversionPatternRewriter &rewriter) const final {\n  ...\n   return rewriter.create<mlir::AffineLoadOp>(loc, input, reverseIvs);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nvoid ToyToAffineLoweringPass::runOnOperation() {\n  ...\n\n  // Now that the conversion target has been defined, we just need to provide\n  // the set of patterns that will lower the Toy operations.\n  mlir::RewritePatternSet patterns(&getContext());\n  patterns.add<..., TransposeOpLowering>(&getContext());\n\n  ...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# Partial Lowering\n\nvoid ToyToAffineLoweringPass::runOnOperation() {\n  ...\n  mlir::applyPartialConversion(getOperation(), target, patterns));\n  ...\n}\n\n\n1\n2\n3\n4\n5\n\n\nDesign Considerations with Partial Lowering\n\nIn this lowering, we lower from value-type(TensorType) to allcoate(or buffer)-type(MemRefType).\n\nHowever, toy.print function is not designed for allocate-type from beginning.\n\nThree ways to solve this:\n\n * Generate load operations from the buffer\n * Generate a new version of toy.print that operates on the lowered type\n * Update toy.print to allow for operating on the lowered type\n\nThe third way is chosen due to its simplicity.\n\ndef PrintOp : Toy_Op<"print"> {\n  ...\n\n  // The print operation takes an input tensor to print.\n  // We also allow a F64MemRef to enable interop during partial lowering.\n  let arguments = (ins AnyTypeOf<[F64Tensor, F64MemRef]>:$input);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# Complete Toy Example\n\nCode\n\nfunc.func @main() {\n  %cst = arith.constant 1.000000e+00 : f64\n  %cst_0 = arith.constant 2.000000e+00 : f64\n  %cst_1 = arith.constant 3.000000e+00 : f64\n  %cst_2 = arith.constant 4.000000e+00 : f64\n  %cst_3 = arith.constant 5.000000e+00 : f64\n  %cst_4 = arith.constant 6.000000e+00 : f64\n\n  // Allocating buffers for the inputs and outputs.\n  %0 = memref.alloc() : memref<3x2xf64>\n  %1 = memref.alloc() : memref<3x2xf64>\n  %2 = memref.alloc() : memref<2x3xf64>\n\n  // Initialize the input buffer with the constant values.\n  affine.store %cst, %2[0, 0] : memref<2x3xf64>\n  affine.store %cst_0, %2[0, 1] : memref<2x3xf64>\n  affine.store %cst_1, %2[0, 2] : memref<2x3xf64>\n  affine.store %cst_2, %2[1, 0] : memref<2x3xf64>\n  affine.store %cst_3, %2[1, 1] : memref<2x3xf64>\n  affine.store %cst_4, %2[1, 2] : memref<2x3xf64>\n\n  // Load the transpose value from the input buffer and store it into the\n  // next input buffer.\n  affine.for %arg0 = 0 to 3 {\n    affine.for %arg1 = 0 to 2 {\n      %3 = affine.load %2[%arg1, %arg0] : memref<2x3xf64>\n      affine.store %3, %1[%arg0, %arg1] : memref<3x2xf64>\n    }\n  }\n\n  // Multiply and store into the output buffer.\n  affine.for %arg0 = 0 to 3 {\n    affine.for %arg1 = 0 to 2 {\n      %3 = affine.load %1[%arg0, %arg1] : memref<3x2xf64>\n      %4 = affine.load %1[%arg0, %arg1] : memref<3x2xf64>\n      %5 = arith.mulf %3, %4 : f64\n      affine.store %5, %0[%arg0, %arg1] : memref<3x2xf64>\n    }\n  }\n\n  // Print the value held by the buffer.\n  toy.print %0 : memref<3x2xf64>\n  memref.dealloc %2 : memref<2x3xf64>\n  memref.dealloc %1 : memref<3x2xf64>\n  memref.dealloc %0 : memref<3x2xf64>\n  return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n# Taking Advantage of Affine Optimization\n\nAdd additional passes to the pipeline to reduce redundant loads.\n\n * LoopFusion\n * AffineScalarReplacement\n\n\n# Chapter 6.Lowering to LLVM and CodeGeneration\n\n# Conversion Target\n\n  mlir::ConversionTarget target(getContext());\n  target.addLegalDialect<mlir::LLVMDialect>();\n  target.addLegalOp<mlir::ModuleOp>();\n\n\n1\n2\n3\n\n\n# Type Converter\n\nThis lowering transform the MemRef types which are currently being operated on into a representation in LLVM.\n\n  LLVMTypeConverter typeConverter(&getContext());\n\n\n1\n\n\n# Conversion Patterns\n\naffine, arith and std has provide set of patterns needed to lower them into llvm dialect\n\n  mlir::RewritePatternSet patterns(&getContext());\n  mlir::populateAffineToStdConversionPatterns(patterns, &getContext());\n  mlir::cf::populateSCFToControlFlowConversionPatterns(patterns, &getContext());\n  mlir::arith::populateArithToLLVMConversionPatterns(typeConverter,\n                                                          patterns);\n  mlir::populateFuncToLLVMConversionPatterns(typeConverter, patterns);\n  mlir::cf::populateControlFlowToLLVMConversionPatterns(patterns, &getContext());\n\n  // The only remaining operation, to lower from the `toy` dialect, is the\n  // PrintOp.\n  patterns.add<PrintOpLowering>(&getContext());\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# Full Lowering\n\nmlir::ModuleOp module = getOperation();\nif (mlir::failed(mlir::applyFullConversion(module, target, patterns)))\n  signalPassFailure();\n\n\n1\n2\n3\n\n\nThe generated llvm dialect:\n\nCode\n\nllvm.func @free(!llvm<"i8*">)\nllvm.func @printf(!llvm<"i8*">, ...) -> i32\nllvm.func @malloc(i64) -> !llvm<"i8*">\nllvm.func @main() {\n  %0 = llvm.mlir.constant(1.000000e+00 : f64) : f64\n  %1 = llvm.mlir.constant(2.000000e+00 : f64) : f64\n\n  ...\n\n^bb16:\n  %221 = llvm.extractvalue %25[0] : !llvm<"{ double*, i64, [2 x i64], [2 x i64] }">\n  %222 = llvm.mlir.constant(0 : index) : i64\n  %223 = llvm.mlir.constant(2 : index) : i64\n  %224 = llvm.mul %214, %223 : i64\n  %225 = llvm.add %222, %224 : i64\n  %226 = llvm.mlir.constant(1 : index) : i64\n  %227 = llvm.mul %219, %226 : i64\n  %228 = llvm.add %225, %227 : i64\n  %229 = llvm.getelementptr %221[%228] : (!llvm."double*">, i64) -> !llvm<"f64*">\n  %230 = llvm.load %229 : !llvm<"double*">\n  %231 = llvm.call @printf(%207, %230) : (!llvm<"i8*">, f64) -> i32\n  %232 = llvm.add %219, %218 : i64\n  llvm.br ^bb15(%232 : i64)\n\n  ...\n\n^bb18:\n  %235 = llvm.extractvalue %65[0] : !llvm<"{ double*, i64, [2 x i64], [2 x i64] }">\n  %236 = llvm.bitcast %235 : !llvm<"double*"> to !llvm<"i8*">\n  llvm.call @free(%236) : (!llvm<"i8*">) -> ()\n  %237 = llvm.extractvalue %45[0] : !llvm<"{ double*, i64, [2 x i64], [2 x i64] }">\n  %238 = llvm.bitcast %237 : !llvm<"double*"> to !llvm<"i8*">\n  llvm.call @free(%238) : (!llvm<"i8*">) -> ()\n  %239 = llvm.extractvalue %25[0] : !llvm<"{ double*, i64, [2 x i64], [2 x i64] }">\n  %240 = llvm.bitcast %239 : !llvm<"double*"> to !llvm<"i8*">\n  llvm.call @free(%240) : (!llvm<"i8*">) -> ()\n  llvm.return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n',normalizedContent:'# chapter 1. toy language and ast\n\nlexer and recursive descent parser construt ast\n\n\n# chapter 2. emit basic mlir\n\noperations: instructions, globals(functions), modules, in llvm\n\nfrom link\n\ntranspose operation\n\n%t_tensor = "toy.transpose"(%tensor) {inplace = true} : (tensor<2x3xf64>) ->\ntensor<3x2xf64> loc("example/file/path":12:1)\n\n\n1\n2\n\n\nmeaning of each part\n\nresult  = name of operation (input operands) dictionary of traits :\ntype of operations (input and output), location\n\n\n1\n2\n\n * a name for the operation.\n * a list of ssa operand values.\n * a list of attributes.\n * a list of types for result values.\n * a source location for debugging purposes.\n * a list of successors blocks (for branches, mostly).\n * a list of regions (for structural operations like functions).\n\nopaque api\n\ndefine a toy dialect\n\ndialect could be defined by c++ or tablegen(declarative specification).\n\nafter the definiation, it could be loaded to mlir context. context.loaddialect();\n\ndefining toy operations\n\nclass constantop : public mlir::op<\n                     /// `mlir::op` is a crtp class, meaning that we provide the\n                     /// derived class as a template parameter.\n                     constantop,\n                     /// the constantop takes zero input operands.\n                     mlir::optrait::zerooperands,\n                     /// the constantop returns a single result.\n                     mlir::optrait::oneresult,\n                     /// we also provide a utility `gettype` accessor that\n                     /// returns the tensortype of the single result.\n                     mlir::optraits::onetypedresult<tensortype>::impl> {\n\n public:\n  /// inherit the constructors from the base op class.\n  using op::op;\n  ...\n  static void build(mlir::opbuilder &builder, mlir::operationstate &state,\n                    mlir::type result, mlir::denseelementsattr value);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\nregister operation:\n\nvoid toydialect::initialize() {\n  addoperations<constantop>();\n}\n\n\n1\n2\n3\n\n\nop vs operation: using mlir operations\n\nusing operation definition specification framwork\n\nbase toy_op\n\nclass toy_op<string mnemonic, list<trait> traits = []> :\n    op<toy_dialect, mnemonic, traits>;\n\n\n1\n2\n\n\nconstantop\n\ndef constantop : toy_op<"constant"> {\n}\n\n\n1\n2\n\n\nattaching build methods\n\nin constantop, it declared a list of build. ods will generate the first build.\nas to other builds, we have to atttach.\n\ndef constantop : toy_op<"constant"> {\n  ...\n\n  // add custom build methods for the constant operation. these methods populate\n  // the `state` that mlir uses to create operations, i.e. these are used when\n  // using `builder.create<constantop>(...)`.\n  let builders = [\n    // build a constant with a given constant tensor value.\n    opbuilder<(ins "denseelementsattr":$value), [{\n      // call into an autogenerated `build` method.\n      build(builder, result, value.gettype(), value);\n    }]>,\n\n    // build a constant with a given constant floating-point value. this builder\n    // creates a declaration for `constantop::build` with the given parameters.\n    opbuilder<(ins "double":$value)>\n  ];\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\nspecifying a custom assembly format\n\nthe printout version of ir has too much information.\n\nwe can strip out by implementing our owne oversion of print and parse function.\n\ntake printop as example.\n\nvoid printop::print(mlir::opasmprinter &printer) {\n  printer << "toy.print " << op.input();\n  printer.printoptionalattrdict(op.getattrs());\n  printer << " : " << op.input().gettype();\n}\n\nmlir::parseresult printop::parse(mlir::opasmparser &parser,\n                                 mlir::operationstate &result) {\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# chapter 3. high-level language-specific analysis and transformation\n\n * imperative, c++ pattern match and rewrite\n * decalrative, rule-based pattern-match and rewrite using table-driven\n\nc++\n\n/// fold transpose(transpose(x)) -> x\nstruct simplifyredundanttranspose : public mlir::oprewritepattern<transposeop> {\n  simplifyredundanttranspose(mlir::mlircontext *context)\n      : oprewritepattern<transposeop>(context, /*benefit=*/1) {}\n  llvm::logicalresult\n  matchandrewrite(transposeop op,\n                  mlir::patternrewriter &rewriter) const override {\n    // look through the input of the current transpose.\n  ....\n  }\n};\n\n// register our patterns for rewrite by the canonicalization framework.\nvoid transposeop::getcanonicalizationpatterns(\n    rewritepatternset &results, mlircontext *context) {\n  results.add<simplifyredundanttranspose>(context);\n}\n\n// add into pass manager\nmlir::passmanager pm(module->getname());\npm.addnestedpass<mlir::toy::funcop>(mlir::createcanonicalizerpass());\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nrule-based pattern-match and rewrite (drr)\n\ndef typesareidentical : constraint<cpred<"$0.gettype() == $1.gettype()">>;\ndef redundantreshapeoptpattern : pat<\n  (reshapeop:$res $arg), (replacewithvalue $arg),\n  [(typesareidentical $res, $arg)]>;\n\n\n1\n2\n3\n4\n\n\n\n# chapter 4. enabling generic transformation with interfaces\n\nif you still remember correctly, in last chapter, we register simplifyredundanttranspose into getcanonicalizationpatterns\nwhich applies transformations defined by operations in a greedy, iterative manner.\n\nvoid transposeop::getcanonicalizationpatterns(\n    rewritepatternset &results, mlircontext *context) {\n  results.add<simplifyredundanttranspose>(context);\n}\n\n\n1\n2\n3\n4\n\n\nthis is not scalable.\n\nshape inference: preparing for code generation\n\nthis starts from an example of inlining all function calls to perform intraprocedural shape propagation.\n\ninlining\n\nmlir provide a generic inliner algorithm that dialects can plug into.\n\nin toy, we need to provide interfaces so the inliner could hook into.\n\n 1. the first, we need to define constraints on inlining operations.\n\ndefine a interface, which is a class containing a set of virtual hooks which the dialects can override.\n\nstruct toyinlinerinterface : public dialectinlinerinterface {\n  using dialectinlinerinterface::dialectinlinerinterface;\n  ...\n  // check whether the region is able to inline\n  islegaltoinline();\n  ...\n  void handleterminator(operation *op,\n                        mutablearrayref<value> valuestorepl)\n  ...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n 2. register dialect interface on to toy dialect.\n\nvoid toydialect::initialize() {\n  addinterfaces<toyinlinerinterface>();\n}\n\n\n1\n2\n3\n\n 3. we need a way to inform inliner that toy.generic_call is a call and toy.func is function.\n\nwe achieve this goal by operation interface, marking they are callable or callop.\n\ninclude "mlir/interfaces/callinterfaces.td"\n\ndef funcop : toy_op<"func",\n    [declareopinterfacemethods<callableopinterface>]> {\n  ...\n}\n\ndef genericcallop : toy_op<"generic_call",\n    [declareopinterfacemethods<callopinterface>]> {\n  ...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nthe above declareopinterfacemethods directive auto-declares all of the interface methods in the class declaration of genericcallop.\n\nthen we need to fill in the definitions:\n\n...\nregion *funcop::getcallableregion()\n...\ncallinterfacecallable genericcallop::getcallableforcallee()\n...\ngenericcallop::setcalleefromcallable(callinterfacecallable callee)\n\n\n1\n2\n3\n4\n5\n6\n\n 4. now the inliner has been informed about the toy dialect.\n\nadd the inlinerpasser to pass manager\n\npm.addpass(mlir::createinlinerpass());\n\n\n1\n\n 5. considering the input of the transpose function is different from argument.\n\nthen they define a cast op to cast between different shapes.\n\ndef castop : toy_op<"cast", [\n    declareopinterfacemethods<castopinterface>,\n    pure,\n    sameoperandsandresultshape]\n\n > {\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nnotice that castop add castopinterface into traits lists.\n\n/// returns true if the given set of input and result types are compatible with\n/// this cast operation. this is required by the `castopinterface` to verify\n/// this operation and provide other additional utilities.\nbool castop::arecastcompatible(typerange inputs, typerange outputs) {\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\nstruct toyinlinerinterface : public dialectinlinerinterface {\n  ...\n\n  /// attempts to materialize a conversion for a type mismatch between a call\n  /// from this dialect, and a callable region. this method should generate an\n  /// operation that takes \'input\' as the only operand, and produces a single\n  /// result of \'resulttype\'. if a conversion can not be generated, nullptr\n  /// should be returned.\n  operation *materializecallconversion(opbuilder &builder, value input,\n                                       type resulttype,\n                                       location conversionloc) const final {\n    return builder.create<castop>(conversionloc, resulttype, input);\n  }\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nthen, the output is as expected.\n\ntoy.func @main() {\n  %0 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>\n  %1 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>\n  %2 = toy.cast %1 : tensor<2x3xf64> to tensor<*xf64>\n  %3 = toy.cast %0 : tensor<2x3xf64> to tensor<*xf64>\n  %4 = toy.transpose(%2 : tensor<*xf64>) to tensor<*xf64>\n  %5 = toy.transpose(%3 : tensor<*xf64>) to tensor<*xf64>\n  %6 = toy.mul %4, %5 : tensor<*xf64>\n  toy.print %6 : tensor<*xf64>\n  toy.return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nintraprocedural shape inference\n\ncurrent state: all function has been inlined and mixed with static and dynamic shaped operations.\n\nthe shape propagation should be generic to many dialects.\n\noperation interface\n\nwe could define this by:\n\ndef shapeinferenceopinterface : opinterface<"shapeinference"> {\n  let description = [{\n    interface to access a registered method to infer the return types for an\n    operation that can be used during type inference.\n  }];\n\n  let methods = [\n    interfacemethod<"infer and set the output shape for the current operation.",\n                    "void", "infershapes">\n  ];\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nthen we add interface to necessary toy operations.\n\nthis is simple to add callopinterface to genericcallop.\n\ndef mulop : toy_op<"mul",\n    [..., declareopinterfacemethods<shapeinferenceopinterface>]> {\n  ...\n}\n\n\n1\n2\n3\n4\n\n\nas to the operations has been added the interface, we need to provide definition of the method.\n\nvoid mulop::infershapes() { getresult().settype(getlhs().gettype()); }\n\n\n1\n\n\nclass shapeinferencepass\n    : public mlir::passwrapper<shapeinferencepass, operationpass<funcop>> {\n  void runonoperation() override {\n    funcop function = getoperation();\n    ...\n  }\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nhelper method\n\nstd::unique_ptr<mlir::pass> mlir::toy::createshapeinferencepass() {\n  return std::make_unique<shapeinferencepass>();\n}\n\n\n1\n2\n3\n\n\npm.addpass(mlir::createshapeinferencepass());\n\n\n1\n\n\nthen the output will be like this:\n\ntoy.func @main() {\n  %0 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>\n  %1 = toy.transpose(%0 : tensor<2x3xf64>) to tensor<3x2xf64>\n  %2 = toy.mul %1, %1 : tensor<3x2xf64>\n  toy.print %2 : tensor<3x2xf64>\n  toy.return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# chapter 5. partial lowering to lower-level dialects for optimization\n\ndialect conversions\n\n 1. conversion target\n\n * formal specification of what operations or dialects are legal for the conversion\n * operations that aren’t legal will require rewrite patterns to perform legalization.\n\n 2. rewrite patterns\n\n * set of patterns used to convert illegal operations into a set of zero or more legal ones.\n\n 3. type convert\n\n# conversion target\n\nconvert toy operations into combination of operations from:\n\n * affine\n * arith\n * func\n * memref\n\nthen we set toydialect to be illegal. so it will be lowered.\n\nand print will be dynamically legal to keep it untouched.\n\nvoid toytoaffineloweringpass::runonoperation() {\n  target.addlegaldialect<affine::affinedialect,\n                         arith::arithdialect,\n                         func::funcdialect,\n                         memref::memrefdialect>();\n\n  target.addillegaldialect<toydialect>();\n  target.adddynamicallylegalop<toy::printop>([](toy::printop op) {\n    return llvm::none_of(op->getoperandtypes(),\n                         [](type type) { return type.isa<tensortype>(); });\n  });\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# conversion patterns\n\nfor operator that needs lowering, define matchandrewrite.\n\n/// lower the `toy.transpose` operation to an affine loop nest.\nstruct transposeoplowering : public mlir::conversionpattern {\n  transposeoplowering(mlir::mlircontext *ctx)\n  llvm::logicalresult\n  matchandrewrite(mlir::operation *op, arrayref<mlir::value> operands,\n                  mlir::conversionpatternrewriter &rewriter) const final {\n  ...\n   return rewriter.create<mlir::affineloadop>(loc, input, reverseivs);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nvoid toytoaffineloweringpass::runonoperation() {\n  ...\n\n  // now that the conversion target has been defined, we just need to provide\n  // the set of patterns that will lower the toy operations.\n  mlir::rewritepatternset patterns(&getcontext());\n  patterns.add<..., transposeoplowering>(&getcontext());\n\n  ...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# partial lowering\n\nvoid toytoaffineloweringpass::runonoperation() {\n  ...\n  mlir::applypartialconversion(getoperation(), target, patterns));\n  ...\n}\n\n\n1\n2\n3\n4\n5\n\n\ndesign considerations with partial lowering\n\nin this lowering, we lower from value-type(tensortype) to allcoate(or buffer)-type(memreftype).\n\nhowever, toy.print function is not designed for allocate-type from beginning.\n\nthree ways to solve this:\n\n * generate load operations from the buffer\n * generate a new version of toy.print that operates on the lowered type\n * update toy.print to allow for operating on the lowered type\n\nthe third way is chosen due to its simplicity.\n\ndef printop : toy_op<"print"> {\n  ...\n\n  // the print operation takes an input tensor to print.\n  // we also allow a f64memref to enable interop during partial lowering.\n  let arguments = (ins anytypeof<[f64tensor, f64memref]>:$input);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# complete toy example\n\ncode\n\nfunc.func @main() {\n  %cst = arith.constant 1.000000e+00 : f64\n  %cst_0 = arith.constant 2.000000e+00 : f64\n  %cst_1 = arith.constant 3.000000e+00 : f64\n  %cst_2 = arith.constant 4.000000e+00 : f64\n  %cst_3 = arith.constant 5.000000e+00 : f64\n  %cst_4 = arith.constant 6.000000e+00 : f64\n\n  // allocating buffers for the inputs and outputs.\n  %0 = memref.alloc() : memref<3x2xf64>\n  %1 = memref.alloc() : memref<3x2xf64>\n  %2 = memref.alloc() : memref<2x3xf64>\n\n  // initialize the input buffer with the constant values.\n  affine.store %cst, %2[0, 0] : memref<2x3xf64>\n  affine.store %cst_0, %2[0, 1] : memref<2x3xf64>\n  affine.store %cst_1, %2[0, 2] : memref<2x3xf64>\n  affine.store %cst_2, %2[1, 0] : memref<2x3xf64>\n  affine.store %cst_3, %2[1, 1] : memref<2x3xf64>\n  affine.store %cst_4, %2[1, 2] : memref<2x3xf64>\n\n  // load the transpose value from the input buffer and store it into the\n  // next input buffer.\n  affine.for %arg0 = 0 to 3 {\n    affine.for %arg1 = 0 to 2 {\n      %3 = affine.load %2[%arg1, %arg0] : memref<2x3xf64>\n      affine.store %3, %1[%arg0, %arg1] : memref<3x2xf64>\n    }\n  }\n\n  // multiply and store into the output buffer.\n  affine.for %arg0 = 0 to 3 {\n    affine.for %arg1 = 0 to 2 {\n      %3 = affine.load %1[%arg0, %arg1] : memref<3x2xf64>\n      %4 = affine.load %1[%arg0, %arg1] : memref<3x2xf64>\n      %5 = arith.mulf %3, %4 : f64\n      affine.store %5, %0[%arg0, %arg1] : memref<3x2xf64>\n    }\n  }\n\n  // print the value held by the buffer.\n  toy.print %0 : memref<3x2xf64>\n  memref.dealloc %2 : memref<2x3xf64>\n  memref.dealloc %1 : memref<3x2xf64>\n  memref.dealloc %0 : memref<3x2xf64>\n  return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n# taking advantage of affine optimization\n\nadd additional passes to the pipeline to reduce redundant loads.\n\n * loopfusion\n * affinescalarreplacement\n\n\n# chapter 6.lowering to llvm and codegeneration\n\n# conversion target\n\n  mlir::conversiontarget target(getcontext());\n  target.addlegaldialect<mlir::llvmdialect>();\n  target.addlegalop<mlir::moduleop>();\n\n\n1\n2\n3\n\n\n# type converter\n\nthis lowering transform the memref types which are currently being operated on into a representation in llvm.\n\n  llvmtypeconverter typeconverter(&getcontext());\n\n\n1\n\n\n# conversion patterns\n\naffine, arith and std has provide set of patterns needed to lower them into llvm dialect\n\n  mlir::rewritepatternset patterns(&getcontext());\n  mlir::populateaffinetostdconversionpatterns(patterns, &getcontext());\n  mlir::cf::populatescftocontrolflowconversionpatterns(patterns, &getcontext());\n  mlir::arith::populatearithtollvmconversionpatterns(typeconverter,\n                                                          patterns);\n  mlir::populatefunctollvmconversionpatterns(typeconverter, patterns);\n  mlir::cf::populatecontrolflowtollvmconversionpatterns(patterns, &getcontext());\n\n  // the only remaining operation, to lower from the `toy` dialect, is the\n  // printop.\n  patterns.add<printoplowering>(&getcontext());\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# full lowering\n\nmlir::moduleop module = getoperation();\nif (mlir::failed(mlir::applyfullconversion(module, target, patterns)))\n  signalpassfailure();\n\n\n1\n2\n3\n\n\nthe generated llvm dialect:\n\ncode\n\nllvm.func @free(!llvm<"i8*">)\nllvm.func @printf(!llvm<"i8*">, ...) -> i32\nllvm.func @malloc(i64) -> !llvm<"i8*">\nllvm.func @main() {\n  %0 = llvm.mlir.constant(1.000000e+00 : f64) : f64\n  %1 = llvm.mlir.constant(2.000000e+00 : f64) : f64\n\n  ...\n\n^bb16:\n  %221 = llvm.extractvalue %25[0] : !llvm<"{ double*, i64, [2 x i64], [2 x i64] }">\n  %222 = llvm.mlir.constant(0 : index) : i64\n  %223 = llvm.mlir.constant(2 : index) : i64\n  %224 = llvm.mul %214, %223 : i64\n  %225 = llvm.add %222, %224 : i64\n  %226 = llvm.mlir.constant(1 : index) : i64\n  %227 = llvm.mul %219, %226 : i64\n  %228 = llvm.add %225, %227 : i64\n  %229 = llvm.getelementptr %221[%228] : (!llvm."double*">, i64) -> !llvm<"f64*">\n  %230 = llvm.load %229 : !llvm<"double*">\n  %231 = llvm.call @printf(%207, %230) : (!llvm<"i8*">, f64) -> i32\n  %232 = llvm.add %219, %218 : i64\n  llvm.br ^bb15(%232 : i64)\n\n  ...\n\n^bb18:\n  %235 = llvm.extractvalue %65[0] : !llvm<"{ double*, i64, [2 x i64], [2 x i64] }">\n  %236 = llvm.bitcast %235 : !llvm<"double*"> to !llvm<"i8*">\n  llvm.call @free(%236) : (!llvm<"i8*">) -> ()\n  %237 = llvm.extractvalue %45[0] : !llvm<"{ double*, i64, [2 x i64], [2 x i64] }">\n  %238 = llvm.bitcast %237 : !llvm<"double*"> to !llvm<"i8*">\n  llvm.call @free(%238) : (!llvm<"i8*">) -> ()\n  %239 = llvm.extractvalue %25[0] : !llvm<"{ double*, i64, [2 x i64], [2 x i64] }">\n  %240 = llvm.bitcast %239 : !llvm<"double*"> to !llvm<"i8*">\n  llvm.call @free(%240) : (!llvm<"i8*">) -> ()\n  llvm.return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"MLIR Compiling Flow of Conv2D",frontmatter:{title:"MLIR Compiling Flow of Conv2D",date:"2025-03-04T00:00:00.000Z",permalink:"/pages/000014/",tags:[null]},regularPath:"/02.compiler/14.mlir_notes_02.html",relativePath:"02.compiler/14.mlir_notes_02.md",key:"v-7ba96ade",path:"/pages/000014/",headers:[{level:2,title:"High-Level Representation",slug:"high-level-representation",normalizedTitle:"high-level representation",charIndex:26},{level:2,title:"Lowering Process through MLIR Dialects",slug:"lowering-process-through-mlir-dialects",normalizedTitle:"lowering process through mlir dialects",charIndex:306},{level:3,title:"1. Torch Dialect (High-Level)",slug:"_1-torch-dialect-high-level",normalizedTitle:"1. torch dialect (high-level)",charIndex:349},{level:3,title:"2. Linalg Dialect Transformation",slug:"_2-linalg-dialect-transformation",normalizedTitle:"2. linalg dialect transformation",charIndex:575},{level:3,title:"3. Memref Dialect",slug:"_3-memref-dialect",normalizedTitle:"3. memref dialect",charIndex:1996},{level:3,title:"4. Vectorization",slug:"_4-vectorization",normalizedTitle:"4. vectorization",charIndex:8676},{level:3,title:"5. Bufferization",slug:"_5-bufferization",normalizedTitle:"5. bufferization",charIndex:14721},{level:3,title:"6. Affine Dialect and Scheduling",slug:"_6-affine-dialect-and-scheduling",normalizedTitle:"6. affine dialect and scheduling",charIndex:16118},{level:3,title:"7. Standard/SCF Dialect",slug:"_7-standard-scf-dialect",normalizedTitle:"7. standard/scf dialect",charIndex:23016},{level:3,title:"8. Final Lowering to PTX",slug:"_8-final-lowering-to-ptx",normalizedTitle:"8. final lowering to ptx",charIndex:29874},{level:2,title:"Key Transformations",slug:"key-transformations",normalizedTitle:"key transformations",charIndex:1798},{level:2,title:"Optimization Considerations",slug:"optimization-considerations",normalizedTitle:"optimization considerations",charIndex:30363}],headersStr:"High-Level Representation Lowering Process through MLIR Dialects 1. Torch Dialect (High-Level) 2. Linalg Dialect Transformation 3. Memref Dialect 4. Vectorization 5. Bufferization 6. Affine Dialect and Scheduling 7. Standard/SCF Dialect 8. Final Lowering to PTX Key Transformations Optimization Considerations",content:"# MLIR Compiling Flow\n\n\n# High-Level Representation\n\nAt the high-level dialect, a Conv2d layer in PyTorch can be represented as a computational graph operation that performs 2D convolution with specific parameters:\n\n * Input tensor\n * Kernel weights\n * Bias (optional)\n * Stride\n * Padding\n * Dilation\n\n\n# Lowering Process through MLIR Dialects\n\n\n# 1. Torch Dialect (High-Level)\n\n * Initial representation of the Conv2d operation\n * Captures semantic intent of the convolution\n * Preserves high-level information about tensor shapes, strides, and computational semantics\n\n\n# 2. Linalg Dialect Transformation\n\n * Converts the high-level operation to more explicit linear algebra operations\n * Represents convolution as nested loops and tensor comprehensions\n * Breaks down the convolution into explicit:\n   * Input sliding window operations\n   * Kernel multiplication\n   * Accumulation of results\n * Introduces explicit iteration spaces and reduction domains\n\n# Demo in Conv2d\n\nAffine Maps (#map, #map1, #map2)\n\n * Define how input tensors are indexed and transformed\n * Specify the iteration spaces for convolution\n * Map between input, kernel, and output tensor dimensions\n\nConvolution Operation Structure\n\n * Input: 1x3x224x224 tensor (batch, channels, height, width)\n * Kernel: 64x3x3x3 tensor (output channels, input channels, kernel height, kernel width)\n * Output: 1x64x222x222 tensor (reduced spatial dimensions due to convolution)\n\nLinalg.generic Operation\n\n * Represents the core convolution computation\n * Uses explicit reduction domains\n * Iterator types show parallel and reduction dimensions\n * Performs element-wise multiplication and reduction\n\nComputation Breakdown\n\n * Initialize output tensor with zeros\n * Perform convolution through nested reductions\n * Optional bias addition\n\nKey Transformations\n\n * Converts high-level convolution to explicit tensor operations\n * Shows computational intent through explicit iterations\n * Prepares for further lowering and optimization\n\n\n# 3. Memref Dialect\n\n * Transforms tensor representations to memory reference (memref) dialect\n * Converts abstract tensor operations to concrete memory layouts\n * Handles:\n   * Memory allocation\n   * Memory access patterns\n   * Contiguous vs. strided memory representations\n * Prepares for lower-level memory optimizations\n\n# Demo in Memref Dialect\n\nMemory Allocation\n\n * Explicit buffer allocation using memref.alloc()\n * Direct memory management instead of tensor abstractions\n * Allows for precise control over memory layout and lifetime\n\nExplicit Nested Loops\n\n * Uses affine.for to represent iteration spaces\n * Breaks down convolution into explicit nested loops\n * Provides fine-grained control over computation\n\nMemory Access Patterns\n\n * memref.load and memref.store for explicit memory interactions\n * Uses affine maps to compute dynamic indices\n * Shows exact memory access and computation steps\n\nComputation Breakdown\n\n * Separate functions for:\n   * Buffer allocation\n   * Convolution computation\n   * Bias addition\n * Enables more explicit memory and computation management**\n\nTransformation Characteristics\n\n * Moves from tensor abstractions to concrete memory references\n * Prepares for lower-level optimizations\n * Enables hardware-specific memory optimizations\n\nKey Differences from Linalg Dialect\n\n * More explicit memory management\n * Concrete buffer allocations\n * Detailed loop structures\n * Direct memory access operations\n\nThe Memref representation provides a lower-level view of the convolution operation, showing how the computation is performed through explicit memory accesses and loop iterations.\n\nCode\n\n// Memref Dialect Representation of Conv2d Operation\n\n// Memref conversion focuses on explicit memory layouts and buffer management\nmodule {\n    // Memory allocation function for input, kernel, and output\n    func.func @allocate_buffers() -> (\n        memref<1x3x224x224xf32>,\n        memref<64x3x3x3xf32>, \n        memref<1x64x222x222xf32>) {\n        // Allocate input buffer\n        %input = memref.alloc() : memref<1x3x224x224xf32>\n        \n        // Allocate kernel buffer\n        %kernel = memref.alloc() : memref<64x3x3x3xf32>\n        \n        // Allocate output buffer (initialized with zeros)\n        %output = memref.alloc() : memref<1x64x222x222xf32>\n        %zero = arith.constant 0.0 : f32\n        linalg.fill ins(%zero : f32) \n            outs(%output : memref<1x64x222x222xf32>)\n\n        return %input, %kernel, %output : \n            memref<1x3x224x224xf32>, \n            memref<64x3x3x3xf32>, \n            memref<1x64x222x222xf32>\n    }\n\n    // Explicit Conv2d implementation using memref\n    func.func @conv2d(\n        %input: memref<1x3x224x224xf32>, \n        %kernel: memref<64x3x3x3xf32>, \n        %output: memref<1x64x222x222xf32>) {\n        affine.for %batch = 0 to 1 {\n            affine.for %out_channel = 0 to 64 {\n                affine.for %out_height = 0 to 222 {\n                    affine.for %out_width = 0 to 222 {\n                        // Reset output value\n                        %init_val = memref.load %output[\n                            %batch, %out_channel, \n                            %out_height, %out_width\n                        ] : memref<1x64x222x222xf32>\n                        \n                        // Inner loops for input channels and kernel\n                        affine.for %in_channel = 0 to 3 {\n                            affine.for %k_height = 0 to 3 {\n                                affine.for %k_width = 0 to 3 {\n                                    // Compute input and kernel indices\n                                    %input_h = affine.apply affine_map<(d0, d1)\n                                                -> (d0 + d1)>(%out_height, %k_height)\n                                    %input_w = affine.apply affine_map<(d0, d1)\n                                                -> (d0 + d1)>(%out_width, %k_width)\n                                    \n                                    // Load input and kernel values\n                                    %input_val = memref.load %input[\n                                        %batch, %in_channel, %input_h, %input_w\n                                    ] : memref<1x3x224x224xf32>\n\n                                    %kernel_val = memref.load %kernel[\n                                        %out_channel, %in_channel, \n                                        %k_height, %k_width\n                                    ] : memref<64x3x3x3xf32>\n                                    \n                                    // Compute convolution\n                                    %mul = arith.mulf %input_val, %kernel_val : f32\n                                    %add = arith.addf %init_val, %mul : f32\n                                    \n                                    // Store updated output\n                                    memref.store %add, %output[\n                                        %batch, %out_channel,\n                                        %out_height, %out_width\n                                    ] : memref<1x64x222x222xf32>\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        return\n    }\n\n    // Bias addition function\n    func.func @add_bias(%output: memref<1x64x222x222xf32>, %bias: memref<64xf32>) {\n        affine.for %batch = 0 to 1 {\n            affine.for %channel = 0 to 64 {\n                affine.for %height = 0 to 222 {\n                    affine.for %width = 0 to 222 {\n                        // Load output and bias values\n                        %output_val = memref.load %output[%batch, %channel, %height, %width]\n                                                                    : memref<1x64x222x222xf32>\n                        %bias_val = memref.load %bias[%channel] : memref<64xf32>\n                        \n                        // Add bias\n                        %added = arith.addf %output_val, %bias_val : f32\n                        \n                        // Store result\n                        memref.store %added, %output[%batch, %channel, %height, %width] :\n                                                                memref<1x64x222x222xf32>\n                    }\n                }\n            }\n        }\n        return\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n\n\n\n# 4. Vectorization\n\n * Transforms loop-based representations into vector operations\n * Applies SIMD (Single Instruction, Multiple Data) transformations\n * Converts scalar computations to vector instructions\n * Optimizes computation by:\n   * Grouping similar operations\n   * Utilizing vector processing units\n   * Reducing instruction overhead\n\n# Demo in Vectorization\n\nVector Type Definitions\n\n * Uses vector registers (e.g., 256-bit AVX2/AVX-512)\n * Defines vector types for different computation sizes\n * Enables SIMD (Single Instruction, Multiple Data) processing\n\nVectorization Strategies\n\n * Loop vectorization with step sizes matching vector width\n * Vector load/store operations\n * SIMD multiplication and reduction\n * Parallel processing of multiple elements\n\nComputation Transformation\n\n * Converts scalar computations to vector operations\n * Uses vector.load, vector.store\n * Applies vector-level operations like vector.mul, vector.reduction\n\nOptimization Techniques\n\n * Processes multiple elements simultaneously\n * Reduces instruction overhead\n * Improves computational efficiency\n * Enables parallel hardware utilization\n\nKey Transformations\n\n * Scalar loops converted to vector operations\n * Explicit SIMD instruction mapping\n * Parallel computation across vector lanes\n\nAdditional Vectorization Utilities\n\n * vector.transfer_read\n * vector.splat\n * Enables type conversions and vector manipulations\n\nCompared to previous representations\n\n * More hardware-specific\n * Explicit parallel computation\n * Focuses on computational efficiency\n * Prepares for low-level code generation\n\nCode\n\n// Vectorization Dialect Representation of Conv2d Operation\n\nmodule {\n    // Vector type definitions\n    // Using 256-bit vector registers (typical for AVX2/AVX-512)\n    // f32 vector with 8 elements per register\n    #vec_8x_f32 = vector.type<8xf32>\n    #vec_64x_f32 = vector.type<64xf32>\n\n    // Vectorized Conv2d Function\n    func.func @vectorized_conv2d(\n        %input: memref<1x3x224x224xf32>, \n        %kernel: memref<64x3x3x3xf32>, \n        %output: memref<1x64x222x222xf32>\n    ) {\n        // Outer loop vectorization dimensions\n        affine.for %batch = 0 to 1 {\n            affine.for %out_channel = 0 to 64 step 8 {\n                affine.for %out_height = 0 to 222 {\n                    affine.for %out_width = 0 to 222 step 8 {\n                        // Vector load output (pre-initialized with zeros)\n                        %output_vec = vector.load %output[%batch, %out_channel : vector<8xf32>] \n                            : memref<1x64x222x222xf32>, vector<8xf32>\n\n                        // Vectorized inner convolution computation\n                        %result_vec = scf.reduce(%output_vec) : vector<8xf32> {\n                        ^bb0(%acc: vector<8xf32>, %_: vector<8xf32>):\n                            // Nested reductions for input channels and kernel\n                            %channel_result = vector.reduction <add>, %acc : vector<8xf32>\n                            scf.reduce.return %channel_result : vector<8xf32>\n                        } : vector<8xf32>\n\n                        // Vectorized kernel and input loading\n                        %kernel_vec = vector.load %kernel[%out_channel, 0, 0, 0 : vector<64xf32>] \n                            : memref<64x3x3x3xf32>, vector<64xf32>\n                        %input_vec = vector.load %input[%batch, 0, 0, 0 : vector<64xf32>] \n                            : memref<1x3x224x224xf32>, vector<64xf32>\n\n                        // SIMD vector multiplication\n                        %mul_vec = vector.mul %input_vec, %kernel_vec : vector<64xf32>\n\n                        // Vectorized reduction\n                        %reduced_vec = vector.reduction <add>, %mul_vec : vector<64xf32>\n\n                        // Vector store result back to output\n                        vector.store %reduced_vec, %output[%batch, %out_channel : vector<8xf32>] \n                            : memref<1x64x222x222xf32>, vector<8xf32>\n                    }\n                }\n            }\n        }\n        return\n    }\n\n    // Vectorized Bias Addition\n    func.func @vectorized_bias_add(\n        %output: memref<1x64x222x222xf32>, \n        %bias: memref<64xf32>\n    ) {\n        // Vectorized bias addition\n        affine.for %batch = 0 to 1 {\n            affine.for %channel = 0 to 64 step 8 {\n                // Load bias vector\n                %bias_vec = vector.load %bias[%channel : vector<8xf32>] \n                    : memref<64xf32>, vector<8xf32>\n\n                affine.for %height = 0 to 222 {\n                    affine.for %width = 0 to 222 step 8 {\n                        // Load output vector\n                        %output_vec = vector.load %output[%batch, %channel, %height, %width : vector<8xf32>] \n                            : memref<1x64x222x222xf32>, vector<8xf32>\n\n                        // Vectorized bias addition\n                        %added_vec = vector.add %output_vec, %bias_vec : vector<8xf32>\n\n                        // Store result back\n                        vector.store %added_vec, %output[%batch, %channel, %height, %width : vector<8xf32>] \n                            : memref<1x64x222x222xf32>, vector<8xf32>\n                    }\n                }\n            }\n        }\n        return\n    }\n\n    // Vectorization Transformation Utility\n    func.func @vectorize_conv2d_transform(%input: memref<1x3x224x224xf32>) {\n        // Vector transfer operations\n        %c0 = arith.constant 0 : index\n        %vec_input = vector.transfer_read %input[%c0, %c0, %c0, %c0], %c0 \n            : memref<1x3x224x224xf32>, vector<8x3x3x3xf32>\n\n        // Vectorized type conversions\n        %splat = vector.splat %c0 : vector<8xf32>\n        \n        return\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n\n\n\n# 5. Bufferization\n\n * Converts tensor operations to explicit buffer allocations\n * Removes tensor abstraction\n * Manages memory allocation and deallocation\n * Converts SSA (Static Single Assignment) values to explicit memory buffers\n * Prepares for hardware-specific memory management\n\n# Demo in Bufferization\n\nTensor to Buffer Conversion\n\n * Transforms tensor operations to explicit memory buffers\n * Uses bufferization.alloc_tensor for memory allocation\n * Tracks memory space and allocation characteristics\n\nMemory Management Techniques\n\n * Explicit buffer views using memref.view\n * Buffer casting with bufferization.buffer_cast\n * One-shot allocation with memory tracking\n * Enables precise memory layout control\n\nComputational Characteristics\n\n * Preserves computational semantics of Conv2d\n * Provides explicit memory access patterns\n * Enables in-place updates and modifications\n\nBufferization Annotations\n\n * copy_memory flag for memory duplication\n * Memory space specification\n * Alias analysis support\n\nTransformation Goals\n\n * Remove tensor abstractions\n * Prepare for hardware-specific optimizations\n * Enable explicit memory management\n * Support efficient memory access patterns\n\nKey Differences from Previous Representations\n\n * More explicit memory management\n * Precise buffer allocation and tracking\n * Prepares for low-level code generation\n * Focuses on memory efficiency\n\n\n# 6. Affine Dialect and Scheduling\n\n * Applies loop transformations and scheduling optimizations\n * Handles:\n   * Loop tiling\n   * Loop fusion\n   * Loop interchange\n   * Data locality improvements\n * Prepares code for efficient hardware execution\n\n# Demo in Affine Dialect and Scheduling\n\nLoop Transformation Techniques\n\n * Loop Tiling: Breaks down large loops into smaller tiles\n * Optimizes cache utilization\n * Improves data locality\n * Enables potential parallelization\n\nScheduling Optimizations\n\n * Explicit loop interchange\n * Potential parallel region marking\n * Fine-grained index computations\n * Boundary condition handling\n\nComputational Characteristics\n\n * Predicated access for boundary conditions\n * Explicit index computations\n * Detailed loop nesting with optimization potential\n\nKey Transformation Strategies\n\n * Spatial locality improvement\n * Cache-aware computation\n * Potential for parallel execution\n * Precise control over computational patterns\n\nAdvanced Features\n\n * Affine maps for index transformations\n * Conditional (predicated) computation\n * Explicit scheduling directives\n\nCompared to Previous Representations\n\n * More focus on computational optimization\n * Explicit scheduling and transformation capabilities\n * Prepares for hardware-specific optimizations\n * Enables fine-grained performance tuning\n\nKey Optimization Techniques\n\n * Tiling (16x16 blocks)\n * Channel-level optimization\n * Boundary-aware computation\n * Potential for parallelization\n\nCode\n\n// Affine Dialect and Scheduling Representation of Conv2d Operation\n\nmodule {\n  // Affine Dialect Convolution with Advanced Scheduling Techniques\n  func.func @conv2d_affine_scheduled(\n      %input: memref<1x3x224x224xf32>, \n      %kernel: memref<64x3x3x3xf32>, \n      %output: memref<1x64x222x222xf32>\n  ) {\n    // Loop Tiling Transformation\n    // Optimize cache utilization and locality\n    affine.for %batch = 0 to 1 {\n      affine.for %out_channel = 0 to 64 step 8 {\n        // Tile size optimization for cache lines\n        affine.for %tile_channel = 0 to 8 {\n          affine.for %out_height = 0 to 222 step 16 {\n            // Height tile optimization\n            affine.for %tile_height = 0 to 16 {\n              affine.for %out_width = 0 to 222 step 16 {\n                // Width tile optimization\n                affine.for %tile_width = 0 to 16 {\n                  // Compute actual indices\n                  %actual_channel = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%out_channel, %tile_channel)\n                  %actual_height = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%out_height, %tile_height)\n                  %actual_width = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%out_width, %tile_width)\n\n                  // Inner convolution computation with locality optimization\n                  affine.for %in_channel = 0 to 3 {\n                    affine.for %k_height = 0 to 3 {\n                      affine.for %k_width = 0 to 3 {\n                        // Compute input indices with boundary checks\n                        %input_h = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%actual_height, %k_height)\n                        %input_w = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%actual_width, %k_width)\n\n                        // Predicated access to handle boundary conditions\n                        %input_val = affine.if %input_h >= 0 and %input_h < 224 and \n                                     %input_w >= 0 and %input_w < 224 \n                            {\n                              %val = memref.load %input[0, %in_channel, %input_h, %input_w] \n                                  : memref<1x3x224x224xf32>\n                              affine.yield %val : f32\n                            } else {\n                              %zero = arith.constant 0.0 : f32\n                              affine.yield %zero : f32\n                            }\n\n                        // Kernel and computation\n                        %kernel_val = memref.load %kernel[%actual_channel, %in_channel, %k_height, %k_width] \n                            : memref<64x3x3x3xf32>\n                        \n                        // Compute and accumulate\n                        %mul = arith.mulf %input_val, %kernel_val : f32\n                        \n                        // Accumulation with potential reduction\n                        %prev_output = memref.load %output[0, %actual_channel, %actual_height, %actual_width] \n                            : memref<1x64x222x222xf32>\n                        %accum = arith.addf %prev_output, %mul : f32\n                        \n                        // Store result\n                        memref.store %accum, %output[0, %actual_channel, %actual_height, %actual_width] \n                            : memref<1x64x222x222xf32>\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    return\n  }\n\n  // Advanced Scheduling Transformation\n  func.func @schedule_conv2d_transformation() {\n    // Define scheduling constraints and mappings\n    %c0 = arith.constant 0 : index\n    %c1 = arith.constant 1 : index\n    %c16 = arith.constant 16 : index\n    %c64 = arith.constant 64 : index\n\n    // Potential scheduling directives\n    // Demonstrates loop interchange and parallelization potential\n    %transformed_map = affine.apply \n        affine_map<(d0, d1, d2) -> (d1, d0, d2)> (%c0, %c16, %c64)\n\n    // Parallel loop marking (conceptual)\n    %parallel_marker = arith.constant 1 : i32\n\n    return\n  }\n\n  // Bias Addition with Affine Scheduling\n  func.func @affine_bias_add(\n      %output: memref<1x64x222x222xf32>, \n      %bias: memref<64xf32>\n  ) {\n    // Vectorized bias addition with affine scheduling\n    affine.for %batch = 0 to 1 {\n      // Channel-level parallelism potential\n      affine.for %channel = 0 to 64 {\n        affine.for %height = 0 to 222 {\n          affine.for %width = 0 to 222 {\n            // Load output and bias\n            %output_val = memref.load %output[%batch, %channel, %height, %width] \n                : memref<1x64x222x222xf32>\n            %bias_val = memref.load %bias[%channel] : memref<64xf32>\n            \n            // Bias addition\n            %added = arith.addf %output_val, %bias_val : f32\n            \n            // Store result\n            memref.store %added, %output[%batch, %channel, %height, %width] \n                : memref<1x64x222x222xf32>\n          }\n        }\n      }\n    }\n    return\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n\n\n\n# 7. Standard/SCF Dialect\n\n * Converts high-level control flow to more explicit representations\n * Handles sequential and parallel execution models\n * Prepares for final code generation\n\n# Demo in Standard/SCF Dialect\n\nStructured Control Flow (SCF) Features\n\n * Explicit control flow constructs\n * Nested loop iterations with accumulation\n * Tensor-based computation\n * Iterative reduction and transformation\n\nComputation Decomposition\n\n * Nested scf.for loops for multi-dimensional computation\n * Explicit iteration arguments\n * Detailed control over computation stages\n\nControl Flow Primitives\n\n * scf.execute_region: Structured computation block\n * scf.reduce: Reduction operations\n * scf.if: Conditional tensor operations\n * scf.while: Conditional loop execution\n * scf.parallel: Potential parallel execution\n\nTensor Manipulation\n\n * tensor.extract: Value extraction\n * tensor.insert: In-place tensor updates\n * Immutable tensor transformations\n\nComputational Characteristics\n\n * Explicit nested reductions\n * Detailed iteration control\n * Boundary condition handling\n * Iterative computation accumulation\n\nKey Differences from Previous Representations\n\n * More explicit control flow\n * Tensor-based computation\n * Detailed iteration management\n * Preparation for lower-level transformations\n\nUnique Aspects\n\n * Nested loop reductions\n * Explicit iteration arguments\n * Conditional tensor operations\n * Potential for parallel execution\n\nCode\n\n// Affine Dialect and Scheduling Representation of Conv2d Operation\n\nmodule {\n  // Affine Dialect Convolution with Advanced Scheduling Techniques\n  func.func @conv2d_affine_scheduled(\n      %input: memref<1x3x224x224xf32>, \n      %kernel: memref<64x3x3x3xf32>, \n      %output: memref<1x64x222x222xf32>\n  ) {\n    // Loop Tiling Transformation\n    // Optimize cache utilization and locality\n    affine.for %batch = 0 to 1 {\n      affine.for %out_channel = 0 to 64 step 8 {\n        // Tile size optimization for cache lines\n        affine.for %tile_channel = 0 to 8 {\n          affine.for %out_height = 0 to 222 step 16 {\n            // Height tile optimization\n            affine.for %tile_height = 0 to 16 {\n              affine.for %out_width = 0 to 222 step 16 {\n                // Width tile optimization\n                affine.for %tile_width = 0 to 16 {\n                  // Compute actual indices\n                  %actual_channel = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%out_channel, %tile_channel)\n                  %actual_height = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%out_height, %tile_height)\n                  %actual_width = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%out_width, %tile_width)\n\n                  // Inner convolution computation with locality optimization\n                  affine.for %in_channel = 0 to 3 {\n                    affine.for %k_height = 0 to 3 {\n                      affine.for %k_width = 0 to 3 {\n                        // Compute input indices with boundary checks\n                        %input_h = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%actual_height, %k_height)\n                        %input_w = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%actual_width, %k_width)\n\n                        // Predicated access to handle boundary conditions\n                        %input_val = affine.if %input_h >= 0 and %input_h < 224 and \n                                     %input_w >= 0 and %input_w < 224 \n                            {\n                              %val = memref.load %input[0, %in_channel, %input_h, %input_w] \n                                  : memref<1x3x224x224xf32>\n                              affine.yield %val : f32\n                            } else {\n                              %zero = arith.constant 0.0 : f32\n                              affine.yield %zero : f32\n                            }\n\n                        // Kernel and computation\n                        %kernel_val = memref.load %kernel[%actual_channel, %in_channel, %k_height, %k_width] \n                            : memref<64x3x3x3xf32>\n                        \n                        // Compute and accumulate\n                        %mul = arith.mulf %input_val, %kernel_val : f32\n                        \n                        // Accumulation with potential reduction\n                        %prev_output = memref.load %output[0, %actual_channel, %actual_height, %actual_width] \n                            : memref<1x64x222x222xf32>\n                        %accum = arith.addf %prev_output, %mul : f32\n                        \n                        // Store result\n                        memref.store %accum, %output[0, %actual_channel, %actual_height, %actual_width] \n                            : memref<1x64x222x222xf32>\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    return\n  }\n\n  // Advanced Scheduling Transformation\n  func.func @schedule_conv2d_transformation() {\n    // Define scheduling constraints and mappings\n    %c0 = arith.constant 0 : index\n    %c1 = arith.constant 1 : index\n    %c16 = arith.constant 16 : index\n    %c64 = arith.constant 64 : index\n\n    // Potential scheduling directives\n    // Demonstrates loop interchange and parallelization potential\n    %transformed_map = affine.apply \n        affine_map<(d0, d1, d2) -> (d1, d0, d2)> (%c0, %c16, %c64)\n\n    // Parallel loop marking (conceptual)\n    %parallel_marker = arith.constant 1 : i32\n\n    return\n  }\n\n  // Bias Addition with Affine Scheduling\n  func.func @affine_bias_add(\n      %output: memref<1x64x222x222xf32>, \n      %bias: memref<64xf32>\n  ) {\n    // Vectorized bias addition with affine scheduling\n    affine.for %batch = 0 to 1 {\n      // Channel-level parallelism potential\n      affine.for %channel = 0 to 64 {\n        affine.for %height = 0 to 222 {\n          affine.for %width = 0 to 222 {\n            // Load output and bias\n            %output_val = memref.load %output[%batch, %channel, %height, %width] \n                : memref<1x64x222x222xf32>\n            %bias_val = memref.load %bias[%channel] : memref<64xf32>\n            \n            // Bias addition\n            %added = arith.addf %output_val, %bias_val : f32\n            \n            // Store result\n            memref.store %added, %output[%batch, %channel, %height, %width] \n                : memref<1x64x222x222xf32>\n          }\n        }\n      }\n    }\n    return\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n\n\n\n# 8. Final Lowering to PTX\n\n * Converts MLIR representation to PTX (Parallel Thread Execution) assembly\n * Generates low-level GPU kernel code\n * Handles:\n   * Thread and block organization\n   * Memory hierarchy management\n   * Kernel launch configuration\n   * GPU-specific optimizations\n\n\n# Key Transformations\n\n 1. Semantic reduction from high-level intent\n 2. Explicit computational decomposition\n 3. Memory layout optimization\n 4. Vectorization\n 5. Hardware-specific code generation\n\n\n# Optimization Considerations\n\n * Each dialect transformation aims to:\n   * Preserve computational semantics\n   * Improve performance\n   * Reduce memory overhead\n   * Utilize hardware capabilities",normalizedContent:"# mlir compiling flow\n\n\n# high-level representation\n\nat the high-level dialect, a conv2d layer in pytorch can be represented as a computational graph operation that performs 2d convolution with specific parameters:\n\n * input tensor\n * kernel weights\n * bias (optional)\n * stride\n * padding\n * dilation\n\n\n# lowering process through mlir dialects\n\n\n# 1. torch dialect (high-level)\n\n * initial representation of the conv2d operation\n * captures semantic intent of the convolution\n * preserves high-level information about tensor shapes, strides, and computational semantics\n\n\n# 2. linalg dialect transformation\n\n * converts the high-level operation to more explicit linear algebra operations\n * represents convolution as nested loops and tensor comprehensions\n * breaks down the convolution into explicit:\n   * input sliding window operations\n   * kernel multiplication\n   * accumulation of results\n * introduces explicit iteration spaces and reduction domains\n\n# demo in conv2d\n\naffine maps (#map, #map1, #map2)\n\n * define how input tensors are indexed and transformed\n * specify the iteration spaces for convolution\n * map between input, kernel, and output tensor dimensions\n\nconvolution operation structure\n\n * input: 1x3x224x224 tensor (batch, channels, height, width)\n * kernel: 64x3x3x3 tensor (output channels, input channels, kernel height, kernel width)\n * output: 1x64x222x222 tensor (reduced spatial dimensions due to convolution)\n\nlinalg.generic operation\n\n * represents the core convolution computation\n * uses explicit reduction domains\n * iterator types show parallel and reduction dimensions\n * performs element-wise multiplication and reduction\n\ncomputation breakdown\n\n * initialize output tensor with zeros\n * perform convolution through nested reductions\n * optional bias addition\n\nkey transformations\n\n * converts high-level convolution to explicit tensor operations\n * shows computational intent through explicit iterations\n * prepares for further lowering and optimization\n\n\n# 3. memref dialect\n\n * transforms tensor representations to memory reference (memref) dialect\n * converts abstract tensor operations to concrete memory layouts\n * handles:\n   * memory allocation\n   * memory access patterns\n   * contiguous vs. strided memory representations\n * prepares for lower-level memory optimizations\n\n# demo in memref dialect\n\nmemory allocation\n\n * explicit buffer allocation using memref.alloc()\n * direct memory management instead of tensor abstractions\n * allows for precise control over memory layout and lifetime\n\nexplicit nested loops\n\n * uses affine.for to represent iteration spaces\n * breaks down convolution into explicit nested loops\n * provides fine-grained control over computation\n\nmemory access patterns\n\n * memref.load and memref.store for explicit memory interactions\n * uses affine maps to compute dynamic indices\n * shows exact memory access and computation steps\n\ncomputation breakdown\n\n * separate functions for:\n   * buffer allocation\n   * convolution computation\n   * bias addition\n * enables more explicit memory and computation management**\n\ntransformation characteristics\n\n * moves from tensor abstractions to concrete memory references\n * prepares for lower-level optimizations\n * enables hardware-specific memory optimizations\n\nkey differences from linalg dialect\n\n * more explicit memory management\n * concrete buffer allocations\n * detailed loop structures\n * direct memory access operations\n\nthe memref representation provides a lower-level view of the convolution operation, showing how the computation is performed through explicit memory accesses and loop iterations.\n\ncode\n\n// memref dialect representation of conv2d operation\n\n// memref conversion focuses on explicit memory layouts and buffer management\nmodule {\n    // memory allocation function for input, kernel, and output\n    func.func @allocate_buffers() -> (\n        memref<1x3x224x224xf32>,\n        memref<64x3x3x3xf32>, \n        memref<1x64x222x222xf32>) {\n        // allocate input buffer\n        %input = memref.alloc() : memref<1x3x224x224xf32>\n        \n        // allocate kernel buffer\n        %kernel = memref.alloc() : memref<64x3x3x3xf32>\n        \n        // allocate output buffer (initialized with zeros)\n        %output = memref.alloc() : memref<1x64x222x222xf32>\n        %zero = arith.constant 0.0 : f32\n        linalg.fill ins(%zero : f32) \n            outs(%output : memref<1x64x222x222xf32>)\n\n        return %input, %kernel, %output : \n            memref<1x3x224x224xf32>, \n            memref<64x3x3x3xf32>, \n            memref<1x64x222x222xf32>\n    }\n\n    // explicit conv2d implementation using memref\n    func.func @conv2d(\n        %input: memref<1x3x224x224xf32>, \n        %kernel: memref<64x3x3x3xf32>, \n        %output: memref<1x64x222x222xf32>) {\n        affine.for %batch = 0 to 1 {\n            affine.for %out_channel = 0 to 64 {\n                affine.for %out_height = 0 to 222 {\n                    affine.for %out_width = 0 to 222 {\n                        // reset output value\n                        %init_val = memref.load %output[\n                            %batch, %out_channel, \n                            %out_height, %out_width\n                        ] : memref<1x64x222x222xf32>\n                        \n                        // inner loops for input channels and kernel\n                        affine.for %in_channel = 0 to 3 {\n                            affine.for %k_height = 0 to 3 {\n                                affine.for %k_width = 0 to 3 {\n                                    // compute input and kernel indices\n                                    %input_h = affine.apply affine_map<(d0, d1)\n                                                -> (d0 + d1)>(%out_height, %k_height)\n                                    %input_w = affine.apply affine_map<(d0, d1)\n                                                -> (d0 + d1)>(%out_width, %k_width)\n                                    \n                                    // load input and kernel values\n                                    %input_val = memref.load %input[\n                                        %batch, %in_channel, %input_h, %input_w\n                                    ] : memref<1x3x224x224xf32>\n\n                                    %kernel_val = memref.load %kernel[\n                                        %out_channel, %in_channel, \n                                        %k_height, %k_width\n                                    ] : memref<64x3x3x3xf32>\n                                    \n                                    // compute convolution\n                                    %mul = arith.mulf %input_val, %kernel_val : f32\n                                    %add = arith.addf %init_val, %mul : f32\n                                    \n                                    // store updated output\n                                    memref.store %add, %output[\n                                        %batch, %out_channel,\n                                        %out_height, %out_width\n                                    ] : memref<1x64x222x222xf32>\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        return\n    }\n\n    // bias addition function\n    func.func @add_bias(%output: memref<1x64x222x222xf32>, %bias: memref<64xf32>) {\n        affine.for %batch = 0 to 1 {\n            affine.for %channel = 0 to 64 {\n                affine.for %height = 0 to 222 {\n                    affine.for %width = 0 to 222 {\n                        // load output and bias values\n                        %output_val = memref.load %output[%batch, %channel, %height, %width]\n                                                                    : memref<1x64x222x222xf32>\n                        %bias_val = memref.load %bias[%channel] : memref<64xf32>\n                        \n                        // add bias\n                        %added = arith.addf %output_val, %bias_val : f32\n                        \n                        // store result\n                        memref.store %added, %output[%batch, %channel, %height, %width] :\n                                                                memref<1x64x222x222xf32>\n                    }\n                }\n            }\n        }\n        return\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n\n\n\n# 4. vectorization\n\n * transforms loop-based representations into vector operations\n * applies simd (single instruction, multiple data) transformations\n * converts scalar computations to vector instructions\n * optimizes computation by:\n   * grouping similar operations\n   * utilizing vector processing units\n   * reducing instruction overhead\n\n# demo in vectorization\n\nvector type definitions\n\n * uses vector registers (e.g., 256-bit avx2/avx-512)\n * defines vector types for different computation sizes\n * enables simd (single instruction, multiple data) processing\n\nvectorization strategies\n\n * loop vectorization with step sizes matching vector width\n * vector load/store operations\n * simd multiplication and reduction\n * parallel processing of multiple elements\n\ncomputation transformation\n\n * converts scalar computations to vector operations\n * uses vector.load, vector.store\n * applies vector-level operations like vector.mul, vector.reduction\n\noptimization techniques\n\n * processes multiple elements simultaneously\n * reduces instruction overhead\n * improves computational efficiency\n * enables parallel hardware utilization\n\nkey transformations\n\n * scalar loops converted to vector operations\n * explicit simd instruction mapping\n * parallel computation across vector lanes\n\nadditional vectorization utilities\n\n * vector.transfer_read\n * vector.splat\n * enables type conversions and vector manipulations\n\ncompared to previous representations\n\n * more hardware-specific\n * explicit parallel computation\n * focuses on computational efficiency\n * prepares for low-level code generation\n\ncode\n\n// vectorization dialect representation of conv2d operation\n\nmodule {\n    // vector type definitions\n    // using 256-bit vector registers (typical for avx2/avx-512)\n    // f32 vector with 8 elements per register\n    #vec_8x_f32 = vector.type<8xf32>\n    #vec_64x_f32 = vector.type<64xf32>\n\n    // vectorized conv2d function\n    func.func @vectorized_conv2d(\n        %input: memref<1x3x224x224xf32>, \n        %kernel: memref<64x3x3x3xf32>, \n        %output: memref<1x64x222x222xf32>\n    ) {\n        // outer loop vectorization dimensions\n        affine.for %batch = 0 to 1 {\n            affine.for %out_channel = 0 to 64 step 8 {\n                affine.for %out_height = 0 to 222 {\n                    affine.for %out_width = 0 to 222 step 8 {\n                        // vector load output (pre-initialized with zeros)\n                        %output_vec = vector.load %output[%batch, %out_channel : vector<8xf32>] \n                            : memref<1x64x222x222xf32>, vector<8xf32>\n\n                        // vectorized inner convolution computation\n                        %result_vec = scf.reduce(%output_vec) : vector<8xf32> {\n                        ^bb0(%acc: vector<8xf32>, %_: vector<8xf32>):\n                            // nested reductions for input channels and kernel\n                            %channel_result = vector.reduction <add>, %acc : vector<8xf32>\n                            scf.reduce.return %channel_result : vector<8xf32>\n                        } : vector<8xf32>\n\n                        // vectorized kernel and input loading\n                        %kernel_vec = vector.load %kernel[%out_channel, 0, 0, 0 : vector<64xf32>] \n                            : memref<64x3x3x3xf32>, vector<64xf32>\n                        %input_vec = vector.load %input[%batch, 0, 0, 0 : vector<64xf32>] \n                            : memref<1x3x224x224xf32>, vector<64xf32>\n\n                        // simd vector multiplication\n                        %mul_vec = vector.mul %input_vec, %kernel_vec : vector<64xf32>\n\n                        // vectorized reduction\n                        %reduced_vec = vector.reduction <add>, %mul_vec : vector<64xf32>\n\n                        // vector store result back to output\n                        vector.store %reduced_vec, %output[%batch, %out_channel : vector<8xf32>] \n                            : memref<1x64x222x222xf32>, vector<8xf32>\n                    }\n                }\n            }\n        }\n        return\n    }\n\n    // vectorized bias addition\n    func.func @vectorized_bias_add(\n        %output: memref<1x64x222x222xf32>, \n        %bias: memref<64xf32>\n    ) {\n        // vectorized bias addition\n        affine.for %batch = 0 to 1 {\n            affine.for %channel = 0 to 64 step 8 {\n                // load bias vector\n                %bias_vec = vector.load %bias[%channel : vector<8xf32>] \n                    : memref<64xf32>, vector<8xf32>\n\n                affine.for %height = 0 to 222 {\n                    affine.for %width = 0 to 222 step 8 {\n                        // load output vector\n                        %output_vec = vector.load %output[%batch, %channel, %height, %width : vector<8xf32>] \n                            : memref<1x64x222x222xf32>, vector<8xf32>\n\n                        // vectorized bias addition\n                        %added_vec = vector.add %output_vec, %bias_vec : vector<8xf32>\n\n                        // store result back\n                        vector.store %added_vec, %output[%batch, %channel, %height, %width : vector<8xf32>] \n                            : memref<1x64x222x222xf32>, vector<8xf32>\n                    }\n                }\n            }\n        }\n        return\n    }\n\n    // vectorization transformation utility\n    func.func @vectorize_conv2d_transform(%input: memref<1x3x224x224xf32>) {\n        // vector transfer operations\n        %c0 = arith.constant 0 : index\n        %vec_input = vector.transfer_read %input[%c0, %c0, %c0, %c0], %c0 \n            : memref<1x3x224x224xf32>, vector<8x3x3x3xf32>\n\n        // vectorized type conversions\n        %splat = vector.splat %c0 : vector<8xf32>\n        \n        return\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n\n\n\n# 5. bufferization\n\n * converts tensor operations to explicit buffer allocations\n * removes tensor abstraction\n * manages memory allocation and deallocation\n * converts ssa (static single assignment) values to explicit memory buffers\n * prepares for hardware-specific memory management\n\n# demo in bufferization\n\ntensor to buffer conversion\n\n * transforms tensor operations to explicit memory buffers\n * uses bufferization.alloc_tensor for memory allocation\n * tracks memory space and allocation characteristics\n\nmemory management techniques\n\n * explicit buffer views using memref.view\n * buffer casting with bufferization.buffer_cast\n * one-shot allocation with memory tracking\n * enables precise memory layout control\n\ncomputational characteristics\n\n * preserves computational semantics of conv2d\n * provides explicit memory access patterns\n * enables in-place updates and modifications\n\nbufferization annotations\n\n * copy_memory flag for memory duplication\n * memory space specification\n * alias analysis support\n\ntransformation goals\n\n * remove tensor abstractions\n * prepare for hardware-specific optimizations\n * enable explicit memory management\n * support efficient memory access patterns\n\nkey differences from previous representations\n\n * more explicit memory management\n * precise buffer allocation and tracking\n * prepares for low-level code generation\n * focuses on memory efficiency\n\n\n# 6. affine dialect and scheduling\n\n * applies loop transformations and scheduling optimizations\n * handles:\n   * loop tiling\n   * loop fusion\n   * loop interchange\n   * data locality improvements\n * prepares code for efficient hardware execution\n\n# demo in affine dialect and scheduling\n\nloop transformation techniques\n\n * loop tiling: breaks down large loops into smaller tiles\n * optimizes cache utilization\n * improves data locality\n * enables potential parallelization\n\nscheduling optimizations\n\n * explicit loop interchange\n * potential parallel region marking\n * fine-grained index computations\n * boundary condition handling\n\ncomputational characteristics\n\n * predicated access for boundary conditions\n * explicit index computations\n * detailed loop nesting with optimization potential\n\nkey transformation strategies\n\n * spatial locality improvement\n * cache-aware computation\n * potential for parallel execution\n * precise control over computational patterns\n\nadvanced features\n\n * affine maps for index transformations\n * conditional (predicated) computation\n * explicit scheduling directives\n\ncompared to previous representations\n\n * more focus on computational optimization\n * explicit scheduling and transformation capabilities\n * prepares for hardware-specific optimizations\n * enables fine-grained performance tuning\n\nkey optimization techniques\n\n * tiling (16x16 blocks)\n * channel-level optimization\n * boundary-aware computation\n * potential for parallelization\n\ncode\n\n// affine dialect and scheduling representation of conv2d operation\n\nmodule {\n  // affine dialect convolution with advanced scheduling techniques\n  func.func @conv2d_affine_scheduled(\n      %input: memref<1x3x224x224xf32>, \n      %kernel: memref<64x3x3x3xf32>, \n      %output: memref<1x64x222x222xf32>\n  ) {\n    // loop tiling transformation\n    // optimize cache utilization and locality\n    affine.for %batch = 0 to 1 {\n      affine.for %out_channel = 0 to 64 step 8 {\n        // tile size optimization for cache lines\n        affine.for %tile_channel = 0 to 8 {\n          affine.for %out_height = 0 to 222 step 16 {\n            // height tile optimization\n            affine.for %tile_height = 0 to 16 {\n              affine.for %out_width = 0 to 222 step 16 {\n                // width tile optimization\n                affine.for %tile_width = 0 to 16 {\n                  // compute actual indices\n                  %actual_channel = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%out_channel, %tile_channel)\n                  %actual_height = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%out_height, %tile_height)\n                  %actual_width = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%out_width, %tile_width)\n\n                  // inner convolution computation with locality optimization\n                  affine.for %in_channel = 0 to 3 {\n                    affine.for %k_height = 0 to 3 {\n                      affine.for %k_width = 0 to 3 {\n                        // compute input indices with boundary checks\n                        %input_h = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%actual_height, %k_height)\n                        %input_w = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%actual_width, %k_width)\n\n                        // predicated access to handle boundary conditions\n                        %input_val = affine.if %input_h >= 0 and %input_h < 224 and \n                                     %input_w >= 0 and %input_w < 224 \n                            {\n                              %val = memref.load %input[0, %in_channel, %input_h, %input_w] \n                                  : memref<1x3x224x224xf32>\n                              affine.yield %val : f32\n                            } else {\n                              %zero = arith.constant 0.0 : f32\n                              affine.yield %zero : f32\n                            }\n\n                        // kernel and computation\n                        %kernel_val = memref.load %kernel[%actual_channel, %in_channel, %k_height, %k_width] \n                            : memref<64x3x3x3xf32>\n                        \n                        // compute and accumulate\n                        %mul = arith.mulf %input_val, %kernel_val : f32\n                        \n                        // accumulation with potential reduction\n                        %prev_output = memref.load %output[0, %actual_channel, %actual_height, %actual_width] \n                            : memref<1x64x222x222xf32>\n                        %accum = arith.addf %prev_output, %mul : f32\n                        \n                        // store result\n                        memref.store %accum, %output[0, %actual_channel, %actual_height, %actual_width] \n                            : memref<1x64x222x222xf32>\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    return\n  }\n\n  // advanced scheduling transformation\n  func.func @schedule_conv2d_transformation() {\n    // define scheduling constraints and mappings\n    %c0 = arith.constant 0 : index\n    %c1 = arith.constant 1 : index\n    %c16 = arith.constant 16 : index\n    %c64 = arith.constant 64 : index\n\n    // potential scheduling directives\n    // demonstrates loop interchange and parallelization potential\n    %transformed_map = affine.apply \n        affine_map<(d0, d1, d2) -> (d1, d0, d2)> (%c0, %c16, %c64)\n\n    // parallel loop marking (conceptual)\n    %parallel_marker = arith.constant 1 : i32\n\n    return\n  }\n\n  // bias addition with affine scheduling\n  func.func @affine_bias_add(\n      %output: memref<1x64x222x222xf32>, \n      %bias: memref<64xf32>\n  ) {\n    // vectorized bias addition with affine scheduling\n    affine.for %batch = 0 to 1 {\n      // channel-level parallelism potential\n      affine.for %channel = 0 to 64 {\n        affine.for %height = 0 to 222 {\n          affine.for %width = 0 to 222 {\n            // load output and bias\n            %output_val = memref.load %output[%batch, %channel, %height, %width] \n                : memref<1x64x222x222xf32>\n            %bias_val = memref.load %bias[%channel] : memref<64xf32>\n            \n            // bias addition\n            %added = arith.addf %output_val, %bias_val : f32\n            \n            // store result\n            memref.store %added, %output[%batch, %channel, %height, %width] \n                : memref<1x64x222x222xf32>\n          }\n        }\n      }\n    }\n    return\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n\n\n\n# 7. standard/scf dialect\n\n * converts high-level control flow to more explicit representations\n * handles sequential and parallel execution models\n * prepares for final code generation\n\n# demo in standard/scf dialect\n\nstructured control flow (scf) features\n\n * explicit control flow constructs\n * nested loop iterations with accumulation\n * tensor-based computation\n * iterative reduction and transformation\n\ncomputation decomposition\n\n * nested scf.for loops for multi-dimensional computation\n * explicit iteration arguments\n * detailed control over computation stages\n\ncontrol flow primitives\n\n * scf.execute_region: structured computation block\n * scf.reduce: reduction operations\n * scf.if: conditional tensor operations\n * scf.while: conditional loop execution\n * scf.parallel: potential parallel execution\n\ntensor manipulation\n\n * tensor.extract: value extraction\n * tensor.insert: in-place tensor updates\n * immutable tensor transformations\n\ncomputational characteristics\n\n * explicit nested reductions\n * detailed iteration control\n * boundary condition handling\n * iterative computation accumulation\n\nkey differences from previous representations\n\n * more explicit control flow\n * tensor-based computation\n * detailed iteration management\n * preparation for lower-level transformations\n\nunique aspects\n\n * nested loop reductions\n * explicit iteration arguments\n * conditional tensor operations\n * potential for parallel execution\n\ncode\n\n// affine dialect and scheduling representation of conv2d operation\n\nmodule {\n  // affine dialect convolution with advanced scheduling techniques\n  func.func @conv2d_affine_scheduled(\n      %input: memref<1x3x224x224xf32>, \n      %kernel: memref<64x3x3x3xf32>, \n      %output: memref<1x64x222x222xf32>\n  ) {\n    // loop tiling transformation\n    // optimize cache utilization and locality\n    affine.for %batch = 0 to 1 {\n      affine.for %out_channel = 0 to 64 step 8 {\n        // tile size optimization for cache lines\n        affine.for %tile_channel = 0 to 8 {\n          affine.for %out_height = 0 to 222 step 16 {\n            // height tile optimization\n            affine.for %tile_height = 0 to 16 {\n              affine.for %out_width = 0 to 222 step 16 {\n                // width tile optimization\n                affine.for %tile_width = 0 to 16 {\n                  // compute actual indices\n                  %actual_channel = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%out_channel, %tile_channel)\n                  %actual_height = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%out_height, %tile_height)\n                  %actual_width = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%out_width, %tile_width)\n\n                  // inner convolution computation with locality optimization\n                  affine.for %in_channel = 0 to 3 {\n                    affine.for %k_height = 0 to 3 {\n                      affine.for %k_width = 0 to 3 {\n                        // compute input indices with boundary checks\n                        %input_h = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%actual_height, %k_height)\n                        %input_w = affine.apply affine_map<(d0, d1) -> (d0 + d1)>(%actual_width, %k_width)\n\n                        // predicated access to handle boundary conditions\n                        %input_val = affine.if %input_h >= 0 and %input_h < 224 and \n                                     %input_w >= 0 and %input_w < 224 \n                            {\n                              %val = memref.load %input[0, %in_channel, %input_h, %input_w] \n                                  : memref<1x3x224x224xf32>\n                              affine.yield %val : f32\n                            } else {\n                              %zero = arith.constant 0.0 : f32\n                              affine.yield %zero : f32\n                            }\n\n                        // kernel and computation\n                        %kernel_val = memref.load %kernel[%actual_channel, %in_channel, %k_height, %k_width] \n                            : memref<64x3x3x3xf32>\n                        \n                        // compute and accumulate\n                        %mul = arith.mulf %input_val, %kernel_val : f32\n                        \n                        // accumulation with potential reduction\n                        %prev_output = memref.load %output[0, %actual_channel, %actual_height, %actual_width] \n                            : memref<1x64x222x222xf32>\n                        %accum = arith.addf %prev_output, %mul : f32\n                        \n                        // store result\n                        memref.store %accum, %output[0, %actual_channel, %actual_height, %actual_width] \n                            : memref<1x64x222x222xf32>\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    return\n  }\n\n  // advanced scheduling transformation\n  func.func @schedule_conv2d_transformation() {\n    // define scheduling constraints and mappings\n    %c0 = arith.constant 0 : index\n    %c1 = arith.constant 1 : index\n    %c16 = arith.constant 16 : index\n    %c64 = arith.constant 64 : index\n\n    // potential scheduling directives\n    // demonstrates loop interchange and parallelization potential\n    %transformed_map = affine.apply \n        affine_map<(d0, d1, d2) -> (d1, d0, d2)> (%c0, %c16, %c64)\n\n    // parallel loop marking (conceptual)\n    %parallel_marker = arith.constant 1 : i32\n\n    return\n  }\n\n  // bias addition with affine scheduling\n  func.func @affine_bias_add(\n      %output: memref<1x64x222x222xf32>, \n      %bias: memref<64xf32>\n  ) {\n    // vectorized bias addition with affine scheduling\n    affine.for %batch = 0 to 1 {\n      // channel-level parallelism potential\n      affine.for %channel = 0 to 64 {\n        affine.for %height = 0 to 222 {\n          affine.for %width = 0 to 222 {\n            // load output and bias\n            %output_val = memref.load %output[%batch, %channel, %height, %width] \n                : memref<1x64x222x222xf32>\n            %bias_val = memref.load %bias[%channel] : memref<64xf32>\n            \n            // bias addition\n            %added = arith.addf %output_val, %bias_val : f32\n            \n            // store result\n            memref.store %added, %output[%batch, %channel, %height, %width] \n                : memref<1x64x222x222xf32>\n          }\n        }\n      }\n    }\n    return\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n\n\n\n# 8. final lowering to ptx\n\n * converts mlir representation to ptx (parallel thread execution) assembly\n * generates low-level gpu kernel code\n * handles:\n   * thread and block organization\n   * memory hierarchy management\n   * kernel launch configuration\n   * gpu-specific optimizations\n\n\n# key transformations\n\n 1. semantic reduction from high-level intent\n 2. explicit computational decomposition\n 3. memory layout optimization\n 4. vectorization\n 5. hardware-specific code generation\n\n\n# optimization considerations\n\n * each dialect transformation aims to:\n   * preserve computational semantics\n   * improve performance\n   * reduce memory overhead\n   * utilize hardware capabilities",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"MLIR Compiling Flow of Transformer-Decoder",frontmatter:{title:"MLIR Compiling Flow of Transformer-Decoder",date:"2025-03-04T00:00:00.000Z",permalink:"/pages/000016/",tags:[null]},regularPath:"/02.compiler/16.mlir_notes_04.html",relativePath:"02.compiler/16.mlir_notes_04.md",key:"v-99a1d2ee",path:"/pages/000016/",headers:[{level:2,title:"1. Frontend Import: TorchScript to Torch Dialect",slug:"_1-frontend-import-torchscript-to-torch-dialect",normalizedTitle:"1. frontend import: torchscript to torch dialect",charIndex:1174},{level:2,title:"2. Computational Pattern Lowering: Torch to Linalg Dialect",slug:"_2-computational-pattern-lowering-torch-to-linalg-dialect",normalizedTitle:"2. computational pattern lowering: torch to linalg dialect",charIndex:1922},{level:2,title:"3. Iteration Space Optimization: Affine Dialect",slug:"_3-iteration-space-optimization-affine-dialect",normalizedTitle:"3. iteration space optimization: affine dialect",charIndex:2818},{level:2,title:"4. Lowering to Vectorization and SCF",slug:"_4-lowering-to-vectorization-and-scf",normalizedTitle:"4. lowering to vectorization and scf",charIndex:825},{level:2,title:"5. Final GPU Lowering: Mapping to GPU Blocks/Threads",slug:"_5-final-gpu-lowering-mapping-to-gpu-blocks-threads",normalizedTitle:"5. final gpu lowering: mapping to gpu blocks/threads",charIndex:3852},{level:2,title:"Conclusion",slug:"conclusion",normalizedTitle:"conclusion",charIndex:4773}],headersStr:"1. Frontend Import: TorchScript to Torch Dialect 2. Computational Pattern Lowering: Torch to Linalg Dialect 3. Iteration Space Optimization: Affine Dialect 4. Lowering to Vectorization and SCF 5. Final GPU Lowering: Mapping to GPU Blocks/Threads Conclusion",content:'# MLIR Compiling Flow of Transformer-Decoder\n\n> This is generated by ChatGPT.\n\nThis document details the MLIR compilation flow for a Transformer decoder, focusing on key computations such as:\n\n * Self-attention (QKV projections, matmul, softmax, output projection)\n * MLP (Feed-forward layers with matmul, activation functions)\n * Layer Normalization (Reduction and elementwise operations)\n\nThe compilation follows the pipeline:\n\n 1. Frontend Import (Torch Dialect): Convert TorchScript to the torch dialect, preserving dynamic semantics.\n 2. Computational Pattern Lowering (Linalg Dialect): Convert tensor computations (matmul, elementwise ops) to structured linalg operations.\n 3. Iteration Space Optimization (Affine Dialect): Introduce affine transformations for loop nest optimization, tiling, and dependency analysis.\n 4. Lowering to Vectorization and SCF: Apply vectorization, loop transformations, and parallelize structured loops (scf dialect).\n 5. Final GPU Lowering (GPU Dialect): Map computation to GPU blocks/threads, introduce shared memory, and lower to NVGPU/LLVM.\n\nEach stage includes MLIR code snippets demonstrating how IR evolves during compilation.\n\n\n# 1. Frontend Import: TorchScript to Torch Dialect\n\nThe Transformer decoder is first exported from PyTorch as a TorchScript model and converted into MLIR’s torch dialect. This preserves dynamic shapes and PyTorch semantics:\n\n%q = torch.aten.matmul %input, %weight_q : (!torch.vtensor<[?,64],f32>, !torch.vtensor<[64,64],f32>) -> !torch.vtensor<[?,64],f32>\n%k = torch.aten.matmul %input, %weight_k : (!torch.vtensor<[?,64],f32>, !torch.vtensor<[64,64],f32>) -> !torch.vtensor<[?,64],f32>\n%v = torch.aten.matmul %input, %weight_v : (!torch.vtensor<[?,64],f32>, !torch.vtensor<[64,64],f32>) -> !torch.vtensor<[?,64],f32>\n\n\n1\n2\n3\n\n\nAt this stage, operations like matrix multiplications, softmax, and elementwise operations remain in torch.aten form.\n\n\n# 2. Computational Pattern Lowering: Torch to Linalg Dialect\n\nNext, torch.aten operations are lowered into linalg operations, which define structured computations over tensors. This allows for explicit loop optimizations.\n\n%q = linalg.matmul ins(%input, %weight_q) outs(%q_init) -> tensor<?x64xf32>\n%k = linalg.matmul ins(%input, %weight_k) outs(%k_init) -> tensor<?x64xf32>\n%v = linalg.matmul ins(%input, %weight_v) outs(%v_init) -> tensor<?x64xf32>\n\n\n1\n2\n3\n\n\nAdditionally, elementwise functions (e.g., softmax, GeLU) are converted into linalg.generic:\n\n%softmax_out = linalg.generic { indexing_maps = [...], iterator_types = ["parallel"] }\n  ins(%logits) outs(%softmax_init) {\n    ^bb(%arg0: f32):\n    %exp = math.exp %arg0 : f32\n    linalg.yield %exp : f32\n  } -> tensor<?x?xf32>\n\n\n1\n2\n3\n4\n5\n6\n\n\nThis explicit representation enables further transformations such as tiling and vectorization.\n\n\n# 3. Iteration Space Optimization: Affine Dialect\n\nAffine transformations introduce loop nest optimization, tiling, and dependency analysis:\n\nscf.for %i = 0 to %M step 16 {\n  scf.for %j = 0 to %N step 16 {\n    %tile_q = affine.load %Q[%i, %j] : memref<?x64xf32>\n    %tile_k = affine.load %K[%i, %j] : memref<?x64xf32>\n    %tile_v = affine.load %V[%i, %j] : memref<?x64xf32>\n    %out = linalg.matmul %tile_q, %tile_k : tensor<16x64xf32>\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nThe affine dialect provides static scheduling guarantees, allowing aggressive compiler optimizations.\n\n\n# 4. Lowering to Vectorization and SCF\n\nAt this stage, computations are vectorized and parallelized using structured loops (scf.for):\n\nvector.transfer_read %A[%i, %j] : memref<?x64xf32> -> vector<4xf32>\n%result = vector.fma %vecA, %vecB, %vecC : vector<4xf32>\nvector.transfer_write %result, %C[%i, %j] : memref<?x64xf32>\n\n\n1\n2\n3\n\n\nBy vectorizing the inner loops, MLIR ensures that matrix multiplications and elementwise ops map efficiently to hardware vector units.\n\n\n# 5. Final GPU Lowering: Mapping to GPU Blocks/Threads\n\nThe final stage maps computations to GPU thread blocks, introduces shared memory buffers, and replaces operations with hardware-specific intrinsics:\n\ngpu.launch blocks(%grid_x, %grid_y, %c1) threads(%block_x, %block_y, %c1) {\n  %q_tile = memref.alloc() : memref<16x16xf32, #gpu.address_space<workgroup>>\n  %k_tile = memref.alloc() : memref<16x16xf32, #gpu.address_space<workgroup>>\n  nvgpu.device_async_copy %q_global, %q_tile\n  nvgpu.device_async_wait\n  %acc = nvgpu.mma.sync %q_tile, %k_tile, %acc_init : tensor<16x16xf32>\n  gpu.barrier\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nHere, we:\n\n * Allocate shared memory tiles (memref.alloc in workgroup memory space).\n * Insert async memory copies from global to shared memory (nvgpu.device_async_copy).\n * Use Tensor Core instructions (nvgpu.mma.sync) for efficient matrix multiplications.\n * Introduce synchronization (gpu.barrier).\n\n\n# Conclusion\n\nThis pipeline enables efficient compilation of Transformer decoder computations for GPUs. By leveraging MLIR’s structured dialects, tiling, vectorization, and hardware mapping, we systematically lower high-level computations into efficient GPU kernels. The final MLIR IR is ready for conversion into LLVM/NVPTX for execution on NVIDIA GPUs, ensuring high performance and optimized memory usage.',normalizedContent:'# mlir compiling flow of transformer-decoder\n\n> this is generated by chatgpt.\n\nthis document details the mlir compilation flow for a transformer decoder, focusing on key computations such as:\n\n * self-attention (qkv projections, matmul, softmax, output projection)\n * mlp (feed-forward layers with matmul, activation functions)\n * layer normalization (reduction and elementwise operations)\n\nthe compilation follows the pipeline:\n\n 1. frontend import (torch dialect): convert torchscript to the torch dialect, preserving dynamic semantics.\n 2. computational pattern lowering (linalg dialect): convert tensor computations (matmul, elementwise ops) to structured linalg operations.\n 3. iteration space optimization (affine dialect): introduce affine transformations for loop nest optimization, tiling, and dependency analysis.\n 4. lowering to vectorization and scf: apply vectorization, loop transformations, and parallelize structured loops (scf dialect).\n 5. final gpu lowering (gpu dialect): map computation to gpu blocks/threads, introduce shared memory, and lower to nvgpu/llvm.\n\neach stage includes mlir code snippets demonstrating how ir evolves during compilation.\n\n\n# 1. frontend import: torchscript to torch dialect\n\nthe transformer decoder is first exported from pytorch as a torchscript model and converted into mlir’s torch dialect. this preserves dynamic shapes and pytorch semantics:\n\n%q = torch.aten.matmul %input, %weight_q : (!torch.vtensor<[?,64],f32>, !torch.vtensor<[64,64],f32>) -> !torch.vtensor<[?,64],f32>\n%k = torch.aten.matmul %input, %weight_k : (!torch.vtensor<[?,64],f32>, !torch.vtensor<[64,64],f32>) -> !torch.vtensor<[?,64],f32>\n%v = torch.aten.matmul %input, %weight_v : (!torch.vtensor<[?,64],f32>, !torch.vtensor<[64,64],f32>) -> !torch.vtensor<[?,64],f32>\n\n\n1\n2\n3\n\n\nat this stage, operations like matrix multiplications, softmax, and elementwise operations remain in torch.aten form.\n\n\n# 2. computational pattern lowering: torch to linalg dialect\n\nnext, torch.aten operations are lowered into linalg operations, which define structured computations over tensors. this allows for explicit loop optimizations.\n\n%q = linalg.matmul ins(%input, %weight_q) outs(%q_init) -> tensor<?x64xf32>\n%k = linalg.matmul ins(%input, %weight_k) outs(%k_init) -> tensor<?x64xf32>\n%v = linalg.matmul ins(%input, %weight_v) outs(%v_init) -> tensor<?x64xf32>\n\n\n1\n2\n3\n\n\nadditionally, elementwise functions (e.g., softmax, gelu) are converted into linalg.generic:\n\n%softmax_out = linalg.generic { indexing_maps = [...], iterator_types = ["parallel"] }\n  ins(%logits) outs(%softmax_init) {\n    ^bb(%arg0: f32):\n    %exp = math.exp %arg0 : f32\n    linalg.yield %exp : f32\n  } -> tensor<?x?xf32>\n\n\n1\n2\n3\n4\n5\n6\n\n\nthis explicit representation enables further transformations such as tiling and vectorization.\n\n\n# 3. iteration space optimization: affine dialect\n\naffine transformations introduce loop nest optimization, tiling, and dependency analysis:\n\nscf.for %i = 0 to %m step 16 {\n  scf.for %j = 0 to %n step 16 {\n    %tile_q = affine.load %q[%i, %j] : memref<?x64xf32>\n    %tile_k = affine.load %k[%i, %j] : memref<?x64xf32>\n    %tile_v = affine.load %v[%i, %j] : memref<?x64xf32>\n    %out = linalg.matmul %tile_q, %tile_k : tensor<16x64xf32>\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nthe affine dialect provides static scheduling guarantees, allowing aggressive compiler optimizations.\n\n\n# 4. lowering to vectorization and scf\n\nat this stage, computations are vectorized and parallelized using structured loops (scf.for):\n\nvector.transfer_read %a[%i, %j] : memref<?x64xf32> -> vector<4xf32>\n%result = vector.fma %veca, %vecb, %vecc : vector<4xf32>\nvector.transfer_write %result, %c[%i, %j] : memref<?x64xf32>\n\n\n1\n2\n3\n\n\nby vectorizing the inner loops, mlir ensures that matrix multiplications and elementwise ops map efficiently to hardware vector units.\n\n\n# 5. final gpu lowering: mapping to gpu blocks/threads\n\nthe final stage maps computations to gpu thread blocks, introduces shared memory buffers, and replaces operations with hardware-specific intrinsics:\n\ngpu.launch blocks(%grid_x, %grid_y, %c1) threads(%block_x, %block_y, %c1) {\n  %q_tile = memref.alloc() : memref<16x16xf32, #gpu.address_space<workgroup>>\n  %k_tile = memref.alloc() : memref<16x16xf32, #gpu.address_space<workgroup>>\n  nvgpu.device_async_copy %q_global, %q_tile\n  nvgpu.device_async_wait\n  %acc = nvgpu.mma.sync %q_tile, %k_tile, %acc_init : tensor<16x16xf32>\n  gpu.barrier\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nhere, we:\n\n * allocate shared memory tiles (memref.alloc in workgroup memory space).\n * insert async memory copies from global to shared memory (nvgpu.device_async_copy).\n * use tensor core instructions (nvgpu.mma.sync) for efficient matrix multiplications.\n * introduce synchronization (gpu.barrier).\n\n\n# conclusion\n\nthis pipeline enables efficient compilation of transformer decoder computations for gpus. by leveraging mlir’s structured dialects, tiling, vectorization, and hardware mapping, we systematically lower high-level computations into efficient gpu kernels. the final mlir ir is ready for conversion into llvm/nvptx for execution on nvidia gpus, ensuring high performance and optimized memory usage.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"MLIR Essential Concepts",frontmatter:{title:"MLIR Essential Concepts",date:"2025-03-04T00:00:00.000Z",permalink:"/pages/000017/",tags:[null]},regularPath:"/02.compiler/17.mlir_notes_05.html",relativePath:"02.compiler/17.mlir_notes_05.md",key:"v-2bb0fc85",path:"/pages/000017/",headers:[{level:2,title:"MLIR Dialects for GPU Code Generation",slug:"mlir-dialects-for-gpu-code-generation",normalizedTitle:"mlir dialects for gpu code generation",charIndex:63},{level:2,title:"Lowering from PyTorch IR to LLVM IR and CUDA PTX",slug:"lowering-from-pytorch-ir-to-llvm-ir-and-cuda-ptx",normalizedTitle:"lowering from pytorch ir to llvm ir and cuda ptx",charIndex:2780},{level:2,title:"WMMA and Tensor Core Optimizations in MLIR",slug:"wmma-and-tensor-core-optimizations-in-mlir",normalizedTitle:"wmma and tensor core optimizations in mlir",charIndex:6553},{level:2,title:"Tools and Frameworks",slug:"tools-and-frameworks",normalizedTitle:"tools and frameworks",charIndex:10062},{level:2,title:"Example Workflow",slug:"example-workflow",normalizedTitle:"example workflow",charIndex:13509},{level:3,title:"1. PyTorch Model Definition",slug:"_1-pytorch-model-definition",normalizedTitle:"1. pytorch model definition",charIndex:13674},{level:3,title:"2. Lower to MLIR (Torch-MLIR)",slug:"_2-lower-to-mlir-torch-mlir",normalizedTitle:"2. lower to mlir (torch-mlir)",charIndex:13917},{level:3,title:"3. Transform to GPU Dialect",slug:"_3-transform-to-gpu-dialect",normalizedTitle:"3. transform to gpu dialect",charIndex:14505},{level:3,title:"4. Lower to NVVM and LLVM:",slug:"_4-lower-to-nvvm-and-llvm",normalizedTitle:"4. lower to nvvm and llvm:",charIndex:15555},{level:3,title:"5. Run on GPU:",slug:"_5-run-on-gpu",normalizedTitle:"5. run on gpu:",charIndex:16476},{level:2,title:"References and Further Reading",slug:"references-and-further-reading",normalizedTitle:"references and further reading",charIndex:17412}],headersStr:"MLIR Dialects for GPU Code Generation Lowering from PyTorch IR to LLVM IR and CUDA PTX WMMA and Tensor Core Optimizations in MLIR Tools and Frameworks Example Workflow 1. PyTorch Model Definition 2. Lower to MLIR (Torch-MLIR) 3. Transform to GPU Dialect 4. Lower to NVVM and LLVM: 5. Run on GPU: References and Further Reading",content:'# MLIR Essential Concepts\n\n> This is generated by ChatGPT.\n\n\n# MLIR Dialects for GPU Code Generation\n\nTorch Dialect (Torch-MLIR): PyTorch operations are first expressed in the Torch dialect (provided by Torch-MLIR).\n\nThis dialect represents PyTorch ops and semantics within MLIR.\n\nFrom there, a series of lowering passes gradually converts Torch dialect ops into lower-level MLIR dialects like linalg (for linear algebra on tensors), arith (basic arithmetic), math, tensor, etc.\n\nFor example, Torch-MLIR provides passes to convert high-level PyTorch ops into linalg or tosa ops , which are easier to optimize and eventually lower to code.\n\nThe goal is to end up with a combination of dialects that MLIR’s backends can consume (e.g., linalg + scf loops, or mhlo, depending on the chosen path ).\n\nOnce the model is represented in these lower dialects, we can target GPU-specific dialects for code generation.\n\nGPU Dialect: MLIR’s GPU dialect provides a mid-level abstraction for GPU kernels and parallel execution, independent of any specific GPU vendor.\n\nIt introduces operations for launching kernels (gpu.launch), thread/block IDs, shared memory, etc., similar to CUDA’s programming model .\n\nOnce the model is represented in these lower dialects, we can target GPU-specific dialects for code generation.\n\nThe GPU dialect abstracts away the driver or backend details and lets us express parallel loops and memory in a CUDA/OpenCL-like fashion.\n\nFor instance, one can map loop nests to GPU grid dimensions (blocks and threads) using passes like --gpu-map-parallel-loops .\n\nThe result is GPU dialect IR containing constructs such as gpu.launch_func and gpu.block_id/gpu.thread_id.\n\nNVGPU and NVVM Dialects: To target NVIDIA GPUs specifically, MLIR provides the NVGPU and NVVM dialects.\n\nThe NVVM dialect corresponds closely to NVIDIA’s PTX ISA (parallel thread execution instructions).\n\nNVVM ops are essentially MLIR’s representation of PTX instructions and intrinsics.\n\nFor example, operations like nvvm.mma.sync might represent warp-level matrix multiply-accumulate (WMMA) PTX instructions.\n\nThe NVGPU dialect sits between the generic GPU dialect and NVVM, offering higher-level NVIDIA-specific operations to simplify programming tensor cores and other advanced features.\n\nNVGPU ops like nvgpu.warpgroup.mma or nvgpu.mma.sync encapsulate complex PTX sequences (for tensor core operations) behind more readable ops.\n\nThis separation means a compiler developer can use NVGPU’s higher-level ops, and then rely on a conversion pass to translate them into the exact NVVM (PTX) instructions.\n\nIn summary, the GPU dialect is vendor-agnostic, while NVGPU/NVVM dialects handle NVIDIA-specific lowering (with NVVM being a direct PTX mirror and NVGPU providing some abstraction on top).\n\n\n# Lowering from PyTorch IR to LLVM IR and CUDA PTX\n\nPipeline Overview: Compiling a PyTorch model to GPU involves several stages of lowering in MLIR, eventually producing LLVM IR and PTX for execution. A typical pipeline might look like:\n\n * PyTorch to MLIR (Torch Dialect)\n   * Use Torch-MLIR to import the PyTorch model into an MLIR module in the torch dialect.\n   * This captures PyTorch ops and data structures in MLIR.\n * High-Level Optimizations\n   * Apply passes to convert from torch dialect to MLIR’s standard ML dialects. For example, convert to linalg on tensors for computations, and use tensor/memref for memory.\n   * Many PyTorch ops (like convolutions, gemm, activations) can be lowered to combinations of linalg operations, loops, etc.\n   * At this stage, device-agnostic optimizations (like fusion, loop tiling, etc.) can be done on the MLIR.\n * Introduce GPU Dialect\n   * Transform the MLIR to use the GPU dialect for portions meant to run on the GPU.\n   * This often involves outlining kernel functions and marking them as gpu.func within a gpu.module.\n   * For example, MLIR provides a pass (gpu-kernel-outlining) that takes computations (loops) and outlines them into separate GPU kernels.\n   * We also map loop iterations to GPU threads/blocks (e.g., one loop dimension to blockIdx.x, another to threadIdx.x, etc.) using mapping passes\n   * After this, the IR contains constructs like gpu.launch_func (on the host side to launch kernels) and gpu.func (device code).\n * Lower to NVVM (PTX Dialect):\n   * Next, we convert the GPU dialect device code to the NVVM dialect. MLIR has a conversion pass (convert-gpu-to-nvvm) that turns GPU dialect ops into NVVM ops.\n   * This is where GPU operations become PTX-equivalent operations.\n   * We also attach target information (such as the CUDA SM version or PTX version) to guide code generation.\n   * For instance, an MLIR pipeline might include an NVVM target attach step like nvvm-attach-target{chip=sm_90} to specify we’re targeting NVIDIA SM90 (Hopper architecture) with a certain PTX version\n   * The NVVM dialect ops now directly correspond to PTX instructions that will run on the GPU.\n * Lower to LLVM IR (NVPTX):\n   * Once we have NVVM ops, MLIR can lower these to the LLVM dialect and utilize LLVM’s NVPTX backend.\n   * A pass like gpu-to-llvm or a translation step converts the MLIR NVVM module into LLVM IR\n   * Essentially, MLIR hands off to LLVM with calls or intrinsics that represent PTX instructions. The LLVM NVPTX backend then generates PTX assembly from that LLVM IR. In MLIR’s tooling, one can use mlir-translate --mlir-to-llvmir to get the final LLVM IR and then use LLVM to emit PTX or even a cubin\n * PTX and Cubin Generation\n   * Finally, the NVPTX backend (part of LLVM) compiles the LLVM IR to PTX code.\n   * MLIR’s gpu-module-to-binary pass can invoke the PTX assembler (ptxas) to produce a cubin or embed the PTX as a binary blob\n   * The result is that we have either PTX text or a compiled binary for the GPU kernel, which the host code can load and execute.\n   * The MLIR GPU compilation documentation shows an example where after conversion to NVVM and LLVM, the gpu.module is serialized to a binary (this is the device code)\n   * The host-side MLIR (or LLVM IR) will contain the necessary runtime calls to launch the kernel (for example, if using CUDA driver APIs or a GPU runtime wrapper) .\n\nThroughout this pipeline, MLIR plays the central role of progressively lowering the representation:\n\nPyTorch IR → high-level MLIR → GPU dialect (with parallelism) → NVVM dialect (PTX) → LLVM IR → machine code.\n\nEach stage leverages MLIR’s modular transformation passes, and the final code generation leverages LLVM. The end product is CUDA PTX or binaries that can run on the GPU.\n\n\n# WMMA and Tensor Core Optimizations in MLIR\n\nNVIDIA’s tensor cores (starting from Volta’s WMMA API up to Ampere/Hopper’s more advanced WMMA and WMMA/WMMA instructions) are critical for accelerating matrix operations.\n\nMLIR has introduced special support to generate code that uses these tensor cores:\n\nWMMA Ops in MLIR: MLIR’s GPU dialect added WMMA (Warp Matrix Multiply-Accumulate) operations to represent matrix-multiplication fragments processed on tensor cores.\n\nFor example, there have been MLIR dialect ops to represent operations like mma.sync (matrix multiply-accumulate on tensor cores).\n\nThese started in the GPU dialect and later also appeared in the NVVM dialect for complete lowering.\n\nBy modeling these as high-level ops, the compiler can reason about matrix tiles and schedule their computation.\n\nNVVM Dialect Tensor Core Ops: The NVVM dialect (targeting PTX) implemented newer PTX instructions corresponding to tensor core operations (e.g., **WMMA instructions, Hopper’s wgmma.mma_async, etc.).\n\nThis means MLIR can emit NVVM ops like nvvm.mma.sync or other PTX intrinsics which map to actual hardware instructions for tensor cores.\n\nFor instance, Ampere and Hopper generation tensor-core ops have been added as NVVM ops.\n\nThese NVVM ops ensure that when lowering to LLVM IR, we generate the correct PTX or call the right LLVM intrinsics for tensor core usage.\n\nNVGPU High-Level Abstractions: To simplify using these powerful but complex instructions, MLIR’s NVGPU dialect provides more human-readable ops.\n\nOne example is nvgpu.warpgroup.mma, which performs a warp-level matrix multiplication (e.g., a 128×128×64 fragment) with FP16 inputs and FP32 accumulation.\n\nThis single op hides the complexity of coordinating 128 threads, loading matrix tiles, and issuing multiple low-level instructions.\n\nThe compiler can lower this one NVGPU op into the appropriate sequence of NVVM ops (like multiple mma.sync or the newer wgmma PTX instructions).\n\nThis separation of concerns allows MLIR to optimize at a high level (treating the tensor-core op as a single unit) and then expand it into hardware-specific code late in the pipeline.\n\nConversion Patterns for WMMA: MLIR includes transformation patterns specifically to handle WMMA ops.\n\nFor instance, the Transform dialect defines a conversion pattern to lower GPU dialect WMMA ops to NVVM ops .\n\nThis ensures any gpu.wmma ops you use (if any exist in the GPU dialect) will reliably turn into correct PTX instructions.\n\nEssentially, MLIR’s compiler passes know how to recognize a high-level “matrix multiply on tensor core” operation and substitute it with the corresponding PTX intrinsic sequence.\n\nOptimizations like Loop Tiling: In addition to direct WMMA ops, MLIR can perform typical loop optimizations (tiling, unrolling, etc.) tailored for tensor cores.\n\nFor example, tiling a matrix multiplication into 16x16 tiles that fit the WMMA fragment size, emitting loads and stores to shared memory, and then using the WMMA ops for the math.\n\nSuch patterns were demonstrated in research using MLIR to achieve near-peak performance .\n\nThe MLIR-based codegen can overlap memory operations with computation, use asynchronous copies, and synchronize warps appropriately – all using MLIR’s structured ops (like nvgpu.device_async_copy, barriers, etc.) and then lowering to PTX .\n\nThe end result is that MLIR can automate the generation of highly optimized GPU kernels that utilize tensor cores, which traditionally required expert-crafted CUDA code.\n\n\n# Tools and Frameworks\n\nSeveral tools and frameworks facilitate compiling PyTorch models via MLIR to GPU:\n\nTorch-MLIR: As described, Torch-MLIR is the primary frontend for PyTorch.\n\nIt provides Python APIs (like torch_mlir.compile(...)) to convert a PyTorch nn.Module into MLIR in a chosen dialect (e.g., Torch dialect or directly to Linalg-on-tensors).\n\nThis is often the first step in an MLIR-based workflow for PyTorch.\n\nThe Torch-MLIR repository also includes example scripts (e.g., lowering a ResNet) that demonstrate obtaining MLIR from PyTorch and then running optimization passes .\n\nTorch-MLIR itself focuses on the front-end conversion and basic lowerings; for full GPU codegen, it can be used in conjunction with other MLIR tools or frameworks (since the MLIR core provides the GPU/LLVM lowerings).\n\nMLIR Core Tools (mlir-opt, mlir-translate): MLIR comes with command-line tools like mlir-opt for applying compiler passes and mlir-translate for converting MLIR to other forms (like LLVM IR or assembly).\n\nDevelopers often take the MLIR module produced by Torch-MLIR and run a sequence of passes to generate GPU code. For example, one could run:\n\nmlir-opt model.mlir -pass-pipeline="builtin.module( \n    torch-lowerings,..., \n    linalg-bufferize, convert-linalg-to-loops, \n    gpu-map-parallel-loops, gpu-kernel-outlining, \n    convert-gpu-to-nvvm, gpu-to-llvm, gpu-module-to-binary \n  )" -o out.mlir\n\n\n1\n2\n3\n4\n5\n6\n\n\nThis hypothetical pipeline would lower Torch ops to linalg, convert to loops, map loops to GPU, outline kernels, lower to NVVM, then to LLVM and embed the binary .\n\nFinally, mlir-translate out.mlir --mlir-to-llvmir > out.ll yields LLVM IR with embedded PTX, which can be run or further compiled .\n\n(In practice, each framework may have its own tuned pipeline, but MLIR’s modular passes make it possible to script these workflows.)\n\nIREE: IREE is an MLIR-based end-to-end compiler that can target multiple backends (CPU, Vulkan, CUDA).\n\nIt can consume models from TensorFlow, TFLite, and via Torch-MLIR for PyTorch. Using Torch-MLIR to import a PyTorch model, one can hand the MLIR to IREE, which then applies its pipeline to generate GPU code.\n\nIREE has its own GPU support (for CUDA it can emit PTX, and for Vulkan it emits SPIR-V).\n\nFor CUDA specifically, IREE leverages MLIR’s NVVM and LLVM pathways similar to what we described .\n\nWhile IREE abstracts a lot, it is a practical example of a framework where PyTorch models (via MLIR) can be compiled to a CUDA driver executable.\n\nTools like IREE’s compile command can take in an MLIR (from Torch-MLIR) and output a module that contains PTX for execution on NVIDIA GPUs.\n\nTriton (OpenAI): Triton is a domain-specific language for writing GPU kernels, and it has recently been leveraging MLIR in its compiler stack.\n\nWhile Triton is not an IR for full models but rather for custom kernels, it’s worth noting because PyTorch 2.x’s TorchInductor uses Triton under the hood for GPU kernel generation.\n\nThe interesting intersection is that Triton’s compiler is moving to use MLIR as well. In fact, there are efforts to align Triton’s PTX generation with MLIR’s NVVM dialect (to avoid duplicating effort).\n\nSo, while not a direct compiler of PyTorch models via MLIR, Triton and MLIR share technology for GPU codegen.\n\nThe LLVM MLIR community has discussed unifying PTX generation approaches – for example, using MLIR’s NVVM dialect instead of Triton’s custom PTX emission.\n\n\n# Example Workflow\n\nTo illustrate how this all comes together, consider a simple example of compiling a PyTorch model with a single matrix multiplication for GPU:\n\n\n# 1. PyTorch Model Definition\n\nYou have a simple model:\n\nimport torch\nclass MyModel(torch.nn.Module):\n    def forward(self, x, y):\n        return x @ y\nmodel = MyModel().eval()\n\n\n1\n2\n3\n4\n5\n\n\nAssume x and y are 2-D tensors (matrix multiply).\n\n\n# 2. Lower to MLIR (Torch-MLIR)\n\nUse Torch-MLIR’s Python API to lower this model:\n\nimport torch_mlir\nmodule = torch_mlir.compile(model, \n            (torch.randn(128, 64), torch.randn(64, 128)), \n            output_type=torch_mlir.OutputType.LINALG_ON_TENSORS)\nmlir_text = str(module)\n\n\n1\n2\n3\n4\n5\n\n\nHere, output_type=LINALG_ON_TENSORS tells Torch-MLIR to produce MLIR where the PyTorch aten::matmul has been lowered to a linalg.matmul op on tensors.\n\nThe resulting MLIR (viewable via mlir_text) will have the high-level structure of the computation in MLIR form (no GPU specifics yet).\n\n\n# 3. Transform to GPU Dialect\n\nNext, we apply MLIR passes (via Python API or command-line) to target the GPU:\n\nimport mlir\nmlir.apply_passes(module, "builtin.module( \n    linalg-bufferize, convert-linalg-to-loops, \n    gpu-map-parallel-loops, gpu-kernel-outlining \n  )")\n\n\n1\n2\n3\n4\n5\n\n\nConvert the linalg op to loops (linalg.matmul → loops and vector ops, for example).\n\nMap those loops to a GPU configuration. For instance, tile the loops and map the outer loops to blockIdx and inner to threadIdx. MLIR’s gpu. map_parallel_loops pass can do this automatically .\n\nimport mlir\nmlir.apply_passes(module, "builtin.module( \n    linalg-bufferize, convert-linalg-to-loops, \n    gpu-map-parallel-loops, gpu-kernel-outlining \n  )")\n\n\n1\n2\n3\n4\n5\n\n\nThis introduces gpu.thread_id and gpu.block_id ops into the loop body (as seen in the NVGPU example code) .\n\nOutline the loop body as a GPU kernel (gpu.kernel_outlining pass), which produces a gpu.module with a device function (gpu.func) containing the math, and a host-side gpu.launch_func that launches it .\n\n\n# 4. Lower to NVVM and LLVM:\n\nAfter outlining, run the conversion to NVVM dialect: The convert-gpu-to-nvvm pass turns things like shared memory allocations, thread ops, etc., into NVVM equivalents .\n\nFor example, gpu.thread_id might become an nvvm.read.ptx.sreg.tid.x intrinsic under the hood.\n\nIf our matmul can use WMMA, this is where high-level ops could be converted to nvvm.mma.sync calls (assuming we used a higher-level op that maps to WMMA).\n\nThen use gpu-to-llvm to finalize conversion to the LLVM dialect .\n\nThis might produce an LLVM function for the host with calls to a kernel launcher, and an LLVM function representing the device code (in NVPTX IR form).\n\nFinally, either use gpu-module-to-binary to generate the actual PTX binary blob inside the MLIR module , or translate to LLVM IR and run LLVM’s llc with the NVPTX target. For example: llc -march=nvptx64 -mcpu=sm_80 -filetype=obj out.ll -o out.ptx.\n\n\n# 5. Run on GPU:\n\nThe end result is that we have a CUDA PTX (or cubin) for the matrix multiply kernel, and the host code can launch it.\n\nIf using MLIR’s runner utilities, mlir-cpu-runner and mlir-runner-utils can execute the module (with a GPU binary embedded) by invoking the CUDA driver under the hood.\n\nAlternatively, one could embed the PTX into a small C++ harness that loads it via the CUDA API and runs the kernel.\n\nThis workflow demonstrates how MLIR serves as the end-to-end compiler: from PyTorch model to optimized GPU code.\n\nAt each stage, different MLIR dialects and passes come into play (Torch → Linalg → GPU → NVVM → LLVM), and specific optimizations like tiling for tensor cores (WMMA) are applied by using the proper ops and passes.\n\nThe combination of Torch-MLIR + MLIR’s GPU compilation pipeline allows targeting CUDA PTX, including advanced instructions for tensor cores, all within a unified compiler framework.\n\n\n# References and Further Reading\n\n * Torch-MLIR project (PyTorch to MLIR): Official GitHub repository and docs. Introduces the Torch dialect and how PyTorch ops are represented in MLIR.\n * MLIR GPU Dialect: Documentation of the GPU dialect and compilation pipeline MLIR.LLVM.ORG MLIR.LLVM.ORG, showing how GPU modules are lowered to NVVM and then to binary/LLVM IR.\n * NVGPU and NVVM Dialects: MLIR docs and discussions on NVIDIA GPU support, explaining how high-level ops map to PTX and how MLIR handles new tensor core instructions.\n * WMMA/Tensor Core Codegen: MLIR community slides and papers, e.g. “High Performance GPU Code Generation for Matrix-Matrix Multiplication using MLIR” ARXIV.ORG (reports near-CuBLAS performance via MLIR), and MLIR’s transform patterns for WMMA MLIR.LLVM.ORG.\n * Example Code: The MLIR code example for using tensor cores (CUDA WMMA) in MLIR’s GitHub tests GITHUB.COM – demonstrates how warp-level matrix mult and async copies are orchestrated in MLIR (Python API usage).\n * TOOLS: IREE’s documentation on PyTorch and CUDA backend IREE.DEV , and the Torch-MLIR docs (e.g., development.md with a ResNet example) GITHUB.COM for practical guidance on using these tools together.',normalizedContent:'# mlir essential concepts\n\n> this is generated by chatgpt.\n\n\n# mlir dialects for gpu code generation\n\ntorch dialect (torch-mlir): pytorch operations are first expressed in the torch dialect (provided by torch-mlir).\n\nthis dialect represents pytorch ops and semantics within mlir.\n\nfrom there, a series of lowering passes gradually converts torch dialect ops into lower-level mlir dialects like linalg (for linear algebra on tensors), arith (basic arithmetic), math, tensor, etc.\n\nfor example, torch-mlir provides passes to convert high-level pytorch ops into linalg or tosa ops , which are easier to optimize and eventually lower to code.\n\nthe goal is to end up with a combination of dialects that mlir’s backends can consume (e.g., linalg + scf loops, or mhlo, depending on the chosen path ).\n\nonce the model is represented in these lower dialects, we can target gpu-specific dialects for code generation.\n\ngpu dialect: mlir’s gpu dialect provides a mid-level abstraction for gpu kernels and parallel execution, independent of any specific gpu vendor.\n\nit introduces operations for launching kernels (gpu.launch), thread/block ids, shared memory, etc., similar to cuda’s programming model .\n\nonce the model is represented in these lower dialects, we can target gpu-specific dialects for code generation.\n\nthe gpu dialect abstracts away the driver or backend details and lets us express parallel loops and memory in a cuda/opencl-like fashion.\n\nfor instance, one can map loop nests to gpu grid dimensions (blocks and threads) using passes like --gpu-map-parallel-loops .\n\nthe result is gpu dialect ir containing constructs such as gpu.launch_func and gpu.block_id/gpu.thread_id.\n\nnvgpu and nvvm dialects: to target nvidia gpus specifically, mlir provides the nvgpu and nvvm dialects.\n\nthe nvvm dialect corresponds closely to nvidia’s ptx isa (parallel thread execution instructions).\n\nnvvm ops are essentially mlir’s representation of ptx instructions and intrinsics.\n\nfor example, operations like nvvm.mma.sync might represent warp-level matrix multiply-accumulate (wmma) ptx instructions.\n\nthe nvgpu dialect sits between the generic gpu dialect and nvvm, offering higher-level nvidia-specific operations to simplify programming tensor cores and other advanced features.\n\nnvgpu ops like nvgpu.warpgroup.mma or nvgpu.mma.sync encapsulate complex ptx sequences (for tensor core operations) behind more readable ops.\n\nthis separation means a compiler developer can use nvgpu’s higher-level ops, and then rely on a conversion pass to translate them into the exact nvvm (ptx) instructions.\n\nin summary, the gpu dialect is vendor-agnostic, while nvgpu/nvvm dialects handle nvidia-specific lowering (with nvvm being a direct ptx mirror and nvgpu providing some abstraction on top).\n\n\n# lowering from pytorch ir to llvm ir and cuda ptx\n\npipeline overview: compiling a pytorch model to gpu involves several stages of lowering in mlir, eventually producing llvm ir and ptx for execution. a typical pipeline might look like:\n\n * pytorch to mlir (torch dialect)\n   * use torch-mlir to import the pytorch model into an mlir module in the torch dialect.\n   * this captures pytorch ops and data structures in mlir.\n * high-level optimizations\n   * apply passes to convert from torch dialect to mlir’s standard ml dialects. for example, convert to linalg on tensors for computations, and use tensor/memref for memory.\n   * many pytorch ops (like convolutions, gemm, activations) can be lowered to combinations of linalg operations, loops, etc.\n   * at this stage, device-agnostic optimizations (like fusion, loop tiling, etc.) can be done on the mlir.\n * introduce gpu dialect\n   * transform the mlir to use the gpu dialect for portions meant to run on the gpu.\n   * this often involves outlining kernel functions and marking them as gpu.func within a gpu.module.\n   * for example, mlir provides a pass (gpu-kernel-outlining) that takes computations (loops) and outlines them into separate gpu kernels.\n   * we also map loop iterations to gpu threads/blocks (e.g., one loop dimension to blockidx.x, another to threadidx.x, etc.) using mapping passes\n   * after this, the ir contains constructs like gpu.launch_func (on the host side to launch kernels) and gpu.func (device code).\n * lower to nvvm (ptx dialect):\n   * next, we convert the gpu dialect device code to the nvvm dialect. mlir has a conversion pass (convert-gpu-to-nvvm) that turns gpu dialect ops into nvvm ops.\n   * this is where gpu operations become ptx-equivalent operations.\n   * we also attach target information (such as the cuda sm version or ptx version) to guide code generation.\n   * for instance, an mlir pipeline might include an nvvm target attach step like nvvm-attach-target{chip=sm_90} to specify we’re targeting nvidia sm90 (hopper architecture) with a certain ptx version\n   * the nvvm dialect ops now directly correspond to ptx instructions that will run on the gpu.\n * lower to llvm ir (nvptx):\n   * once we have nvvm ops, mlir can lower these to the llvm dialect and utilize llvm’s nvptx backend.\n   * a pass like gpu-to-llvm or a translation step converts the mlir nvvm module into llvm ir\n   * essentially, mlir hands off to llvm with calls or intrinsics that represent ptx instructions. the llvm nvptx backend then generates ptx assembly from that llvm ir. in mlir’s tooling, one can use mlir-translate --mlir-to-llvmir to get the final llvm ir and then use llvm to emit ptx or even a cubin\n * ptx and cubin generation\n   * finally, the nvptx backend (part of llvm) compiles the llvm ir to ptx code.\n   * mlir’s gpu-module-to-binary pass can invoke the ptx assembler (ptxas) to produce a cubin or embed the ptx as a binary blob\n   * the result is that we have either ptx text or a compiled binary for the gpu kernel, which the host code can load and execute.\n   * the mlir gpu compilation documentation shows an example where after conversion to nvvm and llvm, the gpu.module is serialized to a binary (this is the device code)\n   * the host-side mlir (or llvm ir) will contain the necessary runtime calls to launch the kernel (for example, if using cuda driver apis or a gpu runtime wrapper) .\n\nthroughout this pipeline, mlir plays the central role of progressively lowering the representation:\n\npytorch ir → high-level mlir → gpu dialect (with parallelism) → nvvm dialect (ptx) → llvm ir → machine code.\n\neach stage leverages mlir’s modular transformation passes, and the final code generation leverages llvm. the end product is cuda ptx or binaries that can run on the gpu.\n\n\n# wmma and tensor core optimizations in mlir\n\nnvidia’s tensor cores (starting from volta’s wmma api up to ampere/hopper’s more advanced wmma and wmma/wmma instructions) are critical for accelerating matrix operations.\n\nmlir has introduced special support to generate code that uses these tensor cores:\n\nwmma ops in mlir: mlir’s gpu dialect added wmma (warp matrix multiply-accumulate) operations to represent matrix-multiplication fragments processed on tensor cores.\n\nfor example, there have been mlir dialect ops to represent operations like mma.sync (matrix multiply-accumulate on tensor cores).\n\nthese started in the gpu dialect and later also appeared in the nvvm dialect for complete lowering.\n\nby modeling these as high-level ops, the compiler can reason about matrix tiles and schedule their computation.\n\nnvvm dialect tensor core ops: the nvvm dialect (targeting ptx) implemented newer ptx instructions corresponding to tensor core operations (e.g., **wmma instructions, hopper’s wgmma.mma_async, etc.).\n\nthis means mlir can emit nvvm ops like nvvm.mma.sync or other ptx intrinsics which map to actual hardware instructions for tensor cores.\n\nfor instance, ampere and hopper generation tensor-core ops have been added as nvvm ops.\n\nthese nvvm ops ensure that when lowering to llvm ir, we generate the correct ptx or call the right llvm intrinsics for tensor core usage.\n\nnvgpu high-level abstractions: to simplify using these powerful but complex instructions, mlir’s nvgpu dialect provides more human-readable ops.\n\none example is nvgpu.warpgroup.mma, which performs a warp-level matrix multiplication (e.g., a 128×128×64 fragment) with fp16 inputs and fp32 accumulation.\n\nthis single op hides the complexity of coordinating 128 threads, loading matrix tiles, and issuing multiple low-level instructions.\n\nthe compiler can lower this one nvgpu op into the appropriate sequence of nvvm ops (like multiple mma.sync or the newer wgmma ptx instructions).\n\nthis separation of concerns allows mlir to optimize at a high level (treating the tensor-core op as a single unit) and then expand it into hardware-specific code late in the pipeline.\n\nconversion patterns for wmma: mlir includes transformation patterns specifically to handle wmma ops.\n\nfor instance, the transform dialect defines a conversion pattern to lower gpu dialect wmma ops to nvvm ops .\n\nthis ensures any gpu.wmma ops you use (if any exist in the gpu dialect) will reliably turn into correct ptx instructions.\n\nessentially, mlir’s compiler passes know how to recognize a high-level “matrix multiply on tensor core” operation and substitute it with the corresponding ptx intrinsic sequence.\n\noptimizations like loop tiling: in addition to direct wmma ops, mlir can perform typical loop optimizations (tiling, unrolling, etc.) tailored for tensor cores.\n\nfor example, tiling a matrix multiplication into 16x16 tiles that fit the wmma fragment size, emitting loads and stores to shared memory, and then using the wmma ops for the math.\n\nsuch patterns were demonstrated in research using mlir to achieve near-peak performance .\n\nthe mlir-based codegen can overlap memory operations with computation, use asynchronous copies, and synchronize warps appropriately – all using mlir’s structured ops (like nvgpu.device_async_copy, barriers, etc.) and then lowering to ptx .\n\nthe end result is that mlir can automate the generation of highly optimized gpu kernels that utilize tensor cores, which traditionally required expert-crafted cuda code.\n\n\n# tools and frameworks\n\nseveral tools and frameworks facilitate compiling pytorch models via mlir to gpu:\n\ntorch-mlir: as described, torch-mlir is the primary frontend for pytorch.\n\nit provides python apis (like torch_mlir.compile(...)) to convert a pytorch nn.module into mlir in a chosen dialect (e.g., torch dialect or directly to linalg-on-tensors).\n\nthis is often the first step in an mlir-based workflow for pytorch.\n\nthe torch-mlir repository also includes example scripts (e.g., lowering a resnet) that demonstrate obtaining mlir from pytorch and then running optimization passes .\n\ntorch-mlir itself focuses on the front-end conversion and basic lowerings; for full gpu codegen, it can be used in conjunction with other mlir tools or frameworks (since the mlir core provides the gpu/llvm lowerings).\n\nmlir core tools (mlir-opt, mlir-translate): mlir comes with command-line tools like mlir-opt for applying compiler passes and mlir-translate for converting mlir to other forms (like llvm ir or assembly).\n\ndevelopers often take the mlir module produced by torch-mlir and run a sequence of passes to generate gpu code. for example, one could run:\n\nmlir-opt model.mlir -pass-pipeline="builtin.module( \n    torch-lowerings,..., \n    linalg-bufferize, convert-linalg-to-loops, \n    gpu-map-parallel-loops, gpu-kernel-outlining, \n    convert-gpu-to-nvvm, gpu-to-llvm, gpu-module-to-binary \n  )" -o out.mlir\n\n\n1\n2\n3\n4\n5\n6\n\n\nthis hypothetical pipeline would lower torch ops to linalg, convert to loops, map loops to gpu, outline kernels, lower to nvvm, then to llvm and embed the binary .\n\nfinally, mlir-translate out.mlir --mlir-to-llvmir > out.ll yields llvm ir with embedded ptx, which can be run or further compiled .\n\n(in practice, each framework may have its own tuned pipeline, but mlir’s modular passes make it possible to script these workflows.)\n\niree: iree is an mlir-based end-to-end compiler that can target multiple backends (cpu, vulkan, cuda).\n\nit can consume models from tensorflow, tflite, and via torch-mlir for pytorch. using torch-mlir to import a pytorch model, one can hand the mlir to iree, which then applies its pipeline to generate gpu code.\n\niree has its own gpu support (for cuda it can emit ptx, and for vulkan it emits spir-v).\n\nfor cuda specifically, iree leverages mlir’s nvvm and llvm pathways similar to what we described .\n\nwhile iree abstracts a lot, it is a practical example of a framework where pytorch models (via mlir) can be compiled to a cuda driver executable.\n\ntools like iree’s compile command can take in an mlir (from torch-mlir) and output a module that contains ptx for execution on nvidia gpus.\n\ntriton (openai): triton is a domain-specific language for writing gpu kernels, and it has recently been leveraging mlir in its compiler stack.\n\nwhile triton is not an ir for full models but rather for custom kernels, it’s worth noting because pytorch 2.x’s torchinductor uses triton under the hood for gpu kernel generation.\n\nthe interesting intersection is that triton’s compiler is moving to use mlir as well. in fact, there are efforts to align triton’s ptx generation with mlir’s nvvm dialect (to avoid duplicating effort).\n\nso, while not a direct compiler of pytorch models via mlir, triton and mlir share technology for gpu codegen.\n\nthe llvm mlir community has discussed unifying ptx generation approaches – for example, using mlir’s nvvm dialect instead of triton’s custom ptx emission.\n\n\n# example workflow\n\nto illustrate how this all comes together, consider a simple example of compiling a pytorch model with a single matrix multiplication for gpu:\n\n\n# 1. pytorch model definition\n\nyou have a simple model:\n\nimport torch\nclass mymodel(torch.nn.module):\n    def forward(self, x, y):\n        return x @ y\nmodel = mymodel().eval()\n\n\n1\n2\n3\n4\n5\n\n\nassume x and y are 2-d tensors (matrix multiply).\n\n\n# 2. lower to mlir (torch-mlir)\n\nuse torch-mlir’s python api to lower this model:\n\nimport torch_mlir\nmodule = torch_mlir.compile(model, \n            (torch.randn(128, 64), torch.randn(64, 128)), \n            output_type=torch_mlir.outputtype.linalg_on_tensors)\nmlir_text = str(module)\n\n\n1\n2\n3\n4\n5\n\n\nhere, output_type=linalg_on_tensors tells torch-mlir to produce mlir where the pytorch aten::matmul has been lowered to a linalg.matmul op on tensors.\n\nthe resulting mlir (viewable via mlir_text) will have the high-level structure of the computation in mlir form (no gpu specifics yet).\n\n\n# 3. transform to gpu dialect\n\nnext, we apply mlir passes (via python api or command-line) to target the gpu:\n\nimport mlir\nmlir.apply_passes(module, "builtin.module( \n    linalg-bufferize, convert-linalg-to-loops, \n    gpu-map-parallel-loops, gpu-kernel-outlining \n  )")\n\n\n1\n2\n3\n4\n5\n\n\nconvert the linalg op to loops (linalg.matmul → loops and vector ops, for example).\n\nmap those loops to a gpu configuration. for instance, tile the loops and map the outer loops to blockidx and inner to threadidx. mlir’s gpu. map_parallel_loops pass can do this automatically .\n\nimport mlir\nmlir.apply_passes(module, "builtin.module( \n    linalg-bufferize, convert-linalg-to-loops, \n    gpu-map-parallel-loops, gpu-kernel-outlining \n  )")\n\n\n1\n2\n3\n4\n5\n\n\nthis introduces gpu.thread_id and gpu.block_id ops into the loop body (as seen in the nvgpu example code) .\n\noutline the loop body as a gpu kernel (gpu.kernel_outlining pass), which produces a gpu.module with a device function (gpu.func) containing the math, and a host-side gpu.launch_func that launches it .\n\n\n# 4. lower to nvvm and llvm:\n\nafter outlining, run the conversion to nvvm dialect: the convert-gpu-to-nvvm pass turns things like shared memory allocations, thread ops, etc., into nvvm equivalents .\n\nfor example, gpu.thread_id might become an nvvm.read.ptx.sreg.tid.x intrinsic under the hood.\n\nif our matmul can use wmma, this is where high-level ops could be converted to nvvm.mma.sync calls (assuming we used a higher-level op that maps to wmma).\n\nthen use gpu-to-llvm to finalize conversion to the llvm dialect .\n\nthis might produce an llvm function for the host with calls to a kernel launcher, and an llvm function representing the device code (in nvptx ir form).\n\nfinally, either use gpu-module-to-binary to generate the actual ptx binary blob inside the mlir module , or translate to llvm ir and run llvm’s llc with the nvptx target. for example: llc -march=nvptx64 -mcpu=sm_80 -filetype=obj out.ll -o out.ptx.\n\n\n# 5. run on gpu:\n\nthe end result is that we have a cuda ptx (or cubin) for the matrix multiply kernel, and the host code can launch it.\n\nif using mlir’s runner utilities, mlir-cpu-runner and mlir-runner-utils can execute the module (with a gpu binary embedded) by invoking the cuda driver under the hood.\n\nalternatively, one could embed the ptx into a small c++ harness that loads it via the cuda api and runs the kernel.\n\nthis workflow demonstrates how mlir serves as the end-to-end compiler: from pytorch model to optimized gpu code.\n\nat each stage, different mlir dialects and passes come into play (torch → linalg → gpu → nvvm → llvm), and specific optimizations like tiling for tensor cores (wmma) are applied by using the proper ops and passes.\n\nthe combination of torch-mlir + mlir’s gpu compilation pipeline allows targeting cuda ptx, including advanced instructions for tensor cores, all within a unified compiler framework.\n\n\n# references and further reading\n\n * torch-mlir project (pytorch to mlir): official github repository and docs. introduces the torch dialect and how pytorch ops are represented in mlir.\n * mlir gpu dialect: documentation of the gpu dialect and compilation pipeline mlir.llvm.org mlir.llvm.org, showing how gpu modules are lowered to nvvm and then to binary/llvm ir.\n * nvgpu and nvvm dialects: mlir docs and discussions on nvidia gpu support, explaining how high-level ops map to ptx and how mlir handles new tensor core instructions.\n * wmma/tensor core codegen: mlir community slides and papers, e.g. “high performance gpu code generation for matrix-matrix multiplication using mlir” arxiv.org (reports near-cublas performance via mlir), and mlir’s transform patterns for wmma mlir.llvm.org.\n * example code: the mlir code example for using tensor cores (cuda wmma) in mlir’s github tests github.com – demonstrates how warp-level matrix mult and async copies are orchestrated in mlir (python api usage).\n * tools: iree’s documentation on pytorch and cuda backend iree.dev , and the torch-mlir docs (e.g., development.md with a resnet example) github.com for practical guidance on using these tools together.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"MLIR Compiling Flow of Conv2D",frontmatter:{title:"MLIR Compiling Flow of Conv2D",date:"2025-03-04T00:00:00.000Z",permalink:"/pages/000015/",tags:[null]},regularPath:"/02.compiler/15.mlir_notes_03.html",relativePath:"02.compiler/15.mlir_notes_03.md",key:"v-3aad308d",path:"/pages/000015/",headers:[{level:2,title:"1. High-Level Dialect (TOSA or MHLO)",slug:"_1-high-level-dialect-tosa-or-mhlo",normalizedTitle:"1. high-level dialect (tosa or mhlo)",charIndex:69},{level:2,title:"2. Lowering to Linalg (Structured Ops)",slug:"_2-lowering-to-linalg-structured-ops",normalizedTitle:"2. lowering to linalg (structured ops)",charIndex:934},{level:2,title:"3. Bufferization & MemRef Dialect",slug:"_3-bufferization-memref-dialect",normalizedTitle:"3. bufferization &amp; memref dialect",charIndex:null},{level:2,title:"4. Tiling, Scheduling, and Vectorization",slug:"_4-tiling-scheduling-and-vectorization",normalizedTitle:"4. tiling, scheduling, and vectorization",charIndex:4237},{level:2,title:"5. Lowering to GPU Dialect (and then to LLVM Dialect)",slug:"_5-lowering-to-gpu-dialect-and-then-to-llvm-dialect",normalizedTitle:"5. lowering to gpu dialect (and then to llvm dialect)",charIndex:4769},{level:2,title:"6. Final PTX Generation",slug:"_6-final-ptx-generation",normalizedTitle:"6. final ptx generation",charIndex:5837},{level:2,title:"Putting It All Together (Conv2D → PTX)",slug:"putting-it-all-together-conv2d-→-ptx",normalizedTitle:"putting it all together (conv2d → ptx)",charIndex:6349}],headersStr:"1. High-Level Dialect (TOSA or MHLO) 2. Lowering to Linalg (Structured Ops) 3. Bufferization & MemRef Dialect 4. Tiling, Scheduling, and Vectorization 5. Lowering to GPU Dialect (and then to LLVM Dialect) 6. Final PTX Generation Putting It All Together (Conv2D → PTX)",content:'# MLIR Compiling Flow of Conv2D\n\n> This is generated by ChatGPT.\n\n\n# 1. High-Level Dialect (TOSA or MHLO)\n\nAt the highest level, the convolution typically appears as a single-purpose op, e.g., TOSA’s tosa.conv2d or MHLO’s mhlo.convolution. Below is a TOSA-style example:\n\nCode\n\n# Simple 2D conv: stride=1, no padding, no dilation\n# Input:  [N=4, H=32, W=32, C_in=3]\n# Filter: [KH=3, KW=3, C_in=3, C_out=8]\n# Output: [N=4, H=30, W=30, C_out=8]\ntosa.conv2d(\n  %input,    // tensor<4x32x32x3xf32>\n  %filter,   // tensor<3x3x3x8xf32>\n  %bias,     // tensor<8xf32>, optional\n  strides = [1, 1],\n  pad = [0, 0, 0, 0],\n  dilation = [1, 1]\n) : (tensor<4x32x32x3xf32>,\n     tensor<3x3x3x8xf32>,\n     tensor<8xf32>) -> tensor<4x30x30x8xf32>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nAt this stage, the operation is still “high level”: the compiler knows it’s a convolution with certain attributes but hasn’t expanded it into explicit loops yet.\n\n\n# 2. Lowering to Linalg (Structured Ops)\n\nA specialized Linalg op may replace the TOSA conv2d. MLIR has built-in named ops like linalg.conv_2d_nhwc_hwcf, but one can also lower to a more generic linalg.generic. Below is a named Linalg form that captures the same convolution:\n\nCode\n\n%output = linalg.conv_2d_nhwc_hwcf\n    ins(%input, %filter : tensor<4x32x32x3xf32>, tensor<3x3x3x8xf32>)\n    outs(%init : tensor<4x30x30x8xf32>) -> tensor<4x30x30x8xf32>\n{\n  // The internal region is typically auto-generated. \n  // The iteration spaces (N, H_out, W_out, C_out, KH, KW, C_in) \n  // are implied by this op’s definition.\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nAlternatively, you may see an expanded linalg.generic with explicit indexing maps that show the iteration space. Here’s a solid example: we define 7 iteration dimensions:\n\n * 4 “parallel” dims: (n, oh, ow, co)\n * 3 “reduction” dims: (kh, kw, ci)\n\nWe then set up affine maps to read from (n, oh + kh, ow + kw, ci) and (kh, kw, ci, co), accumulating into (n, oh, ow, co):\n\nCode\n\n%conv_result = linalg.generic\n    // Define how to map the 7 loop indices to each tensor’s coordinates\n    { indexing_maps = [\n        // Input:  (n, oh, ow, co, kh, kw, ci) -> (n, oh + kh, ow + kw, ci)\n        affine_map<(n, oh, ow, co, kh, kw, ci) -> (n, oh + kh, ow + kw, ci)>,\n\n        // Filter: (n, oh, ow, co, kh, kw, ci) -> (kh, kw, ci, co)\n        affine_map<(n, oh, ow, co, kh, kw, ci) -> (kh, kw, ci, co)>,\n\n        // Output: (n, oh, ow, co, kh, kw, ci) -> (n, oh, ow, co)\n        affine_map<(n, oh, ow, co, kh, kw, ci) -> (n, oh, ow, co)>\n      ],\n      iterator_types = [\n        "parallel", "parallel", "parallel", "parallel", // n, oh, ow, co\n        "reduction", "reduction", "reduction"           // kh, kw, ci\n      ]\n    }\n    ins(%input, %filter : tensor<4x32x32x3xf32>, tensor<3x3x3x8xf32>)\n    outs(%init_out : tensor<4x30x30x8xf32>) -> tensor<4x30x30x8xf32>\n  {\n    // This block is the "fused operation" over each point in the iteration space\n    ^bb0(%in_val: f32, %fil_val: f32, %acc_val: f32):\n      %mul = arith.mulf %in_val, %fil_val : f32\n      %res = arith.addf %acc_val, %mul : f32\n      linalg.yield %res : f32\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\nThis explicitly shows the iteration space (n, oh, ow, co, kh, kw, ci) and how each tensor is indexed. The convolution is essentially a 7D loop nest: 4 parallel loops, 3 reduction loops.\n\n\n# 3. Bufferization & MemRef Dialect\n\nBefore we can generate code for GPUs, we usually transform from “tensor forms” (value semantics) into memref forms (explicit pointers). For instance, each tensor might become a memref<4x32x32x3xf32> allocated in GPU memory. A simplified result might look like:\n\nCode\n\n// Example function using memrefs for input/output/filter:\nfunc.func @conv2d_main(%input: memref<4x32x32x3xf32>, \n                       %filter: memref<3x3x3x8xf32>, \n                       %output: memref<4x30x30x8xf32>) {\n  // The linalg op is now in buffer form:\n  call @linalg_conv_2d_bufferized(%input, %filter, %output)\n\n  func.return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nInternally, linalg_conv_2d_bufferized will do loads and stores (memref.load, memref.store) or eventually llvm.load/llvm.store once further lowered. Bufferization clarifies memory addressing and layouts.\n\n\n# 4. Tiling, Scheduling, and Vectorization\n\nAt this stage (still in Linalg land or in the GPU dialect), a series of passes will:\n\nTile the convolution (e.g., break the iteration space into tile sizes that fit GPU blocks/threads). Schedule the loops to map them to GPU block IDs and thread IDs. Optionally vectorize (using the vector dialect) or fuse elementwise ops (like a subsequent ReLU). Conceptually, this transforms the single big convolution nest into multiple smaller “tiled” computations that run in parallel on the GPU.\n\n\n# 5. Lowering to GPU Dialect (and then to LLVM Dialect)\n\nAfter tiling and scheduling, we insert ops from the GPU dialect—these specify things like gpu.launch, gpu.block_id, gpu.thread_id. For example:\n\nCode\n\ngpu.func @conv2d_kernel(%input: memref<4x32x32x3xf32, 1>,\n                        %filter: memref<3x3x3x8xf32, 1>,\n                        %output: memref<4x30x30x8xf32, 1>) {\n  %bx = gpu.block_id x\n  %tx = gpu.thread_id x\n  // Possibly compute global indices = bx * blockSize + tx, etc.\n\n  // "scf.for" or "gpu.for" loops for the tile\n  // Perform loads, FMAs, stores\n  gpu.return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nOnce in the GPU dialect, a pass lowers it to NVVM or ROCDL dialect ops, which are then turned into the LLVM dialect. For instance, we might see:\n\nCode\n\n// NVVM dialect or final LLVM dialect snippet (illustrative)\nllvm.func @conv2d_kernel_gpu(...) {\n  %0 = llvm.getelementptr ...\n  %1 = llvm.load %0 : !llvm.ptr<f32>\n  %2 = llvm.fmul %1, %some_other_val : f32\n  ...\n  llvm.store %2, %dest_ptr : !llvm.ptr<f32>\n  llvm.return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 6. Final PTX Generation\n\nFinally, the LLVM dialect is handed off to LLVM’s NVPTX backend, which emits PTX code. In practice, you might see (in textual PTX form):\n\nCode\n\n    // A tiny snippet of PTX from the compiled kernel\n    // load r0, [rd0];\n    // ...\n    mul.f32  %f2, %f1, %f0;\n    add.f32  %f3, %f2, %f4;\n    // ...\n    ret;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nThis PTX can be JIT-compiled by the CUDA driver into GPU microcode (cubin). Once this step completes, your conv2d kernel is ready to run on NVIDIA hardware.\n\n\n# Putting It All Together (Conv2D → PTX)\n\nHigh-Level Dialect (TOSA, MHLO)\n\n * Single op: tosa.conv2d / mhlo.convolution.\n\nLinalg (Structured Ops)\n\n * Named op: linalg.conv_2d_nhwc_hwcf, or\n * Expanded op: linalg.generic with explicit (n, oh, ow, co, kh, kw, ci) iteration.\n\nBufferization → MemRef dialect\n\n * Turn tensors into explicit memory references (memref<...>).\n\nTiling & Scheduling\n\n * Transform loops for GPU block/thread distribution, fuse ops, vectorize.\n\nGPU Dialect → NVVM / LLVM Dialect\n\n * Introduce gpu.launch, thread IDs, eventually become LLVM IR.\n\nLLVM NVPTX Backend → PTX\n\n * Final codegen to PTX, run on the NVIDIA GPU.\n\nIn short, each stage refines the representation from an abstract “conv2d” operation down to the PTX instructions that physically perform the multiply-accumulate loops on the GPU.',normalizedContent:'# mlir compiling flow of conv2d\n\n> this is generated by chatgpt.\n\n\n# 1. high-level dialect (tosa or mhlo)\n\nat the highest level, the convolution typically appears as a single-purpose op, e.g., tosa’s tosa.conv2d or mhlo’s mhlo.convolution. below is a tosa-style example:\n\ncode\n\n# simple 2d conv: stride=1, no padding, no dilation\n# input:  [n=4, h=32, w=32, c_in=3]\n# filter: [kh=3, kw=3, c_in=3, c_out=8]\n# output: [n=4, h=30, w=30, c_out=8]\ntosa.conv2d(\n  %input,    // tensor<4x32x32x3xf32>\n  %filter,   // tensor<3x3x3x8xf32>\n  %bias,     // tensor<8xf32>, optional\n  strides = [1, 1],\n  pad = [0, 0, 0, 0],\n  dilation = [1, 1]\n) : (tensor<4x32x32x3xf32>,\n     tensor<3x3x3x8xf32>,\n     tensor<8xf32>) -> tensor<4x30x30x8xf32>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nat this stage, the operation is still “high level”: the compiler knows it’s a convolution with certain attributes but hasn’t expanded it into explicit loops yet.\n\n\n# 2. lowering to linalg (structured ops)\n\na specialized linalg op may replace the tosa conv2d. mlir has built-in named ops like linalg.conv_2d_nhwc_hwcf, but one can also lower to a more generic linalg.generic. below is a named linalg form that captures the same convolution:\n\ncode\n\n%output = linalg.conv_2d_nhwc_hwcf\n    ins(%input, %filter : tensor<4x32x32x3xf32>, tensor<3x3x3x8xf32>)\n    outs(%init : tensor<4x30x30x8xf32>) -> tensor<4x30x30x8xf32>\n{\n  // the internal region is typically auto-generated. \n  // the iteration spaces (n, h_out, w_out, c_out, kh, kw, c_in) \n  // are implied by this op’s definition.\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nalternatively, you may see an expanded linalg.generic with explicit indexing maps that show the iteration space. here’s a solid example: we define 7 iteration dimensions:\n\n * 4 “parallel” dims: (n, oh, ow, co)\n * 3 “reduction” dims: (kh, kw, ci)\n\nwe then set up affine maps to read from (n, oh + kh, ow + kw, ci) and (kh, kw, ci, co), accumulating into (n, oh, ow, co):\n\ncode\n\n%conv_result = linalg.generic\n    // define how to map the 7 loop indices to each tensor’s coordinates\n    { indexing_maps = [\n        // input:  (n, oh, ow, co, kh, kw, ci) -> (n, oh + kh, ow + kw, ci)\n        affine_map<(n, oh, ow, co, kh, kw, ci) -> (n, oh + kh, ow + kw, ci)>,\n\n        // filter: (n, oh, ow, co, kh, kw, ci) -> (kh, kw, ci, co)\n        affine_map<(n, oh, ow, co, kh, kw, ci) -> (kh, kw, ci, co)>,\n\n        // output: (n, oh, ow, co, kh, kw, ci) -> (n, oh, ow, co)\n        affine_map<(n, oh, ow, co, kh, kw, ci) -> (n, oh, ow, co)>\n      ],\n      iterator_types = [\n        "parallel", "parallel", "parallel", "parallel", // n, oh, ow, co\n        "reduction", "reduction", "reduction"           // kh, kw, ci\n      ]\n    }\n    ins(%input, %filter : tensor<4x32x32x3xf32>, tensor<3x3x3x8xf32>)\n    outs(%init_out : tensor<4x30x30x8xf32>) -> tensor<4x30x30x8xf32>\n  {\n    // this block is the "fused operation" over each point in the iteration space\n    ^bb0(%in_val: f32, %fil_val: f32, %acc_val: f32):\n      %mul = arith.mulf %in_val, %fil_val : f32\n      %res = arith.addf %acc_val, %mul : f32\n      linalg.yield %res : f32\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\nthis explicitly shows the iteration space (n, oh, ow, co, kh, kw, ci) and how each tensor is indexed. the convolution is essentially a 7d loop nest: 4 parallel loops, 3 reduction loops.\n\n\n# 3. bufferization & memref dialect\n\nbefore we can generate code for gpus, we usually transform from “tensor forms” (value semantics) into memref forms (explicit pointers). for instance, each tensor might become a memref<4x32x32x3xf32> allocated in gpu memory. a simplified result might look like:\n\ncode\n\n// example function using memrefs for input/output/filter:\nfunc.func @conv2d_main(%input: memref<4x32x32x3xf32>, \n                       %filter: memref<3x3x3x8xf32>, \n                       %output: memref<4x30x30x8xf32>) {\n  // the linalg op is now in buffer form:\n  call @linalg_conv_2d_bufferized(%input, %filter, %output)\n\n  func.return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\ninternally, linalg_conv_2d_bufferized will do loads and stores (memref.load, memref.store) or eventually llvm.load/llvm.store once further lowered. bufferization clarifies memory addressing and layouts.\n\n\n# 4. tiling, scheduling, and vectorization\n\nat this stage (still in linalg land or in the gpu dialect), a series of passes will:\n\ntile the convolution (e.g., break the iteration space into tile sizes that fit gpu blocks/threads). schedule the loops to map them to gpu block ids and thread ids. optionally vectorize (using the vector dialect) or fuse elementwise ops (like a subsequent relu). conceptually, this transforms the single big convolution nest into multiple smaller “tiled” computations that run in parallel on the gpu.\n\n\n# 5. lowering to gpu dialect (and then to llvm dialect)\n\nafter tiling and scheduling, we insert ops from the gpu dialect—these specify things like gpu.launch, gpu.block_id, gpu.thread_id. for example:\n\ncode\n\ngpu.func @conv2d_kernel(%input: memref<4x32x32x3xf32, 1>,\n                        %filter: memref<3x3x3x8xf32, 1>,\n                        %output: memref<4x30x30x8xf32, 1>) {\n  %bx = gpu.block_id x\n  %tx = gpu.thread_id x\n  // possibly compute global indices = bx * blocksize + tx, etc.\n\n  // "scf.for" or "gpu.for" loops for the tile\n  // perform loads, fmas, stores\n  gpu.return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nonce in the gpu dialect, a pass lowers it to nvvm or rocdl dialect ops, which are then turned into the llvm dialect. for instance, we might see:\n\ncode\n\n// nvvm dialect or final llvm dialect snippet (illustrative)\nllvm.func @conv2d_kernel_gpu(...) {\n  %0 = llvm.getelementptr ...\n  %1 = llvm.load %0 : !llvm.ptr<f32>\n  %2 = llvm.fmul %1, %some_other_val : f32\n  ...\n  llvm.store %2, %dest_ptr : !llvm.ptr<f32>\n  llvm.return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 6. final ptx generation\n\nfinally, the llvm dialect is handed off to llvm’s nvptx backend, which emits ptx code. in practice, you might see (in textual ptx form):\n\ncode\n\n    // a tiny snippet of ptx from the compiled kernel\n    // load r0, [rd0];\n    // ...\n    mul.f32  %f2, %f1, %f0;\n    add.f32  %f3, %f2, %f4;\n    // ...\n    ret;\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\nthis ptx can be jit-compiled by the cuda driver into gpu microcode (cubin). once this step completes, your conv2d kernel is ready to run on nvidia hardware.\n\n\n# putting it all together (conv2d → ptx)\n\nhigh-level dialect (tosa, mhlo)\n\n * single op: tosa.conv2d / mhlo.convolution.\n\nlinalg (structured ops)\n\n * named op: linalg.conv_2d_nhwc_hwcf, or\n * expanded op: linalg.generic with explicit (n, oh, ow, co, kh, kw, ci) iteration.\n\nbufferization → memref dialect\n\n * turn tensors into explicit memory references (memref<...>).\n\ntiling & scheduling\n\n * transform loops for gpu block/thread distribution, fuse ops, vectorize.\n\ngpu dialect → nvvm / llvm dialect\n\n * introduce gpu.launch, thread ids, eventually become llvm ir.\n\nllvm nvptx backend → ptx\n\n * final codegen to ptx, run on the nvidia gpu.\n\nin short, each stage refines the representation from an abstract “conv2d” operation down to the ptx instructions that physically perform the multiply-accumulate loops on the gpu.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"MLIR NVGPU Dialect",frontmatter:{title:"MLIR NVGPU Dialect",date:"2025-03-04T00:00:00.000Z",permalink:"/pages/000018/",tags:[null]},regularPath:"/02.compiler/18.mlir_notes_06.html",relativePath:"02.compiler/18.mlir_notes_06.md",key:"v-b79a3afe",path:"/pages/000018/",headers:[{level:2,title:"1. Purpose and Positioning",slug:"_1-purpose-and-positioning",normalizedTitle:"1. purpose and positioning",charIndex:539},{level:2,title:"2. Memory Transfers (TMA)",slug:"_2-memory-transfers-tma",normalizedTitle:"2. memory transfers (tma)",charIndex:1188},{level:2,title:"3. Warpgroup and MMA Operations",slug:"_3-warpgroup-and-mma-operations",normalizedTitle:"3. warpgroup and mma operations",charIndex:2085},{level:2,title:"4. Synchronization and Barriers",slug:"_4-synchronization-and-barriers",normalizedTitle:"4. synchronization and barriers",charIndex:3003},{level:2,title:"5. Putting It All Together in a GEMM",slug:"_5-putting-it-all-together-in-a-gemm",normalizedTitle:"5. putting it all together in a gemm",charIndex:3547},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:4481}],headersStr:"1. Purpose and Positioning 2. Memory Transfers (TMA) 3. Warpgroup and MMA Operations 4. Synchronization and Barriers 5. Putting It All Together in a GEMM Summary",content:"# MLIR NVGPU Dialect\n\n> This is generated by ChatGPT.\n\nThe nvgpu dialect is an MLIR dialect specifically designed for NVIDIA GPU–targeted code.\n\nIt provides MLIR operations and types that map onto NVIDIA’s GPU hardware features—such as warp-level matrix–multiply–accumulate (MMA) instructions, asynchronous “TMA” (Tensor Memory Access) data transfers, and warpgroup synchronization—while staying higher-level and more structured than raw PTX.\n\nBelow is a concise explanation tying together the main points shown in the pages/diagrams:\n\n\n# 1. Purpose and Positioning\n\nHardware‐aware MLIR dialect\n\nThe nvgpu dialect introduces operations and attributes that directly match NVIDIA GPU hardware capabilities (like Tensor Cores, warp‐level shared memory operations, etc.) but are still valid MLIR operations.\n\nThis makes it possible to generate correct, optimized GPU code in a structured way before finally lowering to NVVM or PTX.\n\nBridging to NVVM/PTX\n\nUnder the hood, nvgpu dialect operations eventually get lowered to NVVM (LLVM’s NVIDIA GPU backend) or directly to PTX instructions.\n\nThis allows you to write higher-level MLIR code while still leveraging specialized GPU intrinsics.\n\n\n# 2. Memory Transfers (TMA)\n\nTMA load/store ops\n\nIn the examples (e.g., nvgpu.tma.async.load or nvgpu.tma.async.commit), the dialect provides operations that handle asynchronous bulk transfers between global memory and shared memory.\n\nGlobal–Shared–Register path\n\nA “TMA descriptor” (for example, nvgpu.warpgroup.generate.descriptor %view) configures how data should be read from or written to global memory.\n\nAsynchronous TMA load An asynchronous TMA load operation triggers the hardware to bring data from global memory into shared memory without stalling the thread. A synchronization primitive (e.g., mbar_group[0].wait(...)) can be used afterward to ensure the data is ready in shared memory.\n\nCooperative TMA\n\nThese loads are typically done per warpgroup so that multiple threads can coordinate and amortize overhead when fetching large tiles of data (e.g., a 128×64 block of the matrix).\n\n\n# 3. Warpgroup and MMA Operations\n\nnvgpu.warpgroup.mma\n\nThe dialect introduces warpgroup-level instructions for Tensor Core “Matrix Multiply–Accumulate” (MMA).\n\nThese let you multiply two tile fragments (e.g., 128×64×f16 × 64×128×f16) and accumulate results into a 128×128 f32 fragment.\n\nFragments\n\nDescriptor fragments represent tiles loaded into registers (part of a warp’s register file). Accumulator fragments hold the intermediate or final results (often in a higher precision).\n\nUsage pattern in the GEMM example\n\n * Load: TMA ops bring matrix tiles A and B into shared memory.\n * Convert them into warp-level descriptors, e.g. A = WGMMAMatrix(..., shape=[128, 64]).\n * MMA: C += A @ B uses the warpgroup-level MMA operation to compute partial products in registers.\n * Store: Results in accumulator fragments are then stored back out to global memory (often through TMA or another set of store instructions).\n\n\n# 4. Synchronization and Barriers\n\nMemory barrier groups\n\nIn the snippets, you see calls such as mbar_group[0].init(...) or mbar_group[0].wait(...).\n\nThese are ways of synchronizing the asynchronous TMA loads and ensuring that all threads in a warp (or warpgroup) see the same data at the right time.\n\nnvgpu.wgma.commit & nvgpu.wgma.wait\n\nThese are specialized instructions in the dialect for committing asynchronous MMA operations and waiting for them to complete—again letting the code remain high-level but express the needed sync points.\n\n\n# 5. Putting It All Together in a GEMM\n\nFrom the example “Ch3.py: GEMM 128x128x64”:\n\n * TMA Descriptors are generated for each of the input matrices A and B.\n * Asynchronous TMA loads bring blocks of A and B into shared memory.\n * A warp or warpgroup waits on those loads to complete, ensuring data is now ready.\n * Matrix descriptors (e.g., WGMMAMatrix.Descriptor) are formed so the warp can read the shared memory tiles.\n * Accumulator (e.g., WGMMAMatrix.Accumulator) is initialized for the output (C).\n * The warpgroup performs C += A @ B via nvgpu.warpgroup.mma instructions (Tensor Cores).\n * Store the final accumulator tile back out to global memory (possibly again using TMA).\n\nThis flow showcases how the nvgpu dialect encodes all stages of the classic GPU GEMM pipeline—global memory tile loads, shared memory tiling, warp-level MMA, and final writes—while leveraging MLIR’s structured representation and transformations.\n\n\n# Summary\n\nIn short, the nvgpu dialect in MLIR gives you a structured, GPU‐focused IR that directly corresponds to NVIDIA hardware’s memory hierarchy, warp cooperative groups, and Tensor Core instructions.\n\nIt provides:\n\n * Custom ops for asynchronous TMA data movement,\n * Warpgroup cooperation and synchronization,\n * MMA (matrix multiply–accumulate) ops targeting Tensor Cores, and\n * A path to lower down to NVVM/PTX with minimal hand-tuning while still matching NVIDIA’s performance features.\n\nThis makes it a powerful tool for writing high‐performance GPU kernels in an MLIR framework without dropping down to raw assembly-level code.",normalizedContent:"# mlir nvgpu dialect\n\n> this is generated by chatgpt.\n\nthe nvgpu dialect is an mlir dialect specifically designed for nvidia gpu–targeted code.\n\nit provides mlir operations and types that map onto nvidia’s gpu hardware features—such as warp-level matrix–multiply–accumulate (mma) instructions, asynchronous “tma” (tensor memory access) data transfers, and warpgroup synchronization—while staying higher-level and more structured than raw ptx.\n\nbelow is a concise explanation tying together the main points shown in the pages/diagrams:\n\n\n# 1. purpose and positioning\n\nhardware‐aware mlir dialect\n\nthe nvgpu dialect introduces operations and attributes that directly match nvidia gpu hardware capabilities (like tensor cores, warp‐level shared memory operations, etc.) but are still valid mlir operations.\n\nthis makes it possible to generate correct, optimized gpu code in a structured way before finally lowering to nvvm or ptx.\n\nbridging to nvvm/ptx\n\nunder the hood, nvgpu dialect operations eventually get lowered to nvvm (llvm’s nvidia gpu backend) or directly to ptx instructions.\n\nthis allows you to write higher-level mlir code while still leveraging specialized gpu intrinsics.\n\n\n# 2. memory transfers (tma)\n\ntma load/store ops\n\nin the examples (e.g., nvgpu.tma.async.load or nvgpu.tma.async.commit), the dialect provides operations that handle asynchronous bulk transfers between global memory and shared memory.\n\nglobal–shared–register path\n\na “tma descriptor” (for example, nvgpu.warpgroup.generate.descriptor %view) configures how data should be read from or written to global memory.\n\nasynchronous tma load an asynchronous tma load operation triggers the hardware to bring data from global memory into shared memory without stalling the thread. a synchronization primitive (e.g., mbar_group[0].wait(...)) can be used afterward to ensure the data is ready in shared memory.\n\ncooperative tma\n\nthese loads are typically done per warpgroup so that multiple threads can coordinate and amortize overhead when fetching large tiles of data (e.g., a 128×64 block of the matrix).\n\n\n# 3. warpgroup and mma operations\n\nnvgpu.warpgroup.mma\n\nthe dialect introduces warpgroup-level instructions for tensor core “matrix multiply–accumulate” (mma).\n\nthese let you multiply two tile fragments (e.g., 128×64×f16 × 64×128×f16) and accumulate results into a 128×128 f32 fragment.\n\nfragments\n\ndescriptor fragments represent tiles loaded into registers (part of a warp’s register file). accumulator fragments hold the intermediate or final results (often in a higher precision).\n\nusage pattern in the gemm example\n\n * load: tma ops bring matrix tiles a and b into shared memory.\n * convert them into warp-level descriptors, e.g. a = wgmmamatrix(..., shape=[128, 64]).\n * mma: c += a @ b uses the warpgroup-level mma operation to compute partial products in registers.\n * store: results in accumulator fragments are then stored back out to global memory (often through tma or another set of store instructions).\n\n\n# 4. synchronization and barriers\n\nmemory barrier groups\n\nin the snippets, you see calls such as mbar_group[0].init(...) or mbar_group[0].wait(...).\n\nthese are ways of synchronizing the asynchronous tma loads and ensuring that all threads in a warp (or warpgroup) see the same data at the right time.\n\nnvgpu.wgma.commit & nvgpu.wgma.wait\n\nthese are specialized instructions in the dialect for committing asynchronous mma operations and waiting for them to complete—again letting the code remain high-level but express the needed sync points.\n\n\n# 5. putting it all together in a gemm\n\nfrom the example “ch3.py: gemm 128x128x64”:\n\n * tma descriptors are generated for each of the input matrices a and b.\n * asynchronous tma loads bring blocks of a and b into shared memory.\n * a warp or warpgroup waits on those loads to complete, ensuring data is now ready.\n * matrix descriptors (e.g., wgmmamatrix.descriptor) are formed so the warp can read the shared memory tiles.\n * accumulator (e.g., wgmmamatrix.accumulator) is initialized for the output (c).\n * the warpgroup performs c += a @ b via nvgpu.warpgroup.mma instructions (tensor cores).\n * store the final accumulator tile back out to global memory (possibly again using tma).\n\nthis flow showcases how the nvgpu dialect encodes all stages of the classic gpu gemm pipeline—global memory tile loads, shared memory tiling, warp-level mma, and final writes—while leveraging mlir’s structured representation and transformations.\n\n\n# summary\n\nin short, the nvgpu dialect in mlir gives you a structured, gpu‐focused ir that directly corresponds to nvidia hardware’s memory hierarchy, warp cooperative groups, and tensor core instructions.\n\nit provides:\n\n * custom ops for asynchronous tma data movement,\n * warpgroup cooperation and synchronization,\n * mma (matrix multiply–accumulate) ops targeting tensor cores, and\n * a path to lower down to nvvm/ptx with minimal hand-tuning while still matching nvidia’s performance features.\n\nthis makes it a powerful tool for writing high‐performance gpu kernels in an mlir framework without dropping down to raw assembly-level code.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"MLIR Bufferization",frontmatter:{title:"MLIR Bufferization",date:"2025-03-05T00:00:00.000Z",permalink:"/pages/000020/",tags:[null]},regularPath:"/02.compiler/20.mlir_notes_08.html",relativePath:"02.compiler/20.mlir_notes_08.md",key:"v-5841ee0f",path:"/pages/000020/",headers:[{level:2,title:"1️⃣ What is Bufferization?",slug:"_1️⃣-what-is-bufferization",normalizedTitle:"1️⃣ what is bufferization?",charIndex:97},{level:3,title:"✅ Why is Bufferization Needed?",slug:"✅-why-is-bufferization-needed",normalizedTitle:"✅ why is bufferization needed?",charIndex:450},{level:2,title:"2️⃣ Bufferization Pipeline",slug:"_2️⃣-bufferization-pipeline",normalizedTitle:"2️⃣ bufferization pipeline",charIndex:749},{level:3,title:"🔹 High-Level Tensor Computation (Functional Style)",slug:"🔹-high-level-tensor-computation-functional-style",normalizedTitle:"🔹 high-level tensor computation (functional style)",charIndex:879},{level:3,title:"🔹 Bufferization (Converting Tensor to MemRef)",slug:"🔹-bufferization-converting-tensor-to-memref",normalizedTitle:"🔹 bufferization (converting tensor to memref)",charIndex:1192},{level:3,title:"🔹 Lowering to LLVM (Final Execution)",slug:"🔹-lowering-to-llvm-final-execution",normalizedTitle:"🔹 lowering to llvm (final execution)",charIndex:1640},{level:2,title:"3️⃣ Types of Bufferization",slug:"_3️⃣-types-of-bufferization",normalizedTitle:"3️⃣ types of bufferization",charIndex:1878},{level:3,title:"🔹 1. One-Shot Bufferization",slug:"🔹-1-one-shot-bufferization",normalizedTitle:"🔹 1. one-shot bufferization",charIndex:1952},{level:3,title:"🔹 2. Progressive (Partial) Bufferization",slug:"🔹-2-progressive-partial-bufferization",normalizedTitle:"🔹 2. progressive (partial) bufferization",charIndex:2148},{level:2,title:"4️⃣ Bufferization Analysis: Handling Aliasing & Copies",slug:"_4️⃣-bufferization-analysis-handling-aliasing-copies",normalizedTitle:"4️⃣ bufferization analysis: handling aliasing &amp; copies",charIndex:null},{level:3,title:"🔹 Case 1: No Copy Needed (In-Place Bufferization)",slug:"🔹-case-1-no-copy-needed-in-place-bufferization",normalizedTitle:"🔹 case 1: no copy needed (in-place bufferization)",charIndex:2521},{level:3,title:"🔹 Case 2: Copy Needed (Aliased Data)",slug:"🔹-case-2-copy-needed-aliased-data",normalizedTitle:"🔹 case 2: copy needed (aliased data)",charIndex:3156},{level:2,title:"5️⃣ Bufferization for GPU Execution",slug:"_5️⃣-bufferization-for-gpu-execution",normalizedTitle:"5️⃣ bufferization for gpu execution",charIndex:4199},{level:2,title:"6️⃣ Bufferization Passes in MLIR",slug:"_6️⃣-bufferization-passes-in-mlir",normalizedTitle:"6️⃣ bufferization passes in mlir",charIndex:5017},{level:2,title:"7️⃣ Summary",slug:"_7️⃣-summary",normalizedTitle:"7️⃣ summary",charIndex:5291},{level:2,title:"8️⃣ Conclusion",slug:"_8️⃣-conclusion",normalizedTitle:"8️⃣ conclusion",charIndex:5610}],headersStr:"1️⃣ What is Bufferization? ✅ Why is Bufferization Needed? 2️⃣ Bufferization Pipeline 🔹 High-Level Tensor Computation (Functional Style) 🔹 Bufferization (Converting Tensor to MemRef) 🔹 Lowering to LLVM (Final Execution) 3️⃣ Types of Bufferization 🔹 1. One-Shot Bufferization 🔹 2. Progressive (Partial) Bufferization 4️⃣ Bufferization Analysis: Handling Aliasing & Copies 🔹 Case 1: No Copy Needed (In-Place Bufferization) 🔹 Case 2: Copy Needed (Aliased Data) 5️⃣ Bufferization for GPU Execution 6️⃣ Bufferization Passes in MLIR 7️⃣ Summary 8️⃣ Conclusion",content:"# MLIR MemRef Dialect\n\n> This is generated by ChatGPT.\n\nDeep Dive into Bufferization in MLIR\n\n\n# 1️⃣ What is Bufferization?\n\nBufferization is the process of converting tensor-based computations into memory-based (memref) computations in MLIR.\n\nIt allows transitioning from a high-level functional-style representation (immutable tensors) to explicit memory management (mutable memrefs), which is required for hardware execution (CPU, GPU, etc.).\n\n\n# ✅ Why is Bufferization Needed?\n\nTensors are immutable: Every tensor operation creates a new tensor. MemRefs are mutable: Avoids unnecessary copies, enabling in-place updates. Hardware requires explicit memory management: Low-level backends (LLVM, CUDA) work with pointers, not abstract tensors.\n\n\n# 2️⃣ Bufferization Pipeline\n\nBufferization transforms tensor-based operations into memref-based operations in multiple stages.\n\n\n# 🔹 High-Level Tensor Computation (Functional Style)\n\nOperations use immutable tensor values. Example:\n\n%B = linalg.matmul ins(%A, %A : tensor<4x4xf32>, tensor<4x4xf32>)\n                   outs(%C : tensor<4x4xf32>) -> tensor<4x4xf32>\n\n\n1\n2\n\n\n%B is not modified in-place. A new tensor is allocated implicitly.\n\n\n# 🔹 Bufferization (Converting Tensor to MemRef)\n\nExplicit memory allocation (memref.alloc). Uses mutability for in-place updates. Example:\n\n%B_mem = memref.alloc() : memref<4x4xf32>\nlinalg.matmul ins(%A_mem, %A_mem : memref<4x4xf32>, memref<4x4xf32>)\n             outs(%B_mem : memref<4x4xf32>)\nmemref.dealloc %B_mem\n\n\n1\n2\n3\n4\n\n\ntensor<4x4xf32> → memref<4x4xf32>. Explicit memory allocation (memref.alloc). Manual deallocation (memref.dealloc).\n\n\n# 🔹 Lowering to LLVM (Final Execution)\n\nMemRef is converted into LLVM pointers (llvm.ptr). Example:\n\n%ptr = llvm.getelementptr %B_mem[%i, %j] : (!llvm.ptr<f32>, i32, i32) -> !llvm.ptr<f32>\n%val = llvm.load %ptr : !llvm.ptr<f32>\n\n\n1\n2\n\n\n\n# 3️⃣ Types of Bufferization\n\nMLIR provides two types of bufferization:\n\n\n# 🔹 1. One-Shot Bufferization\n\nConverts all tensors into memrefs in a single pass. Less flexible but efficient for static memory allocation. Example Pass:\n\nmlir-opt --bufferize input.mlir\n\n\n1\n\n\n\n# 🔹 2. Progressive (Partial) Bufferization\n\nConverts tensors incrementally, allowing analysis-based optimizations. Handles aliasing and inplace updates safely. Example:\n\nmlir-opt --partial-bufferize input.mlir\n\n\n1\n\n\n\n# 4️⃣ Bufferization Analysis: Handling Aliasing & Copies\n\nBufferization must analyze if a tensor operation can be safely replaced with a mutable memref.\n\n\n# 🔹 Case 1: No Copy Needed (In-Place Bufferization)\n\nIf only one operation writes to a tensor, it can be directly mapped to a memref. Example: In-Place Bufferization (No Copy Needed)\n\nfunc.func @inplace_add(%A: tensor<4xf32>) -> tensor<4xf32> {\n  %B = tensor.add %A, %A : tensor<4xf32>\n  return %B\n}\n\n\n1\n2\n3\n4\n\n\n➡ After Bufferization\n\nfunc.func @inplace_add(%A_mem: memref<4xf32>) {\n  %B_mem = %A_mem  // No copy needed\n  scf.for %i = 0 to 4 {\n    %a = memref.load %A_mem[%i] : memref<4xf32>\n    %b = arith.addf %a, %a : f32\n    memref.store %b, %A_mem[%i] : memref<4xf32>\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n📌 No additional alloc() needed!\n\n\n# 🔹 Case 2: Copy Needed (Aliased Data)\n\nIf a tensor is used multiple times, a copy is required to prevent unintended modifications.\n\nExample: Copy Required Due to Aliasing\n\nfunc.func @aliasing_problem(%A: tensor<4xf32>) -> tensor<4xf32> {\n  %B = tensor.add %A, %A : tensor<4xf32>\n  %C = tensor.add %B, %A : tensor<4xf32>\n  return %C\n}\n\n\n1\n2\n3\n4\n5\n\n\n➡ After Bufferization\n\nfunc.func @aliasing_problem(%A_mem: memref<4xf32>) {\n  %B_mem = memref.alloc() : memref<4xf32>\n  scf.for %i = 0 to 4 {\n    %a = memref.load %A_mem[%i] : memref<4xf32>\n    %b = arith.addf %a, %a : f32\n    memref.store %b, %B_mem[%i] : memref<4xf32>\n  }\n\n  %C_mem = memref.alloc() : memref<4xf32>\n  scf.for %i = 0 to 4 {\n    %b = memref.load %B_mem[%i] : memref<4xf32>\n    %a = memref.load %A_mem[%i] : memref<4xf32>\n    %c = arith.addf %b, %a : f32\n    memref.store %c, %C_mem[%i] : memref<4xf32>\n  }\n\n  memref.dealloc %B_mem\n  return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n📌 A copy (memref.alloc()) is required for %B_mem because %A_mem is still used!\n\n\n# 5️⃣ Bufferization for GPU Execution\n\nBufferization is critical for GPU execution because:\n\nTensors cannot be used in GPU kernels (they are immutable). MemRefs explicitly allocate GPU memory (#gpu.memory_space). Bufferization ensures correct memory aliasing for parallel execution. Example: Bufferizing a GPU Kernel Before Bufferization (Tensor Representation)\n\ngpu.func @kernel(%A: tensor<1024xf32>) {\n  %B = linalg.matmul ins(%A, %A) -> tensor<1024xf32>\n  return %B\n}\n\n\n1\n2\n3\n4\n\n\nAfter Bufferization (MemRef for GPU Execution)\n\ngpu.func @kernel(%A_mem: memref<1024xf32, #gpu.memory_space<global>>) {\n  %B_mem = memref.alloc() : memref<1024xf32, #gpu.memory_space<global>>\n  linalg.matmul ins(%A_mem, %A_mem) outs(%B_mem)\n  return\n}\n\n\n1\n2\n3\n4\n5\n\n\n📌 MemRefs are placed in #gpu.memory_space to allocate GPU memory.\n\n\n# 6️⃣ Bufferization Passes in MLIR\n\nTo apply bufferization, MLIR provides passes:\n\nPASS NAME                COMMAND\nOne-Shot Bufferization   mlir-opt --bufferize\nPartial Bufferization    mlir-opt --partial-bufferize\nBuffer Deallocation      mlir-opt --buffer-deallocation\n\n\n# 7️⃣ Summary\n\nFEATURE             BUFFERIZATION\nPurpose             Converts immutable tensors → mutable memrefs\nAvoids Copies?      ✅ Yes (if no aliasing)\nHandles Aliasing?   ✅ Yes (inserts memref.alloc if needed)\nNeeded for GPU?     ✅ Yes (memrefs required for GPU execution)\nFinal Target?       ✅ LLVM (llvm.ptr)\n\n\n# 8️⃣ Conclusion\n\n✅ Bufferization is essential for transitioning from high-level tensor computations to hardware execution. ✅ It minimizes memory allocations, reducing overhead and improving efficiency. ✅ Works with CPU and GPU lowering (via MemRef dialect).",normalizedContent:"# mlir memref dialect\n\n> this is generated by chatgpt.\n\ndeep dive into bufferization in mlir\n\n\n# 1️⃣ what is bufferization?\n\nbufferization is the process of converting tensor-based computations into memory-based (memref) computations in mlir.\n\nit allows transitioning from a high-level functional-style representation (immutable tensors) to explicit memory management (mutable memrefs), which is required for hardware execution (cpu, gpu, etc.).\n\n\n# ✅ why is bufferization needed?\n\ntensors are immutable: every tensor operation creates a new tensor. memrefs are mutable: avoids unnecessary copies, enabling in-place updates. hardware requires explicit memory management: low-level backends (llvm, cuda) work with pointers, not abstract tensors.\n\n\n# 2️⃣ bufferization pipeline\n\nbufferization transforms tensor-based operations into memref-based operations in multiple stages.\n\n\n# 🔹 high-level tensor computation (functional style)\n\noperations use immutable tensor values. example:\n\n%b = linalg.matmul ins(%a, %a : tensor<4x4xf32>, tensor<4x4xf32>)\n                   outs(%c : tensor<4x4xf32>) -> tensor<4x4xf32>\n\n\n1\n2\n\n\n%b is not modified in-place. a new tensor is allocated implicitly.\n\n\n# 🔹 bufferization (converting tensor to memref)\n\nexplicit memory allocation (memref.alloc). uses mutability for in-place updates. example:\n\n%b_mem = memref.alloc() : memref<4x4xf32>\nlinalg.matmul ins(%a_mem, %a_mem : memref<4x4xf32>, memref<4x4xf32>)\n             outs(%b_mem : memref<4x4xf32>)\nmemref.dealloc %b_mem\n\n\n1\n2\n3\n4\n\n\ntensor<4x4xf32> → memref<4x4xf32>. explicit memory allocation (memref.alloc). manual deallocation (memref.dealloc).\n\n\n# 🔹 lowering to llvm (final execution)\n\nmemref is converted into llvm pointers (llvm.ptr). example:\n\n%ptr = llvm.getelementptr %b_mem[%i, %j] : (!llvm.ptr<f32>, i32, i32) -> !llvm.ptr<f32>\n%val = llvm.load %ptr : !llvm.ptr<f32>\n\n\n1\n2\n\n\n\n# 3️⃣ types of bufferization\n\nmlir provides two types of bufferization:\n\n\n# 🔹 1. one-shot bufferization\n\nconverts all tensors into memrefs in a single pass. less flexible but efficient for static memory allocation. example pass:\n\nmlir-opt --bufferize input.mlir\n\n\n1\n\n\n\n# 🔹 2. progressive (partial) bufferization\n\nconverts tensors incrementally, allowing analysis-based optimizations. handles aliasing and inplace updates safely. example:\n\nmlir-opt --partial-bufferize input.mlir\n\n\n1\n\n\n\n# 4️⃣ bufferization analysis: handling aliasing & copies\n\nbufferization must analyze if a tensor operation can be safely replaced with a mutable memref.\n\n\n# 🔹 case 1: no copy needed (in-place bufferization)\n\nif only one operation writes to a tensor, it can be directly mapped to a memref. example: in-place bufferization (no copy needed)\n\nfunc.func @inplace_add(%a: tensor<4xf32>) -> tensor<4xf32> {\n  %b = tensor.add %a, %a : tensor<4xf32>\n  return %b\n}\n\n\n1\n2\n3\n4\n\n\n➡ after bufferization\n\nfunc.func @inplace_add(%a_mem: memref<4xf32>) {\n  %b_mem = %a_mem  // no copy needed\n  scf.for %i = 0 to 4 {\n    %a = memref.load %a_mem[%i] : memref<4xf32>\n    %b = arith.addf %a, %a : f32\n    memref.store %b, %a_mem[%i] : memref<4xf32>\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n📌 no additional alloc() needed!\n\n\n# 🔹 case 2: copy needed (aliased data)\n\nif a tensor is used multiple times, a copy is required to prevent unintended modifications.\n\nexample: copy required due to aliasing\n\nfunc.func @aliasing_problem(%a: tensor<4xf32>) -> tensor<4xf32> {\n  %b = tensor.add %a, %a : tensor<4xf32>\n  %c = tensor.add %b, %a : tensor<4xf32>\n  return %c\n}\n\n\n1\n2\n3\n4\n5\n\n\n➡ after bufferization\n\nfunc.func @aliasing_problem(%a_mem: memref<4xf32>) {\n  %b_mem = memref.alloc() : memref<4xf32>\n  scf.for %i = 0 to 4 {\n    %a = memref.load %a_mem[%i] : memref<4xf32>\n    %b = arith.addf %a, %a : f32\n    memref.store %b, %b_mem[%i] : memref<4xf32>\n  }\n\n  %c_mem = memref.alloc() : memref<4xf32>\n  scf.for %i = 0 to 4 {\n    %b = memref.load %b_mem[%i] : memref<4xf32>\n    %a = memref.load %a_mem[%i] : memref<4xf32>\n    %c = arith.addf %b, %a : f32\n    memref.store %c, %c_mem[%i] : memref<4xf32>\n  }\n\n  memref.dealloc %b_mem\n  return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n📌 a copy (memref.alloc()) is required for %b_mem because %a_mem is still used!\n\n\n# 5️⃣ bufferization for gpu execution\n\nbufferization is critical for gpu execution because:\n\ntensors cannot be used in gpu kernels (they are immutable). memrefs explicitly allocate gpu memory (#gpu.memory_space). bufferization ensures correct memory aliasing for parallel execution. example: bufferizing a gpu kernel before bufferization (tensor representation)\n\ngpu.func @kernel(%a: tensor<1024xf32>) {\n  %b = linalg.matmul ins(%a, %a) -> tensor<1024xf32>\n  return %b\n}\n\n\n1\n2\n3\n4\n\n\nafter bufferization (memref for gpu execution)\n\ngpu.func @kernel(%a_mem: memref<1024xf32, #gpu.memory_space<global>>) {\n  %b_mem = memref.alloc() : memref<1024xf32, #gpu.memory_space<global>>\n  linalg.matmul ins(%a_mem, %a_mem) outs(%b_mem)\n  return\n}\n\n\n1\n2\n3\n4\n5\n\n\n📌 memrefs are placed in #gpu.memory_space to allocate gpu memory.\n\n\n# 6️⃣ bufferization passes in mlir\n\nto apply bufferization, mlir provides passes:\n\npass name                command\none-shot bufferization   mlir-opt --bufferize\npartial bufferization    mlir-opt --partial-bufferize\nbuffer deallocation      mlir-opt --buffer-deallocation\n\n\n# 7️⃣ summary\n\nfeature             bufferization\npurpose             converts immutable tensors → mutable memrefs\navoids copies?      ✅ yes (if no aliasing)\nhandles aliasing?   ✅ yes (inserts memref.alloc if needed)\nneeded for gpu?     ✅ yes (memrefs required for gpu execution)\nfinal target?       ✅ llvm (llvm.ptr)\n\n\n# 8️⃣ conclusion\n\n✅ bufferization is essential for transitioning from high-level tensor computations to hardware execution. ✅ it minimizes memory allocations, reducing overhead and improving efficiency. ✅ works with cpu and gpu lowering (via memref dialect).",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"MLIR Linalg Dialect",frontmatter:{title:"MLIR Linalg Dialect",date:"2025-03-04T00:00:00.000Z",permalink:"/pages/000019/",tags:[null]},regularPath:"/02.compiler/19.mlir_notes_07.html",relativePath:"02.compiler/19.mlir_notes_07.md",key:"v-1cb4c87d",path:"/pages/000019/",headers:[{level:2,title:"1. Function of the Linalg Dialect",slug:"_1-function-of-the-linalg-dialect",normalizedTitle:"1. function of the linalg dialect",charIndex:430},{level:3,title:"1. Core Operations",slug:"_1-core-operations",normalizedTitle:"1. core operations",charIndex:732},{level:3,title:"2. Key Features",slug:"_2-key-features",normalizedTitle:"2. key features",charIndex:1426},{level:3,title:"3. Design Principles",slug:"_3-design-principles",normalizedTitle:"3. design principles",charIndex:2119},{level:2,title:"2. Principles of the Linalg Dialect",slug:"_2-principles-of-the-linalg-dialect",normalizedTitle:"2. principles of the linalg dialect",charIndex:2474},{level:3,title:"2.1. Structured Operations",slug:"_2-1-structured-operations",normalizedTitle:"2.1. structured operations",charIndex:2622},{level:3,title:"2.2. Progressive Lowering Strategy",slug:"_2-2-progressive-lowering-strategy",normalizedTitle:"2.2. progressive lowering strategy",charIndex:2867},{level:2,title:"3. Cooperation with Other Dialects",slug:"_3-cooperation-with-other-dialects",normalizedTitle:"3. cooperation with other dialects",charIndex:3354},{level:3,title:"3.1. Tensor Dialect",slug:"_3-1-tensor-dialect",normalizedTitle:"3.1. tensor dialect",charIndex:3444},{level:3,title:"3.2. MemRef Dialect",slug:"_3-2-memref-dialect",normalizedTitle:"3.2. memref dialect",charIndex:3684},{level:3,title:"3.3. SCF (Structured Control Flow) Dialect",slug:"_3-3-scf-structured-control-flow-dialect",normalizedTitle:"3.3. scf (structured control flow) dialect",charIndex:3860},{level:3,title:"3.4. Affine Dialect",slug:"_3-4-affine-dialect",normalizedTitle:"3.4. affine dialect",charIndex:4280},{level:3,title:"3.5. Vector Dialect",slug:"_3-5-vector-dialect",normalizedTitle:"3.5. vector dialect",charIndex:4773},{level:2,title:"4. Example: Linalg Dialect in Action",slug:"_4-example-linalg-dialect-in-action",normalizedTitle:"4. example: linalg dialect in action",charIndex:5208},{level:3,title:"Lowering Steps",slug:"lowering-steps",normalizedTitle:"lowering steps",charIndex:5599},{level:2,title:"Conclusion",slug:"conclusion",normalizedTitle:"conclusion",charIndex:5928}],headersStr:"1. Function of the Linalg Dialect 1. Core Operations 2. Key Features 3. Design Principles 2. Principles of the Linalg Dialect 2.1. Structured Operations 2.2. Progressive Lowering Strategy 3. Cooperation with Other Dialects 3.1. Tensor Dialect 3.2. MemRef Dialect 3.3. SCF (Structured Control Flow) Dialect 3.4. Affine Dialect 3.5. Vector Dialect 4. Example: Linalg Dialect in Action Lowering Steps Conclusion",content:'# MLIR Linalg Dialect\n\n> This is generated by ChatGPT.\n\nThe Linalg dialect in MLIR (Multi-Level Intermediate Representation) is a structured abstraction for expressing operations on tensors, buffers, and memories in a way that facilitates transformations and optimizations.\n\nIt is crucial in MLIR’s progressive lowering strategy, acting as a bridge between high-level tensor algebra and low-level machine-specific operations.\n\n\n# 1. Function of the Linalg Dialect\n\nThe Linalg dialect serves as a powerful intermediate representation for tensor and buffer computations in MLIR. It provides a structured way to express and optimize linear algebra operations while facilitating progressive lowering to lower-level representations.\n\n\n# 1. Core Operations\n\nThe dialect supports several high-level operations:\n\n * Matrix operations (matmul, convolutions)\n * Elementwise computations (add, multiply)\n * Reduction operations (sum, min, max)\n * Generic structured operations\n\n// Example of a generic LINALG operation\nlinalg.generic {\n  indexing_maps = [\n    affine_map<(i,j) -> (i,j)>,   // Input matrix\n    affine_map<(i,j) -> (i,j)>    // Output matrix\n  ],\n  iterator_types = ["parallel", "parallel"]\n} ins(%input : tensor<4x4xf32>) \n  outs(%output : tensor<4x4xf32>) {\n  ^bb0(%in: f32, %out: f32):\n    // Computation body\n    %result = some_computation(%in)\n    linalg.yield %result : f32\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 2. Key Features\n\n# 2.1 Optimization-Friendly Design\n\n * Enables sophisticated transformations:\n   * Tiling and fusion\n   * Vectorization\n   * Distribution across compute units\n * Provides analyzable computation patterns\n * Supports performance optimization strategies\n\n# 2.2 Progressive Lowering Support\n\n * Systematic lowering to loops and vectors\n * Conversion to hardware-specific instructions\n * Flexible targeting of different architectures\n\n# 2.3 Rich Interoperability\n\n * Seamless integration with other MLIR dialects:\n   * Tensor dialect for tensor operations\n   * MemRef dialect for memory operations\n   * Affine dialect for loop transformations\n   * SCF dialect for control flow\n\n\n# 3. Design Principles\n\n# 3.1 Abstraction Level\n\n * Captures high-level semantic intent\n * Preserves optimization opportunities\n * Maintains transformation flexibility\n\n# 3.2 Implementation Characteristics\n\n * Parametric operation support\n * Structured computation representation\n * Transformation-friendly design\n * Multi-level optimization capability\n\n\n# 2. Principles of the Linalg Dialect\n\nThe design principles of the Linalg dialect revolve around structured operations and progressive lowering.\n\n\n# 2.1. Structured Operations\n\nLinalg operations are loop nests over multi-dimensional data structures. They define explicit iteration spaces and access patterns. Example operations include:\n\n * linalg.matmul\n * linalg.conv_2d\n * linalg.reduce\n\n\n# 2.2. Progressive Lowering Strategy\n\nLinalg serves as an intermediate step in the lowering pipeline:\n\n * High-Level IR (TOSA, Tensor, MHLO, etc.) → Lowered to Linalg for structured optimization.\n * Linalg Transformations (tiling, fusion, etc.) → Optimized at the Linalg level.\n * Lowering to Loops, SCF, and Vector Dialect → Further optimized and hardware-aware transformations applied.\n * Lowering to LLVM Dialect and Machine Code → Final code generation via LLVM or other backends.\n\n\n# 3. Cooperation with Other Dialects\n\nLinalg works closely with multiple MLIR dialects:\n\n\n# 3.1. Tensor Dialect\n\nLinalg operates on tensor values. Tensor-level transformations like bufferization convert tensor operations into memref operations. Example:\n\n%A = tensor.from_memref %A_mem : memref<4x4xf32> -> tensor<4x4xf32>\n\n\n1\n\n\n\n# 3.2. MemRef Dialect\n\nWhen moving to a memory-aware representation, Linalg lowers operations from tensor to memref. Example:\n\n%A_mem = memref.alloc() : memref<4x4xf32>\n\n\n1\n\n\n\n# 3.3. SCF (Structured Control Flow) Dialect\n\nLinalg operations can be lowered into explicit loops using SCF. Example: Lowering linalg.matmul to SCF loops:\n\nscf.for %i = 0 to 4 {\n  scf.for %j = 0 to 4 {\n    scf.for %k = 0 to 4 {\n      %prod = arith.mulf %A[%i, %k], %B[%k, %j] : f32\n      %sum = arith.addf %C[%i, %j], %prod : f32\n      memref.store %sum, %C[%i, %j] : memref<4x4xf32>\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 3.4. Affine Dialect\n\nAffine dialect provides advanced loop and memory access transformations.\n\nWhen lowering to hardware-specific memory layouts, Linalg may use Affine transformations for loop optimizations.\n\nExample:\n\naffine.for %i = 0 to 4 {\n  affine.for %j = 0 to 4 {\n    affine.for %k = 0 to 4 {\n      %prod = arith.mulf %A[%i, %k], %B[%k, %j] : f32\n      %sum = arith.addf %C[%i, %j], %prod : f32\n      affine.store %sum, %C[%i, %j] : memref<4x4xf32>\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 3.5. Vector Dialect\n\nLinalg lowering can introduce vectorized operations. Example: linalg.matmul lowering to vector.contract:\n\n%C = vector.contract {indexing_maps = [affine_map<(m, n, k) -> (m, k)>,\n                                      affine_map<(m, n, k) -> (k, n)>,\n                                      affine_map<(m, n, k) -> (m, n)>],\n                      iterator_types = ["parallel", "parallel", "reduction"]}\n\n\n1\n2\n3\n4\n\n\n\n# 4. Example: Linalg Dialect in Action\n\nHere’s an example MLIR program using linalg.matmul:\n\nfunc.func @matmul(%A: tensor<4x4xf32>, %B: tensor<4x4xf32>, %C: tensor<4x4xf32>) -> tensor<4x4xf32> {\n  %result = linalg.matmul ins(%A, %B : tensor<4x4xf32>, tensor<4x4xf32>)\n                         outs(%C : tensor<4x4xf32>) -> tensor<4x4xf32>\n  return %result : tensor<4x4xf32>\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# Lowering Steps\n\n * Linalg Dialect\n   * Uses structured linalg.matmul for clear semantics.\n * Lower to SCF (Loops)\n   * Transforms linalg.matmul into explicit nested loops.\n * Lower to Affine or Vector Dialect\n   * Optimizes for CPU or GPU execution.\n * Lower to LLVM Dialect\n   * Converts into final machine-executable code.\n\n\n# Conclusion\n\nThe Linalg dialect is a structured and optimization-friendly representation in MLIR, bridging high-level tensor computations and low-level execution.\n\nIt works alongside the Tensor, MemRef, SCF, Affine, and Vector dialects to progressively lower operations into efficient machine code.\n\nBy leveraging tiling, fusion, vectorization, and hardware mapping, Linalg plays a critical role in modern compiler optimization.',normalizedContent:'# mlir linalg dialect\n\n> this is generated by chatgpt.\n\nthe linalg dialect in mlir (multi-level intermediate representation) is a structured abstraction for expressing operations on tensors, buffers, and memories in a way that facilitates transformations and optimizations.\n\nit is crucial in mlir’s progressive lowering strategy, acting as a bridge between high-level tensor algebra and low-level machine-specific operations.\n\n\n# 1. function of the linalg dialect\n\nthe linalg dialect serves as a powerful intermediate representation for tensor and buffer computations in mlir. it provides a structured way to express and optimize linear algebra operations while facilitating progressive lowering to lower-level representations.\n\n\n# 1. core operations\n\nthe dialect supports several high-level operations:\n\n * matrix operations (matmul, convolutions)\n * elementwise computations (add, multiply)\n * reduction operations (sum, min, max)\n * generic structured operations\n\n// example of a generic linalg operation\nlinalg.generic {\n  indexing_maps = [\n    affine_map<(i,j) -> (i,j)>,   // input matrix\n    affine_map<(i,j) -> (i,j)>    // output matrix\n  ],\n  iterator_types = ["parallel", "parallel"]\n} ins(%input : tensor<4x4xf32>) \n  outs(%output : tensor<4x4xf32>) {\n  ^bb0(%in: f32, %out: f32):\n    // computation body\n    %result = some_computation(%in)\n    linalg.yield %result : f32\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 2. key features\n\n# 2.1 optimization-friendly design\n\n * enables sophisticated transformations:\n   * tiling and fusion\n   * vectorization\n   * distribution across compute units\n * provides analyzable computation patterns\n * supports performance optimization strategies\n\n# 2.2 progressive lowering support\n\n * systematic lowering to loops and vectors\n * conversion to hardware-specific instructions\n * flexible targeting of different architectures\n\n# 2.3 rich interoperability\n\n * seamless integration with other mlir dialects:\n   * tensor dialect for tensor operations\n   * memref dialect for memory operations\n   * affine dialect for loop transformations\n   * scf dialect for control flow\n\n\n# 3. design principles\n\n# 3.1 abstraction level\n\n * captures high-level semantic intent\n * preserves optimization opportunities\n * maintains transformation flexibility\n\n# 3.2 implementation characteristics\n\n * parametric operation support\n * structured computation representation\n * transformation-friendly design\n * multi-level optimization capability\n\n\n# 2. principles of the linalg dialect\n\nthe design principles of the linalg dialect revolve around structured operations and progressive lowering.\n\n\n# 2.1. structured operations\n\nlinalg operations are loop nests over multi-dimensional data structures. they define explicit iteration spaces and access patterns. example operations include:\n\n * linalg.matmul\n * linalg.conv_2d\n * linalg.reduce\n\n\n# 2.2. progressive lowering strategy\n\nlinalg serves as an intermediate step in the lowering pipeline:\n\n * high-level ir (tosa, tensor, mhlo, etc.) → lowered to linalg for structured optimization.\n * linalg transformations (tiling, fusion, etc.) → optimized at the linalg level.\n * lowering to loops, scf, and vector dialect → further optimized and hardware-aware transformations applied.\n * lowering to llvm dialect and machine code → final code generation via llvm or other backends.\n\n\n# 3. cooperation with other dialects\n\nlinalg works closely with multiple mlir dialects:\n\n\n# 3.1. tensor dialect\n\nlinalg operates on tensor values. tensor-level transformations like bufferization convert tensor operations into memref operations. example:\n\n%a = tensor.from_memref %a_mem : memref<4x4xf32> -> tensor<4x4xf32>\n\n\n1\n\n\n\n# 3.2. memref dialect\n\nwhen moving to a memory-aware representation, linalg lowers operations from tensor to memref. example:\n\n%a_mem = memref.alloc() : memref<4x4xf32>\n\n\n1\n\n\n\n# 3.3. scf (structured control flow) dialect\n\nlinalg operations can be lowered into explicit loops using scf. example: lowering linalg.matmul to scf loops:\n\nscf.for %i = 0 to 4 {\n  scf.for %j = 0 to 4 {\n    scf.for %k = 0 to 4 {\n      %prod = arith.mulf %a[%i, %k], %b[%k, %j] : f32\n      %sum = arith.addf %c[%i, %j], %prod : f32\n      memref.store %sum, %c[%i, %j] : memref<4x4xf32>\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 3.4. affine dialect\n\naffine dialect provides advanced loop and memory access transformations.\n\nwhen lowering to hardware-specific memory layouts, linalg may use affine transformations for loop optimizations.\n\nexample:\n\naffine.for %i = 0 to 4 {\n  affine.for %j = 0 to 4 {\n    affine.for %k = 0 to 4 {\n      %prod = arith.mulf %a[%i, %k], %b[%k, %j] : f32\n      %sum = arith.addf %c[%i, %j], %prod : f32\n      affine.store %sum, %c[%i, %j] : memref<4x4xf32>\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 3.5. vector dialect\n\nlinalg lowering can introduce vectorized operations. example: linalg.matmul lowering to vector.contract:\n\n%c = vector.contract {indexing_maps = [affine_map<(m, n, k) -> (m, k)>,\n                                      affine_map<(m, n, k) -> (k, n)>,\n                                      affine_map<(m, n, k) -> (m, n)>],\n                      iterator_types = ["parallel", "parallel", "reduction"]}\n\n\n1\n2\n3\n4\n\n\n\n# 4. example: linalg dialect in action\n\nhere’s an example mlir program using linalg.matmul:\n\nfunc.func @matmul(%a: tensor<4x4xf32>, %b: tensor<4x4xf32>, %c: tensor<4x4xf32>) -> tensor<4x4xf32> {\n  %result = linalg.matmul ins(%a, %b : tensor<4x4xf32>, tensor<4x4xf32>)\n                         outs(%c : tensor<4x4xf32>) -> tensor<4x4xf32>\n  return %result : tensor<4x4xf32>\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# lowering steps\n\n * linalg dialect\n   * uses structured linalg.matmul for clear semantics.\n * lower to scf (loops)\n   * transforms linalg.matmul into explicit nested loops.\n * lower to affine or vector dialect\n   * optimizes for cpu or gpu execution.\n * lower to llvm dialect\n   * converts into final machine-executable code.\n\n\n# conclusion\n\nthe linalg dialect is a structured and optimization-friendly representation in mlir, bridging high-level tensor computations and low-level execution.\n\nit works alongside the tensor, memref, scf, affine, and vector dialects to progressively lower operations into efficient machine code.\n\nby leveraging tiling, fusion, vectorization, and hardware mapping, linalg plays a critical role in modern compiler optimization.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"MLIR Bufferization Passes",frontmatter:{title:"MLIR Bufferization Passes",date:"2025-03-05T00:00:00.000Z",permalink:"/pages/000021/",tags:[null]},regularPath:"/02.compiler/21.mlir_notes_09.html",relativePath:"02.compiler/21.mlir_notes_09.md",key:"v-5e7857ea",path:"/pages/000021/",headers:[{level:2,title:"1️⃣ What is Bufferization?",slug:"_1️⃣-what-is-bufferization",normalizedTitle:"1️⃣ what is bufferization?",charIndex:65},{level:3,title:"✅ Why is Bufferization Needed?",slug:"✅-why-is-bufferization-needed",normalizedTitle:"✅ why is bufferization needed?",charIndex:417},{level:2,title:"2️⃣ Bufferization Pipeline",slug:"_2️⃣-bufferization-pipeline",normalizedTitle:"2️⃣ bufferization pipeline",charIndex:725},{level:3,title:"🔹 High-Level Tensor Computation (Functional Style)",slug:"🔹-high-level-tensor-computation-functional-style",normalizedTitle:"🔹 high-level tensor computation (functional style)",charIndex:855},{level:3,title:"🔹 Bufferization (Converting Tensor to MemRef)",slug:"🔹-bufferization-converting-tensor-to-memref",normalizedTitle:"🔹 bufferization (converting tensor to memref)",charIndex:1169},{level:3,title:"🔹 Lowering to LLVM (Final Execution)",slug:"🔹-lowering-to-llvm-final-execution",normalizedTitle:"🔹 lowering to llvm (final execution)",charIndex:1617},{level:2,title:"3️⃣ Types of Bufferization",slug:"_3️⃣-types-of-bufferization",normalizedTitle:"3️⃣ types of bufferization",charIndex:1855},{level:3,title:"🔹 1. One-Shot Bufferization",slug:"🔹-1-one-shot-bufferization",normalizedTitle:"🔹 1. one-shot bufferization",charIndex:1929},{level:3,title:"🔹 2. Progressive (Partial) Bufferization",slug:"🔹-2-progressive-partial-bufferization",normalizedTitle:"🔹 2. progressive (partial) bufferization",charIndex:2125},{level:2,title:"4️⃣ Bufferization Analysis: Handling Aliasing & Copies",slug:"_4️⃣-bufferization-analysis-handling-aliasing-copies",normalizedTitle:"4️⃣ bufferization analysis: handling aliasing &amp; copies",charIndex:null},{level:3,title:"🔹 Case 1: No Copy Needed (In-Place Bufferization)",slug:"🔹-case-1-no-copy-needed-in-place-bufferization",normalizedTitle:"🔹 case 1: no copy needed (in-place bufferization)",charIndex:2498},{level:3,title:"🔹 Case 2: Copy Needed (Aliased Data)",slug:"🔹-case-2-copy-needed-aliased-data",normalizedTitle:"🔹 case 2: copy needed (aliased data)",charIndex:3133},{level:2,title:"5️⃣ Bufferization for GPU Execution",slug:"_5️⃣-bufferization-for-gpu-execution",normalizedTitle:"5️⃣ bufferization for gpu execution",charIndex:4176},{level:2,title:"6️⃣ Bufferization Passes in MLIR",slug:"_6️⃣-bufferization-passes-in-mlir",normalizedTitle:"6️⃣ bufferization passes in mlir",charIndex:5001},{level:2,title:"7️⃣ Summary",slug:"_7️⃣-summary",normalizedTitle:"7️⃣ summary",charIndex:5250},{level:2,title:"8️⃣ Conclusion",slug:"_8️⃣-conclusion",normalizedTitle:"8️⃣ conclusion",charIndex:5569}],headersStr:"1️⃣ What is Bufferization? ✅ Why is Bufferization Needed? 2️⃣ Bufferization Pipeline 🔹 High-Level Tensor Computation (Functional Style) 🔹 Bufferization (Converting Tensor to MemRef) 🔹 Lowering to LLVM (Final Execution) 3️⃣ Types of Bufferization 🔹 1. One-Shot Bufferization 🔹 2. Progressive (Partial) Bufferization 4️⃣ Bufferization Analysis: Handling Aliasing & Copies 🔹 Case 1: No Copy Needed (In-Place Bufferization) 🔹 Case 2: Copy Needed (Aliased Data) 5️⃣ Bufferization for GPU Execution 6️⃣ Bufferization Passes in MLIR 7️⃣ Summary 8️⃣ Conclusion",content:"# MLIR Bufferization Passes\n\n> This is generated by ChatGPT.\n\n\n# 1️⃣ What is Bufferization?\n\nBufferization is the process of converting tensor-based computations into memory-based (memref) computations in MLIR. It allows transitioning from a high-level functional-style representation (immutable tensors) to explicit memory management (mutable memrefs), which is required for hardware execution (CPU, GPU, etc.).\n\n\n# ✅ Why is Bufferization Needed?\n\n * Tensors are immutable: Every tensor operation creates a new tensor.\n * MemRefs are mutable: Avoids unnecessary copies, enabling in-place updates.\n * Hardware requires explicit memory management: Low-level backends (LLVM, CUDA) work with pointers, not abstract tensors.\n\n\n# 2️⃣ Bufferization Pipeline\n\nBufferization transforms tensor-based operations into memref-based operations in multiple stages.\n\n\n# 🔹 High-Level Tensor Computation (Functional Style)\n\nOperations use immutable tensor values. Example:\n\n%B = linalg.matmul ins(%A, %A : tensor<4x4xf32>, tensor<4x4xf32>)\n                   outs(%C : tensor<4x4xf32>) -> tensor<4x4xf32>\n\n\n1\n2\n\n\n%B is not modified in-place.\n\nA new tensor is allocated implicitly.\n\n\n# 🔹 Bufferization (Converting Tensor to MemRef)\n\nExplicit memory allocation (memref.alloc). Uses mutability for in-place updates. Example:\n\n%B_mem = memref.alloc() : memref<4x4xf32>\nlinalg.matmul ins(%A_mem, %A_mem : memref<4x4xf32>, memref<4x4xf32>)\n             outs(%B_mem : memref<4x4xf32>)\nmemref.dealloc %B_mem\n\n\n1\n2\n3\n4\n\n\ntensor<4x4xf32> → memref<4x4xf32>. Explicit memory allocation (memref.alloc). Manual deallocation (memref.dealloc).\n\n\n# 🔹 Lowering to LLVM (Final Execution)\n\nMemRef is converted into LLVM pointers (llvm.ptr). Example:\n\n%ptr = llvm.getelementptr %B_mem[%i, %j] : (!llvm.ptr<f32>, i32, i32) -> !llvm.ptr<f32>\n%val = llvm.load %ptr : !llvm.ptr<f32>\n\n\n1\n2\n\n\n\n# 3️⃣ Types of Bufferization\n\nMLIR provides two types of bufferization:\n\n\n# 🔹 1. One-Shot Bufferization\n\nConverts all tensors into memrefs in a single pass. Less flexible but efficient for static memory allocation. Example Pass:\n\nmlir-opt --bufferize input.mlir\n\n\n1\n\n\n\n# 🔹 2. Progressive (Partial) Bufferization\n\nConverts tensors incrementally, allowing analysis-based optimizations. Handles aliasing and inplace updates safely. Example:\n\nmlir-opt --partial-bufferize input.mlir\n\n\n1\n\n\n\n# 4️⃣ Bufferization Analysis: Handling Aliasing & Copies\n\nBufferization must analyze if a tensor operation can be safely replaced with a mutable memref.\n\n\n# 🔹 Case 1: No Copy Needed (In-Place Bufferization)\n\nIf only one operation writes to a tensor, it can be directly mapped to a memref. Example: In-Place Bufferization (No Copy Needed)\n\nfunc.func @inplace_add(%A: tensor<4xf32>) -> tensor<4xf32> {\n  %B = tensor.add %A, %A : tensor<4xf32>\n  return %B\n}\n\n\n1\n2\n3\n4\n\n\n➡ After Bufferization\n\nfunc.func @inplace_add(%A_mem: memref<4xf32>) {\n  %B_mem = %A_mem  // No copy needed\n  scf.for %i = 0 to 4 {\n    %a = memref.load %A_mem[%i] : memref<4xf32>\n    %b = arith.addf %a, %a : f32\n    memref.store %b, %A_mem[%i] : memref<4xf32>\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n📌 No additional alloc() needed!\n\n\n# 🔹 Case 2: Copy Needed (Aliased Data)\n\nIf a tensor is used multiple times, a copy is required to prevent unintended modifications.\n\nExample: Copy Required Due to Aliasing\n\nfunc.func @aliasing_problem(%A: tensor<4xf32>) -> tensor<4xf32> {\n  %B = tensor.add %A, %A : tensor<4xf32>\n  %C = tensor.add %B, %A : tensor<4xf32>\n  return %C\n}\n\n\n1\n2\n3\n4\n5\n\n\n➡ After Bufferization\n\nfunc.func @aliasing_problem(%A_mem: memref<4xf32>) {\n  %B_mem = memref.alloc() : memref<4xf32>\n  scf.for %i = 0 to 4 {\n    %a = memref.load %A_mem[%i] : memref<4xf32>\n    %b = arith.addf %a, %a : f32\n    memref.store %b, %B_mem[%i] : memref<4xf32>\n  }\n\n  %C_mem = memref.alloc() : memref<4xf32>\n  scf.for %i = 0 to 4 {\n    %b = memref.load %B_mem[%i] : memref<4xf32>\n    %a = memref.load %A_mem[%i] : memref<4xf32>\n    %c = arith.addf %b, %a : f32\n    memref.store %c, %C_mem[%i] : memref<4xf32>\n  }\n\n  memref.dealloc %B_mem\n  return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n📌 A copy (memref.alloc()) is required for %B_mem because %A_mem is still used!\n\n\n# 5️⃣ Bufferization for GPU Execution\n\nBufferization is critical for GPU execution because:\n\n * Tensors cannot be used in GPU kernels (they are immutable).\n * MemRefs explicitly allocate GPU memory (#gpu.memory_space). Bufferization ensures correct memory aliasing for parallel execution. Example: Bufferizing a GPU Kernel Before Bufferization (Tensor Representation)\n\ngpu.func @kernel(%A: tensor<1024xf32>) {\n  %B = linalg.matmul ins(%A, %A) -> tensor<1024xf32>\n  return %B\n}\n\n\n1\n2\n3\n4\n\n\nAfter Bufferization (MemRef for GPU Execution)\n\ngpu.func @kernel(%A_mem: memref<1024xf32, #gpu.memory_space<global>>) {\n  %B_mem = memref.alloc() : memref<1024xf32, #gpu.memory_space<global>>\n  linalg.matmul ins(%A_mem, %A_mem) outs(%B_mem)\n  return\n} \n\n\n1\n2\n3\n4\n5\n\n\n📌 MemRefs are placed in #gpu.memory_space to allocate GPU memory.\n\n\n# 6️⃣ Bufferization Passes in MLIR\n\nTo apply bufferization, MLIR provides passes:\n\nPass Name Command One-Shot Bufferization mlir-opt --bufferize Partial Bufferization mlir-opt --partial-bufferize Buffer Deallocation mlir-opt --buffer-deallocation\n\n\n# 7️⃣ Summary\n\nFEATURE             BUFFERIZATION\nPurpose             Converts immutable tensors → mutable memrefs\nAvoids Copies?      ✅ Yes (if no aliasing)\nHandles Aliasing?   ✅ Yes (inserts memref.alloc if needed)\nNeeded for GPU?     ✅ Yes (memrefs required for GPU execution)\nFinal Target?       ✅ LLVM (llvm.ptr)\n\n\n# 8️⃣ Conclusion\n\n✅ Bufferization is essential for transitioning from high-level tensor computations to hardware execution. ✅ It minimizes memory allocations, reducing overhead and improving efficiency. ✅ Works with CPU and GPU lowering (via MemRef dialect).",normalizedContent:"# mlir bufferization passes\n\n> this is generated by chatgpt.\n\n\n# 1️⃣ what is bufferization?\n\nbufferization is the process of converting tensor-based computations into memory-based (memref) computations in mlir. it allows transitioning from a high-level functional-style representation (immutable tensors) to explicit memory management (mutable memrefs), which is required for hardware execution (cpu, gpu, etc.).\n\n\n# ✅ why is bufferization needed?\n\n * tensors are immutable: every tensor operation creates a new tensor.\n * memrefs are mutable: avoids unnecessary copies, enabling in-place updates.\n * hardware requires explicit memory management: low-level backends (llvm, cuda) work with pointers, not abstract tensors.\n\n\n# 2️⃣ bufferization pipeline\n\nbufferization transforms tensor-based operations into memref-based operations in multiple stages.\n\n\n# 🔹 high-level tensor computation (functional style)\n\noperations use immutable tensor values. example:\n\n%b = linalg.matmul ins(%a, %a : tensor<4x4xf32>, tensor<4x4xf32>)\n                   outs(%c : tensor<4x4xf32>) -> tensor<4x4xf32>\n\n\n1\n2\n\n\n%b is not modified in-place.\n\na new tensor is allocated implicitly.\n\n\n# 🔹 bufferization (converting tensor to memref)\n\nexplicit memory allocation (memref.alloc). uses mutability for in-place updates. example:\n\n%b_mem = memref.alloc() : memref<4x4xf32>\nlinalg.matmul ins(%a_mem, %a_mem : memref<4x4xf32>, memref<4x4xf32>)\n             outs(%b_mem : memref<4x4xf32>)\nmemref.dealloc %b_mem\n\n\n1\n2\n3\n4\n\n\ntensor<4x4xf32> → memref<4x4xf32>. explicit memory allocation (memref.alloc). manual deallocation (memref.dealloc).\n\n\n# 🔹 lowering to llvm (final execution)\n\nmemref is converted into llvm pointers (llvm.ptr). example:\n\n%ptr = llvm.getelementptr %b_mem[%i, %j] : (!llvm.ptr<f32>, i32, i32) -> !llvm.ptr<f32>\n%val = llvm.load %ptr : !llvm.ptr<f32>\n\n\n1\n2\n\n\n\n# 3️⃣ types of bufferization\n\nmlir provides two types of bufferization:\n\n\n# 🔹 1. one-shot bufferization\n\nconverts all tensors into memrefs in a single pass. less flexible but efficient for static memory allocation. example pass:\n\nmlir-opt --bufferize input.mlir\n\n\n1\n\n\n\n# 🔹 2. progressive (partial) bufferization\n\nconverts tensors incrementally, allowing analysis-based optimizations. handles aliasing and inplace updates safely. example:\n\nmlir-opt --partial-bufferize input.mlir\n\n\n1\n\n\n\n# 4️⃣ bufferization analysis: handling aliasing & copies\n\nbufferization must analyze if a tensor operation can be safely replaced with a mutable memref.\n\n\n# 🔹 case 1: no copy needed (in-place bufferization)\n\nif only one operation writes to a tensor, it can be directly mapped to a memref. example: in-place bufferization (no copy needed)\n\nfunc.func @inplace_add(%a: tensor<4xf32>) -> tensor<4xf32> {\n  %b = tensor.add %a, %a : tensor<4xf32>\n  return %b\n}\n\n\n1\n2\n3\n4\n\n\n➡ after bufferization\n\nfunc.func @inplace_add(%a_mem: memref<4xf32>) {\n  %b_mem = %a_mem  // no copy needed\n  scf.for %i = 0 to 4 {\n    %a = memref.load %a_mem[%i] : memref<4xf32>\n    %b = arith.addf %a, %a : f32\n    memref.store %b, %a_mem[%i] : memref<4xf32>\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n📌 no additional alloc() needed!\n\n\n# 🔹 case 2: copy needed (aliased data)\n\nif a tensor is used multiple times, a copy is required to prevent unintended modifications.\n\nexample: copy required due to aliasing\n\nfunc.func @aliasing_problem(%a: tensor<4xf32>) -> tensor<4xf32> {\n  %b = tensor.add %a, %a : tensor<4xf32>\n  %c = tensor.add %b, %a : tensor<4xf32>\n  return %c\n}\n\n\n1\n2\n3\n4\n5\n\n\n➡ after bufferization\n\nfunc.func @aliasing_problem(%a_mem: memref<4xf32>) {\n  %b_mem = memref.alloc() : memref<4xf32>\n  scf.for %i = 0 to 4 {\n    %a = memref.load %a_mem[%i] : memref<4xf32>\n    %b = arith.addf %a, %a : f32\n    memref.store %b, %b_mem[%i] : memref<4xf32>\n  }\n\n  %c_mem = memref.alloc() : memref<4xf32>\n  scf.for %i = 0 to 4 {\n    %b = memref.load %b_mem[%i] : memref<4xf32>\n    %a = memref.load %a_mem[%i] : memref<4xf32>\n    %c = arith.addf %b, %a : f32\n    memref.store %c, %c_mem[%i] : memref<4xf32>\n  }\n\n  memref.dealloc %b_mem\n  return\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n📌 a copy (memref.alloc()) is required for %b_mem because %a_mem is still used!\n\n\n# 5️⃣ bufferization for gpu execution\n\nbufferization is critical for gpu execution because:\n\n * tensors cannot be used in gpu kernels (they are immutable).\n * memrefs explicitly allocate gpu memory (#gpu.memory_space). bufferization ensures correct memory aliasing for parallel execution. example: bufferizing a gpu kernel before bufferization (tensor representation)\n\ngpu.func @kernel(%a: tensor<1024xf32>) {\n  %b = linalg.matmul ins(%a, %a) -> tensor<1024xf32>\n  return %b\n}\n\n\n1\n2\n3\n4\n\n\nafter bufferization (memref for gpu execution)\n\ngpu.func @kernel(%a_mem: memref<1024xf32, #gpu.memory_space<global>>) {\n  %b_mem = memref.alloc() : memref<1024xf32, #gpu.memory_space<global>>\n  linalg.matmul ins(%a_mem, %a_mem) outs(%b_mem)\n  return\n} \n\n\n1\n2\n3\n4\n5\n\n\n📌 memrefs are placed in #gpu.memory_space to allocate gpu memory.\n\n\n# 6️⃣ bufferization passes in mlir\n\nto apply bufferization, mlir provides passes:\n\npass name command one-shot bufferization mlir-opt --bufferize partial bufferization mlir-opt --partial-bufferize buffer deallocation mlir-opt --buffer-deallocation\n\n\n# 7️⃣ summary\n\nfeature             bufferization\npurpose             converts immutable tensors → mutable memrefs\navoids copies?      ✅ yes (if no aliasing)\nhandles aliasing?   ✅ yes (inserts memref.alloc if needed)\nneeded for gpu?     ✅ yes (memrefs required for gpu execution)\nfinal target?       ✅ llvm (llvm.ptr)\n\n\n# 8️⃣ conclusion\n\n✅ bufferization is essential for transitioning from high-level tensor computations to hardware execution. ✅ it minimizes memory allocations, reducing overhead and improving efficiency. ✅ works with cpu and gpu lowering (via memref dialect).",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Operand Collector",frontmatter:{title:"Operand Collector",date:"2022-07-18T17:25:49.000Z",permalink:"/pages/cc7034/",tags:[null]},regularPath:"/03.gpu/01.operand_collector.html",relativePath:"03.gpu/01.operand_collector.md",key:"v-6ffd4505",path:"/pages/cc7034/",headersStr:null,content:"Warped-Compression: Enabling Power Efficient GPUs through Register Compression\n\n\n\neach register bank entry can store up to four 32-bit register values. All thread registers in a warp are statically allocated on consecutive banks with the same entry index. Therefore, to read one operand of a warp instruction,a buffering unit called operand collector needs to access up to eight register banks with the same index within each bank. While operands from different banks may be concurrently read, operands that access the same bank lead to bank conflicts.\n\nCORF: Coalescing Operand Register File for GPUs\n\n\n\nFigure 1 shows our baseline register file organization for the Fermi generation of Nvidia GPUs. It has a register file size of 128 KB per SM split across four banks. A bank is made up of 8 sub-banks that are 128 bits wide each. All 32 registers belonging to the 32 threads in the same warp are statically allocated to consecutive sub-banks (in a single bank) with the same entry index. Thus, a full register for all the threads within a warp can be striped using one entry of one bank, allowing it to be operated on in a single cycle. Each bank can store up to 256 warp-registers.\n\nSummary They all assume that 128bit entry in each bank will supply 4 register to 4 thread in 32 thread per swap.\n\nWarped-Compression assumes that 8 bank will supply 32 registers for a warp. CORF assumes that 8 subbank in each bank will supply 32 registers for a warp.",normalizedContent:"warped-compression: enabling power efficient gpus through register compression\n\n\n\neach register bank entry can store up to four 32-bit register values. all thread registers in a warp are statically allocated on consecutive banks with the same entry index. therefore, to read one operand of a warp instruction,a buffering unit called operand collector needs to access up to eight register banks with the same index within each bank. while operands from different banks may be concurrently read, operands that access the same bank lead to bank conflicts.\n\ncorf: coalescing operand register file for gpus\n\n\n\nfigure 1 shows our baseline register file organization for the fermi generation of nvidia gpus. it has a register file size of 128 kb per sm split across four banks. a bank is made up of 8 sub-banks that are 128 bits wide each. all 32 registers belonging to the 32 threads in the same warp are statically allocated to consecutive sub-banks (in a single bank) with the same entry index. thus, a full register for all the threads within a warp can be striped using one entry of one bank, allowing it to be operated on in a single cycle. each bank can store up to 256 warp-registers.\n\nsummary they all assume that 128bit entry in each bank will supply 4 register to 4 thread in 32 thread per swap.\n\nwarped-compression assumes that 8 bank will supply 32 registers for a warp. corf assumes that 8 subbank in each bank will supply 32 registers for a warp.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU WARP Scheduler",frontmatter:{title:"GPU WARP Scheduler",date:"2023-09-20T00:00:00.000Z",permalink:"/pages/2476ae/",tags:[null]},regularPath:"/03.gpu/02.warp_execution.html",relativePath:"03.gpu/02.warp_execution.md",key:"v-b25c491a",path:"/pages/2476ae/",headers:[{level:3,title:"1. Thread Block Compaction for Efficient SIMT Control Flow",slug:"_1-thread-block-compaction-for-efficient-simt-control-flow",normalizedTitle:"1. thread block compaction for efficient simt control flow",charIndex:1}],headersStr:"1. Thread Block Compaction for Efficient SIMT Control Flow",content:" 1. Thread Block Compaction for Efficient SIMT Control Flow\n\n----------------------------------------\n\n\n# 1. Thread Block Compaction for Efficient SIMT Control Flow",normalizedContent:" 1. thread block compaction for efficient simt control flow\n\n----------------------------------------\n\n\n# 1. thread block compaction for efficient simt control flow",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Precision Exception",frontmatter:{title:"Precision Exception",date:"2023-11-11T00:00:00.000Z",permalink:"/pages/14769f/",tags:[null]},regularPath:"/03.gpu/03.Precise%20Exception.html",relativePath:"03.gpu/03.Precise Exception.md",key:"v-f96426b0",path:"/pages/14769f/",headers:[{level:3,title:"1. Supporting Virtual Memory in GPGPU without Supporting Precise Exception [2012]",slug:"_1-supporting-virtual-memory-in-gpgpu-without-supporting-precise-exception-2012",normalizedTitle:"1. supporting virtual memory in gpgpu without supporting precise exception [2012]",charIndex:489},{level:3,title:"6. Efficient Exception Handling Support for GPUs [2017]",slug:"_6-efficient-exception-handling-support-for-gpus-2017",normalizedTitle:"6. efficient exception handling support for gpus [2017]",charIndex:2960}],headersStr:"1. Supporting Virtual Memory in GPGPU without Supporting Precise Exception [2012] 6. Efficient Exception Handling Support for GPUs [2017]",content:" 1. Supporting Virtual Memory in GPGPU without Supporting Precise Exception 2012\n 2. Idempotent Processor Architecture 2011\n 3. iGPU: Exception Support and Speculative Execution on GPUs 2012\n 4. Implementing Virtual Memory in a Vector Processor with Software Restart Markers 2006 Not Read\n 5. Imprecise Store Exceptions 2023 ISCA\n 6. Efficient Exception Handling Support for GPUs 2017\n 7. Simple Out of Order Core for GPGPUs\n 8. Other Papers.\n\n----------------------------------------\n\n\n# 1. Supporting Virtual Memory in GPGPU without Supporting Precise Exception [2012]\n\n👍\n\nIntroduction: GPU is designed for grahics. Supporting precise exceptions is not needed at all and it is extremely expensive due to the high number of registers. Other Designs:\n\n 1. Software restart Remarker Implementing virtual memory in a vector processor with software restart markers.[4] 2006 Reducing Exception Management Overhead with Software Restart Markers 2008\n 2. Idempotent Idempotent processor architecture [2] 2011 igpu: Exception support and speculative execution on gpus. [3] 2012\n\na) set start_maker set start_marker indicates a place where a program can be restarted after a page fault exception handler is serviced.\n\nb) LD.pfchk An LD.pfchk instruction sets pfbit, when it generates a page fault. The pfbit registers behave like predicate registers in IA-64. Instructions that can potentially change program’s states are predicated with pfbit.\n\nc) sw_call sw_call is composed of barrier and call instructions. When a processor fetches an sw_call instruction, it enforces an execution barrier.\n\nInstructions after sw_call can be fetched/renamed, but none of the instructions will be executed. call instructions invoke page fault handler. Implementing this execution barrier is very easy, but it reduces the benefit of a fully out-of-order scheduling processor.\n\nLD.pfchk will set pfbits. Instructions that can potentially change program's state are predicated with pfbit. Similar to idempotent processors, instructions that can be safely reexecuted without changing the program’s results do not need to be predicated. If all instructions are predicated, those instructions cannot be executed until the load instruction is completed, thereby degrading performance significantly.\n\n 1. Not all load/store instruction will be set as LD.pfck. Compiler's job to distinguish Static, Malloc, Large Arrays, Stack Operations, Pointers, and so on\n 2. Only those instructions that can safely reexecuted can be predicated.\n\n/* original C-code */\nfor (int ii=0; ii<N; ii++)\na[ii] = b[ii]*2;\n/* new code */\nfor (int ii=0; ii<N; ii++) {\nif (!(ii%kk)) {\n// kk = page size%(size of(a[0]))\npfchk(&(a[0])+ii*kk));\npfchk(&(b[0])+ii*kk));\n}\na[ii] = b[ii]*2;\n}\nvoid pfchk(int addr) {\n/* use intrinsics to insert assembly code */\nset start_marker;\nLD.pfchk(addr);\n(pfbit) sw_call(start_marker);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n----------------------------------------\n\n\n# 6. Efficient Exception Handling Support for GPUs [2017]\n\n👍 👍 👍 👍\n\nThis paper summerize [1] [2] [3] [4] and discuss why altough GPU solves the problem of dependency, it still meets the problem of RAW Hazard on replay\n\n\n\nIn short, since R4 has been read by C, D can issue and might overwrite R4 before C is done. Thus if we resume from C, C might read the value of new R4, which means hazard.\n\nIt propose three method to solve this:\n\n 1. Warp Disable\n\n\n\n 2. Replay Queue\n\n\n\n 3. Operand Log\n\n\n\nThis is a good paper, that deserves reading throughly. 👏",normalizedContent:" 1. supporting virtual memory in gpgpu without supporting precise exception 2012\n 2. idempotent processor architecture 2011\n 3. igpu: exception support and speculative execution on gpus 2012\n 4. implementing virtual memory in a vector processor with software restart markers 2006 not read\n 5. imprecise store exceptions 2023 isca\n 6. efficient exception handling support for gpus 2017\n 7. simple out of order core for gpgpus\n 8. other papers.\n\n----------------------------------------\n\n\n# 1. supporting virtual memory in gpgpu without supporting precise exception [2012]\n\n👍\n\nintroduction: gpu is designed for grahics. supporting precise exceptions is not needed at all and it is extremely expensive due to the high number of registers. other designs:\n\n 1. software restart remarker implementing virtual memory in a vector processor with software restart markers.[4] 2006 reducing exception management overhead with software restart markers 2008\n 2. idempotent idempotent processor architecture [2] 2011 igpu: exception support and speculative execution on gpus. [3] 2012\n\na) set start_maker set start_marker indicates a place where a program can be restarted after a page fault exception handler is serviced.\n\nb) ld.pfchk an ld.pfchk instruction sets pfbit, when it generates a page fault. the pfbit registers behave like predicate registers in ia-64. instructions that can potentially change program’s states are predicated with pfbit.\n\nc) sw_call sw_call is composed of barrier and call instructions. when a processor fetches an sw_call instruction, it enforces an execution barrier.\n\ninstructions after sw_call can be fetched/renamed, but none of the instructions will be executed. call instructions invoke page fault handler. implementing this execution barrier is very easy, but it reduces the benefit of a fully out-of-order scheduling processor.\n\nld.pfchk will set pfbits. instructions that can potentially change program's state are predicated with pfbit. similar to idempotent processors, instructions that can be safely reexecuted without changing the program’s results do not need to be predicated. if all instructions are predicated, those instructions cannot be executed until the load instruction is completed, thereby degrading performance significantly.\n\n 1. not all load/store instruction will be set as ld.pfck. compiler's job to distinguish static, malloc, large arrays, stack operations, pointers, and so on\n 2. only those instructions that can safely reexecuted can be predicated.\n\n/* original c-code */\nfor (int ii=0; ii<n; ii++)\na[ii] = b[ii]*2;\n/* new code */\nfor (int ii=0; ii<n; ii++) {\nif (!(ii%kk)) {\n// kk = page size%(size of(a[0]))\npfchk(&(a[0])+ii*kk));\npfchk(&(b[0])+ii*kk));\n}\na[ii] = b[ii]*2;\n}\nvoid pfchk(int addr) {\n/* use intrinsics to insert assembly code */\nset start_marker;\nld.pfchk(addr);\n(pfbit) sw_call(start_marker);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n----------------------------------------\n\n\n# 6. efficient exception handling support for gpus [2017]\n\n👍 👍 👍 👍\n\nthis paper summerize [1] [2] [3] [4] and discuss why altough gpu solves the problem of dependency, it still meets the problem of raw hazard on replay\n\n\n\nin short, since r4 has been read by c, d can issue and might overwrite r4 before c is done. thus if we resume from c, c might read the value of new r4, which means hazard.\n\nit propose three method to solve this:\n\n 1. warp disable\n\n\n\n 2. replay queue\n\n\n\n 3. operand log\n\n\n\nthis is a good paper, that deserves reading throughly. 👏",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"TensorCore Paper List",frontmatter:{title:"TensorCore Paper List",date:"2023-11-17T00:00:00.000Z",permalink:"/pages/44871e/",tags:[null]},regularPath:"/03.gpu/05.TensorCore.html",relativePath:"03.gpu/05.TensorCore.md",key:"v-2861ec8b",path:"/pages/44871e/",headers:[{level:3,title:"1. Modeling Deep Learning Accelerator Enabled GPUs",slug:"_1-modeling-deep-learning-accelerator-enabled-gpus",normalizedTitle:"1. modeling deep learning accelerator enabled gpus",charIndex:1}],headersStr:"1. Modeling Deep Learning Accelerator Enabled GPUs",content:" 1. Modeling Deep Learning Accelerator Enabled GPUs 2019\n 2. Dissecting Tensor Cores via Microbenchmarks: Latency, Throughput and Numeric Behaviors 2023\n 3. [84 Year:2020 Not Read Yet] Demystifying Tensor Cores to Optimize Half-Precision Matrix Multiply 2020\n 4. CS 380 - GPU and GPGPU ProgrammingLecture 26: Programming Tensor Cores\n 5. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmark 2018\n\n----------------------------------------\n\n\n# 1. Modeling Deep Learning Accelerator Enabled GPUs",normalizedContent:" 1. modeling deep learning accelerator enabled gpus 2019\n 2. dissecting tensor cores via microbenchmarks: latency, throughput and numeric behaviors 2023\n 3. [84 year:2020 not read yet] demystifying tensor cores to optimize half-precision matrix multiply 2020\n 4. cs 380 - gpu and gpgpu programminglecture 26: programming tensor cores\n 5. dissecting the nvidia volta gpu architecture via microbenchmark 2018\n\n----------------------------------------\n\n\n# 1. modeling deep learning accelerator enabled gpus",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Memory Behaviour Paper List",frontmatter:{title:"Memory Behaviour Paper List",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/45871e/",tags:[null]},regularPath:"/03.gpu/06.MemoryBehaviour.html",relativePath:"03.gpu/06.MemoryBehaviour.md",key:"v-04292a45",path:"/pages/45871e/",headers:[{level:3,title:"1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior",slug:"_1-a-comparative-analysis-of-microarchitecture-effects-on-cpu-and-gpu-memory-system-behavior",normalizedTitle:"1. a comparative analysis of microarchitecture effects on cpu and gpu memory system behavior",charIndex:1}],headersStr:"1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior",content:" 1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior 2014\n 2. \n\n----------------------------------------\n\n\n# 1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior\n\nCPU cores must extract very wide ILP in order to expose MLP to the memory hierarchy, and this MLP can be limited to lower levels of the memory hierarchy due to L1 cache locality. On the other hand, GPU cores and caches aim to mitigate MLP limitations, allowing the programmer to focus their efforts on leveraging the available MLP.\n\n\n\nRodinia is categorized into the above features.\n\n * Pipleline\n * Iterative\n * Head Reads(I/c)\n\nA fairly common factor in compute and memory op count differences between system configurations is due to register handling. For x86 CPU applications, the small architected register set (16) can cause register spilling to the stack and recomputation of previously computed values.\n\nIn contrast, GPU cores have some flexibility in register use due to their core multithreading. By running fewer GPU threads per core and late binding register specifiers to physical registers, there is more flexibility for each thread to access more registers, which can avoid spilling and recomputation.\n\nMemory Access Pattern\n\nCPU cores use a small set of deep per-thread instruction windows, and high-frequency pipelines and caches to expose parallel memory accesses. In contrast, GPU cores expose parallel memory accesses by executing 100s–1000s more threads at lower frequencies, and threads are grouped for smaller perthread instruction windows and memory request coalescing.\n\nwhile CPU cores rely heavily on L1 caches to capture locality, GPU cores capture most locality with coalescing and lessen the L1 cache responsibilities by providing scratch memory.\n\nBeyond the L1 caches, the memory systems tend to capture very similar locality. Further, we see that different core threading and cache filtering result in extreme differences in instantaneous memory access rates; CPU caches tend to filter accesses down to regular intervals, while GPU cores tend to issue bursts of accesses.\n\n\n\nScratch memory: GPU cores provide scratch memory, which can function as local storage for groups of threads to expand the space of local storage with register-like accessibility. In CUDA benchmarks that use the GPU scratch memory, kernels are typically organized into three stages: (1) read a small portion of data from global memory into the scratch memory, (2) compute on the data in scratch memory, and (3) write results back to global memory.\n\nSince GPU request coalescing behaves similarly to CPU single-instruction, multiple-data (SIMD) vectorization, vectorization reduces the total number of memory accesses by 1.32–1.69× (1.44× geometric mean), and that most of the eliminated accesses are to heap data.\n\nOverall, GPU scratch memory and request coalescing reduce the number of global memory accesses by 18–100× compared to CPU applications (27× in the geometric mean).\n\nCompared to CPU cores, this reduction alleviates pressure on caches, which in turn allows GPU cores to operate at lower frequencies while still serving data to threads at rates comparable to or greater than CPU cores.\n\nSpatial Locality\n\nCPU threads have extremely high spatial locality, typically striding through all elements in a heap cache line in subsequent algorithm loop iterations. These access patterns, which also include accesses to stack/local memory that is persistent over many loop iterations, result in high L1 cache hit rates that even exceed those expected by simple strided read memory access.\n\nFor GPU, small number of remaining spatially local accesses is likely due to separate thread groups accessing the same data rather than thread groups being unable to fully coalesce accesses.\n\nTemporal Locality\n\nFor CPU, this leaves the L2 caches mostly responsible for capturing temporally local accesses to data shared across cores rather than temporally or spatially local accesses to data previously evicted from the L1 caches due to limited capacity.\n\nFor GPU, This indicates that instead of competing for L1 capacity, GPU threads from separate cores are generating most of the temporally local accesses to single cache lines, similar to the CPU L2.\n\nBased on the above observations, we find that CPU and GPU L1 caches have very different importance, though their filtering roles are similar.\n\nIn the aggregate for data-parallel workloads, CPU L1 caches have many responsibilities; they must be designed to capture both the spatial locality for heap data accesses and the temporal locality of stack accesses. Fortunately for data-parallel workloads, these responsibilities rarely conflict given sufficient L1 capacity, so CPU L1s are quite effective and important for capturing locality.\n\nFor GPU applications, register and scratch memory can shift local variable accesses away from the caches, which eliminates the L1 responsibility for capturing temporally local stack requests. Further, GPU coalescing greatly reduces the importance of spatial locality across separate heap accesses, so the L1 caches are mostly responsible for capturing the small number of temporally local accesses from separate GPU threads on the same core(this is different from cpu), diminishing the overall responsibility of the GPU L1s compared to CPU L1s.\n\nBandwidth Demands\n\n\n\nThe key takeaway here is that GPU burst access behavior results from the way that GPUs group and launch threads. Specifically, at the beginning of a kernel, all capable thread block contexts begin executing at roughly the same time, so this can cause very large bursts of independent accesses.\n\nFollowing this initial burst, smaller but still significant access bursts occur each time a new thread block begins executing or when thread groups pass synchronization events.\n\nBy contrast, CPU cache access filtering tends to modulate the core’s ability to issue nearly as many parallel accesses to off-chip memory.\n\nLatency Sensitivity and Bandwidth Sensitivity\n\n\n\nThis Figue show CPU is sensitive to latency, but gpu to bandwidth.\n\nInteresting thoughts*\n\n-cache shared by CPU and GPU -interconnect and off-chip memory scheduling\n\nThese should take different characteristic of CPU and GPU into consideration.",normalizedContent:" 1. a comparative analysis of microarchitecture effects on cpu and gpu memory system behavior 2014\n 2. \n\n----------------------------------------\n\n\n# 1. a comparative analysis of microarchitecture effects on cpu and gpu memory system behavior\n\ncpu cores must extract very wide ilp in order to expose mlp to the memory hierarchy, and this mlp can be limited to lower levels of the memory hierarchy due to l1 cache locality. on the other hand, gpu cores and caches aim to mitigate mlp limitations, allowing the programmer to focus their efforts on leveraging the available mlp.\n\n\n\nrodinia is categorized into the above features.\n\n * pipleline\n * iterative\n * head reads(i/c)\n\na fairly common factor in compute and memory op count differences between system configurations is due to register handling. for x86 cpu applications, the small architected register set (16) can cause register spilling to the stack and recomputation of previously computed values.\n\nin contrast, gpu cores have some flexibility in register use due to their core multithreading. by running fewer gpu threads per core and late binding register specifiers to physical registers, there is more flexibility for each thread to access more registers, which can avoid spilling and recomputation.\n\nmemory access pattern\n\ncpu cores use a small set of deep per-thread instruction windows, and high-frequency pipelines and caches to expose parallel memory accesses. in contrast, gpu cores expose parallel memory accesses by executing 100s–1000s more threads at lower frequencies, and threads are grouped for smaller perthread instruction windows and memory request coalescing.\n\nwhile cpu cores rely heavily on l1 caches to capture locality, gpu cores capture most locality with coalescing and lessen the l1 cache responsibilities by providing scratch memory.\n\nbeyond the l1 caches, the memory systems tend to capture very similar locality. further, we see that different core threading and cache filtering result in extreme differences in instantaneous memory access rates; cpu caches tend to filter accesses down to regular intervals, while gpu cores tend to issue bursts of accesses.\n\n\n\nscratch memory: gpu cores provide scratch memory, which can function as local storage for groups of threads to expand the space of local storage with register-like accessibility. in cuda benchmarks that use the gpu scratch memory, kernels are typically organized into three stages: (1) read a small portion of data from global memory into the scratch memory, (2) compute on the data in scratch memory, and (3) write results back to global memory.\n\nsince gpu request coalescing behaves similarly to cpu single-instruction, multiple-data (simd) vectorization, vectorization reduces the total number of memory accesses by 1.32–1.69× (1.44× geometric mean), and that most of the eliminated accesses are to heap data.\n\noverall, gpu scratch memory and request coalescing reduce the number of global memory accesses by 18–100× compared to cpu applications (27× in the geometric mean).\n\ncompared to cpu cores, this reduction alleviates pressure on caches, which in turn allows gpu cores to operate at lower frequencies while still serving data to threads at rates comparable to or greater than cpu cores.\n\nspatial locality\n\ncpu threads have extremely high spatial locality, typically striding through all elements in a heap cache line in subsequent algorithm loop iterations. these access patterns, which also include accesses to stack/local memory that is persistent over many loop iterations, result in high l1 cache hit rates that even exceed those expected by simple strided read memory access.\n\nfor gpu, small number of remaining spatially local accesses is likely due to separate thread groups accessing the same data rather than thread groups being unable to fully coalesce accesses.\n\ntemporal locality\n\nfor cpu, this leaves the l2 caches mostly responsible for capturing temporally local accesses to data shared across cores rather than temporally or spatially local accesses to data previously evicted from the l1 caches due to limited capacity.\n\nfor gpu, this indicates that instead of competing for l1 capacity, gpu threads from separate cores are generating most of the temporally local accesses to single cache lines, similar to the cpu l2.\n\nbased on the above observations, we find that cpu and gpu l1 caches have very different importance, though their filtering roles are similar.\n\nin the aggregate for data-parallel workloads, cpu l1 caches have many responsibilities; they must be designed to capture both the spatial locality for heap data accesses and the temporal locality of stack accesses. fortunately for data-parallel workloads, these responsibilities rarely conflict given sufficient l1 capacity, so cpu l1s are quite effective and important for capturing locality.\n\nfor gpu applications, register and scratch memory can shift local variable accesses away from the caches, which eliminates the l1 responsibility for capturing temporally local stack requests. further, gpu coalescing greatly reduces the importance of spatial locality across separate heap accesses, so the l1 caches are mostly responsible for capturing the small number of temporally local accesses from separate gpu threads on the same core(this is different from cpu), diminishing the overall responsibility of the gpu l1s compared to cpu l1s.\n\nbandwidth demands\n\n\n\nthe key takeaway here is that gpu burst access behavior results from the way that gpus group and launch threads. specifically, at the beginning of a kernel, all capable thread block contexts begin executing at roughly the same time, so this can cause very large bursts of independent accesses.\n\nfollowing this initial burst, smaller but still significant access bursts occur each time a new thread block begins executing or when thread groups pass synchronization events.\n\nby contrast, cpu cache access filtering tends to modulate the core’s ability to issue nearly as many parallel accesses to off-chip memory.\n\nlatency sensitivity and bandwidth sensitivity\n\n\n\nthis figue show cpu is sensitive to latency, but gpu to bandwidth.\n\ninteresting thoughts*\n\n-cache shared by cpu and gpu -interconnect and off-chip memory scheduling\n\nthese should take different characteristic of cpu and gpu into consideration.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Unified Memory Paper List",frontmatter:{title:"Unified Memory Paper List",date:"2023-11-11T00:00:00.000Z",permalink:"/pages/44771e/",tags:[null]},regularPath:"/03.gpu/04.Unified_Memory.html",relativePath:"03.gpu/04.Unified_Memory.md",key:"v-5849f946",path:"/pages/44771e/",headers:[{level:3,title:"1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory",slug:"_1-holistic-performance-analysis-and-optimization-of-unified-virtual-holistic-performance-analysis-and-optimization-of-unified-virtual-memory",normalizedTitle:"1. holistic performance analysis and optimization of unified virtual holistic performance analysis and optimization of unified virtual memory",charIndex:3797},{level:3,title:"3. Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions",slug:"_3-oversubscribing-gpu-unified-virtual-memory-implications-and-suggestions",normalizedTitle:"3. oversubscribing gpu unified virtual memory: implications and suggestions",charIndex:4084},{level:3,title:"4. Performance Evaluation of Advanced Features in CUDA Unified Memory",slug:"_4-performance-evaluation-of-advanced-features-in-cuda-unified-memory",normalizedTitle:"4. performance evaluation of advanced features in cuda unified memory",charIndex:6364},{level:3,title:"5. Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory",slug:"_5-interplay-between-hardware-prefetcher-and-page-eviction-policy-in-cpu-gpu-unified-virtual-memory",normalizedTitle:"5. interplay between hardware prefetcher and page eviction policy in cpu-gpu unified virtual memory",charIndex:9887},{level:3,title:"7. Batch-Aware Unified Memory Management in GPUs for Irregular Workloads 2020",slug:"_7-batch-aware-unified-memory-management-in-gpus-for-irregular-workloads-2020",normalizedTitle:"7. batch-aware unified memory management in gpus for irregular workloads 2020",charIndex:11620},{level:3,title:"10. Machine Learning Guided Optimal Use of GPU Unified Memory 2019",slug:"_10-machine-learning-guided-optimal-use-of-gpu-unified-memory-2019",normalizedTitle:"10. machine learning guided optimal use of gpu unified memory 2019",charIndex:18629},{level:3,title:"14. Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory [2024] 👍👍👍👍",slug:"_14-fine-grain-quantitative-analysis-of-demand-paging-in-unified-virtual-memory-2024",normalizedTitle:"14. fine-grain quantitative analysis of demand paging in unified virtual memory [2024] 👍👍👍👍",charIndex:21192}],headersStr:"1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory 3. Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions 4. Performance Evaluation of Advanced Features in CUDA Unified Memory 5. Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory 7. Batch-Aware Unified Memory Management in GPUs for Irregular Workloads 2020 10. Machine Learning Guided Optimal Use of GPU Unified Memory 2019 14. Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory [2024] 👍👍👍👍",content:' 1.  Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory\n 2.  In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing\n 3.  Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions\n 4.  Performance Evaluation of Advanced Features in CUDA Unified Memory\n 5.  Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory\n 6.  Unified Memory: GPGPU-Sim/UVM Smart Integration\n 7.  Batch-Aware Unified Memory Management in GPUs for Irregular Workloads\n 8.  An Intelligent Framework for Oversubscription Management in CPU-GPU Unified Memory\n 9.  Architectural Support for Address Translation on GPUs Designing Memory Management Units for CPU/GPUs with Unified Address Spaces\n 10. Machine Learning Guided Optimal Use of GPU Unified Memory\n 11. Towards High Performance Paged Memory for GPUs\n 12. [Virtualization] Virtual Thread: Maximizing Thread-Level Parallelism beyond GPU Scheduling Limit.\n 13. [Virtualization] A Survey of GPU Multitasking Methods Supported by Hardware Architecture\n 14. Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory\n\nPlan to read\n\n 1. Early-Adaptor: An Adaptive Framework for Proactive UVM Memory Management\n 2. Liberator: A Data Reuse Framework for Out-of-Memory Graph Computing on GPUs\n 3. [HPCA] Enabling Large Dynamic Neural Network Training with Learning-based Memory Management\n 4. GPUswap: Enabling Oversubscription of GPU Memory through Transparent Swapping\n\n----------------------------------------\n\nUnified Memory History copied from Evolution of Nvidia GPU from microarchitectures Pascal to Ampere\n\nCUDA 4 introduced UVA (Unified Virtual Addressing) to provide a single virtual memory address space for both CPU and GPU memory and enable pointers to be accessed from GPU code no matter where in the system they reside. UVA enables Zero-Copy memory, a pinned CPU memory accessible by GPU code directly, over PCIe, without the need for memory copy. This provides some of the convince of Unified Memory, but at the cost of worse performance, because GPU always accesses it with PCIe’s low bandwidth and high latency.[1]\n\nLater, CUDA 6 introduced Unified Memory, which creates a pool of managed memory that programs running on the CPU and GPU can access without explicit data movement. However, only when CPU and GPU processes are not running together because of the limitation of the Kepler and Maxwell GPU microarchitecture. Also, the Unified Memory address space was limited to the size of the GPU memory.[1, 3]\n\nCUDA 8 and Pascal microarchitectures improve Unified Memory functionality by adding 49-bit virtual addressing and page faulting capability. The larger 49-bit virtual addresses are sufficient to enable GPUs to access the entire system memory plus the memory of all GPUs in the system. Because of the memory page faulting functionality, the CUDA system software does not need to synchronize all managed memory allocations to the GPU before each kernel lunch. Instead, when a thread running on GPU faults on non-resident memory access(demanding page), it stalls until the page can be migrated and the page table updated. Alternatively, the page may be mapped for remote access over PCIe or NVLink interconnects.[1, 3, 6]\n\nThese new features of Unified Memory enable oversubscription of memory, which means that application running on a GPU can use data sets larger than ten their device memory.[1] While the Unified Memory model makes GPU programming more convenient, it comes at a cost; handling page faults and page migrations can be expensive. CUDA 8 addresses this issue with features like prefetch and memory advice.\n\n----------------------------------------\n\n\n# 1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory\n\nSame author with In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing\n\n----------------------------------------\n\n\n# 3. Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions\n\nUVM supports memory oversubscription, giving GPU programs the ability to use a larger amount of memory than the physical memory, without worrying about the problem of memory shortage.\n\nAdvanced optimization techniques, mainly prefetching and memory usage hints [1], can be used to fine-tune the performance of UVM applications, mitigating the overheads caused by UVM.\n\n\n\n2）Prefetching and Hints Prefetching and UVM hints are the major approaches provided by CUDA, with the hope that page faults and memory thrashing could be prevented by fine-tuning the behavior of UVM at runtime.\n\nBy calling cudaMemPrefetchAsync (PF), a memory block could be prefetched to GPU. UVM hints provide informed decisions on page handling by indicating the access patterns of data.\n\nChanging UVM hints is done by invoking cudaMemAdvise with one of the following policies：\n\n• cudaMemAdviseSetAccessedBy (AB) implies that the device keeps a direct mapping in its page table. When the data is migrated, the mapping is re-established.\n• cudaMemAdviseSetPreferredLocation (PL) pins the data and prevents the page to be migrated, which is useful when the page is mainly accessed on one side.\n• cudaMemAdviseSetReadMostly (RM) indicates the data region is read-intensive. It creates a read-only copy of the page on the faulting side, allowing on current access on both sides.\n\n\nOnly one policy (AB, PL, or RM) could be specified for each memory block, but each policy can be used along with prefetching.\n\nSuggestions: To ensure performance under all oversubscription conditions, programmer needs to choose the UVM hints dynamically based on the application’s memory usage and available GPU memory. As a prerequisite, the size of the FALL pages needs to be estimated or measured by experiment. Before kernel launch, the program should first check the size of available GPU memory (e.g. via the cudaMemGetInfo API). If no oversubscription will happen, or the available memory is larger than the size of FALL pages, the programmer could set hints based on the conclusions provided by related researches [24]. Otherwise, based on our findings, applying the hint AB is a preferable choice.\n\n----------------------------------------\n\n\n# 4. Performance Evaluation of Advanced Features in CUDA Unified Memory\n\nCUDA has introduced new features for optimizing the data migration on UM, i.e., memory advises and prefetch. Instead of solely relying on page faults, the memory advises feature allows the programmer to provide data access pattern for each memory object so that the runtime can optimize migration decisions. The prefetch proactively triggers asynchronous data migration to GPU before the data is accessed, which reduces page faults and, consequently, the overhead in handling page faults.\n\n-Using memory advises improves application performance in oversubscription execution on the Intel platform and in-memory executions on the IBM platform.\n\n-UM prefetch provides a significant performance improvement on the Intel-Volta/Pascal-PCI-E based systems while it does not show a performance improvement on the Power9-Volta-NVLink based system\n\nUM was first introduced in CUDA 6.0 [21]. Only until the recent Nvidia Pascal microarchitecture that has hardware support for page faults.\n\n\n\n• cudaMemAdviseSetAccessedBy establishes a direct mapping of data to a specified device. Figure 2c illustrates an example of a physical page on GPU being remotely access from the host. When cudaMemAdviseSetPreferredLocation is applied, CUDA runtime tries to build a direct mapping to the page to avoid data migration so that the destination can access data remotely. Differently from cudaMemAdviseSetPreferredLocation, this cudaMemAdviseSetAccessedBy does not try to pin pages on a specific device; instead, its main effect is to establish mapping on the remote device. This advice takes effect on the creation of the memory pages. The mapping will be re-established after the pages are migrated.\n\n• cudaMemAdviseSetPreferredLocation sets the preferred physical location of pages. This advice pins a page and prevents it from migrating to other memories. Figure 2b illustrates a page preferred on the host side, and GPU uses remote mapping to access the page. This advice established a direct (remote) mapping to the memory page. When accessing the page remotely, data is fetched through the remote memory instead of generating a page fault. If the underlying hardware does not support the remote mapping, the page will be migrated as in the standard UM. cudaMemAdviseSetPreferredLocation is useful for applications with little data sharing between CPU and GPU, i.e., part of the application is executed completely on the GPU, and the rest of the application executes on the host. Data that is being used mostly by the GPU can be pinned to the GPU with the advice, avoiding memory thrashing.\n\n• cudaMemAdviseSetReadMostly implies a read-intensive data region. In the basic UM, accessing a page on a remote side triggers page migration. However, with cudaMemAdviseSetReadMostly, a read-only duplicate of the page will be created on the faulting side, which prevents page faults and data migration in the future. Figure 2a illustrates an example, where the second access (step 5) has no page fault and is local access. This mechanism, however, results in a high overhead if there is any update to this memory region because all copies of the corresponding page will be invalidated to preserve consistency between different copies. Thus, this advice is often used in read-only data structures, such as lookup tables and application parameters.\n\nIn general, we found both memory advises and prefetch to be simple and effective.\n\n----------------------------------------\n\n\n# 5. Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory\n\nCons in traditional GPU: Complicated asynchronous user-directed constructs to overlap data migration and kernel execution are used to address this issue. The second challenge is memory over-subscription. When the working set of the GPU kernel cannot fit in the device memory, the programmers have to painstakingly redefine the data structures and tile the data to transfer back and forth in chunks.\n\nThis flow is inspired by -> 11. Towards High Performance Paged Memory for GPU.\n\n1 Scheduled threads generate global memory accesses.\n\n2 Each SM has its own load/store unit. Every load/store unit has its own TLB. Load/store unit performs a TLB look up to find whether the translation for the issued memory access is cached in TLB or not. A TLB miss is relayed to the GMMU.\n\n3 The GMMU walks through the page table looking for a PTE corresponding to the requested page with valid flag set. A far-fault occurs if there is no PTE for the requested page or the valid flag is not set. Then the far-fault is registered in the Far-fault Miss Status Handling Registers (MSHRs).\n\n4 The page is scheduled for transfer over CPU-GPU PCI-e interconnect.\n\n5 A 4KB page is allocated on demand and data is migrated from host to device memory.\n\n6 The MSHRs are consulted to notify the corresponding load/store unit and the memory access is replayed. A new PTE entry is added to the page table with valid\n\n\n\nThis paper introduces random, sequential and tree-based Neighborhood prefetcher in detail.\n\nAnd come up with pre-eviction for tree-based Neighborhood, different from LRU eviction used in Nvidia.\n\n\n\n----------------------------------------\n\n\n# 7. Batch-Aware Unified Memory Management in GPUs for Irregular Workloads 2020\n\nPropose:\n\n(1) increases the batch size (i.e., the number of page faults handled together), thereby amortizing the GPU runtime fault handling time, and reduces the number of batches by supporting CPU-like thread block context switching\n\nThread Oversubscription (TO), a CPU-like thread block context switching technique, to effectively amortize the GPU runtime fault handling time by increasing the batch size (i.e., the number of page faults handled together).\n\n(2) takes page eviction off the critical path with no hardware changes by overlapping evictions with CPU-to-GPU page migrations. Unobtrusive Eviction (UE) to take GPU page evictions off the critical path with no hardware changes based on the idea of overlapping page evictions with CPU-to-GPU page migrations.\n\nPrior work reports that page fault handling latency ranges from 20µs to 50µs [53]. We find that these numbers are conservative and can be worse depending on the applications and systems. Unfortunately, this page fault latency, which is in the order of microseconds, cannot be easily hidden even with ample thread-level parallelism (TLP) in GPUs, especially when GPU memory is oversubscribed.\n\n\n\nThe GPU runtime processes a group of GPU page faults together, rather than processing each individual one, in order to amortize the overhead of multiple round-trip latencies over the PCIe bus and to avoid invoking multiple interrupt service routines (ISRs) in the operating system (OS). To efficiently process an excessive number of page faults, the GPU runtime performs a series of operations such as preprocessing all the page faults and inserting page prefetching requests, which takes a significant amount of time (in the range of tens to hundreds of microseconds). Once all the operations (e.g., CPU page table walks for all the page faults, page allocation and eviction scheduling, etc.) are finished, page migrations between the CPU and the GPU begin.\n\nThis page fault handling is expensive because (1) it requires long latency communications between the CPU and GPU over the PCIe bus, and (2) the GPU runtime performs a very expensive fault handling service routine.\n\nTo amortize the overhead, the GPU runtime processes a group of page faults together, which we refer to as batch processing.\n\nWhen a page fault exception is raised by the GPU memory management unit (MMU), the GPU runtime begins to handle the exception, shown in 1.\n\n\n\nFrom this, we conclude that page evictions and new page allocations are serialized in modern GPUs to prevent the new pages from overwriting the evicted pages. Note that an eviction is required on every page fault once the pages resident in the GPU’s memory are at capacity.\n\n\n\nThis preprocessing includes sorting the page faults in ascending order of page addresses (to accelerate the page table walks) and the analysis of page addresses to insert page prefetching requests.1 We refer to the time taken by the GPU runtime to perform a collection of operations to handle many page faults together as GPU runtime fault handling time.\n\nhttps://github.com/acsl-technion/gaia_nvidia/blob/e23e4d926f576c2c4169664b6add89e1368ee849/kernel/nvidia-uvm/uvm8_gpu_replayable_faults.c#L787\n\n// Fault cache preprocessing for fault coalescing\n//\n// This function generates an ordered view of the given fault_cache in which faults are sorted by VA space, fault\n// address (aligned to 4K) and access type "intrusiveness" (atomic - write - read - prefetch). In order to minimize\n// the number of instance_ptr to VA space translations we perform a first sort by instance_ptr.\n//\n// This function returns NV_WARN_MORE_PROCESSING_REQUIRED if a fault buffer flush occurred during instance_ptr\n// translation and executed successfully, or the error code if it failed. NV_OK otherwise.\n//\n// Current scheme:\n// 1) sort by instance_ptr\n// 2) translate all instance_ptrs to VA spaces\n// 3) sort by va_space, fault address (GPU already reports 4K-aligned address) and access type\nstatic NV_STATUS preprocess_fault_batch(uvm_gpu_t *gpu, uvm_fault_service_batch_context_t *batch_context)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nThe batch processing time is measured to be in the range of 223µs to 553µs with a median of 313µs, of which, GPU runtime fault handling accounts for an average of 46.69% of the time (measured to be in the range of 50µs to 430µs with a median of 140µs).\n\n1）Thread Oversubscription\n\nWe enable thread oversubscription from the beginning of the execution by allocating one additional thread block to each SM ( 1 ). The thread block additionally allocated to each SM is inactive at first. It is important to note that the number of active thread blocks does not exceed that of the baseline, which is determined by the physical resource constraints. Once all of the warps in an active thread block are stalled due to page faults, the thread oversubscription mechanism context switches the active (but stalled) thread block with an inactive thread block ( 2 ). The thread oversubscription mechanism can be detrimental if it causes premature evictions. To prevent this, the GPU runtime monitors the premature eviction rates by periodically estimating the running average of the lifetime of pages by tracking when each page is allocated and evicted. We use the running average as an indicator of premature evictions. If the running average is decreased by a certain threshold, the thread oversubscription mechanism does not allow any more context switching by decrementing (and limiting) the number of concurrently runnable thread blocks ( 3 ).6 Otherwise, thread oversubscription allocates one additional thread block to each SM in an incremental manner.\n\n\n\n 2. Unobstrusive Eviction\n\nWhen a page fault interrupt is raised by the GPU MMU, the top-half interrupt service routine (ISR) responds. It checks whether the number of GPU resident pages is at capacity via the GPU memory status tracker. If so, it sends a preemptive eviction request to the GPU. The rest of the fault handling (e.g., preprocessing of the page faults, CPU-side page table walks) is performed by the bottom-half ISR.\n\n\n\n\n\nWhen the GPU runtime begins a batch’s processing, it checks the GPU memory status. If it is at capacity, it initiates a single page eviction ( 1 ). Once page X is evicted from the GPU’s memory, both CPU and GPU page tables are updated ( 2 ). Unlike the baseline case (Figure 4), page A can be migrated to the GPU memory without any delay ( 3 ). At the same time, page Y can be evicted using bidirectional transfers. Since the data transfer speed from the GPU to CPU memory is faster than the other way around [29], eviction is completely unobtrusive and migrations to the GPU can occur without any delay.\n\nIn short, thread oversubscription increase the batch size by switching in in-active thread block. and unobstrusive eviction avoid the serialization of swap pages between host and device.\n\n----------------------------------------\n\n\n# 10. Machine Learning Guided Optimal Use of GPU Unified Memory 2019\n\nTo enable better performance of UM, CUDA allows developers to give the UM driver additional advice on managing a given GPU memory range via an API function named cudaMemAdvise(const void *, size_t, enum cudaMemoryAdvise, int). The first two parameters of this function accept a pointer to a memory range with a specified size. The memory range should be allocated via cudaMallocManaged or declared via __managed__variables. The third parameter sets the advice for the memory range. The last parameter indicates the associated device’s id, which can indicate either a CPU or GPU device. The details and differences of these four kinds of advice are presented as follows:\n\n• Default: This represents the default on-demand page migration to accessing processor, using the first-touch policy.\n\n• cudaMemAdviseSetReadMostly: This advice is used for the data which is mostly going to be read from and only occasionally written to. The UM driver may create read-only copies of the data in a processor’s memory when that processor accesses it. If this region encounters any write requests, then only the write occurred page will be valid and other copies will be invalid.\n\n• cudaMemAdviseSetPreferredLocation: Once a target device is specified, this device memory can be set as the preferred location for the allocated data. The host memory can also be specified as the preferred location. Setting the preferred location does not cause data to migrate to that location immediately. The policy only guides what will happen when a fault occurs on the specified memory region: if data is already in the preferred location, the faulting processor will try to directly establish a mapping to the region without causing page migration. Otherwise, the data will be migrated to the processor accessing it if the data is not in the preferred location or if a direct mapping cannot be established.\n\n• cudaMemAdviseSetAccessedBy: This advice implies that the data will be accessed by a specified CPU or GPU device. It has no impact on the data location and will not cause data migration. It only causes the data to be always mapped in the specified processor’s page tables, when applicable. The mapping will be accordingly updated if the data is migrated somehow. This advice is useful to indicate that avoiding faults is important for some data, especially when the data is accessed by a GPU within a system containing multiple GPUs with peer-to-peer access enabled.\n\n----------------------------------------\n\n\n# 14. Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory [2024] 👍👍👍👍\n\nSame author: In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing[2021]\n\n\n\nThe UVM host driver on the host is open source with dependencies on the proprietary nvidia driver/resource manager and the host OS for memory management. This driver is a runtime fault servicing engine and the memory manager for managed memory allocations.\n\n\n 1. the fault is generated and handled by the hardware thread’s corresponding µTLB. The thread may continue executing instructions not blocked by a memory dependency. The fault propagates to the GPU memory management unit (GMMU), which writes the corresponding fault information into the GPU Fault Buffer and sends a hardware interrupt to the host. The fault buffer acts as a circular array, configured and managed by the UVM driver.\n    \n 2. The nvidia-uvm driver fetches the fault information, caches it on the host, and services the faults through\n 3. page processing: page table update and TLB shootdown on the host and GPU page table update\n 4. page migration: involves page migration.\n\nThe GPU exposes two functionalities to the host via the GPU command push-buffer—host-to GPU memory copy and fault replay.\nAs part of the fault servicing process, the driver instructs the GPU to copy pages into its memory, generally using high-performance hardware “copy engines.”\nOnce the GPU’s page tables are updated and the data is successfully migrated, the driver issues a fault replay, which clears the waiting status of µTLB, causing them to “replay” the prior miss.\n\nFault Handling:\n\nFirst, the GPU sends an interrupt over the interconnect to alert the host UVM driver of a page fault. The interrupt wakes up a worker thread to begin fault servicing if none is awake.\nSecond, the host retrieves the complete fault information from the GPU Fault Buffer.\nThe default fault retrieval policy reads faults until the batch size limit (i.e., 256 faults) is reached or no faults remain in the buffer.\n\n\nThese VABlocks serve as logical boundaries; the driver processes all batch faults within a single VABlock together, and each VABlock within a batch requires a distinct processing step.\n\n\n\n\n\nNotes:\n\n 1. when prefetching is not enabled, Service Faults is the major part of delay in CPU-GPU system. In this case, even NVlink does not matter. The reason is that unmapping and tlb-shut down in multi-cpu costs a lot.\n    \n 2. Pretching reduce the overhead by reduction of page fault and also increase the efficiency of NVlink.\n    \n 3. Oversubscription worse the case by finding empty space failed first and then evictim block to GPU. This worsen the performance.\n    \n\n1 & 2 explained:\n\n(1) unmapping host-side data takes place on the fault path and incurs significant overhead\n(2) certain hostside parallelizations of an application using UVM can exaggerate these unmapping costs.\nThe host OS performs this operation, and the costs likely stem from issues with virtual mappings across CPU cores, flushing dirty pages from caches and TLBs, NUMA, and other memory-adjacent issues.\nAdditionally, these operations do not take place in bulk due to the logical separation of VABlocks within UVM.\nThis is an area that deserves particular scrutiny as HMM also performs host page unmapping on the fault path using host OS mechanisms, implying a similar cost could be applied to all devices when using HMM [15, 26].\n\ncompared to cpu-gpu case, GPU-GPU on-demand page migration is faster due to the actual page table updates offloaded to the source GPU.\nFault servicing includes operations such as page unmapping and TLB shootdown on the source device.\nGPU page table updates and TLB shootdown are hardware based and relatively much faster.\n\n\n 3. Explained Process: (1) fail allocation\n    (2) evict a VABlock and migrate the data back to the host\n    (3) restart the block migration process, including host unmapping, data transfer, GPU mapping, page population, a process by which pages are filled with zero values before data is migrated to them.\n\nInterestingly, oversubscription diminishes the benefits of NVLink2. Oversubscription, as it is currently implemented, always evicts pages back to the host memory. This causes the CPU-GPU PCIe interconnect to become active for data eviction.\n\n👉 In short, in CPU-GPU system, service faults are major issue due to tlb shutdown and page table update. This even diminish the power of NVLink. Memory Oversubscription worsen the situation by failing to allocate memory in GPU, find eviction and eviction to CPU, adding these operation worsen the performance.\n👉 GPU-GPU does not have the service faults problem since page table update and tlb shutdown are handled by faster gpu hardware.\n👉 Besides, prefetching helps to improve performance a lot by reducing fault and better bandwitdh efficiency.\n',normalizedContent:' 1.  holistic performance analysis and optimization of unified virtual holistic performance analysis and optimization of unified virtual memory\n 2.  in-depth analyses of unified virtual memory system for gpu accelerated computing\n 3.  oversubscribing gpu unified virtual memory: implications and suggestions\n 4.  performance evaluation of advanced features in cuda unified memory\n 5.  interplay between hardware prefetcher and page eviction policy in cpu-gpu unified virtual memory\n 6.  unified memory: gpgpu-sim/uvm smart integration\n 7.  batch-aware unified memory management in gpus for irregular workloads\n 8.  an intelligent framework for oversubscription management in cpu-gpu unified memory\n 9.  architectural support for address translation on gpus designing memory management units for cpu/gpus with unified address spaces\n 10. machine learning guided optimal use of gpu unified memory\n 11. towards high performance paged memory for gpus\n 12. [virtualization] virtual thread: maximizing thread-level parallelism beyond gpu scheduling limit.\n 13. [virtualization] a survey of gpu multitasking methods supported by hardware architecture\n 14. fine-grain quantitative analysis of demand paging in unified virtual memory\n\nplan to read\n\n 1. early-adaptor: an adaptive framework for proactive uvm memory management\n 2. liberator: a data reuse framework for out-of-memory graph computing on gpus\n 3. [hpca] enabling large dynamic neural network training with learning-based memory management\n 4. gpuswap: enabling oversubscription of gpu memory through transparent swapping\n\n----------------------------------------\n\nunified memory history copied from evolution of nvidia gpu from microarchitectures pascal to ampere\n\ncuda 4 introduced uva (unified virtual addressing) to provide a single virtual memory address space for both cpu and gpu memory and enable pointers to be accessed from gpu code no matter where in the system they reside. uva enables zero-copy memory, a pinned cpu memory accessible by gpu code directly, over pcie, without the need for memory copy. this provides some of the convince of unified memory, but at the cost of worse performance, because gpu always accesses it with pcie’s low bandwidth and high latency.[1]\n\nlater, cuda 6 introduced unified memory, which creates a pool of managed memory that programs running on the cpu and gpu can access without explicit data movement. however, only when cpu and gpu processes are not running together because of the limitation of the kepler and maxwell gpu microarchitecture. also, the unified memory address space was limited to the size of the gpu memory.[1, 3]\n\ncuda 8 and pascal microarchitectures improve unified memory functionality by adding 49-bit virtual addressing and page faulting capability. the larger 49-bit virtual addresses are sufficient to enable gpus to access the entire system memory plus the memory of all gpus in the system. because of the memory page faulting functionality, the cuda system software does not need to synchronize all managed memory allocations to the gpu before each kernel lunch. instead, when a thread running on gpu faults on non-resident memory access(demanding page), it stalls until the page can be migrated and the page table updated. alternatively, the page may be mapped for remote access over pcie or nvlink interconnects.[1, 3, 6]\n\nthese new features of unified memory enable oversubscription of memory, which means that application running on a gpu can use data sets larger than ten their device memory.[1] while the unified memory model makes gpu programming more convenient, it comes at a cost; handling page faults and page migrations can be expensive. cuda 8 addresses this issue with features like prefetch and memory advice.\n\n----------------------------------------\n\n\n# 1. holistic performance analysis and optimization of unified virtual holistic performance analysis and optimization of unified virtual memory\n\nsame author with in-depth analyses of unified virtual memory system for gpu accelerated computing\n\n----------------------------------------\n\n\n# 3. oversubscribing gpu unified virtual memory: implications and suggestions\n\nuvm supports memory oversubscription, giving gpu programs the ability to use a larger amount of memory than the physical memory, without worrying about the problem of memory shortage.\n\nadvanced optimization techniques, mainly prefetching and memory usage hints [1], can be used to fine-tune the performance of uvm applications, mitigating the overheads caused by uvm.\n\n\n\n2）prefetching and hints prefetching and uvm hints are the major approaches provided by cuda, with the hope that page faults and memory thrashing could be prevented by fine-tuning the behavior of uvm at runtime.\n\nby calling cudamemprefetchasync (pf), a memory block could be prefetched to gpu. uvm hints provide informed decisions on page handling by indicating the access patterns of data.\n\nchanging uvm hints is done by invoking cudamemadvise with one of the following policies：\n\n• cudamemadvisesetaccessedby (ab) implies that the device keeps a direct mapping in its page table. when the data is migrated, the mapping is re-established.\n• cudamemadvisesetpreferredlocation (pl) pins the data and prevents the page to be migrated, which is useful when the page is mainly accessed on one side.\n• cudamemadvisesetreadmostly (rm) indicates the data region is read-intensive. it creates a read-only copy of the page on the faulting side, allowing on current access on both sides.\n\n\nonly one policy (ab, pl, or rm) could be specified for each memory block, but each policy can be used along with prefetching.\n\nsuggestions: to ensure performance under all oversubscription conditions, programmer needs to choose the uvm hints dynamically based on the application’s memory usage and available gpu memory. as a prerequisite, the size of the fall pages needs to be estimated or measured by experiment. before kernel launch, the program should first check the size of available gpu memory (e.g. via the cudamemgetinfo api). if no oversubscription will happen, or the available memory is larger than the size of fall pages, the programmer could set hints based on the conclusions provided by related researches [24]. otherwise, based on our findings, applying the hint ab is a preferable choice.\n\n----------------------------------------\n\n\n# 4. performance evaluation of advanced features in cuda unified memory\n\ncuda has introduced new features for optimizing the data migration on um, i.e., memory advises and prefetch. instead of solely relying on page faults, the memory advises feature allows the programmer to provide data access pattern for each memory object so that the runtime can optimize migration decisions. the prefetch proactively triggers asynchronous data migration to gpu before the data is accessed, which reduces page faults and, consequently, the overhead in handling page faults.\n\n-using memory advises improves application performance in oversubscription execution on the intel platform and in-memory executions on the ibm platform.\n\n-um prefetch provides a significant performance improvement on the intel-volta/pascal-pci-e based systems while it does not show a performance improvement on the power9-volta-nvlink based system\n\num was first introduced in cuda 6.0 [21]. only until the recent nvidia pascal microarchitecture that has hardware support for page faults.\n\n\n\n• cudamemadvisesetaccessedby establishes a direct mapping of data to a specified device. figure 2c illustrates an example of a physical page on gpu being remotely access from the host. when cudamemadvisesetpreferredlocation is applied, cuda runtime tries to build a direct mapping to the page to avoid data migration so that the destination can access data remotely. differently from cudamemadvisesetpreferredlocation, this cudamemadvisesetaccessedby does not try to pin pages on a specific device; instead, its main effect is to establish mapping on the remote device. this advice takes effect on the creation of the memory pages. the mapping will be re-established after the pages are migrated.\n\n• cudamemadvisesetpreferredlocation sets the preferred physical location of pages. this advice pins a page and prevents it from migrating to other memories. figure 2b illustrates a page preferred on the host side, and gpu uses remote mapping to access the page. this advice established a direct (remote) mapping to the memory page. when accessing the page remotely, data is fetched through the remote memory instead of generating a page fault. if the underlying hardware does not support the remote mapping, the page will be migrated as in the standard um. cudamemadvisesetpreferredlocation is useful for applications with little data sharing between cpu and gpu, i.e., part of the application is executed completely on the gpu, and the rest of the application executes on the host. data that is being used mostly by the gpu can be pinned to the gpu with the advice, avoiding memory thrashing.\n\n• cudamemadvisesetreadmostly implies a read-intensive data region. in the basic um, accessing a page on a remote side triggers page migration. however, with cudamemadvisesetreadmostly, a read-only duplicate of the page will be created on the faulting side, which prevents page faults and data migration in the future. figure 2a illustrates an example, where the second access (step 5) has no page fault and is local access. this mechanism, however, results in a high overhead if there is any update to this memory region because all copies of the corresponding page will be invalidated to preserve consistency between different copies. thus, this advice is often used in read-only data structures, such as lookup tables and application parameters.\n\nin general, we found both memory advises and prefetch to be simple and effective.\n\n----------------------------------------\n\n\n# 5. interplay between hardware prefetcher and page eviction policy in cpu-gpu unified virtual memory\n\ncons in traditional gpu: complicated asynchronous user-directed constructs to overlap data migration and kernel execution are used to address this issue. the second challenge is memory over-subscription. when the working set of the gpu kernel cannot fit in the device memory, the programmers have to painstakingly redefine the data structures and tile the data to transfer back and forth in chunks.\n\nthis flow is inspired by -> 11. towards high performance paged memory for gpu.\n\n1 scheduled threads generate global memory accesses.\n\n2 each sm has its own load/store unit. every load/store unit has its own tlb. load/store unit performs a tlb look up to find whether the translation for the issued memory access is cached in tlb or not. a tlb miss is relayed to the gmmu.\n\n3 the gmmu walks through the page table looking for a pte corresponding to the requested page with valid flag set. a far-fault occurs if there is no pte for the requested page or the valid flag is not set. then the far-fault is registered in the far-fault miss status handling registers (mshrs).\n\n4 the page is scheduled for transfer over cpu-gpu pci-e interconnect.\n\n5 a 4kb page is allocated on demand and data is migrated from host to device memory.\n\n6 the mshrs are consulted to notify the corresponding load/store unit and the memory access is replayed. a new pte entry is added to the page table with valid\n\n\n\nthis paper introduces random, sequential and tree-based neighborhood prefetcher in detail.\n\nand come up with pre-eviction for tree-based neighborhood, different from lru eviction used in nvidia.\n\n\n\n----------------------------------------\n\n\n# 7. batch-aware unified memory management in gpus for irregular workloads 2020\n\npropose:\n\n(1) increases the batch size (i.e., the number of page faults handled together), thereby amortizing the gpu runtime fault handling time, and reduces the number of batches by supporting cpu-like thread block context switching\n\nthread oversubscription (to), a cpu-like thread block context switching technique, to effectively amortize the gpu runtime fault handling time by increasing the batch size (i.e., the number of page faults handled together).\n\n(2) takes page eviction off the critical path with no hardware changes by overlapping evictions with cpu-to-gpu page migrations. unobtrusive eviction (ue) to take gpu page evictions off the critical path with no hardware changes based on the idea of overlapping page evictions with cpu-to-gpu page migrations.\n\nprior work reports that page fault handling latency ranges from 20µs to 50µs [53]. we find that these numbers are conservative and can be worse depending on the applications and systems. unfortunately, this page fault latency, which is in the order of microseconds, cannot be easily hidden even with ample thread-level parallelism (tlp) in gpus, especially when gpu memory is oversubscribed.\n\n\n\nthe gpu runtime processes a group of gpu page faults together, rather than processing each individual one, in order to amortize the overhead of multiple round-trip latencies over the pcie bus and to avoid invoking multiple interrupt service routines (isrs) in the operating system (os). to efficiently process an excessive number of page faults, the gpu runtime performs a series of operations such as preprocessing all the page faults and inserting page prefetching requests, which takes a significant amount of time (in the range of tens to hundreds of microseconds). once all the operations (e.g., cpu page table walks for all the page faults, page allocation and eviction scheduling, etc.) are finished, page migrations between the cpu and the gpu begin.\n\nthis page fault handling is expensive because (1) it requires long latency communications between the cpu and gpu over the pcie bus, and (2) the gpu runtime performs a very expensive fault handling service routine.\n\nto amortize the overhead, the gpu runtime processes a group of page faults together, which we refer to as batch processing.\n\nwhen a page fault exception is raised by the gpu memory management unit (mmu), the gpu runtime begins to handle the exception, shown in 1.\n\n\n\nfrom this, we conclude that page evictions and new page allocations are serialized in modern gpus to prevent the new pages from overwriting the evicted pages. note that an eviction is required on every page fault once the pages resident in the gpu’s memory are at capacity.\n\n\n\nthis preprocessing includes sorting the page faults in ascending order of page addresses (to accelerate the page table walks) and the analysis of page addresses to insert page prefetching requests.1 we refer to the time taken by the gpu runtime to perform a collection of operations to handle many page faults together as gpu runtime fault handling time.\n\nhttps://github.com/acsl-technion/gaia_nvidia/blob/e23e4d926f576c2c4169664b6add89e1368ee849/kernel/nvidia-uvm/uvm8_gpu_replayable_faults.c#l787\n\n// fault cache preprocessing for fault coalescing\n//\n// this function generates an ordered view of the given fault_cache in which faults are sorted by va space, fault\n// address (aligned to 4k) and access type "intrusiveness" (atomic - write - read - prefetch). in order to minimize\n// the number of instance_ptr to va space translations we perform a first sort by instance_ptr.\n//\n// this function returns nv_warn_more_processing_required if a fault buffer flush occurred during instance_ptr\n// translation and executed successfully, or the error code if it failed. nv_ok otherwise.\n//\n// current scheme:\n// 1) sort by instance_ptr\n// 2) translate all instance_ptrs to va spaces\n// 3) sort by va_space, fault address (gpu already reports 4k-aligned address) and access type\nstatic nv_status preprocess_fault_batch(uvm_gpu_t *gpu, uvm_fault_service_batch_context_t *batch_context)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nthe batch processing time is measured to be in the range of 223µs to 553µs with a median of 313µs, of which, gpu runtime fault handling accounts for an average of 46.69% of the time (measured to be in the range of 50µs to 430µs with a median of 140µs).\n\n1）thread oversubscription\n\nwe enable thread oversubscription from the beginning of the execution by allocating one additional thread block to each sm ( 1 ). the thread block additionally allocated to each sm is inactive at first. it is important to note that the number of active thread blocks does not exceed that of the baseline, which is determined by the physical resource constraints. once all of the warps in an active thread block are stalled due to page faults, the thread oversubscription mechanism context switches the active (but stalled) thread block with an inactive thread block ( 2 ). the thread oversubscription mechanism can be detrimental if it causes premature evictions. to prevent this, the gpu runtime monitors the premature eviction rates by periodically estimating the running average of the lifetime of pages by tracking when each page is allocated and evicted. we use the running average as an indicator of premature evictions. if the running average is decreased by a certain threshold, the thread oversubscription mechanism does not allow any more context switching by decrementing (and limiting) the number of concurrently runnable thread blocks ( 3 ).6 otherwise, thread oversubscription allocates one additional thread block to each sm in an incremental manner.\n\n\n\n 2. unobstrusive eviction\n\nwhen a page fault interrupt is raised by the gpu mmu, the top-half interrupt service routine (isr) responds. it checks whether the number of gpu resident pages is at capacity via the gpu memory status tracker. if so, it sends a preemptive eviction request to the gpu. the rest of the fault handling (e.g., preprocessing of the page faults, cpu-side page table walks) is performed by the bottom-half isr.\n\n\n\n\n\nwhen the gpu runtime begins a batch’s processing, it checks the gpu memory status. if it is at capacity, it initiates a single page eviction ( 1 ). once page x is evicted from the gpu’s memory, both cpu and gpu page tables are updated ( 2 ). unlike the baseline case (figure 4), page a can be migrated to the gpu memory without any delay ( 3 ). at the same time, page y can be evicted using bidirectional transfers. since the data transfer speed from the gpu to cpu memory is faster than the other way around [29], eviction is completely unobtrusive and migrations to the gpu can occur without any delay.\n\nin short, thread oversubscription increase the batch size by switching in in-active thread block. and unobstrusive eviction avoid the serialization of swap pages between host and device.\n\n----------------------------------------\n\n\n# 10. machine learning guided optimal use of gpu unified memory 2019\n\nto enable better performance of um, cuda allows developers to give the um driver additional advice on managing a given gpu memory range via an api function named cudamemadvise(const void *, size_t, enum cudamemoryadvise, int). the first two parameters of this function accept a pointer to a memory range with a specified size. the memory range should be allocated via cudamallocmanaged or declared via __managed__variables. the third parameter sets the advice for the memory range. the last parameter indicates the associated device’s id, which can indicate either a cpu or gpu device. the details and differences of these four kinds of advice are presented as follows:\n\n• default: this represents the default on-demand page migration to accessing processor, using the first-touch policy.\n\n• cudamemadvisesetreadmostly: this advice is used for the data which is mostly going to be read from and only occasionally written to. the um driver may create read-only copies of the data in a processor’s memory when that processor accesses it. if this region encounters any write requests, then only the write occurred page will be valid and other copies will be invalid.\n\n• cudamemadvisesetpreferredlocation: once a target device is specified, this device memory can be set as the preferred location for the allocated data. the host memory can also be specified as the preferred location. setting the preferred location does not cause data to migrate to that location immediately. the policy only guides what will happen when a fault occurs on the specified memory region: if data is already in the preferred location, the faulting processor will try to directly establish a mapping to the region without causing page migration. otherwise, the data will be migrated to the processor accessing it if the data is not in the preferred location or if a direct mapping cannot be established.\n\n• cudamemadvisesetaccessedby: this advice implies that the data will be accessed by a specified cpu or gpu device. it has no impact on the data location and will not cause data migration. it only causes the data to be always mapped in the specified processor’s page tables, when applicable. the mapping will be accordingly updated if the data is migrated somehow. this advice is useful to indicate that avoiding faults is important for some data, especially when the data is accessed by a gpu within a system containing multiple gpus with peer-to-peer access enabled.\n\n----------------------------------------\n\n\n# 14. fine-grain quantitative analysis of demand paging in unified virtual memory [2024] 👍👍👍👍\n\nsame author: in-depth analyses of unified virtual memory system for gpu accelerated computing[2021]\n\n\n\nthe uvm host driver on the host is open source with dependencies on the proprietary nvidia driver/resource manager and the host os for memory management. this driver is a runtime fault servicing engine and the memory manager for managed memory allocations.\n\n\n 1. the fault is generated and handled by the hardware thread’s corresponding µtlb. the thread may continue executing instructions not blocked by a memory dependency. the fault propagates to the gpu memory management unit (gmmu), which writes the corresponding fault information into the gpu fault buffer and sends a hardware interrupt to the host. the fault buffer acts as a circular array, configured and managed by the uvm driver.\n    \n 2. the nvidia-uvm driver fetches the fault information, caches it on the host, and services the faults through\n 3. page processing: page table update and tlb shootdown on the host and gpu page table update\n 4. page migration: involves page migration.\n\nthe gpu exposes two functionalities to the host via the gpu command push-buffer—host-to gpu memory copy and fault replay.\nas part of the fault servicing process, the driver instructs the gpu to copy pages into its memory, generally using high-performance hardware “copy engines.”\nonce the gpu’s page tables are updated and the data is successfully migrated, the driver issues a fault replay, which clears the waiting status of µtlb, causing them to “replay” the prior miss.\n\nfault handling:\n\nfirst, the gpu sends an interrupt over the interconnect to alert the host uvm driver of a page fault. the interrupt wakes up a worker thread to begin fault servicing if none is awake.\nsecond, the host retrieves the complete fault information from the gpu fault buffer.\nthe default fault retrieval policy reads faults until the batch size limit (i.e., 256 faults) is reached or no faults remain in the buffer.\n\n\nthese vablocks serve as logical boundaries; the driver processes all batch faults within a single vablock together, and each vablock within a batch requires a distinct processing step.\n\n\n\n\n\nnotes:\n\n 1. when prefetching is not enabled, service faults is the major part of delay in cpu-gpu system. in this case, even nvlink does not matter. the reason is that unmapping and tlb-shut down in multi-cpu costs a lot.\n    \n 2. pretching reduce the overhead by reduction of page fault and also increase the efficiency of nvlink.\n    \n 3. oversubscription worse the case by finding empty space failed first and then evictim block to gpu. this worsen the performance.\n    \n\n1 & 2 explained:\n\n(1) unmapping host-side data takes place on the fault path and incurs significant overhead\n(2) certain hostside parallelizations of an application using uvm can exaggerate these unmapping costs.\nthe host os performs this operation, and the costs likely stem from issues with virtual mappings across cpu cores, flushing dirty pages from caches and tlbs, numa, and other memory-adjacent issues.\nadditionally, these operations do not take place in bulk due to the logical separation of vablocks within uvm.\nthis is an area that deserves particular scrutiny as hmm also performs host page unmapping on the fault path using host os mechanisms, implying a similar cost could be applied to all devices when using hmm [15, 26].\n\ncompared to cpu-gpu case, gpu-gpu on-demand page migration is faster due to the actual page table updates offloaded to the source gpu.\nfault servicing includes operations such as page unmapping and tlb shootdown on the source device.\ngpu page table updates and tlb shootdown are hardware based and relatively much faster.\n\n\n 3. explained process: (1) fail allocation\n    (2) evict a vablock and migrate the data back to the host\n    (3) restart the block migration process, including host unmapping, data transfer, gpu mapping, page population, a process by which pages are filled with zero values before data is migrated to them.\n\ninterestingly, oversubscription diminishes the benefits of nvlink2. oversubscription, as it is currently implemented, always evicts pages back to the host memory. this causes the cpu-gpu pcie interconnect to become active for data eviction.\n\n👉 in short, in cpu-gpu system, service faults are major issue due to tlb shutdown and page table update. this even diminish the power of nvlink. memory oversubscription worsen the situation by failing to allocate memory in gpu, find eviction and eviction to cpu, adding these operation worsen the performance.\n👉 gpu-gpu does not have the service faults problem since page table update and tlb shutdown are handled by faster gpu hardware.\n👉 besides, prefetching helps to improve performance a lot by reducing fault and better bandwitdh efficiency.\n',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Large Language Model Paper List",frontmatter:{title:"Large Language Model Paper List",date:"2023-12-19T00:00:00.000Z",permalink:"/pages/458720/",tags:[null]},regularPath:"/03.gpu/08.LLM.html",relativePath:"03.gpu/08.LLM.md",key:"v-65bd31e5",path:"/pages/458720/",headers:[{level:3,title:"1. Efficient Memory Management for Large Language Model Serving with PagedAttention",slug:"_1-efficient-memory-management-for-large-language-model-serving-with-pagedattention",normalizedTitle:"1. efficient memory management for large language model serving with pagedattention",charIndex:1},{level:3,title:"2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory",slug:"_2-llm-in-a-flash-efficient-large-language-model-inference-with-limited-memory",normalizedTitle:"2. llm in a flash: efficient large language model inference with limited memory",charIndex:93},{level:3,title:"1. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",slug:"_1-a-survey-on-hallucination-in-large-language-models-principles-taxonomy-challenges-and-open-questions",normalizedTitle:"1. a survey on hallucination in large language models: principles, taxonomy, challenges, and open questions",charIndex:4496}],headersStr:"1. Efficient Memory Management for Large Language Model Serving with PagedAttention 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory 1. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",content:" 1. Efficient Memory Management for Large Language Model Serving with PagedAttention [2023]\n 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory [Apple 2023]\n 3. [591 Year: 2021] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM Not Read\n\n----------------------------------------\n\n\n# 1. Efficient Memory Management for Large Language Model Serving with PagedAttention\n\nDisscussed the GEMM in prompt and GEMV in auto regression. In GEMV, LLM is memory bound. There is lot of fragment in KVCache. It also quantize the memory necessity for parameter in KV Cache. They came up the method similar to paging in OS to manage KV in KV cache, reducing the fragment.\n\n\n# 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory\n\nUpproject matrix and downprojection matrix: https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/ Related paper: Parameter-Efficient Transfer Learning for NLP This introduce low-rank.\n\nsliding window.\n\n 1. high sparsity in FeedForward Layers, more than 90% Selectively only load parameters from memory either no-zero input or predicted have non-zero output\n\n 2. Minimize data transfer and maximize flash memory throughout Window sliding: Load parameters for only the past few tokens, reusing activations from recently computed tokens. This sliding window approach reduces the number of IO requests to load weights. Row-column bundling: We store a concatenated row and column of the up-projection and down-projection layers to read bigger contiguous chunks from flash memory. This increases throughput by reading larger chunks.\n\n 3. Predict FFN sparsity and avoid loading zeroed-out parameter to minimize the number of weights to be transferred from flash memory to DRAM.\n\n 4. Static memory preallocation\n\nAlso a model to predict the tradeoff between loading less data and reading larger chunks\n\nLoad only 2% of FFN layer from flash\n\n 1. Larger chunk Although throughput growth is not linear (larger chunks take longer to transfer), the latency for the initial byte becomes a smaller fraction of the total request time, resulting in more efficient data reading.\n\n 2. Load From Flash\n\n2.1 inherent sparsity found in Feed-Forward Network (FFN) model\n\nSelective Persistence Strategy Retain the embeddings and matrices within the attention mechanism of the transformer constant.Attentions weights 1/3 of the model size.For the Feed-Forward Network (FFN) portions, only the non-sparse segments are dynamically loaded into DRAM as needed.\n\nAnticipating ReLU Sparsity\nRelu activation can induce 90% sparsity. Optimize preceding layer, up project by low-rank predictor to identify the zeroed elements post-ReLU.\nIn contrast to their work, our predictor needs only the output of the current layer’s attention module and not the previous layer’s FFN module.\n\nNeuron Data Management via Sliding Window Technique Our approach focuses on managing neuron data by employing a Sliding Window Technique. This methodology entails maintaining neuron data only for a recent subset of input tokens in the memory.\nThe key aspect of this technique is the selective loading of neuron data that differs between the current input token and its immediate predecessors.\nFrees up memory resources previously allocated to neuron data from older tokens that are no longer within the sliding window\n\nLet sagg(k) denote the cumulative use of neuron data across a sequence of k input tokens. This reduction in data loading is counterbalanced by the memory cost associated with storing sagg(k). In determining the size of the sliding window, the aim is to maximize it within the constraints imposed by the available memory capacity.\n\n2.2 Improve Transfer Throughput with Increased Chunk Sizes\n\nBundling Columns and Rows for upward and downward projection\n\nBundling Based on Co-activation fetch neuron with its cloest friend. But there is WARM-GUY problem.\n\n2.3 Optimized Data Management in DRAM\n\nWhen a substantial portion (approximately 25%) of the Feed-Forward Networks (FFNs) in DRAM needs to be rewritten.\n\nWhen introducing data for new neurons, reallocating the matrix and appending new matrices can lead to significant overhead due to the need for rewriting existing neurons data in DRAM. This involves the preallocation of all necessary memory and the establishment of a corresponding data structure for efficient management.\n\n----------------------------------------\n\nLLM Principles\n\n\n# 1. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions\n\n",normalizedContent:" 1. efficient memory management for large language model serving with pagedattention [2023]\n 2. llm in a flash: efficient large language model inference with limited memory [apple 2023]\n 3. [591 year: 2021] efficient large-scale language model training on gpu clusters using megatron-lm not read\n\n----------------------------------------\n\n\n# 1. efficient memory management for large language model serving with pagedattention\n\ndisscussed the gemm in prompt and gemv in auto regression. in gemv, llm is memory bound. there is lot of fragment in kvcache. it also quantize the memory necessity for parameter in kv cache. they came up the method similar to paging in os to manage kv in kv cache, reducing the fragment.\n\n\n# 2. llm in a flash: efficient large language model inference with limited memory\n\nupproject matrix and downprojection matrix: https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/ related paper: parameter-efficient transfer learning for nlp this introduce low-rank.\n\nsliding window.\n\n 1. high sparsity in feedforward layers, more than 90% selectively only load parameters from memory either no-zero input or predicted have non-zero output\n\n 2. minimize data transfer and maximize flash memory throughout window sliding: load parameters for only the past few tokens, reusing activations from recently computed tokens. this sliding window approach reduces the number of io requests to load weights. row-column bundling: we store a concatenated row and column of the up-projection and down-projection layers to read bigger contiguous chunks from flash memory. this increases throughput by reading larger chunks.\n\n 3. predict ffn sparsity and avoid loading zeroed-out parameter to minimize the number of weights to be transferred from flash memory to dram.\n\n 4. static memory preallocation\n\nalso a model to predict the tradeoff between loading less data and reading larger chunks\n\nload only 2% of ffn layer from flash\n\n 1. larger chunk although throughput growth is not linear (larger chunks take longer to transfer), the latency for the initial byte becomes a smaller fraction of the total request time, resulting in more efficient data reading.\n\n 2. load from flash\n\n2.1 inherent sparsity found in feed-forward network (ffn) model\n\nselective persistence strategy retain the embeddings and matrices within the attention mechanism of the transformer constant.attentions weights 1/3 of the model size.for the feed-forward network (ffn) portions, only the non-sparse segments are dynamically loaded into dram as needed.\n\nanticipating relu sparsity\nrelu activation can induce 90% sparsity. optimize preceding layer, up project by low-rank predictor to identify the zeroed elements post-relu.\nin contrast to their work, our predictor needs only the output of the current layer’s attention module and not the previous layer’s ffn module.\n\nneuron data management via sliding window technique our approach focuses on managing neuron data by employing a sliding window technique. this methodology entails maintaining neuron data only for a recent subset of input tokens in the memory.\nthe key aspect of this technique is the selective loading of neuron data that differs between the current input token and its immediate predecessors.\nfrees up memory resources previously allocated to neuron data from older tokens that are no longer within the sliding window\n\nlet sagg(k) denote the cumulative use of neuron data across a sequence of k input tokens. this reduction in data loading is counterbalanced by the memory cost associated with storing sagg(k). in determining the size of the sliding window, the aim is to maximize it within the constraints imposed by the available memory capacity.\n\n2.2 improve transfer throughput with increased chunk sizes\n\nbundling columns and rows for upward and downward projection\n\nbundling based on co-activation fetch neuron with its cloest friend. but there is warm-guy problem.\n\n2.3 optimized data management in dram\n\nwhen a substantial portion (approximately 25%) of the feed-forward networks (ffns) in dram needs to be rewritten.\n\nwhen introducing data for new neurons, reallocating the matrix and appending new matrices can lead to significant overhead due to the need for rewriting existing neurons data in dram. this involves the preallocation of all necessary memory and the establishment of a corresponding data structure for efficient management.\n\n----------------------------------------\n\nllm principles\n\n\n# 1. a survey on hallucination in large language models: principles, taxonomy, challenges, and open questions\n\n",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU Virtualization Paper List",frontmatter:{title:"GPU Virtualization Paper List",date:"2023-12-18T00:00:00.000Z",permalink:"/pages/45871f/",tags:[null]},regularPath:"/03.gpu/07.GPUVirtualization.html",relativePath:"03.gpu/07.GPUVirtualization.md",key:"v-d22a70f6",path:"/pages/45871f/",headers:[{level:3,title:"1.TimeGraph: GPU Scheduling for Real-Time Multi-Tasking Environments",slug:"_1-timegraph-gpu-scheduling-for-real-time-multi-tasking-environments",normalizedTitle:"1.timegraph: gpu scheduling for real-time multi-tasking environments",charIndex:500},{level:3,title:"2.Hardware Compute Partitioning on NVIDIA GPUs",slug:"_2-hardware-compute-partitioning-on-nvidia-gpus",normalizedTitle:"2.hardware compute partitioning on nvidia gpus",charIndex:622},{level:3,title:"3.GPUvm: Why Not Virtualizing GPUs at the Hypervisor [Year 2014 Citation]",slug:"_3-gpuvm-why-not-virtualizing-gpus-at-the-hypervisor-year-2014-citation",normalizedTitle:"3.gpuvm: why not virtualizing gpus at the hypervisor [year 2014 citation]",charIndex:722},{level:3,title:"4.Implementing Open-Source CUDA Runtime",slug:"_4-implementing-open-source-cuda-runtime",normalizedTitle:"4.implementing open-source cuda runtime",charIndex:923},{level:3,title:"5.Gdev: First-Class GPU Resource Management in the Operating System",slug:"_5-gdev-first-class-gpu-resource-management-in-the-operating-system",normalizedTitle:"5.gdev: first-class gpu resource management in the operating system",charIndex:1016}],headersStr:"1.TimeGraph: GPU Scheduling for Real-Time Multi-Tasking Environments 2.Hardware Compute Partitioning on NVIDIA GPUs 3.GPUvm: Why Not Virtualizing GPUs at the Hypervisor [Year 2014 Citation] 4.Implementing Open-Source CUDA Runtime 5.Gdev: First-Class GPU Resource Management in the Operating System",content:" 1. TimeGraph: GPU Scheduling for Real-Time Multi-Tasking Environments [Citation 372]\n 2. Hardware Compute Partitioning on NVIDIA GPUs [Year 2022 Citation 3]\n 3. GPUvm: Why Not Virtualizing GPUs at the Hypervisor [Year 2014 Citation 142]\n 4. Implementing Open-Source CUDA Runtime [Year 2013 Citation 13]\n 5. Gdev: First-Class GPU Resource Management in the Operating System [Year 2012 Citation 272]\n\n1.3.4.5 are written by the same author: Shinpei Kato.\n\n----------------------------------------\n\n\n# 1.TimeGraph: GPU Scheduling for Real-Time Multi-Tasking Environments\n\nDone.\n\n----------------------------------------\n\n\n# 2.Hardware Compute Partitioning on NVIDIA GPUs\n\nDone.\n\n----------------------------------------\n\n\n# 3.GPUvm: Why Not Virtualizing GPUs at the Hypervisor [Year 2014 Citation]\n\nhttps://cseweb.ucsd.edu/~yiying/cse291j-winter20/reading/GPU-Virtualization.pdf\n\n----------------------------------------\n\n\n# 4.Implementing Open-Source CUDA Runtime\n\nDone.\n\n----------------------------------------\n\n\n# 5.Gdev: First-Class GPU Resource Management in the Operating System\n\nDone.",normalizedContent:" 1. timegraph: gpu scheduling for real-time multi-tasking environments [citation 372]\n 2. hardware compute partitioning on nvidia gpus [year 2022 citation 3]\n 3. gpuvm: why not virtualizing gpus at the hypervisor [year 2014 citation 142]\n 4. implementing open-source cuda runtime [year 2013 citation 13]\n 5. gdev: first-class gpu resource management in the operating system [year 2012 citation 272]\n\n1.3.4.5 are written by the same author: shinpei kato.\n\n----------------------------------------\n\n\n# 1.timegraph: gpu scheduling for real-time multi-tasking environments\n\ndone.\n\n----------------------------------------\n\n\n# 2.hardware compute partitioning on nvidia gpus\n\ndone.\n\n----------------------------------------\n\n\n# 3.gpuvm: why not virtualizing gpus at the hypervisor [year 2014 citation]\n\nhttps://cseweb.ucsd.edu/~yiying/cse291j-winter20/reading/gpu-virtualization.pdf\n\n----------------------------------------\n\n\n# 4.implementing open-source cuda runtime\n\ndone.\n\n----------------------------------------\n\n\n# 5.gdev: first-class gpu resource management in the operating system\n\ndone.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU Simulator",frontmatter:{title:"GPU Simulator",date:"2024-01-01T00:00:00.000Z",permalink:"/pages/458721/",tags:[null]},regularPath:"/03.gpu/09.Simulator.html",relativePath:"03.gpu/09.Simulator.md",key:"v-e33712b6",path:"/pages/458721/",headers:[{level:3,title:"1.DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis",slug:"_1-delta-gpu-performance-model-for-deep-learning-applications-with-in-depth-memory-system-traffic-analysis",normalizedTitle:"1.delta: gpu performance model for deep learning applications with in-depth memory system traffic analysis",charIndex:263},{level:3,title:"2.Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]",slug:"_2-lost-in-abstraction-pitfalls-of-analyzing-gpus-at-the-intermediate-language-level-hpca",normalizedTitle:"2.lost in abstraction: pitfalls of analyzing gpus at the intermediate language level [hpca]",charIndex:401}],headersStr:"1.DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis 2.Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]",content:" 1. DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis [Citation 39]\n 2. Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]\n\n----------------------------------------\n\n\n# 1.DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis\n\nStill reading in process.\n\n\n# 2.Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]\n\nDone. Gem5 GPU Introduction.\n\nReference Materials\n\nAMD_gem5_APU_simulator_isca_2018_gem5_wiki.pdf",normalizedContent:" 1. delta: gpu performance model for deep learning applications with in-depth memory system traffic analysis [citation 39]\n 2. lost in abstraction: pitfalls of analyzing gpus at the intermediate language level [hpca]\n\n----------------------------------------\n\n\n# 1.delta: gpu performance model for deep learning applications with in-depth memory system traffic analysis\n\nstill reading in process.\n\n\n# 2.lost in abstraction: pitfalls of analyzing gpus at the intermediate language level [hpca]\n\ndone. gem5 gpu introduction.\n\nreference materials\n\namd_gem5_apu_simulator_isca_2018_gem5_wiki.pdf",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Architectural Survey",frontmatter:{title:"Architectural Survey",date:"2024-03-30T00:00:00.000Z",permalink:"/pages/458722/",tags:[null]},regularPath:"/03.gpu/10.%20Architectural%20Survey.html",relativePath:"03.gpu/10. Architectural Survey.md",key:"v-a0731602",path:"/pages/458722/",headers:[{level:2,title:"Control flow divergence",slug:"control-flow-divergence",normalizedTitle:"control flow divergence",charIndex:643},{level:3,title:"1. Regrouping Divergent warps",slug:"_1-regrouping-divergent-warps",normalizedTitle:"1. regrouping divergent warps",charIndex:1633},{level:3,title:"2.  Large Warp/CTA compaction",slug:"_2-large-warp-cta-compaction",normalizedTitle:"2.  large warp/cta compaction",charIndex:null},{level:3,title:"3. Multi-path execution",slug:"_3-multi-path-execution",normalizedTitle:"3. multi-path execution",charIndex:2727},{level:3,title:"4. MIMD-like architecture",slug:"_4-mimd-like-architecture",normalizedTitle:"4. mimd-like architecture",charIndex:2815},{level:3,title:"5. Dynamic kernels/threads",slug:"_5-dynamic-kernels-threads",normalizedTitle:"5. dynamic kernels/threads",charIndex:3221},{level:2,title:"Efficient utilization of memory bandwidth",slug:"efficient-utilization-of-memory-bandwidth",normalizedTitle:"efficient utilization of memory bandwidth",charIndex:5054},{level:3,title:"1. Alleviating cache thrashing, and resource contention",slug:"_1-alleviating-cache-thrashing-and-resource-contention",normalizedTitle:"1. alleviating cache thrashing, and resource contention",charIndex:5100},{level:3,title:"2. High-bandwidth many-thread-aware memory hierarchy",slug:"_2-high-bandwidth-many-thread-aware-memory-hierarchy",normalizedTitle:"2. high-bandwidth many-thread-aware memory hierarchy",charIndex:11365},{level:2,title:"Increasing parallelism and improving execution pipelining",slug:"increasing-parallelism-and-improving-execution-pipelining",normalizedTitle:"increasing parallelism and improving execution pipelining",charIndex:14420},{level:3,title:"1. Reducing resource fragmentation and increasing parallelism",slug:"_1-reducing-resource-fragmentation-and-increasing-parallelism",normalizedTitle:"1. reducing resource fragmentation and increasing parallelism",charIndex:15371},{level:3,title:"2. GPU multitasking",slug:"_2-gpu-multitasking",normalizedTitle:"2. gpu multitasking",charIndex:15701},{level:3,title:"3. Exploiting scalar and value similarity opportunities",slug:"_3-exploiting-scalar-and-value-similarity-opportunities",normalizedTitle:"3. exploiting scalar and value similarity opportunities",charIndex:16480},{level:3,title:"4. Improving execution pipelining",slug:"_4-improving-execution-pipelining",normalizedTitle:"4. improving execution pipelining",charIndex:17379},{level:2,title:"Enhancing GPGPU programmability",slug:"enhancing-gpgpu-programmability",normalizedTitle:"enhancing gpgpu programmability",charIndex:18458},{level:3,title:"1. Coherence and consistency model",slug:"_1-coherence-and-consistency-model",normalizedTitle:"1. coherence and consistency model",charIndex:18496},{level:3,title:"2. Transactional memory",slug:"_2-transactional-memory",normalizedTitle:"2. transactional memory",charIndex:18795},{level:3,title:"3. Deterministic GPU",slug:"_3-deterministic-gpu",normalizedTitle:"3. deterministic gpu",charIndex:19057},{level:3,title:"4. Memory management",slug:"_4-memory-management",normalizedTitle:"4. memory management",charIndex:19082},{level:2,title:"CPU–GPU heterogeneous architecture",slug:"cpu-gpu-heterogeneous-architecture",normalizedTitle:"cpu–gpu heterogeneous architecture",charIndex:19645},{level:3,title:"1. Impacts of CPU–GPU integration",slug:"_1-impacts-of-cpu-gpu-integration",normalizedTitle:"1. impacts of cpu–gpu integration",charIndex:19684},{level:3,title:"2. CPU–GPU programmability",slug:"_2-cpu-gpu-programmability",normalizedTitle:"2. cpu–gpu programmability",charIndex:20168},{level:3,title:"3. Exploiting heterogeneity",slug:"_3-exploiting-heterogeneity",normalizedTitle:"3. exploiting heterogeneity",charIndex:20369},{level:3,title:"4. Shared resources management",slug:"_4-shared-resources-management",normalizedTitle:"4. shared resources management",charIndex:20696}],headersStr:"Control flow divergence 1. Regrouping Divergent warps 2.  Large Warp/CTA compaction 3. Multi-path execution 4. MIMD-like architecture 5. Dynamic kernels/threads Efficient utilization of memory bandwidth 1. Alleviating cache thrashing, and resource contention 2. High-bandwidth many-thread-aware memory hierarchy Increasing parallelism and improving execution pipelining 1. Reducing resource fragmentation and increasing parallelism 2. GPU multitasking 3. Exploiting scalar and value similarity opportunities 4. Improving execution pipelining Enhancing GPGPU programmability 1. Coherence and consistency model 2. Transactional memory 3. Deterministic GPU 4. Memory management CPU–GPU heterogeneous architecture 1. Impacts of CPU–GPU integration 2. CPU–GPU programmability 3. Exploiting heterogeneity 4. Shared resources management",content:' 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity\n\n----------------------------------------\n\n\n# 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity\n\nFour major improvement\n\n * mitigating the impact of control flow divergence\n * alleviating resource contention and efficient utilization of memory bandwidth across the entire memory hierarchy, including caches, interconnection and main memory\n * increasing the available parallelism and concurrency\n * improving pipeline execution and exploiting scalarization opportunities.\n\n\n\n\n# Control flow divergence\n\n 1. First, GPUs employ PDOM stack-based mechanism that serializes the execution of divergent paths. This serialization of divergent paths reduces the available thread level parallelism (i.e., the number of active warps at a time) which limits the ability of GPUs to hide long memory instruction latency.\n 2. Control divergence limits the number of active threads in the running warps. As a result, SIMD execution units are not efficiently utilized when a diverged warp is executed.\n 3. Control divergence may also lead memory divergence wherein threads in the same warp access different regions of memory and thus the memory coalescing unit fails to reduce memory requests. Memory divergence causes huge pressure on memory resources and leads long memory latency and performance degradation.\n 4. Irregular applications tend to cause workload imbalance in such a way that assigned work (i.e., active threads per CTAs) to some GPU cores are larger than others.\n\n\n\n\n# 1. Regrouping Divergent warps\n\n\nInstead, DWF dynamically re-forms divergent warps into new non-divergent warps on the fly.\nMoreover, DWF does not reconverge diverged warp at IPDOM in order to amortize coalesced memory address of converged warps.\n\n\n\n\n\n\n# 2. Large Warp/CTA compaction\n\n * Thread Block Compaction (TBC)\n   Allows a group of warps, that belong to the same thread block, to share the same PDOM stack.\n   However, TBC stalls all warps within a CTA on any potentially divergent branch until all warps reach the branch point.\n   \n   \n\nThe major difference between 1) and TBC is that 1) can only merge threads in a warp when they are ready in a queue. Thus it miss some potentials.\n\nTBC replace per-warp convergence stack with in-threadblock stack.\n\n * CAPRI\n   CAPRI dynamically identifies the compaction effectiveness of a branch and only stalls threads that are predicted to benefit from compaction.\n   \n\n * SLP proposed SIMD lane permutation (SLP) as an optimization to expand the applicability of compaction in case of conventional compaction technique is ineffective.\n   \n   \n\n\n# 3. Multi-path execution\n\n\n * DPS\n   Dual-path Stack\n   \n * Multi-path Execution\n   \n\n\n# 4. MIMD-like architecture\n\n\nRogers et al. [194] observed that regular applications perform better with a wider warp size, whereas divergent applications achieve better performance with a smaller warp size.\nVWS groups sets of these smaller warps together by ganging their execution in the warp scheduler and thus amortizing the energy consumed by fetch, decode, and warp scheduling across more threads.\n\n\n# 5. Dynamic kernels/threads\n\n\nRelated Paper: Characterization and Analysis of Dynamic Parallelism in Unstructured GPU Applications. [108]\nDynamic Thread Block Launch: A Lightweight Execution Mechanism to Support Irregular Applications on GPUs. [85]\nBy wang jing NVIDIA\n\n👍 👍 👍 These two paper has very thorough explanation of how kernels are launched, kernel parameters are gained and how thread create subkernel.\n\n\n\n\n\nCUDA enables dynamic parallsim, creating subkernels from each thread.\n\n> Copied from "Characterization"\n> \n> When a child kernel is launched, the parameter buffer pointer of the kernel is retrieved through the device runtime API cudaGetParameterBuffer.\n> \n> Then the argument values are stored in the parameter buffer and the kernel is launched by calling cudaLaunchDevice.\n> \n> After that, the device runtime manager appends the child kernels to an execution queue and dispatches the kernel to SMXs according to a certain scheduling policy.\n> \n> The CDP kernel launching overhead comprises of kernel parameter parsing, calling cudaGetParameterBuffer and cudaLaunchDevice, as well as the process that device runtime manager setups,enqueues, manages and dispatches the child kernels.\n> \n> however, the huge kernel launching overhead could negate the performance benefit of DFP. The overhead is due to the large number of launched kernels, the associated memory footprint and the low number of running warps per core.The CPU launches GPU kernels by dispatching kernel launching commands. Kernel parameters are passed from CPU to GPU at the kernel launching time and stored in the GPU global.\n> \n> Wang et al. [236] proposed new mechanism, called Dynamic Thread Block Launch (DTBL), that employs light-weight thread block rather than heavy-weight device kernel for DFP.\n\n----------------------------------------\n\n\n# Efficient utilization of memory bandwidth\n\n\n# 1. Alleviating cache thrashing, and resource contention\n\n# 1. Two-level warp scheduling\n\n * TLRR\n   \n\nThey proposed two-level round-robin warp scheduling (TL-RR), in which the warps are split into fetch groups.\nTL-RR executes only one fetch group at a time and it schedules warps from the same fetch group in a round-robin fashion.\nWhen the running warps reach a long latency operation, then the next fetch group is prioritized.\nThey try to alleviate the issue of threads in all warps arrive the same memory latency instruction at the same time.\n\n\n\n\n\n\n * OWL OWL augments the TL-RR with CTA-awareness, such that warps are split into groups of CTAs basis rather than warps basis, resulting in increased intra-CTA locality.\n   OWL gives a group of CTAs higher priority when their data exist at the L1 cache such that they get the opportunity to reuse it, therefore improving L1 hit rates and alleviating cache contention.\n   \n\n# 2. Coarse-grained CTA throttling\n\n\n\n\n\nDYNCTA\nNeither More Nor Less: Optimizing Thread-level Parallelism for GPGPUs\n👍 👍 Illustrated CTA and WARP mapping.\n\n * Always executing the maximum possible number of CTAs on a GPU core (i.e., increasing TLP to the maximum) does not always lead to better performance.\n * To alleviate resource contention, they proposed dynamic CTA scheduling mechanism (DYNCTA), which aims to allocate the optimal number of CTAs per GPU core that alleviate memory contention according to an application characteristics.\n * DYNCTA dynamically adjusts over sampling periods the number of active CTAs per GPU core that reduces the memory latency without sacrificing the available TLP.\n\nLCS\nIn contrast to DYNCTA that monitors the workload behavior for the entire kernel execution, LCS leverages GTO scheduler to find the optimal number of thread blocks at the early beginning of kernel execution.\n\n# 3. Fine-grained warp throttling\n\ndue to the massive multithreading and the limited capacity of L1 cache, divergent GPGPU applications cause severe cache contention.\n\n\n * CCWS\n   uses a victim tag array, called lost locality detector, to detect warps that have lost locality due to thrashing. These warps are prioritized till they exploit their locality while other warps are descheduled.\n   \n * DAWS\n   DAWS is a divergence-based cache footprint predictor to calculate the amount of locality in loops required by each warp.\n   DAWS uses these predictions to prioritize a group of warps such that the cache footprint of these warps do not exceed the capacity of the L1 cache.\n   \n\n# 4. Throttling and cache bypassing\n\nprevious CTA or warp throttling techniques leave memory bandwidth and other chip resources (L2 cache, interconnection and execution units) significantly underutilized.\n\n * PCAL\n   At the beginning of kernel execution, PCAL executes an optimal number of active warps, that alleviates thrashing and conflicts, then extra inactive warps are allowed to bypass cache and utilize the other on-chip resources. Thus, PCAL reduces cache thrashing and effectively utilizes the chip resources that would otherwise go unused by a pure thread throttling approach.\n * CCA\n   CCA improves DAWS by allowing extra inactive warps and some streaming memory instructions from the active warps to bypass the L1 cache and utilize on-chip resources.\n\n# 5. Critical warp awareness\n\nsome warps may be assigned more workload and exhibit longer latency compared to other warps within the same Thread Block. Hence, fast warps are idle at a synchronization barrier or at the end of kernel execution until the critical (i.e., the slowest) warp finishes execution. Thus, the overall execution time is dominated by the performance of these critical warps.\n\nCAWA dynamically identifies critical warps and coordinates warp scheduling and cache prioritization to accelerate the critical warp execution.\n\n * Workload Imbalance In a GPGPU kernel function, tasks are not always uniformly distributed to each thread/warp, and thereby some threads/warps have heavier workloads than others. Intuitively, the threads/warps with heavier workloads require longer time to process their tasks. Consequently, warps with heavier workloads often become the slowest running/critical warps.\n * Diverging Branch Behavior At runtime, warps can undergo different con trol paths leading to different number of dynamic instructions across different warps. This problem could be worsened if threads in a warp also take diverging control paths, i.e., the branch divergence problem, leading to a larger instruction execution gap between warps.\n * Contention in the Memory Subsystem Jog et al. ob served that the memory subsystem has a significant impact on GPGPU applications [15, 16]. . Jia et al. also pointed out that interference in the Ll data cache as well as in the interconnection between the Ll data caches and the L2 cache are the major factors that limit GPU performance.More than 60% of the cache blocks that could be reused by the slower-running, critical warps are evicted before the re-references by the critical warps.\n * Latency Introduced by the Warp Scheduler Because of the particular warp execution order determined by the scheduler, when a warp becomes ready for execution, it can experience up to N cycles of scheduling delay, where N represents the number of warps.\n\n# 6. Cache management and bypassing\n\nGCache To detect thrashing, they equip L2 cache tag array with extra bits (victim bits) to provide L1 cache with some information about the hot lines that have been evicted before. An adaptive cache replacement policy is used by L1 cache to protect these hot lines.\n\n# 7. Ordering buffers\n\nThe idea of MRPB is two-fold.\nFirst, a FIFO requests buffer is used to reorder memory references so that requests from the same warp are grouped and sent to the cache together in a more cache-friendly order. This results in drastically reducing cache contention and improving use of the limited per-thread cache capacity.\nSecond, MRPB allows memory request that encounters associativity stall to bypass L1 cache.\n\n\n\n\n\n\n# 8. Resource tuning\n\nEqualizer, a dynamic runtime system that tunes number of thread blocks, core and memory frequency to match the requirements of the running kernel, leading to efficient execution and energy saving.\n\n\n# 2. High-bandwidth many-thread-aware memory hierarchy\n\n# 1. Mitigating off-chip bandwidth bottleneck\n\nLAMAR Emerging irregular workloads benefit from fine-grain (FG) memory access by avoiding unnecessary data transfers, that may be happened under CG policy,\n\nthey proposed a locality-aware memory hierarchy (LAMAR) that adaptively tunes the memory access granularity for the running kernel.\n\nLAMAR employs CG accesses for kernels with high temporal and spatial locality, while applying FG accesses for irregular divergent workloads in attempt to reduce memory over-fetching.\n\nCABA Vijaykumar et al. [231] proposed, Core-Assisted Bottleneck Acceleration (CABA) framework, that exploits the underutilized computational resources to perform useful work and alleviate different bottlenecks in GPU execution.\n\nFor instance, to alleviate memory bandwidth bottleneck, CABA dynamically creates assist warps that execute with the original warps side by side on the same GPU core.\n\nAssist warps opportunistically use idle computational units to perform data decompression for the incoming compressed cache blocks and compression for the outgoing cache blocks, leading to less transferring data from memory and mitigating memory bandwidth problem.\n\nApproximation An approximation technique in which the GPU drops some portion of load requests which miss in the cache after approximating their values.\n\n# 2. Memory divergence normalization\n\nOrchestrated Scheduling and Prefetching for GPGPUs\n\nthey proposed prefetch-aware warp scheduling, that coordinates simple data prefetcher and warp scheduling in an intelligent manner such that the scheduling of two consecutive warps are separated in time, and thus prefetching becomes more effective.\n\n\n\n# 3. Interconnection network\n\n# 4. Main memory scheduling\n\ninterconnection network which is between cores and memory controllers can destroy memory access row-buffer locality.\n\nTo reserve row locality and reduce complexity circuit design of FR-FCFS DRAM controller, they employ an interconnection arbitration scheme to prioritize memory requests accessing the same row first.\n\n# 5. Heterogeneous memory management\n\nAgarwal et al. [5] showed that applying traditional Linux page placements policies, which have been used for CPUonly NUMA systems and aim to minimize the memory request latency, may not be effective in CPU–GPU NUMA systems. This is due to the fact that GPU performance is more sensitive to memory bandwidth.\n\nBandwidth-aware placement that maximizes GPU performance by balancing page placement across the memories based on the aggregate memory bandwidth available in a system.\n\n# 6. CPU–GPU memory transfer overhead\n\nfine-grained CPU–GPU synchronization enabled by a hardware-managed full-empty bits to track when regions of data have been transferred.\n\nThus, the GPU is able to start execution once the required block of data is available.\n\nSoftware level APIs are proposed to allow programmer to launch kernel earlier and overlap data transfer with execution.\n\n----------------------------------------\n\n\n# Increasing parallelism and improving execution pipelining\n\nSome applications have a low number of active thread blocks due to the small input size or the unavailability of some required resources in SM (e.g. registers or shared memory), thus they fail to efficiently utilize the execution units. This results in inefficient utilization of execution unit and hinders the GPU ability to hide long memory latency.\n\n\nPrevious works proposed new techniques in order to reduce resource fragmentation and run the maximum number of warps per core.\n\n\nFurther, other approaches proposed running multiple applications on the same GPU to exploit these underutilized resources and increase overall throughput.\n\n\nAnother way to improve execution efficiency and increase parallelism is to exploit scalar opportunities and value similarity between the running warps such that scalar instructions can be executed con currently along with other SIMT instructions.\n\n\n\n# 1. Reducing resource fragmentation and increasing parallelism\n\n👍 👍 👍\n\nUnifyingthey proposed a unified local memory which integrates the register file, L1 cache, and scratchpad memory into one large on-chip storage. Then, the hardware can dynamically partition the on-chip storage according to each application’s needs.\n\n\n\n\n\n\n# 2. GPU multitasking\n\nBetter Utilization and Virtualization\n\n\n * multiple applica tions execute simultaneously on different cores within the same GPU substrate.\n   \n * mixed concurrent kernels execution, in which two applications execute concurrently on the same core. especially, mixture of memory-intensive and compute-intensive workloads\n   \n\nDefault strategy may cause high-priority application suffering from a long latency to execute. a task preemption strategy is required to improve GPU ultitasking\n\n * context switching and draining\n * To further reduce preemption latency, Park et al. [178] intro duced core flushing which drops an execution of a thread block without context saving and re-executes the dropped thread block from the beginning when it is relaunched.\n\n\n# 3. Exploiting scalar and value similarity opportunities\n\nmany GPGPU workloads have scalar instructions in which computation is identical across mul tiple threads within the same warp instruction (i.e., operands are identical for all the threads in a warp).\n\nmodern GPU mi croarchitecture, like AMD’s GCN [10], leverages these scalar op portunities by statically detecting scalar instructions and executing them on a separate scalar unit attached with each GPU core.\n\nA vector is defined as an affine, when the vector contains a consecutive strided values, i.e., the vector values can be represented as V(i) = b + i ∗ s, where b is the base, s is the stride and i is the thread index.\n\n👍 👍 👍 Microarchitectural mechanisms to exploit value structure in SIMT architectures\n\nThis Paper has detailed explanation of microarchitecture in gpu execution core, how ALU and register files operates.\n\n\n\n\n\n\n# 4. Improving execution pipelining\n\nmany GPGPU applications do not have enough active threads that are ready to issue instructions and hide short read-after-write (RAW) dependencies caused by deep execution pipeline stages.\n\n * a low-power forwarded network that can considerably improve the performance of many compute-intensive GPGPU ap plications.\n\n\n\n * improve GPU performance by splitting the existing 32-bit datapath into two 16-bit datapath slices. As a result, the GPU instruction throughput can be increased by issuing dual 16-bit instructions from two different warps in parallel using the sliced 32-bit datapath.\n\n * a pre-execution approach for improving GPU latency hiding and performance by employing run-ahead out of-order execution [158].\n\n\n\n\n\nwhen a warp stalls for a long-latency operation such as off-chip memory accesses, it continues to fetch and pre-execute successive instructions that are not on the long latency dependence chain resulting in hiding processing delay of operations and performance improvement.\n\n----------------------------------------\n\n\n# Enhancing GPGPU programmability\n\n\n\n\n# 1. Coherence and consistency model\n\nCurrent GPUs lack hardware cache coherence and require dis abling of private L1 caches or employing software-based bulk coherence decisions (i.e., flush/invalidate all private L1 caches at synchronization points) if an application needs coherent memory view.\n\n\n# 2. Transactional memory\n\nKILO TM does not rely on cache coherence nor global atomic operations.\n\nInstead, it detects conflicts via a fine-grain value-based approach that supports thousands of concurrent transactions and requires negligible storage overhead.\n\n\n# 3. Deterministic GPU\n\n\n# 4. Memory management\n\nKim et al. [109] proposed GPUdmm, a high-performance dynamic memory management for GPU architecture. GPUdmm enables dynamic memory management for discrete GPU environ ments by using GPU memory as a cache of CPU memory with on demand CPU–GPU data transfers.\n\n\n\nPichai et al. [183] 👍 👍 👍 augmenting CCWS and TBC with TLB-awareness and a few simple adjustments can recover most of this lost performance and move address translation overheads into a range considered acceptable in the CPU world.\n\n----------------------------------------\n\n\n# CPU–GPU heterogeneous architecture\n\n\n# 1. Impacts of CPU–GPU integration\n\nremaining CPU code tends to have lower instruction-level parallelism (ILP), more complex load/store operations to prefetch and more difficult branch rediction.\n\n\nFurther, the serial code will not benefit significantly from SIMD instructions or increasing the number of CPU cores, owing to the limited availability of thread level parallelism (TLP) and data-level parallelism (DLP) that will be already captured and exploited by the GPU instead.\n\n\n# 2. CPU–GPU programmability\n\nHeterogeneous System Coherence for Integrated CPU-GPU Systems\n\nthey replace the fine-grained 64B-block-level directory with a coarse-grained 1KB-region-level directory.\n\n\n# 3. Exploiting heterogeneity\n\nCOMPASS uses idle GPU core resources to act as data prefetchers for CPU execution and success fully improve the memory performance of single-thread applications.\nWoo and Lee [247] proposed to collaboratively utilize CPU resources to act as programmable data prefetchers for GPGPU applications.\n\n\n# 4. Shared resources management\n\nTwo kinds of approaches have been explored to mitigate inter ference:\n\n * application-aware resource management\n * throttling based management.\n\n\n\nSMS decouples memory controller into three stages.\n\n\n * The first stage of SMS groups requests based on row buffer locality.\n * At the second stage, SMS ensures fairness between CPU and GPU memory requests by applying CPU-biased shortest job first scheduling policy or GPU-biased round robin scheduling policy. A dynamically configurable parameter is used to select between the two policies based on the system’s needs.\n * The last stage consists of simple per-bank FIFO queue to issue low-level memory commands.\n\nTAP: A TLP-Aware Cache Management Policy for a CPU-GPU Heterogeneous\n\n\n\n * A core-sampling technique, which applies a different cache management policy to each GPU core and regularly collects statistics on the performance\n   of these cores to see how these polices affect GPU applications.\n   \n * GPU cores typically access caches much more frequently than CPU cores.\n   \n * enforces a similar cache lifetime to both CPU and GPGPU appli cations and prevent GPGPU application to monopolize the shared cache.\n\nOne (CM-CPU) for boosting CPU performance in the presence of GPU interference.\nThe other (CM-BAL) for improving both CPU and GPU performance in a balanced manner and thus overall system performance.\n\n\n\n\npropose GPU concurrency management that dynamically throttles/boosts TLP (i.e., number of active warps) of GPU cores in order to minimize shared resources interference between CPU and GPU.',normalizedContent:' 1. a survey of architectural approaches for improving gpgpu performance, programmability and heterogeneity\n\n----------------------------------------\n\n\n# 1. a survey of architectural approaches for improving gpgpu performance, programmability and heterogeneity\n\nfour major improvement\n\n * mitigating the impact of control flow divergence\n * alleviating resource contention and efficient utilization of memory bandwidth across the entire memory hierarchy, including caches, interconnection and main memory\n * increasing the available parallelism and concurrency\n * improving pipeline execution and exploiting scalarization opportunities.\n\n\n\n\n# control flow divergence\n\n 1. first, gpus employ pdom stack-based mechanism that serializes the execution of divergent paths. this serialization of divergent paths reduces the available thread level parallelism (i.e., the number of active warps at a time) which limits the ability of gpus to hide long memory instruction latency.\n 2. control divergence limits the number of active threads in the running warps. as a result, simd execution units are not efficiently utilized when a diverged warp is executed.\n 3. control divergence may also lead memory divergence wherein threads in the same warp access different regions of memory and thus the memory coalescing unit fails to reduce memory requests. memory divergence causes huge pressure on memory resources and leads long memory latency and performance degradation.\n 4. irregular applications tend to cause workload imbalance in such a way that assigned work (i.e., active threads per ctas) to some gpu cores are larger than others.\n\n\n\n\n# 1. regrouping divergent warps\n\n\ninstead, dwf dynamically re-forms divergent warps into new non-divergent warps on the fly.\nmoreover, dwf does not reconverge diverged warp at ipdom in order to amortize coalesced memory address of converged warps.\n\n\n\n\n\n\n# 2. large warp/cta compaction\n\n * thread block compaction (tbc)\n   allows a group of warps, that belong to the same thread block, to share the same pdom stack.\n   however, tbc stalls all warps within a cta on any potentially divergent branch until all warps reach the branch point.\n   \n   \n\nthe major difference between 1) and tbc is that 1) can only merge threads in a warp when they are ready in a queue. thus it miss some potentials.\n\ntbc replace per-warp convergence stack with in-threadblock stack.\n\n * capri\n   capri dynamically identifies the compaction effectiveness of a branch and only stalls threads that are predicted to benefit from compaction.\n   \n\n * slp proposed simd lane permutation (slp) as an optimization to expand the applicability of compaction in case of conventional compaction technique is ineffective.\n   \n   \n\n\n# 3. multi-path execution\n\n\n * dps\n   dual-path stack\n   \n * multi-path execution\n   \n\n\n# 4. mimd-like architecture\n\n\nrogers et al. [194] observed that regular applications perform better with a wider warp size, whereas divergent applications achieve better performance with a smaller warp size.\nvws groups sets of these smaller warps together by ganging their execution in the warp scheduler and thus amortizing the energy consumed by fetch, decode, and warp scheduling across more threads.\n\n\n# 5. dynamic kernels/threads\n\n\nrelated paper: characterization and analysis of dynamic parallelism in unstructured gpu applications. [108]\ndynamic thread block launch: a lightweight execution mechanism to support irregular applications on gpus. [85]\nby wang jing nvidia\n\n👍 👍 👍 these two paper has very thorough explanation of how kernels are launched, kernel parameters are gained and how thread create subkernel.\n\n\n\n\n\ncuda enables dynamic parallsim, creating subkernels from each thread.\n\n> copied from "characterization"\n> \n> when a child kernel is launched, the parameter buffer pointer of the kernel is retrieved through the device runtime api cudagetparameterbuffer.\n> \n> then the argument values are stored in the parameter buffer and the kernel is launched by calling cudalaunchdevice.\n> \n> after that, the device runtime manager appends the child kernels to an execution queue and dispatches the kernel to smxs according to a certain scheduling policy.\n> \n> the cdp kernel launching overhead comprises of kernel parameter parsing, calling cudagetparameterbuffer and cudalaunchdevice, as well as the process that device runtime manager setups,enqueues, manages and dispatches the child kernels.\n> \n> however, the huge kernel launching overhead could negate the performance benefit of dfp. the overhead is due to the large number of launched kernels, the associated memory footprint and the low number of running warps per core.the cpu launches gpu kernels by dispatching kernel launching commands. kernel parameters are passed from cpu to gpu at the kernel launching time and stored in the gpu global.\n> \n> wang et al. [236] proposed new mechanism, called dynamic thread block launch (dtbl), that employs light-weight thread block rather than heavy-weight device kernel for dfp.\n\n----------------------------------------\n\n\n# efficient utilization of memory bandwidth\n\n\n# 1. alleviating cache thrashing, and resource contention\n\n# 1. two-level warp scheduling\n\n * tlrr\n   \n\nthey proposed two-level round-robin warp scheduling (tl-rr), in which the warps are split into fetch groups.\ntl-rr executes only one fetch group at a time and it schedules warps from the same fetch group in a round-robin fashion.\nwhen the running warps reach a long latency operation, then the next fetch group is prioritized.\nthey try to alleviate the issue of threads in all warps arrive the same memory latency instruction at the same time.\n\n\n\n\n\n\n * owl owl augments the tl-rr with cta-awareness, such that warps are split into groups of ctas basis rather than warps basis, resulting in increased intra-cta locality.\n   owl gives a group of ctas higher priority when their data exist at the l1 cache such that they get the opportunity to reuse it, therefore improving l1 hit rates and alleviating cache contention.\n   \n\n# 2. coarse-grained cta throttling\n\n\n\n\n\ndyncta\nneither more nor less: optimizing thread-level parallelism for gpgpus\n👍 👍 illustrated cta and warp mapping.\n\n * always executing the maximum possible number of ctas on a gpu core (i.e., increasing tlp to the maximum) does not always lead to better performance.\n * to alleviate resource contention, they proposed dynamic cta scheduling mechanism (dyncta), which aims to allocate the optimal number of ctas per gpu core that alleviate memory contention according to an application characteristics.\n * dyncta dynamically adjusts over sampling periods the number of active ctas per gpu core that reduces the memory latency without sacrificing the available tlp.\n\nlcs\nin contrast to dyncta that monitors the workload behavior for the entire kernel execution, lcs leverages gto scheduler to find the optimal number of thread blocks at the early beginning of kernel execution.\n\n# 3. fine-grained warp throttling\n\ndue to the massive multithreading and the limited capacity of l1 cache, divergent gpgpu applications cause severe cache contention.\n\n\n * ccws\n   uses a victim tag array, called lost locality detector, to detect warps that have lost locality due to thrashing. these warps are prioritized till they exploit their locality while other warps are descheduled.\n   \n * daws\n   daws is a divergence-based cache footprint predictor to calculate the amount of locality in loops required by each warp.\n   daws uses these predictions to prioritize a group of warps such that the cache footprint of these warps do not exceed the capacity of the l1 cache.\n   \n\n# 4. throttling and cache bypassing\n\nprevious cta or warp throttling techniques leave memory bandwidth and other chip resources (l2 cache, interconnection and execution units) significantly underutilized.\n\n * pcal\n   at the beginning of kernel execution, pcal executes an optimal number of active warps, that alleviates thrashing and conflicts, then extra inactive warps are allowed to bypass cache and utilize the other on-chip resources. thus, pcal reduces cache thrashing and effectively utilizes the chip resources that would otherwise go unused by a pure thread throttling approach.\n * cca\n   cca improves daws by allowing extra inactive warps and some streaming memory instructions from the active warps to bypass the l1 cache and utilize on-chip resources.\n\n# 5. critical warp awareness\n\nsome warps may be assigned more workload and exhibit longer latency compared to other warps within the same thread block. hence, fast warps are idle at a synchronization barrier or at the end of kernel execution until the critical (i.e., the slowest) warp finishes execution. thus, the overall execution time is dominated by the performance of these critical warps.\n\ncawa dynamically identifies critical warps and coordinates warp scheduling and cache prioritization to accelerate the critical warp execution.\n\n * workload imbalance in a gpgpu kernel function, tasks are not always uniformly distributed to each thread/warp, and thereby some threads/warps have heavier workloads than others. intuitively, the threads/warps with heavier workloads require longer time to process their tasks. consequently, warps with heavier workloads often become the slowest running/critical warps.\n * diverging branch behavior at runtime, warps can undergo different con trol paths leading to different number of dynamic instructions across different warps. this problem could be worsened if threads in a warp also take diverging control paths, i.e., the branch divergence problem, leading to a larger instruction execution gap between warps.\n * contention in the memory subsystem jog et al. ob served that the memory subsystem has a significant impact on gpgpu applications [15, 16]. . jia et al. also pointed out that interference in the ll data cache as well as in the interconnection between the ll data caches and the l2 cache are the major factors that limit gpu performance.more than 60% of the cache blocks that could be reused by the slower-running, critical warps are evicted before the re-references by the critical warps.\n * latency introduced by the warp scheduler because of the particular warp execution order determined by the scheduler, when a warp becomes ready for execution, it can experience up to n cycles of scheduling delay, where n represents the number of warps.\n\n# 6. cache management and bypassing\n\ngcache to detect thrashing, they equip l2 cache tag array with extra bits (victim bits) to provide l1 cache with some information about the hot lines that have been evicted before. an adaptive cache replacement policy is used by l1 cache to protect these hot lines.\n\n# 7. ordering buffers\n\nthe idea of mrpb is two-fold.\nfirst, a fifo requests buffer is used to reorder memory references so that requests from the same warp are grouped and sent to the cache together in a more cache-friendly order. this results in drastically reducing cache contention and improving use of the limited per-thread cache capacity.\nsecond, mrpb allows memory request that encounters associativity stall to bypass l1 cache.\n\n\n\n\n\n\n# 8. resource tuning\n\nequalizer, a dynamic runtime system that tunes number of thread blocks, core and memory frequency to match the requirements of the running kernel, leading to efficient execution and energy saving.\n\n\n# 2. high-bandwidth many-thread-aware memory hierarchy\n\n# 1. mitigating off-chip bandwidth bottleneck\n\nlamar emerging irregular workloads benefit from fine-grain (fg) memory access by avoiding unnecessary data transfers, that may be happened under cg policy,\n\nthey proposed a locality-aware memory hierarchy (lamar) that adaptively tunes the memory access granularity for the running kernel.\n\nlamar employs cg accesses for kernels with high temporal and spatial locality, while applying fg accesses for irregular divergent workloads in attempt to reduce memory over-fetching.\n\ncaba vijaykumar et al. [231] proposed, core-assisted bottleneck acceleration (caba) framework, that exploits the underutilized computational resources to perform useful work and alleviate different bottlenecks in gpu execution.\n\nfor instance, to alleviate memory bandwidth bottleneck, caba dynamically creates assist warps that execute with the original warps side by side on the same gpu core.\n\nassist warps opportunistically use idle computational units to perform data decompression for the incoming compressed cache blocks and compression for the outgoing cache blocks, leading to less transferring data from memory and mitigating memory bandwidth problem.\n\napproximation an approximation technique in which the gpu drops some portion of load requests which miss in the cache after approximating their values.\n\n# 2. memory divergence normalization\n\norchestrated scheduling and prefetching for gpgpus\n\nthey proposed prefetch-aware warp scheduling, that coordinates simple data prefetcher and warp scheduling in an intelligent manner such that the scheduling of two consecutive warps are separated in time, and thus prefetching becomes more effective.\n\n\n\n# 3. interconnection network\n\n# 4. main memory scheduling\n\ninterconnection network which is between cores and memory controllers can destroy memory access row-buffer locality.\n\nto reserve row locality and reduce complexity circuit design of fr-fcfs dram controller, they employ an interconnection arbitration scheme to prioritize memory requests accessing the same row first.\n\n# 5. heterogeneous memory management\n\nagarwal et al. [5] showed that applying traditional linux page placements policies, which have been used for cpuonly numa systems and aim to minimize the memory request latency, may not be effective in cpu–gpu numa systems. this is due to the fact that gpu performance is more sensitive to memory bandwidth.\n\nbandwidth-aware placement that maximizes gpu performance by balancing page placement across the memories based on the aggregate memory bandwidth available in a system.\n\n# 6. cpu–gpu memory transfer overhead\n\nfine-grained cpu–gpu synchronization enabled by a hardware-managed full-empty bits to track when regions of data have been transferred.\n\nthus, the gpu is able to start execution once the required block of data is available.\n\nsoftware level apis are proposed to allow programmer to launch kernel earlier and overlap data transfer with execution.\n\n----------------------------------------\n\n\n# increasing parallelism and improving execution pipelining\n\nsome applications have a low number of active thread blocks due to the small input size or the unavailability of some required resources in sm (e.g. registers or shared memory), thus they fail to efficiently utilize the execution units. this results in inefficient utilization of execution unit and hinders the gpu ability to hide long memory latency.\n\n\nprevious works proposed new techniques in order to reduce resource fragmentation and run the maximum number of warps per core.\n\n\nfurther, other approaches proposed running multiple applications on the same gpu to exploit these underutilized resources and increase overall throughput.\n\n\nanother way to improve execution efficiency and increase parallelism is to exploit scalar opportunities and value similarity between the running warps such that scalar instructions can be executed con currently along with other simt instructions.\n\n\n\n# 1. reducing resource fragmentation and increasing parallelism\n\n👍 👍 👍\n\nunifyingthey proposed a unified local memory which integrates the register file, l1 cache, and scratchpad memory into one large on-chip storage. then, the hardware can dynamically partition the on-chip storage according to each application’s needs.\n\n\n\n\n\n\n# 2. gpu multitasking\n\nbetter utilization and virtualization\n\n\n * multiple applica tions execute simultaneously on different cores within the same gpu substrate.\n   \n * mixed concurrent kernels execution, in which two applications execute concurrently on the same core. especially, mixture of memory-intensive and compute-intensive workloads\n   \n\ndefault strategy may cause high-priority application suffering from a long latency to execute. a task preemption strategy is required to improve gpu ultitasking\n\n * context switching and draining\n * to further reduce preemption latency, park et al. [178] intro duced core flushing which drops an execution of a thread block without context saving and re-executes the dropped thread block from the beginning when it is relaunched.\n\n\n# 3. exploiting scalar and value similarity opportunities\n\nmany gpgpu workloads have scalar instructions in which computation is identical across mul tiple threads within the same warp instruction (i.e., operands are identical for all the threads in a warp).\n\nmodern gpu mi croarchitecture, like amd’s gcn [10], leverages these scalar op portunities by statically detecting scalar instructions and executing them on a separate scalar unit attached with each gpu core.\n\na vector is defined as an affine, when the vector contains a consecutive strided values, i.e., the vector values can be represented as v(i) = b + i ∗ s, where b is the base, s is the stride and i is the thread index.\n\n👍 👍 👍 microarchitectural mechanisms to exploit value structure in simt architectures\n\nthis paper has detailed explanation of microarchitecture in gpu execution core, how alu and register files operates.\n\n\n\n\n\n\n# 4. improving execution pipelining\n\nmany gpgpu applications do not have enough active threads that are ready to issue instructions and hide short read-after-write (raw) dependencies caused by deep execution pipeline stages.\n\n * a low-power forwarded network that can considerably improve the performance of many compute-intensive gpgpu ap plications.\n\n\n\n * improve gpu performance by splitting the existing 32-bit datapath into two 16-bit datapath slices. as a result, the gpu instruction throughput can be increased by issuing dual 16-bit instructions from two different warps in parallel using the sliced 32-bit datapath.\n\n * a pre-execution approach for improving gpu latency hiding and performance by employing run-ahead out of-order execution [158].\n\n\n\n\n\nwhen a warp stalls for a long-latency operation such as off-chip memory accesses, it continues to fetch and pre-execute successive instructions that are not on the long latency dependence chain resulting in hiding processing delay of operations and performance improvement.\n\n----------------------------------------\n\n\n# enhancing gpgpu programmability\n\n\n\n\n# 1. coherence and consistency model\n\ncurrent gpus lack hardware cache coherence and require dis abling of private l1 caches or employing software-based bulk coherence decisions (i.e., flush/invalidate all private l1 caches at synchronization points) if an application needs coherent memory view.\n\n\n# 2. transactional memory\n\nkilo tm does not rely on cache coherence nor global atomic operations.\n\ninstead, it detects conflicts via a fine-grain value-based approach that supports thousands of concurrent transactions and requires negligible storage overhead.\n\n\n# 3. deterministic gpu\n\n\n# 4. memory management\n\nkim et al. [109] proposed gpudmm, a high-performance dynamic memory management for gpu architecture. gpudmm enables dynamic memory management for discrete gpu environ ments by using gpu memory as a cache of cpu memory with on demand cpu–gpu data transfers.\n\n\n\npichai et al. [183] 👍 👍 👍 augmenting ccws and tbc with tlb-awareness and a few simple adjustments can recover most of this lost performance and move address translation overheads into a range considered acceptable in the cpu world.\n\n----------------------------------------\n\n\n# cpu–gpu heterogeneous architecture\n\n\n# 1. impacts of cpu–gpu integration\n\nremaining cpu code tends to have lower instruction-level parallelism (ilp), more complex load/store operations to prefetch and more difficult branch rediction.\n\n\nfurther, the serial code will not benefit significantly from simd instructions or increasing the number of cpu cores, owing to the limited availability of thread level parallelism (tlp) and data-level parallelism (dlp) that will be already captured and exploited by the gpu instead.\n\n\n# 2. cpu–gpu programmability\n\nheterogeneous system coherence for integrated cpu-gpu systems\n\nthey replace the fine-grained 64b-block-level directory with a coarse-grained 1kb-region-level directory.\n\n\n# 3. exploiting heterogeneity\n\ncompass uses idle gpu core resources to act as data prefetchers for cpu execution and success fully improve the memory performance of single-thread applications.\nwoo and lee [247] proposed to collaboratively utilize cpu resources to act as programmable data prefetchers for gpgpu applications.\n\n\n# 4. shared resources management\n\ntwo kinds of approaches have been explored to mitigate inter ference:\n\n * application-aware resource management\n * throttling based management.\n\n\n\nsms decouples memory controller into three stages.\n\n\n * the first stage of sms groups requests based on row buffer locality.\n * at the second stage, sms ensures fairness between cpu and gpu memory requests by applying cpu-biased shortest job first scheduling policy or gpu-biased round robin scheduling policy. a dynamically configurable parameter is used to select between the two policies based on the system’s needs.\n * the last stage consists of simple per-bank fifo queue to issue low-level memory commands.\n\ntap: a tlp-aware cache management policy for a cpu-gpu heterogeneous\n\n\n\n * a core-sampling technique, which applies a different cache management policy to each gpu core and regularly collects statistics on the performance\n   of these cores to see how these polices affect gpu applications.\n   \n * gpu cores typically access caches much more frequently than cpu cores.\n   \n * enforces a similar cache lifetime to both cpu and gpgpu appli cations and prevent gpgpu application to monopolize the shared cache.\n\none (cm-cpu) for boosting cpu performance in the presence of gpu interference.\nthe other (cm-bal) for improving both cpu and gpu performance in a balanced manner and thus overall system performance.\n\n\n\n\npropose gpu concurrency management that dynamically throttles/boosts tlp (i.e., number of active warps) of gpu cores in order to minimize shared resources interference between cpu and gpu.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper",frontmatter:{title:"Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper",date:"2024-08-10T00:00:00.000Z",permalink:"/pages/458724/",tags:[null]},regularPath:"/03.gpu/11.IntegratedCPUGPUMemory.html",relativePath:"03.gpu/11.IntegratedCPUGPUMemory.md",key:"v-75a07a01",path:"/pages/458724/",headers:[{level:3,title:"Harnessing Integrated CPU-GPU System Memory for HPC: a first look into Grace Hopper",slug:"harnessing-integrated-cpu-gpu-system-memory-for-hpc-a-first-look-into-grace-hopper",normalizedTitle:"harnessing integrated cpu-gpu system memory for hpc: a first look into grace hopper",charIndex:2},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:100},{level:3,title:"Grace Hooper Unified Memory System",slug:"grace-hooper-unified-memory-system",normalizedTitle:"grace hooper unified memory system",charIndex:1293},{level:3,title:"Methodology",slug:"methodology",normalizedTitle:"methodology",charIndex:8699},{level:3,title:"Overview",slug:"overview",normalizedTitle:"overview",charIndex:9633},{level:3,title:"CPU-GPU Integrated System Page Table",slug:"cpu-gpu-integrated-system-page-table",normalizedTitle:"cpu-gpu integrated system page table",charIndex:12057},{level:3,title:"System Page Size",slug:"system-page-size",normalizedTitle:"system page size",charIndex:14458},{level:3,title:"Page Migration",slug:"page-migration",normalizedTitle:"page migration",charIndex:16671},{level:3,title:"Memory Oversubscription",slug:"memory-oversubscription",normalizedTitle:"memory oversubscription",charIndex:19172}],headersStr:"Harnessing Integrated CPU-GPU System Memory for HPC: a first look into Grace Hopper Introduction Grace Hooper Unified Memory System Methodology Overview CPU-GPU Integrated System Page Table System Page Size Page Migration Memory Oversubscription",content:"# Harnessing Integrated CPU-GPU System Memory for HPC: a first look into Grace Hopper\n\n👍 👍 👍\n\n\n# Introduction\n\n# limitation of state-of-art\n\n 1. UVM large overheads in handling page faults in GPU and suffers from read/write amplification due to page-level swapping.\n 2. Data object offloading requires offline profiling and application refactoring, limiting solution generality.\n 3. The performance of both solutions is constrained by data transfer bottlenecks between the CPU and GPU.\n\n# Grace Hopper Superchip\n\n * NVLink-C2C (chip-to-chip) cache-coherent interconnect\n * a single virtual memory space is shared between the CPU and GPU (i.e., system memory)\n * address translation is accelerated by hardware.\n   1. direct remote accesses at cacheline granularity\n   2. heuristic-guided page migrations.\n\nBy leveraging cacheline level access and Address Translation Service (ATS), which enables full access to all CPU and GPU memory allocations, the system memory eliminates the page-fault handling overhead needed in managed memory in UVM, and minimizes the need for memory migrations.\n\nWhile managed memory splits the virtual memory space into both the system page table and GPU page table, system memory relies on a single system-wide page table, shared between the CPU and the GPU.\n\n\n# Grace Hooper Unified Memory System\n\n * system-allocated memory\n * CUDA managed memory\n\n# Memory Subsystem\n\nThe CPU is connected to 480 GB of LPDDR5X memory, while the GPU is equipped with 96 GB of HBM3 memory.\nThese two processors, GPU and CPU, are interconnected via the Nvidia NVLink-C2C interconnect.\n\nResults show that the GPU’s HBM3 memory achieved a bandwidth of 3.4 TB/s, compared to its theoretical bandwidth of 4 TB/s.\nThe CPU’s LPDDR5X memory reached a bandwidth of 486 GB/s, close to its theoretical bandwidth of 500 GB/s.\n\nWe achieved a bandwidth of 375 GB/s for host-to-device (H2D) transfers and 297 GB/s for device-to-host (D2H) transfers, compared to the interconnect’s theoretical bandwidth of 450 GB/s.\n\n# NVLink-C2C Interconnect\n\nIn the Grace Hopper system, a processor (CPU or GPU) can directly access the other processor’s physical memory over the NVLink-C2C interconnect.\nCacheline granularity, with transfer sizes as small as 64 bytes on the CPU side and 128 bytes on the GPU side.\n\n# System-level Address Translation\n\nGrace CPU features a unique hardware unit called the System Memory Management Unit (SMMU).\n\nThe SMMU is responsible for translating virtual addresses to physical addresses by performing page table walks.\nCompared to a traditional MMU, the SMMU provides additional support for virtual-to-physical address translation requests from the GPU.\n\nThis is the flow that GPU TLB cached mapping and GPU wish to access physical memory stored in CPU memory system.\n\n\n\nAccess Flow:\n\n * A GPU thread accesses a virtual address.\n * The data is not cached in the GPU cache hierarchy. This generates a cache miss.\n * The virtual address is looked up in the GPU TLBs (Translation Lookaside Buffers) for virtual-to-physical translation.\n   As the translation is already cached, it is used to perform an access to physical memory.\n * The GMMU initiates a direct memory access (DMA) over the NVLink-C2C interconnect, at the cacheline granularity.\n * The requested access is performed from CPU memory, and send back to the GPU.\n * The access is completed, and memory is cached in the regular GPU cache hierarchy.\n\nCompared to pre-Grace Hopper systems, which rely on GPU page fault handling to access CPU memory, this new approach has two main implications.\n\n * First, GPU accesses to CPU-located memory no longer systematically trigger GPU page faults.\n * Second, page faults are now generated by the SMMU and can be directly handled by the operating system’s page fault handling mechanism, simplifying the overall process.\n\n# Memory Management in Grace Hopper\n\ntwo distinct page tables\n\n * system-wide page table\n * GPU-exclusive page table\n\nMemory allocation\n\n * allocations in CPU physical memory only\n * allocations in GPU physical memory only\n * allocations that can reside in either CPU or GPU physical memory\n\na system-wide page table, located in CPU memory.\nThe operating system directly accesses this page table, creates and manages page table entries (PTEs).\nThe SMMU uses this page table to provide virtual-to-physical address translation for both the CPU (when required by user applications) and the GPU (when requested over the NVLink-C2C interconnect).\nMemory pages in the system-wide page table can be physically located in either CPU or GPU memory, and they use the system page size, which is defined at the operating system level and constrained by the CPU architecture capabilities.\nWhen using the Grace CPU, the page size is either 4 KB or 64 KB.\n\nGPU-exclusive Page Table The Grace Hopper system retains the local GPU page table from previous generations of Nvidia GPUs.\nThis page table, located in GPU memory and only accessible by the GPU, stores virtual-to-physical translations for cudaMalloc allocations and cudaMallocManaged allocations when the physical location of the managed memory is on the GPU.\nThe page size used by this page table is 2 MB.\n\n# System-Allocated Memory\n\nIn general, when malloc is called, the operating system creates page table entries in the system page table without assigning physical memory to those pages. During the first access to a virtual address in the allocation, known as first-touch, a page fault is triggered since the accessed virtual page is not mapped to physical memory. Classic first-touch. On Grace Hopper, this process applies to both CPU and GPU first-touch accesses.\n\n * When a GPU thread generates a first-touch access to a virtual address, a GPU TLB miss is triggered.\n * As a result, the GPU’s ATSTBU (Translation Buffer Unit) generates an address translation request and sends it to the SMMU over NVLink-C2C.\n * To answer the request, the SMMU performs a page table walk in the system page table.\n * If no physical memory is allocated to the page, the SMMU issues a page fault.\n * OS handles the fault by updating the page table entry to point to GPU physical memory, as the first-touch originated from a GPU thread.\n * Once the physical address is stored in the GPU’s TLB, GPU threads can perform memory access using direct memory access to the physical memory address, potentially located in CPU memory, over NVLink-C2C.\n\n# Automatic Delayed Access-counter-based Migrations\n\nFor system-allocated memory, the Grace Hopper system can be configured to automatically migrate memory regions between GPU and CPU physical memory.\nThe default migration strategy, detailed in Nvidia’s open-source GPU driver, relies on hardware counters to track GPU accesses to memory ranges.\nWhen a counter value exceeds a user-defined threshold (by default, 256), the GPU issues a notification in the form of a hardware interrupt, which is handled by the GPU driver on the CPU.\nThe driver then determines whether to migrate the pages belonging to the associated virtual memory region.\n\n# CUDA Managed Memory\n\nCUDA managed memory is primarily a software abstraction, implemented as part of the CUDA runtime libraries and the Nvidia GPU driver. Programmers create managed memory allocations using the cudaMallocManaged() function.\nSimilar to malloc, for post-Pascal systems, the virtual memory is not immediately mapped to physical memory.\nInstead, the location of the first-touch triggers this mapping operation.\n\n> Please Notice that CUDA Managed Memory doesn't guarantee memory is allocated in GPU. It just means that the memory is allocated by this API.\n\n# On-demand page migration\n\nCUDA managed memory relies on on-demand page migration to enable both GPU and CPU to access the shared virtual memory range.\nWhen the GPU tries to access a page, a page fault is triggered if a GPU TLB miss occurs and the GMMU fails to find the virtual address in the GPU-exclusive page table.\nThis page fault causes a page migration from CPU memory to GPU memory.\nwhen GPU memory is overwhelmed, pages can also be evicted to CPU memory.\\\n\nCoherent dynamic memory allocation was introduced on Power9 platforms in CUDA 9.2. This feature is supported by the ATS, which enables hardware-level address translations by allowing direct communication between CPU and GPU MMUs and eliminates the need for software-level address translation.\n\n# Speculative prefetching\n\nbefore they are accessed, in order to reduce the page fault handling overhead of CUDA managed memory on the critical path.\nThese strategies include explicit prefetching, triggered through the cudaMemPrefetchAsync API, and implicit prefetching performed by GPU hardware prefetchers.\n\n\n# Methodology\n\nA snippet of code transformation from a typical CUDA code with explicit memory copy to Unified Memory.\n\nWe derive two versions for each application,\n\n * one using CUDA managed memory\n * one using system-allocated memory.\n\nFor this purpose, we first identify candidate memory allocations to replace, by locating explicit host-to-device data movements in the code.\nWe replace the destination and source buffers in those data transfers by a single buffer, allocated using one of the two unified memory allocators, either the system-level allocator (malloc) or CUDA managed memory allocator (cudaMallocManaged).\nGPU-only buffers, which are never meant be accessed by the CPU, and are typically only used for storing intermediary results on the GPU, are still allocated with cudaMalloc.\n\nPhases:\nGPU context initialization and argument parsing, allocation, CPU-side buffers initialization, computation, and de-allocation.\n\n\n# Overview\n\ncategorized into two classes.\n\n * In some applications, the system memory version outperforms the managed memory version.\n   The managed memory will trigger page faults when the GPU accesses data that is not in GPU memory, and start on-demand page migration.\n   As pointed in multiple existing works [2, 9], the page fault handling can cause higher overhead than the data migration itself.\n   The new cache-coherent NVlink-C2C enables direct data access to CPU memory at cacheline level without involving the expensive page fault mechanism, attributing to the observed speedup.\n   the system memory version even outperforms the original explicit version. The significant difference in the allocation and de-allocation time depending on the type of memory management in use.\n * In contrast, for SRAD and Quantum Volume simulations of 21-23 qubits, the managed memory version outperforms the system memory version.\n\nOur in-depth analysis in Section 5 identifies the main factors coming from\n\n * the data structures that are initialized on GPU\n * the different sizes of the integrated system pages and GPU-exclusive pages.\n\nWe also identified a difference in behavior for the GPU context initialization.\nIn the traditional explicit version and managed memory version, memory allocations, and data transfer are done through specific CUDA APIs before kernel launches, which implicitly initialize GPU context.\nHowever, in the system memory version, due to the absence of explicit CUDA memory allocation and data copy API calls, GPU context initialization occurs within the first kernel launch, apparently prolonging the computation time.\n\n\n\nCUDA Managed Mode: once in the computation phase, GPU access to data triggers page migration, and a steep decrease in system memory and a sharp increase in GPU memory usage is observed. hotspot represents a typical class of existing GPU applications, where data structures used in GPU computation are initialized on CPU.\n\n\n\nIn this application, the end-to-end execution is significantly prolonged in the system memory version, compared to the managed memory version.\nHowever, we also notice that the main difference is only constrained in the initialization phase,\nwhere the GPU memory usage slowly ramps up in the system memory version (orange) but quickly reaches the peak in the managed memory version (blue).\nIn fact, the computation phase in both versions are similar.\n\n\n# CPU-GPU Integrated System Page Table\n\nThe following two factor will affect integrated page table.\nSystem memory uses a first-touch placement policy and pages always reside in the system page table.\nManaged memory also uses a first-touch placement policy but pages may reside in either the system page table or GPU page table, depending on its physical location.\n\n# CPU-side Initialization\n\nCommon HPC perform data initialization, often including pre-processing, on the CPU before offloading data onto the GPU for computation.\n\nIn such a pattern, the first-touch policy will cause pages to be placed on the CPU during initialization.\nWhen the computation phase starts, in managed memory, data is migrated on demand to the GPU memory often with additional pages from speculative prefetching, which will result in both traffic on the NVLink-C2C and increased GPU memory utilization.\nInstead, in the system memory, data will not be migrated on access but deferred, which will result in only traffic on the NVLink-C2C link and no immediately increased GPU memory utilization. using system memory (left) and CUDA managed memory (right).\n\nIn short:\n\n * Managed, introduce additional prefetch\n * System, access-counter-based migration\n\n# GPU-side Initialization\n\n> Please Notice that Malloc and CUDA can both allocate memory, then initialized by GPU, which is far more different.\n\nWith CUDA managed memory, the initialization is much shorter than that in the system memory version, and no page migration is performed during the computation phase, as the first touch by GPU has directly mapped data to GPU memory.\n\nWith system memory, the GPU first-touch policy triggers a replayable page fault, as the page being first-touched is neither present in the GMMU page table, nor through address translation.\nThe CPU then handles the page fault and populates the system page table, therefore slowing down the initialization time on the GPU.\n\nIn short:\n\n * Managed, no page migration\n * System, CPU intervene the process, handle page fault and populate the system page table.\n\n\n\nsystem-allocated memory performs better in cases of CPU-side initialization as the page faults are both triggered and handled on the CPU side.\nIn the GPU-side initialization, page table initialization on the CPU-side significantly slows down the execution. In the latter cases, we observed that CUDA managed memory performed better.\n\n\n# System Page Size\n\nAll pages in a system allocation use the system page size, while only pages resident on CPU memory in the managed memory uses the system page size.\nThe system page sizes mainly impacts the page initialization overhead that often occurs in application initialization phase, and migration performance between CPU and GPU memory that often occurs in the computation phase.\n\nWe run each application in the system memory version by configuring the system pages in 4 KB and 64 KB.\n\nA noticeable difference between 4 KB and 64 KB pages lies in the de-allocation time, which is significantly higher in 4 KB system pages, for all applications.\n\nRodinia applications, with the exception of SRAD, exhibit lower compute time for 4 KB pages compared to 64 KB pages (1.1×-2.1×).\n\n\n\nOne possible reason for the lower performance in 64 KB pages pages is the granularity of migrated pages may cause amplification, resulting in unused data being migrated.\nThis performance loss could also partially be attributed to the automatic migrations that might incur temporary latency increase when the computation accesses on pages that are being migrated, reducing performance.\nIn Rodinia applications, this is particularly noticeable as applications have a short computation time, where migrated data may not be sufficiently reused.\n\n\n\nWith an increasing problem size, the speedup in the managed memory version is decreasing while the speedup in the system memory version is increasing.\n\nIn CUDA managed memory, when using 64 KB pages, the execution time is 10% lower than with 4 KB pages. This limited impact of the system page size is expected, as Qiskit has GPU-side data initialization, and CUDA managed memory uses the GPU page table for GPU-resident data, with a constant 2 MB page size, independent of system page size.\n\nWhile the computation time remains stable between page sizes, the initialization time is drastically reduced with 64 KB pages, with a 5× improvement.\nThis difference highlights the cost of GPU-side page initialization, where memory pages are first-touched on the GPU-side, and page table initialization is performed on the CPU-side, representing a notable bottleneck in the application.\n\n\n# Page Migration\n\nwe compare the new automatic access-counter-based strategy in system-allocated memory on Grace Hopper with the on-demand page migration strategy in CUDA managed memory.\n\n * access-counter-based An application needs to have access patterns that can clearly expose hot pages to exploit the access-counter-based strategy in system-allocated memory.\n   We examined all the test applications and choose SRAD as this application uses an iterative algorithm in its computation phase.\n   Therefore, with a sufficient number of iterations, the access-counter-based page migration should migrate pages repeatedly accessed during computation iterations into GPU memory. \\\n * on-demand migration the on-demand page migration in the managed system version should migrate all accessed pages on their first access.\n\nFor the managed memory version, due to page migration in the first iteration, the execution time of this iteration is significantly higher than the other iterations.\n\nIn the system memory version, from a performance standpoint, the computational phase consists of three sub-phases, as separated by dashed line on Figure 10.\n\n * The first phase corresponds to the first iteration, with high execution time, primarily caused by the overhead of GPU first-touch on system-allocated data, as memory pages must be initialized on the CPU-side.\n * The second phase (iteration 2-4), exhibits a decreasing iteration time but still slower than that of the managed memory version.\n * In the final phase (iteration 5 and above), the iteration time stabilizes and outperform the managed memory version.\n\n\n\nIn the managed memory version, all reads are performed from GPU memory, even for the first iteration, where pages are being migrated, and exhibit non-zero reads over NVLink-C2C.\nThis is because in managed memory, pages are first migrated, and then read from local GPU memory.\nIn the system memory version, we observe that memory reads over NVLink-C2C decreases as reads from GPU memory increases gradually in iteration 1-4.\nThis observation confirms that the ccess-pattern-based automatic migrations are being triggered in this stage, which hinders performance in this period.\nAfter the entire working set has been migrated to GPU memory, that is, for iterations 5-12, memory reads over NVLink-C2C remain nearly zero while reads from GPU memory stabilize at 1.5 GB per iteration.\nConsequently, the performance in iterations 5-12 improves to outperform that of the managed memory version.\n\n\n# Memory Oversubscription\n\n * First, pages can be evicted from GPU memory, and the required pages can be migrated into GPU. This is the expected behavior for CUDA managed memory.\n * In addition, as Grace Hopper supports direct memory access over NVLink-C2C, data in CPU memory can be remotely accessed without migration.\n\n\n\nThe system memory version of Rodinia applications, BFS, hotspot, needle, pathfinder, are less affected by oversubscription than the managed versions, as indicated by the increased speedup at increased over-subscription.\nThis trend is because that the system-memory version always places data on CPU memory, and performs accesses over NVLink-C2C link.\nHowever, in the managed memory version, data is being migrated to the GPU, and evicted when the GPU memory has been exhausted.\nThis eviction and migration process significantly impacts the performance.\n\nFor the 34-qubits quantum volume simulation (about 130% GPU memory oversubscription), a significant slowdown with managed memory is observed compared to the explicit copy version. Further analysis reveals that no page is migrated and all data is accessed over NVLink-C2C at a low bandwidth.\nWe optimize the managed memory version using CUDA managed memory prefetching to transform the majority of data access to be read locally from GPU memory.\n\nIn previous in-memory scenarios, CUDA managed memory in both 4 KB and 64 KB pages exhibits similar execution times.\nHowever, in oversubscription scenarios, the system page size shows a high impact on execution time.\n\n\n\nIn the 34-qubit quantum simulation, switching from 4 KB to 64 KB system pages shortens initialization and accelerates page migration by 58%.\nInterestingly, the 30-qubit simulation shows a different preference on the system page size, nearly 3× slower computation when using 64 KB system pages as shown in Figure 13.\\ This is unexpected, as the page size for GPU-resident memory is 2 MB in managed memory, and is not modified by the system page size. We suggest that this difference is due to some pages being evicted to CPU memory where the system page size is used. In the case of 64 KB pages, when those pages are migrated back to the GPU, the amount of migrated memory at a time is higher than 4 KB, affecting performance.",normalizedContent:"# harnessing integrated cpu-gpu system memory for hpc: a first look into grace hopper\n\n👍 👍 👍\n\n\n# introduction\n\n# limitation of state-of-art\n\n 1. uvm large overheads in handling page faults in gpu and suffers from read/write amplification due to page-level swapping.\n 2. data object offloading requires offline profiling and application refactoring, limiting solution generality.\n 3. the performance of both solutions is constrained by data transfer bottlenecks between the cpu and gpu.\n\n# grace hopper superchip\n\n * nvlink-c2c (chip-to-chip) cache-coherent interconnect\n * a single virtual memory space is shared between the cpu and gpu (i.e., system memory)\n * address translation is accelerated by hardware.\n   1. direct remote accesses at cacheline granularity\n   2. heuristic-guided page migrations.\n\nby leveraging cacheline level access and address translation service (ats), which enables full access to all cpu and gpu memory allocations, the system memory eliminates the page-fault handling overhead needed in managed memory in uvm, and minimizes the need for memory migrations.\n\nwhile managed memory splits the virtual memory space into both the system page table and gpu page table, system memory relies on a single system-wide page table, shared between the cpu and the gpu.\n\n\n# grace hooper unified memory system\n\n * system-allocated memory\n * cuda managed memory\n\n# memory subsystem\n\nthe cpu is connected to 480 gb of lpddr5x memory, while the gpu is equipped with 96 gb of hbm3 memory.\nthese two processors, gpu and cpu, are interconnected via the nvidia nvlink-c2c interconnect.\n\nresults show that the gpu’s hbm3 memory achieved a bandwidth of 3.4 tb/s, compared to its theoretical bandwidth of 4 tb/s.\nthe cpu’s lpddr5x memory reached a bandwidth of 486 gb/s, close to its theoretical bandwidth of 500 gb/s.\n\nwe achieved a bandwidth of 375 gb/s for host-to-device (h2d) transfers and 297 gb/s for device-to-host (d2h) transfers, compared to the interconnect’s theoretical bandwidth of 450 gb/s.\n\n# nvlink-c2c interconnect\n\nin the grace hopper system, a processor (cpu or gpu) can directly access the other processor’s physical memory over the nvlink-c2c interconnect.\ncacheline granularity, with transfer sizes as small as 64 bytes on the cpu side and 128 bytes on the gpu side.\n\n# system-level address translation\n\ngrace cpu features a unique hardware unit called the system memory management unit (smmu).\n\nthe smmu is responsible for translating virtual addresses to physical addresses by performing page table walks.\ncompared to a traditional mmu, the smmu provides additional support for virtual-to-physical address translation requests from the gpu.\n\nthis is the flow that gpu tlb cached mapping and gpu wish to access physical memory stored in cpu memory system.\n\n\n\naccess flow:\n\n * a gpu thread accesses a virtual address.\n * the data is not cached in the gpu cache hierarchy. this generates a cache miss.\n * the virtual address is looked up in the gpu tlbs (translation lookaside buffers) for virtual-to-physical translation.\n   as the translation is already cached, it is used to perform an access to physical memory.\n * the gmmu initiates a direct memory access (dma) over the nvlink-c2c interconnect, at the cacheline granularity.\n * the requested access is performed from cpu memory, and send back to the gpu.\n * the access is completed, and memory is cached in the regular gpu cache hierarchy.\n\ncompared to pre-grace hopper systems, which rely on gpu page fault handling to access cpu memory, this new approach has two main implications.\n\n * first, gpu accesses to cpu-located memory no longer systematically trigger gpu page faults.\n * second, page faults are now generated by the smmu and can be directly handled by the operating system’s page fault handling mechanism, simplifying the overall process.\n\n# memory management in grace hopper\n\ntwo distinct page tables\n\n * system-wide page table\n * gpu-exclusive page table\n\nmemory allocation\n\n * allocations in cpu physical memory only\n * allocations in gpu physical memory only\n * allocations that can reside in either cpu or gpu physical memory\n\na system-wide page table, located in cpu memory.\nthe operating system directly accesses this page table, creates and manages page table entries (ptes).\nthe smmu uses this page table to provide virtual-to-physical address translation for both the cpu (when required by user applications) and the gpu (when requested over the nvlink-c2c interconnect).\nmemory pages in the system-wide page table can be physically located in either cpu or gpu memory, and they use the system page size, which is defined at the operating system level and constrained by the cpu architecture capabilities.\nwhen using the grace cpu, the page size is either 4 kb or 64 kb.\n\ngpu-exclusive page table the grace hopper system retains the local gpu page table from previous generations of nvidia gpus.\nthis page table, located in gpu memory and only accessible by the gpu, stores virtual-to-physical translations for cudamalloc allocations and cudamallocmanaged allocations when the physical location of the managed memory is on the gpu.\nthe page size used by this page table is 2 mb.\n\n# system-allocated memory\n\nin general, when malloc is called, the operating system creates page table entries in the system page table without assigning physical memory to those pages. during the first access to a virtual address in the allocation, known as first-touch, a page fault is triggered since the accessed virtual page is not mapped to physical memory. classic first-touch. on grace hopper, this process applies to both cpu and gpu first-touch accesses.\n\n * when a gpu thread generates a first-touch access to a virtual address, a gpu tlb miss is triggered.\n * as a result, the gpu’s atstbu (translation buffer unit) generates an address translation request and sends it to the smmu over nvlink-c2c.\n * to answer the request, the smmu performs a page table walk in the system page table.\n * if no physical memory is allocated to the page, the smmu issues a page fault.\n * os handles the fault by updating the page table entry to point to gpu physical memory, as the first-touch originated from a gpu thread.\n * once the physical address is stored in the gpu’s tlb, gpu threads can perform memory access using direct memory access to the physical memory address, potentially located in cpu memory, over nvlink-c2c.\n\n# automatic delayed access-counter-based migrations\n\nfor system-allocated memory, the grace hopper system can be configured to automatically migrate memory regions between gpu and cpu physical memory.\nthe default migration strategy, detailed in nvidia’s open-source gpu driver, relies on hardware counters to track gpu accesses to memory ranges.\nwhen a counter value exceeds a user-defined threshold (by default, 256), the gpu issues a notification in the form of a hardware interrupt, which is handled by the gpu driver on the cpu.\nthe driver then determines whether to migrate the pages belonging to the associated virtual memory region.\n\n# cuda managed memory\n\ncuda managed memory is primarily a software abstraction, implemented as part of the cuda runtime libraries and the nvidia gpu driver. programmers create managed memory allocations using the cudamallocmanaged() function.\nsimilar to malloc, for post-pascal systems, the virtual memory is not immediately mapped to physical memory.\ninstead, the location of the first-touch triggers this mapping operation.\n\n> please notice that cuda managed memory doesn't guarantee memory is allocated in gpu. it just means that the memory is allocated by this api.\n\n# on-demand page migration\n\ncuda managed memory relies on on-demand page migration to enable both gpu and cpu to access the shared virtual memory range.\nwhen the gpu tries to access a page, a page fault is triggered if a gpu tlb miss occurs and the gmmu fails to find the virtual address in the gpu-exclusive page table.\nthis page fault causes a page migration from cpu memory to gpu memory.\nwhen gpu memory is overwhelmed, pages can also be evicted to cpu memory.\\\n\ncoherent dynamic memory allocation was introduced on power9 platforms in cuda 9.2. this feature is supported by the ats, which enables hardware-level address translations by allowing direct communication between cpu and gpu mmus and eliminates the need for software-level address translation.\n\n# speculative prefetching\n\nbefore they are accessed, in order to reduce the page fault handling overhead of cuda managed memory on the critical path.\nthese strategies include explicit prefetching, triggered through the cudamemprefetchasync api, and implicit prefetching performed by gpu hardware prefetchers.\n\n\n# methodology\n\na snippet of code transformation from a typical cuda code with explicit memory copy to unified memory.\n\nwe derive two versions for each application,\n\n * one using cuda managed memory\n * one using system-allocated memory.\n\nfor this purpose, we first identify candidate memory allocations to replace, by locating explicit host-to-device data movements in the code.\nwe replace the destination and source buffers in those data transfers by a single buffer, allocated using one of the two unified memory allocators, either the system-level allocator (malloc) or cuda managed memory allocator (cudamallocmanaged).\ngpu-only buffers, which are never meant be accessed by the cpu, and are typically only used for storing intermediary results on the gpu, are still allocated with cudamalloc.\n\nphases:\ngpu context initialization and argument parsing, allocation, cpu-side buffers initialization, computation, and de-allocation.\n\n\n# overview\n\ncategorized into two classes.\n\n * in some applications, the system memory version outperforms the managed memory version.\n   the managed memory will trigger page faults when the gpu accesses data that is not in gpu memory, and start on-demand page migration.\n   as pointed in multiple existing works [2, 9], the page fault handling can cause higher overhead than the data migration itself.\n   the new cache-coherent nvlink-c2c enables direct data access to cpu memory at cacheline level without involving the expensive page fault mechanism, attributing to the observed speedup.\n   the system memory version even outperforms the original explicit version. the significant difference in the allocation and de-allocation time depending on the type of memory management in use.\n * in contrast, for srad and quantum volume simulations of 21-23 qubits, the managed memory version outperforms the system memory version.\n\nour in-depth analysis in section 5 identifies the main factors coming from\n\n * the data structures that are initialized on gpu\n * the different sizes of the integrated system pages and gpu-exclusive pages.\n\nwe also identified a difference in behavior for the gpu context initialization.\nin the traditional explicit version and managed memory version, memory allocations, and data transfer are done through specific cuda apis before kernel launches, which implicitly initialize gpu context.\nhowever, in the system memory version, due to the absence of explicit cuda memory allocation and data copy api calls, gpu context initialization occurs within the first kernel launch, apparently prolonging the computation time.\n\n\n\ncuda managed mode: once in the computation phase, gpu access to data triggers page migration, and a steep decrease in system memory and a sharp increase in gpu memory usage is observed. hotspot represents a typical class of existing gpu applications, where data structures used in gpu computation are initialized on cpu.\n\n\n\nin this application, the end-to-end execution is significantly prolonged in the system memory version, compared to the managed memory version.\nhowever, we also notice that the main difference is only constrained in the initialization phase,\nwhere the gpu memory usage slowly ramps up in the system memory version (orange) but quickly reaches the peak in the managed memory version (blue).\nin fact, the computation phase in both versions are similar.\n\n\n# cpu-gpu integrated system page table\n\nthe following two factor will affect integrated page table.\nsystem memory uses a first-touch placement policy and pages always reside in the system page table.\nmanaged memory also uses a first-touch placement policy but pages may reside in either the system page table or gpu page table, depending on its physical location.\n\n# cpu-side initialization\n\ncommon hpc perform data initialization, often including pre-processing, on the cpu before offloading data onto the gpu for computation.\n\nin such a pattern, the first-touch policy will cause pages to be placed on the cpu during initialization.\nwhen the computation phase starts, in managed memory, data is migrated on demand to the gpu memory often with additional pages from speculative prefetching, which will result in both traffic on the nvlink-c2c and increased gpu memory utilization.\ninstead, in the system memory, data will not be migrated on access but deferred, which will result in only traffic on the nvlink-c2c link and no immediately increased gpu memory utilization. using system memory (left) and cuda managed memory (right).\n\nin short:\n\n * managed, introduce additional prefetch\n * system, access-counter-based migration\n\n# gpu-side initialization\n\n> please notice that malloc and cuda can both allocate memory, then initialized by gpu, which is far more different.\n\nwith cuda managed memory, the initialization is much shorter than that in the system memory version, and no page migration is performed during the computation phase, as the first touch by gpu has directly mapped data to gpu memory.\n\nwith system memory, the gpu first-touch policy triggers a replayable page fault, as the page being first-touched is neither present in the gmmu page table, nor through address translation.\nthe cpu then handles the page fault and populates the system page table, therefore slowing down the initialization time on the gpu.\n\nin short:\n\n * managed, no page migration\n * system, cpu intervene the process, handle page fault and populate the system page table.\n\n\n\nsystem-allocated memory performs better in cases of cpu-side initialization as the page faults are both triggered and handled on the cpu side.\nin the gpu-side initialization, page table initialization on the cpu-side significantly slows down the execution. in the latter cases, we observed that cuda managed memory performed better.\n\n\n# system page size\n\nall pages in a system allocation use the system page size, while only pages resident on cpu memory in the managed memory uses the system page size.\nthe system page sizes mainly impacts the page initialization overhead that often occurs in application initialization phase, and migration performance between cpu and gpu memory that often occurs in the computation phase.\n\nwe run each application in the system memory version by configuring the system pages in 4 kb and 64 kb.\n\na noticeable difference between 4 kb and 64 kb pages lies in the de-allocation time, which is significantly higher in 4 kb system pages, for all applications.\n\nrodinia applications, with the exception of srad, exhibit lower compute time for 4 kb pages compared to 64 kb pages (1.1×-2.1×).\n\n\n\none possible reason for the lower performance in 64 kb pages pages is the granularity of migrated pages may cause amplification, resulting in unused data being migrated.\nthis performance loss could also partially be attributed to the automatic migrations that might incur temporary latency increase when the computation accesses on pages that are being migrated, reducing performance.\nin rodinia applications, this is particularly noticeable as applications have a short computation time, where migrated data may not be sufficiently reused.\n\n\n\nwith an increasing problem size, the speedup in the managed memory version is decreasing while the speedup in the system memory version is increasing.\n\nin cuda managed memory, when using 64 kb pages, the execution time is 10% lower than with 4 kb pages. this limited impact of the system page size is expected, as qiskit has gpu-side data initialization, and cuda managed memory uses the gpu page table for gpu-resident data, with a constant 2 mb page size, independent of system page size.\n\nwhile the computation time remains stable between page sizes, the initialization time is drastically reduced with 64 kb pages, with a 5× improvement.\nthis difference highlights the cost of gpu-side page initialization, where memory pages are first-touched on the gpu-side, and page table initialization is performed on the cpu-side, representing a notable bottleneck in the application.\n\n\n# page migration\n\nwe compare the new automatic access-counter-based strategy in system-allocated memory on grace hopper with the on-demand page migration strategy in cuda managed memory.\n\n * access-counter-based an application needs to have access patterns that can clearly expose hot pages to exploit the access-counter-based strategy in system-allocated memory.\n   we examined all the test applications and choose srad as this application uses an iterative algorithm in its computation phase.\n   therefore, with a sufficient number of iterations, the access-counter-based page migration should migrate pages repeatedly accessed during computation iterations into gpu memory. \\\n * on-demand migration the on-demand page migration in the managed system version should migrate all accessed pages on their first access.\n\nfor the managed memory version, due to page migration in the first iteration, the execution time of this iteration is significantly higher than the other iterations.\n\nin the system memory version, from a performance standpoint, the computational phase consists of three sub-phases, as separated by dashed line on figure 10.\n\n * the first phase corresponds to the first iteration, with high execution time, primarily caused by the overhead of gpu first-touch on system-allocated data, as memory pages must be initialized on the cpu-side.\n * the second phase (iteration 2-4), exhibits a decreasing iteration time but still slower than that of the managed memory version.\n * in the final phase (iteration 5 and above), the iteration time stabilizes and outperform the managed memory version.\n\n\n\nin the managed memory version, all reads are performed from gpu memory, even for the first iteration, where pages are being migrated, and exhibit non-zero reads over nvlink-c2c.\nthis is because in managed memory, pages are first migrated, and then read from local gpu memory.\nin the system memory version, we observe that memory reads over nvlink-c2c decreases as reads from gpu memory increases gradually in iteration 1-4.\nthis observation confirms that the ccess-pattern-based automatic migrations are being triggered in this stage, which hinders performance in this period.\nafter the entire working set has been migrated to gpu memory, that is, for iterations 5-12, memory reads over nvlink-c2c remain nearly zero while reads from gpu memory stabilize at 1.5 gb per iteration.\nconsequently, the performance in iterations 5-12 improves to outperform that of the managed memory version.\n\n\n# memory oversubscription\n\n * first, pages can be evicted from gpu memory, and the required pages can be migrated into gpu. this is the expected behavior for cuda managed memory.\n * in addition, as grace hopper supports direct memory access over nvlink-c2c, data in cpu memory can be remotely accessed without migration.\n\n\n\nthe system memory version of rodinia applications, bfs, hotspot, needle, pathfinder, are less affected by oversubscription than the managed versions, as indicated by the increased speedup at increased over-subscription.\nthis trend is because that the system-memory version always places data on cpu memory, and performs accesses over nvlink-c2c link.\nhowever, in the managed memory version, data is being migrated to the gpu, and evicted when the gpu memory has been exhausted.\nthis eviction and migration process significantly impacts the performance.\n\nfor the 34-qubits quantum volume simulation (about 130% gpu memory oversubscription), a significant slowdown with managed memory is observed compared to the explicit copy version. further analysis reveals that no page is migrated and all data is accessed over nvlink-c2c at a low bandwidth.\nwe optimize the managed memory version using cuda managed memory prefetching to transform the majority of data access to be read locally from gpu memory.\n\nin previous in-memory scenarios, cuda managed memory in both 4 kb and 64 kb pages exhibits similar execution times.\nhowever, in oversubscription scenarios, the system page size shows a high impact on execution time.\n\n\n\nin the 34-qubit quantum simulation, switching from 4 kb to 64 kb system pages shortens initialization and accelerates page migration by 58%.\ninterestingly, the 30-qubit simulation shows a different preference on the system page size, nearly 3× slower computation when using 64 kb system pages as shown in figure 13.\\ this is unexpected, as the page size for gpu-resident memory is 2 mb in managed memory, and is not modified by the system page size. we suggest that this difference is due to some pages being evicted to cpu memory where the system page size is used. in the case of 64 kb pages, when those pages are migrated back to the gpu, the amount of migrated memory at a time is higher than 4 kb, affecting performance.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Understanding GPGPU-SIM 1 How to get Instruction",frontmatter:{title:"Understanding GPGPU-SIM 1 How to get Instruction",date:"2024-08-12T00:00:00.000Z",permalink:"/pages/458725/",tags:[null]},regularPath:"/03.gpu/12.gpgpusim.html",relativePath:"03.gpu/12.gpgpusim.md",key:"v-5474106a",path:"/pages/458725/",headers:[{level:3,title:"How did GPGPU-sim get instruction from CUDA?",slug:"how-did-gpgpu-sim-get-instruction-from-cuda",normalizedTitle:"how did gpgpu-sim get instruction from cuda?",charIndex:3278}],headersStr:"How did GPGPU-sim get instruction from CUDA?",content:" * libcuda\n   * cuda_runtime_api.cc\n * src\n   * abstract_hardware_model.h/cpp\n   * gpgpusim_entrypoint.h/cpp\n   * stream_manager.h/cpp\n   * cuda-sim\n     \n     * cuda-sim.h/cc 🐝\n       \n       Code\n       \n       \n       \n       void function_info::ptx_assemble() {\n        for ( i=m_instructions.begin(); i != m_instructions.end(); i++ ) {\n         // map pc to instruction\n         g_pc_to_finfo[PC] = this;\n         // This is a uniform array, each entry is one instruction\n         m_instr_mem[n] = pI; \n         s_g_pc_to_insn.push_back(pI);\n         ssert(pI == s_g_pc_to_insn[PC]);\n         pI->set_m_instr_mem_index(n);\n         pI->set_PC(PC);\n        }\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       \n     \n     * memory.h/cc\n     \n     * opcode.h/def\n     \n     * ptx_loader.h/cc\n     \n     * ptx.y\n       It is a Yacc/Bison grammar file used to parse PTX (Parallel Thread Execution) assembly code, which is a low-level intermediate representation used by NVIDIA GPUs.\n       It defines the grammar rules for PTX assembly code and specifies how different components of PTX code should be interpreted and processed.\n       This includes recognizing various PTX instructions, operands, directives, and control structures, and translating them into an internal representation that the simulator can work with.\n       \n       When it meets instruction statement it will call add_instruction.\n       \n       Code\n       \n       \n       \n       statement_list: directive_statement { add_directive(); }\n       | instruction_statement { add_instruction();}\n       ...\n       \n       \n       1\n       2\n       3\n       \n     \n     * ptx_parser.h/cc the add_instruction used in ptx.y will call the following instruction.\n       \n       Code\n       \n       \n       \n       void add_instruction() \n       {\n       ptx_instruction *i = new ptx_instruction(**);\n       g_instructions.push_back(i);\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       \n       in the end of function it will add all the instructins into function infomation Code\n       \n       void end_function()\n       {\n       ...\n       g_func_info->add_inst( g_instructions );\n       ...\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       \n     \n     * ptx_ir.h/cc\n       \n       Code\n       \n       \n       \n       //@@@@@@ ptx_ir.h\n       ...\n       std::vector<const symbol*> m_args;\n       // end_function will put function into this list\n       std::list<ptx_instruction*> m_instructions;\n       std::vector<basic_block_t*> m_basic_blocks;\n       \n       //@@@@@@ ptx_ir.cc\n       void gpgpu_ptx_assemble( std::string kname, void *kinfo ) {\n        function_info *func_info = (function_info *)kinfo;\n        // This will call cuda_sim ptx_assemble function\n        func_info->ptx_assemble();\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       13\n       \n     \n     * ptx_sim.h/cc\n   \n   * gpgpu-sim\n     * gpgpu-sim.h/cc\n     * shader.h/shader.cc\n     * mem_fetch.h/cc\n     * stack.h/cc\n     * addrdec.h/cc\n     * dram.h/cc\n     * traffic_breakdown.h/cc\n\n\n# How did GPGPU-sim get instruction from CUDA?\n\n🐝 show how Yacc/Bison grammar file is used to add instruction in function_info. Now we will describe how GPGPU-sim execute each instructin.\n\n# Function Mode\n\nabstract_hardware_model.cc implement this function, if you provde a warpId, it can return warp_instruction.\nThus, if we know next pc, we can use ptx_fetch_inst to get instruction.\n\nCode\n\n\n\n// @@@@@@ abstract_hardware_model.cc\n//! Get the warp to be executed using the data taken form the SIMT stack\nwarp_inst_t core_t::getExecuteWarp(unsigned warpId)\n{\n    unsigned pc,rpc;\n    m_simt_stack[warpId]->get_pdom_stack_top_info(&pc,&rpc);\n    warp_inst_t wi= *ptx_fetch_inst(pc);\n    wi.set_active(m_simt_stack[warpId]->get_active_mask());\n    return wi;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n      gpgpu*_sim_main_function\n                 |\n    gpgpu_cuda_ptx_sim_main_func\n                 |\n              execute\n                 |\n            executeWarp\n                 |\n   getExecuteWarp execute_warp_inst_t\n\n\nCode\n\n// @@@@@@ gpgpusim_entrypoint.cc\nint gpgpu_opencl_ptx_sim_main_func( kernel_info_t *grid )\n{\n    //calling the CUDA PTX simulator, sending the kernel by reference and a flag set to true,\n    //the flag used by the function to distinguish OpenCL calls from the CUDA simulation calls which\n    //it is needed by the called function to not register the exit the exit of OpenCL kernel as it doesn't register entering in the first place as the CUDA kernels does\n   gpgpu_cuda_ptx_sim_main_func( *grid, true );\n   return 0;\n}\n\n// @@@@@@ cuda-sim.cc\n/*!\nThis function simulates the CUDA code functionally, it takes a kernel_info_t parameter \nwhich holds the data for the CUDA kernel to be executed\n!*/\nvoid gpgpu_cuda_ptx_sim_main_func( kernel_info_t &kernel, bool openCL ) {\n  while(!kernel.no_more_ctas_to_run()){\n    functionalCoreSim cta(&kernel,g_the_gpu,\n    g_the_gpu->getShaderCoreConfig()->warp_size\n    );\n    cta.execute();\n }\n}\n\n\nvoid functionalCoreSim::execute()\n {\n    ...\n    //start executing the CTA\n    while(true){\n        ...\n        for(unsigned i=0;i<m_warp_count;i++){\n            executeWarp(i,allAtBarrier,someOneLive);\n        }\n        ...\n    }\n }\n\nvoid functionalCoreSim::executeWarp(unsigned i, bool &allAtBarrier, bool & someOneLive)\n{\n ...\n warp_inst_t inst =getExecuteWarp(i);\n //!!!!! Attention !!!!!!!!\n execute_warp_inst_t(inst,i);\n ...\n updateSIMTStack( i, &inst );\n}\n\nconst warp_inst_t *ptx_fetch_inst( address_type pc )\n{\n    return function_info::pc_to_instruction(pc);\n}\n\n// @@@@@@ ptx_ir.h\nstatic const ptx_instruction* pc_to_instruction(unsigned pc) \n{\n  if( pc < s_g_pc_to_insn.size() )\n      return s_g_pc_to_insn[pc];\n  else\n      return NULL;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\n# Timing Mode\n\n * shader.cc decode() fill instruction into ibuffer\n * shader.h filled into m_ibuffer\n * shader.cc cycle issue warp\n\nCode\n\n// @@@@@@ shader.cc\nvoid shader_core_ctx::decode()\n{\n    if( m_inst_fetch_buffer.m_valid ) {\n        // decode 1 or 2 instructions and place them into ibuffer\n        address_type pc = m_inst_fetch_buffer.m_pc;\n        const warp_inst_t* pI1 = ptx_fetch_inst(pc);\n        m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(0,pI1);\n        m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();\n        ...\n    }\n}\n\n// @@@@@@ shader.h\n    void ibuffer_fill( unsigned slot, const warp_inst_t *pI )\n    {\n       m_ibuffer[slot].m_inst=pI;\n       m_ibuffer[slot].m_valid=true;\n    }\n\n    const warp_inst_t *ibuffer_next_inst() { return m_ibuffer[m_next].m_inst; }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nEvery cycle, if current warp is done, it will pick form the m_next_cycle_prioritized_warps to schedule next warp.\nThe instruction is obtained from m_ibuffer.\n\nCode\n\nvoid scheduler_unit::cycle()\n{\n    SCHED_DPRINTF( \"scheduler_unit::cycle()\\n\" );\n    bool valid_inst = false;  // there was one warp with a valid instruction to issue (didn't require flush due to control hazard)\n    bool ready_inst = false;  // of the valid instructions, there was one not waiting for pending register writes\n    bool issued_inst = false; // of these we issued one\n\n    order_warps();\n    for ( std::vector< shd_warp_t* >::const_iterator iter = m_next_cycle_prioritized_warps.begin();\n          iter != m_next_cycle_prioritized_warps.end();\n          iter++ ) {\n        // Don't consider warps that are not yet valid\n        if ( (*iter) == NULL || (*iter)->done_exit() ) {\n            continue;\n        }\n        while( !warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty() && (checked < max_issue) && (checked <= issued) && (issued < max_issue) ) {\n            const warp_inst_t *pI = warp(warp_id).ibuffer_next_inst();\n            if( pI ) {\n            ...\n              if ( (pI->op == LOAD_OP) || (pI->op == STORE_OP) || (pI->op == MEMORY_BARRIER_OP) ) {\n                m_shader->issue_warp(*m_mem_out,pI,active_mask,warp_id);\n                issued++;\n                issued_inst=true;\n                warp_inst_issued = true;\n              } else if ( (pI->op == SFU_OP) || (pI->op == ALU_SFU_OP) ) {\n                m_shader->issue_warp(*m_sfu_out,pI,active_mask,warp_id);\n              }\n            }\n        }\n    }\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n",normalizedContent:" * libcuda\n   * cuda_runtime_api.cc\n * src\n   * abstract_hardware_model.h/cpp\n   * gpgpusim_entrypoint.h/cpp\n   * stream_manager.h/cpp\n   * cuda-sim\n     \n     * cuda-sim.h/cc 🐝\n       \n       code\n       \n       \n       \n       void function_info::ptx_assemble() {\n        for ( i=m_instructions.begin(); i != m_instructions.end(); i++ ) {\n         // map pc to instruction\n         g_pc_to_finfo[pc] = this;\n         // this is a uniform array, each entry is one instruction\n         m_instr_mem[n] = pi; \n         s_g_pc_to_insn.push_back(pi);\n         ssert(pi == s_g_pc_to_insn[pc]);\n         pi->set_m_instr_mem_index(n);\n         pi->set_pc(pc);\n        }\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       \n     \n     * memory.h/cc\n     \n     * opcode.h/def\n     \n     * ptx_loader.h/cc\n     \n     * ptx.y\n       it is a yacc/bison grammar file used to parse ptx (parallel thread execution) assembly code, which is a low-level intermediate representation used by nvidia gpus.\n       it defines the grammar rules for ptx assembly code and specifies how different components of ptx code should be interpreted and processed.\n       this includes recognizing various ptx instructions, operands, directives, and control structures, and translating them into an internal representation that the simulator can work with.\n       \n       when it meets instruction statement it will call add_instruction.\n       \n       code\n       \n       \n       \n       statement_list: directive_statement { add_directive(); }\n       | instruction_statement { add_instruction();}\n       ...\n       \n       \n       1\n       2\n       3\n       \n     \n     * ptx_parser.h/cc the add_instruction used in ptx.y will call the following instruction.\n       \n       code\n       \n       \n       \n       void add_instruction() \n       {\n       ptx_instruction *i = new ptx_instruction(**);\n       g_instructions.push_back(i);\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       \n       in the end of function it will add all the instructins into function infomation code\n       \n       void end_function()\n       {\n       ...\n       g_func_info->add_inst( g_instructions );\n       ...\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       \n     \n     * ptx_ir.h/cc\n       \n       code\n       \n       \n       \n       //@@@@@@ ptx_ir.h\n       ...\n       std::vector<const symbol*> m_args;\n       // end_function will put function into this list\n       std::list<ptx_instruction*> m_instructions;\n       std::vector<basic_block_t*> m_basic_blocks;\n       \n       //@@@@@@ ptx_ir.cc\n       void gpgpu_ptx_assemble( std::string kname, void *kinfo ) {\n        function_info *func_info = (function_info *)kinfo;\n        // this will call cuda_sim ptx_assemble function\n        func_info->ptx_assemble();\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       13\n       \n     \n     * ptx_sim.h/cc\n   \n   * gpgpu-sim\n     * gpgpu-sim.h/cc\n     * shader.h/shader.cc\n     * mem_fetch.h/cc\n     * stack.h/cc\n     * addrdec.h/cc\n     * dram.h/cc\n     * traffic_breakdown.h/cc\n\n\n# how did gpgpu-sim get instruction from cuda?\n\n🐝 show how yacc/bison grammar file is used to add instruction in function_info. now we will describe how gpgpu-sim execute each instructin.\n\n# function mode\n\nabstract_hardware_model.cc implement this function, if you provde a warpid, it can return warp_instruction.\nthus, if we know next pc, we can use ptx_fetch_inst to get instruction.\n\ncode\n\n\n\n// @@@@@@ abstract_hardware_model.cc\n//! get the warp to be executed using the data taken form the simt stack\nwarp_inst_t core_t::getexecutewarp(unsigned warpid)\n{\n    unsigned pc,rpc;\n    m_simt_stack[warpid]->get_pdom_stack_top_info(&pc,&rpc);\n    warp_inst_t wi= *ptx_fetch_inst(pc);\n    wi.set_active(m_simt_stack[warpid]->get_active_mask());\n    return wi;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n      gpgpu*_sim_main_function\n                 |\n    gpgpu_cuda_ptx_sim_main_func\n                 |\n              execute\n                 |\n            executewarp\n                 |\n   getexecutewarp execute_warp_inst_t\n\n\ncode\n\n// @@@@@@ gpgpusim_entrypoint.cc\nint gpgpu_opencl_ptx_sim_main_func( kernel_info_t *grid )\n{\n    //calling the cuda ptx simulator, sending the kernel by reference and a flag set to true,\n    //the flag used by the function to distinguish opencl calls from the cuda simulation calls which\n    //it is needed by the called function to not register the exit the exit of opencl kernel as it doesn't register entering in the first place as the cuda kernels does\n   gpgpu_cuda_ptx_sim_main_func( *grid, true );\n   return 0;\n}\n\n// @@@@@@ cuda-sim.cc\n/*!\nthis function simulates the cuda code functionally, it takes a kernel_info_t parameter \nwhich holds the data for the cuda kernel to be executed\n!*/\nvoid gpgpu_cuda_ptx_sim_main_func( kernel_info_t &kernel, bool opencl ) {\n  while(!kernel.no_more_ctas_to_run()){\n    functionalcoresim cta(&kernel,g_the_gpu,\n    g_the_gpu->getshadercoreconfig()->warp_size\n    );\n    cta.execute();\n }\n}\n\n\nvoid functionalcoresim::execute()\n {\n    ...\n    //start executing the cta\n    while(true){\n        ...\n        for(unsigned i=0;i<m_warp_count;i++){\n            executewarp(i,allatbarrier,someonelive);\n        }\n        ...\n    }\n }\n\nvoid functionalcoresim::executewarp(unsigned i, bool &allatbarrier, bool & someonelive)\n{\n ...\n warp_inst_t inst =getexecutewarp(i);\n //!!!!! attention !!!!!!!!\n execute_warp_inst_t(inst,i);\n ...\n updatesimtstack( i, &inst );\n}\n\nconst warp_inst_t *ptx_fetch_inst( address_type pc )\n{\n    return function_info::pc_to_instruction(pc);\n}\n\n// @@@@@@ ptx_ir.h\nstatic const ptx_instruction* pc_to_instruction(unsigned pc) \n{\n  if( pc < s_g_pc_to_insn.size() )\n      return s_g_pc_to_insn[pc];\n  else\n      return null;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\n# timing mode\n\n * shader.cc decode() fill instruction into ibuffer\n * shader.h filled into m_ibuffer\n * shader.cc cycle issue warp\n\ncode\n\n// @@@@@@ shader.cc\nvoid shader_core_ctx::decode()\n{\n    if( m_inst_fetch_buffer.m_valid ) {\n        // decode 1 or 2 instructions and place them into ibuffer\n        address_type pc = m_inst_fetch_buffer.m_pc;\n        const warp_inst_t* pi1 = ptx_fetch_inst(pc);\n        m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(0,pi1);\n        m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();\n        ...\n    }\n}\n\n// @@@@@@ shader.h\n    void ibuffer_fill( unsigned slot, const warp_inst_t *pi )\n    {\n       m_ibuffer[slot].m_inst=pi;\n       m_ibuffer[slot].m_valid=true;\n    }\n\n    const warp_inst_t *ibuffer_next_inst() { return m_ibuffer[m_next].m_inst; }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nevery cycle, if current warp is done, it will pick form the m_next_cycle_prioritized_warps to schedule next warp.\nthe instruction is obtained from m_ibuffer.\n\ncode\n\nvoid scheduler_unit::cycle()\n{\n    sched_dprintf( \"scheduler_unit::cycle()\\n\" );\n    bool valid_inst = false;  // there was one warp with a valid instruction to issue (didn't require flush due to control hazard)\n    bool ready_inst = false;  // of the valid instructions, there was one not waiting for pending register writes\n    bool issued_inst = false; // of these we issued one\n\n    order_warps();\n    for ( std::vector< shd_warp_t* >::const_iterator iter = m_next_cycle_prioritized_warps.begin();\n          iter != m_next_cycle_prioritized_warps.end();\n          iter++ ) {\n        // don't consider warps that are not yet valid\n        if ( (*iter) == null || (*iter)->done_exit() ) {\n            continue;\n        }\n        while( !warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty() && (checked < max_issue) && (checked <= issued) && (issued < max_issue) ) {\n            const warp_inst_t *pi = warp(warp_id).ibuffer_next_inst();\n            if( pi ) {\n            ...\n              if ( (pi->op == load_op) || (pi->op == store_op) || (pi->op == memory_barrier_op) ) {\n                m_shader->issue_warp(*m_mem_out,pi,active_mask,warp_id);\n                issued++;\n                issued_inst=true;\n                warp_inst_issued = true;\n              } else if ( (pi->op == sfu_op) || (pi->op == alu_sfu_op) ) {\n                m_shader->issue_warp(*m_sfu_out,pi,active_mask,warp_id);\n              }\n            }\n        }\n    }\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n",charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"TO READ",frontmatter:{title:"TO READ",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/47871e/",tags:[null]},regularPath:"/03.gpu/1234.TODO.html",relativePath:"03.gpu/1234.TODO.md",key:"v-02aef12e",path:"/pages/47871e/",headersStr:null,content:" 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity",normalizedContent:" 1. a survey of architectural approaches for improving gpgpu performance, programmability and heterogeneity",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Understanding GPGPU-SIM 2 Instruction Execution",frontmatter:{title:"Understanding GPGPU-SIM 2 Instruction Execution",date:"2024-08-13T00:00:00.000Z",permalink:"/pages/458726/",tags:[null]},regularPath:"/03.gpu/13.gpgpusim.html",relativePath:"03.gpu/13.gpgpusim.md",key:"v-4349358d",path:"/pages/458726/",headers:[{level:3,title:"How do CUDA instructions get executed ?",slug:"how-do-cuda-instructions-get-executed",normalizedTitle:"how do cuda instructions get executed ?",charIndex:482}],headersStr:"How do CUDA instructions get executed ?",content:' * libcuda\n   * cuda_runtime_api.cc\n * src\n   * abstract_hardware_model.h/cpp\n   * gpgpusim_entrypoint.h/cpp\n   * stream_manager.h/cpp\n   * cuda-sim\n     * cuda-sim.h/cc\n     * memory.h/cc\n     * opcode.h/def\n     * ptx_loader.h/cc\n     * ptx.y\\\n     * ptx_parser.h/cc\n     * ptx_ir.h/cc\n     * ptx_sim.h/cc\n   * gpgpu-sim\n     * gpgpu-sim.h/cc\n     * shader.h/shader.cc\n     * mem_fetch.h/cc\n     * stack.h/cc\n     * addrdec.h/cc\n     * dram.h/cc\n     * traffic_breakdown.h/cc\n\n\n# How do CUDA instructions get executed ?\n\n# Instruction Level\n\nIn opcodes.def, it defines hook for each type of instruction.\n\nIn instruction.cc, it implements the detail of each function. If this is a Load instruction, mem->read() is exectued.\n\nCode\n\n// @@@@@@ opcodes.def\nOP_DEF(LD_OP,ld_impl,"ld",1,5)\nOP_DEF(ST_OP,st_impl,"st",0,5)\n\n// @@@@@@ instructions.cc\nvoid ld_exec( const ptx_instruction *pI, ptx_thread_info *thread ) \n{ \n   const operand_info &dst = pI->dst();\n   const operand_info &src1 = pI->src1();\n   ...\n   mem->read(addr,size/8,&data.s64);\n}\n\nvoid ld_impl( const ptx_instruction *pI, ptx_thread_info *thread ) \n{\n   ld_exec(pI,thread);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# Abstract Level\n\nThe abstract core calls execute_warp_inst_t, which will execute each thread in the warp.\n\nAs to each instruction, it will call a "DEFINE" macro, which will invoke function, described in previous Section.\n\nCode\n\n// @@@@@@ abstract_hardware_model.cc\n\nvoid core_t::execute_warp_inst_t(warp_inst_t &inst, unsigned warpId)\n{\n    for ( unsigned t=0; t < m_warp_size; t++ ) {\n        if( inst.active(t) ) {\n            if(warpId==(unsigned (-1)))\n                warpId = inst.warp_id();\n            unsigned tid=m_warp_size*warpId+t;\n            m_thread[tid]->ptx_exec_inst(inst,t);\n            \n            //virtual function\n            checkExecutionStatusAndUpdate(inst,t,tid);\n        }\n    } \n}\n\n// @@@@@@ cuda-sim.cc\nvoid ptx_thread_info::ptx_exec_inst( warp_inst_t &inst, unsigned lane_id)\n{\n      ...\n      switch ( pI->get_opcode() ) {\n      #define OP_DEF(OP,FUNC,STR,DST,CLASSIFICATION) case OP: FUNC(pI,this); op_classification = CLASSIFICATION; break;\n      ...\n      #include "opcodes.def"\n      #undef OP_DEF\n      default: printf( "Execution error: Invalid opcode (0x%x)\\n", pI->get_opcode() ); break;\n      }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n# Detail GPU Mode\n\nIn detail GPU, programmed in shader.cc, instruction is executed at issue time.\n\nThis is reasonable, as long as the latency and bandwidth is modeled correctly, it is accurate.\n\nCode\n\n// @@@@@@ shader.cc\nvoid shader_core_ctx::issue_warp( register_set& pipe_reg_set, const warp_inst_t* next_inst, const active_mask_t &active_mask, unsigned warp_id )\n{\n    ...\n    func_exec_inst( **pipe_reg );\n}\n\nvoid shader_core_ctx::func_exec_inst( warp_inst_t &inst )\n{\n    execute_warp_inst_t(inst);\n    // !!!!!! Notice that as to memory access instruction, it will generate memory access\n    if( inst.is_load() || inst.is_store() )\n        inst.generate_mem_accesses();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nThen we can go further, how is issue_warp called?\n\nIn shader.cc, for each cycle, it will check is the status of for each warp.\n\nAs to each warp, it will check whether the first instruction in the instruction buffer, after checking scoreboard, if this is no hazard and issue width is not saturated, the warp could be issued.\n\nCode\n\n// @@@@@@ shader.cc\nvoid scheduler_unit::cycle()\n{\n    SCHED_DPRINTF( "scheduler_unit::cycle()\\n" );\n    bool valid_inst = false;  // there was one warp with a valid instruction to issue (didn\'t require flush due to control hazard)\n    bool ready_inst = false;  // of the valid instructions, there was one not waiting for pending register writes\n    bool issued_inst = false; // of these we issued one\n\n    for ( std::vector< shd_warp_t* >::const_iterator iter = m_next_cycle_prioritized_warps.begin();\n          iter != m_next_cycle_prioritized_warps.end();\n          iter++ ) {\n        // Don\'t consider warps that are not yet valid\n        if ( (*iter) == NULL || (*iter)->done_exit() ) {\n            continue;\n        }\n        while( !warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty() ... && (checked <= issued) && (issued < max_issue) ) {\n         const warp_inst_t *pI = warp(warp_id).ibuffer_next_inst();\n         ...\n         if ( (pI->op == LOAD_OP) || (pI->op == STORE_OP) || (pI->op == MEMORY_BARRIER_OP) ) {\n             if( m_mem_out->has_free() ) {\n                 m_shader->issue_warp(*m_mem_out,pI,active_mask,warp_id);\n             }\n         } else {\n             \n         }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\nIn the above code, u can see the m_shader->issue_warp() . Underneath the issue_warp, it is func_exec_inst, as shown in the following code.\n\nCode\n\n// @@@@@@ shader.cc\n\nvoid shader_core_ctx::func_exec_inst( warp_inst_t &inst )\n{\n    execute_warp_inst_t(inst);\n    if( inst.is_load() || inst.is_store() )\n        inst.generate_mem_accesses();\n}\n\nvoid shader_core_ctx::issue_warp( register_set& pipe_reg_set, const warp_inst_t* next_inst, const active_mask_t &active_mask, unsigned warp_id )\n{\n    warp_inst_t** pipe_reg = pipe_reg_set.get_free();\n    assert(pipe_reg);\n\n    m_warp[warp_id].ibuffer_free();\n    assert(next_inst->valid());\n    **pipe_reg = *next_inst; // static instruction information\n    (*pipe_reg)->issue( active_mask, warp_id, gpu_tot_sim_cycle + gpu_sim_cycle, m_warp[warp_id].get_dynamic_warp_id() ); // dynamic instruction information\n    m_stats->shader_cycle_distro[2+(*pipe_reg)->active_count()]++;\n    func_exec_inst( **pipe_reg );\n    if( next_inst->op == BARRIER_OP ){\n        m_warp[warp_id].store_info_of_last_inst_at_barrier(*pipe_reg);\n        m_barriers.warp_reaches_barrier(m_warp[warp_id].get_cta_id(),warp_id,const_cast<warp_inst_t*> (next_inst));\n\n    }else if( next_inst->op == MEMORY_BARRIER_OP ){\n        m_warp[warp_id].set_membar();\n    }\n\n    updateSIMTStack(warp_id,*pipe_reg);\n    m_scoreboard->reserveRegisters(*pipe_reg);\n    m_warp[warp_id].set_next_pc(next_inst->pc + next_inst->isize);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n# Register Hazard\n\nIt seems like gpgpu-sim does not use register dependency wakeup, like ROB, which contains a register ID that if dependency instruction has executed, the result transfer through bypassnet work and wakeup pending instructions.\n\nIn gpgpu-sim, in the writeback stage, it will release its target registers in scoreboard.\n\nCode\n\n\n\n// @@@@@@ shader.cc\nvoid shader_core_ctx::writeback()\n{\n    warp_inst_t** preg = m_pipeline_reg[EX_WB].get_ready();\n    warp_inst_t* pipe_reg = (preg==NULL)? NULL:*preg;\n    while( preg and !pipe_reg->empty()) {\n        m_scoreboard->releaseRegisters( pipe_reg );\n        warp_inst_complete(*pipe_reg);\n    }\n}\n\n// @@@@@@ scoreboard.cc\n// Release target registers for an instruction\nvoid Scoreboard::releaseRegisters(const class warp_inst_t *inst) \n{\n    for( unsigned r=0; r < 4; r++) {\n        if(inst->out[r] > 0) {\n            releaseRegister(inst->warp_id(), inst->out[r]);\n        }\n    }\n}\n\n// It can be seen that in this function, there is no ready or pending status. Only bookkeeping.\n// Unmark register as write-pending\nvoid Scoreboard::releaseRegister(unsigned wid, unsigned regnum) \n{\n      reg_table[wid].erase(regnum);\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\nSo the scoreboard will maintains all pending registers. Once it is written back, the register will be released.\n\nIf another instruction is about to issue, it will check whether the register it reads or writes matches with any of the registes.\n\nCode\n\n\n\n/** \n * Checks to see if registers used by an instruction are reserved in the scoreboard\n *  \n * @return \n * true if WAW or RAW hazard (no WAR since in-order issue)\n **/ \nbool Scoreboard::checkCollision( unsigned wid, const class inst_t *inst ) const\n{\n\t// Get list of all input and output registers\n\tstd::set<int> inst_regs;\n\n\t// from 0 to 3\n\tif(inst->out[0] > 0) inst_regs.insert(inst->out[0]);\n\n\t// from 0 to 3\n\tif(inst->in[0] > 0) inst_regs.insert(inst->in[0]);\n\n\tif(inst->pred > 0) inst_regs.insert(inst->pred);\n\tif(inst->ar1 > 0) inst_regs.insert(inst->ar1);\n\tif(inst->ar2 > 0) inst_regs.insert(inst->ar2);\n\n\t// Check for collision, get the intersection of reserved registers and instruction registers\n\tstd::set<int>::const_iterator it2;\n\tfor ( it2=inst_regs.begin() ; it2 != inst_regs.end(); it2++ )\n\t\tif(reg_table[wid].find(*it2) != reg_table[wid].end()) {\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n# Function GPU Mode\n\nfunction mode is described in previous blog.',normalizedContent:' * libcuda\n   * cuda_runtime_api.cc\n * src\n   * abstract_hardware_model.h/cpp\n   * gpgpusim_entrypoint.h/cpp\n   * stream_manager.h/cpp\n   * cuda-sim\n     * cuda-sim.h/cc\n     * memory.h/cc\n     * opcode.h/def\n     * ptx_loader.h/cc\n     * ptx.y\\\n     * ptx_parser.h/cc\n     * ptx_ir.h/cc\n     * ptx_sim.h/cc\n   * gpgpu-sim\n     * gpgpu-sim.h/cc\n     * shader.h/shader.cc\n     * mem_fetch.h/cc\n     * stack.h/cc\n     * addrdec.h/cc\n     * dram.h/cc\n     * traffic_breakdown.h/cc\n\n\n# how do cuda instructions get executed ?\n\n# instruction level\n\nin opcodes.def, it defines hook for each type of instruction.\n\nin instruction.cc, it implements the detail of each function. if this is a load instruction, mem->read() is exectued.\n\ncode\n\n// @@@@@@ opcodes.def\nop_def(ld_op,ld_impl,"ld",1,5)\nop_def(st_op,st_impl,"st",0,5)\n\n// @@@@@@ instructions.cc\nvoid ld_exec( const ptx_instruction *pi, ptx_thread_info *thread ) \n{ \n   const operand_info &dst = pi->dst();\n   const operand_info &src1 = pi->src1();\n   ...\n   mem->read(addr,size/8,&data.s64);\n}\n\nvoid ld_impl( const ptx_instruction *pi, ptx_thread_info *thread ) \n{\n   ld_exec(pi,thread);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# abstract level\n\nthe abstract core calls execute_warp_inst_t, which will execute each thread in the warp.\n\nas to each instruction, it will call a "define" macro, which will invoke function, described in previous section.\n\ncode\n\n// @@@@@@ abstract_hardware_model.cc\n\nvoid core_t::execute_warp_inst_t(warp_inst_t &inst, unsigned warpid)\n{\n    for ( unsigned t=0; t < m_warp_size; t++ ) {\n        if( inst.active(t) ) {\n            if(warpid==(unsigned (-1)))\n                warpid = inst.warp_id();\n            unsigned tid=m_warp_size*warpid+t;\n            m_thread[tid]->ptx_exec_inst(inst,t);\n            \n            //virtual function\n            checkexecutionstatusandupdate(inst,t,tid);\n        }\n    } \n}\n\n// @@@@@@ cuda-sim.cc\nvoid ptx_thread_info::ptx_exec_inst( warp_inst_t &inst, unsigned lane_id)\n{\n      ...\n      switch ( pi->get_opcode() ) {\n      #define op_def(op,func,str,dst,classification) case op: func(pi,this); op_classification = classification; break;\n      ...\n      #include "opcodes.def"\n      #undef op_def\n      default: printf( "execution error: invalid opcode (0x%x)\\n", pi->get_opcode() ); break;\n      }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n# detail gpu mode\n\nin detail gpu, programmed in shader.cc, instruction is executed at issue time.\n\nthis is reasonable, as long as the latency and bandwidth is modeled correctly, it is accurate.\n\ncode\n\n// @@@@@@ shader.cc\nvoid shader_core_ctx::issue_warp( register_set& pipe_reg_set, const warp_inst_t* next_inst, const active_mask_t &active_mask, unsigned warp_id )\n{\n    ...\n    func_exec_inst( **pipe_reg );\n}\n\nvoid shader_core_ctx::func_exec_inst( warp_inst_t &inst )\n{\n    execute_warp_inst_t(inst);\n    // !!!!!! notice that as to memory access instruction, it will generate memory access\n    if( inst.is_load() || inst.is_store() )\n        inst.generate_mem_accesses();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nthen we can go further, how is issue_warp called?\n\nin shader.cc, for each cycle, it will check is the status of for each warp.\n\nas to each warp, it will check whether the first instruction in the instruction buffer, after checking scoreboard, if this is no hazard and issue width is not saturated, the warp could be issued.\n\ncode\n\n// @@@@@@ shader.cc\nvoid scheduler_unit::cycle()\n{\n    sched_dprintf( "scheduler_unit::cycle()\\n" );\n    bool valid_inst = false;  // there was one warp with a valid instruction to issue (didn\'t require flush due to control hazard)\n    bool ready_inst = false;  // of the valid instructions, there was one not waiting for pending register writes\n    bool issued_inst = false; // of these we issued one\n\n    for ( std::vector< shd_warp_t* >::const_iterator iter = m_next_cycle_prioritized_warps.begin();\n          iter != m_next_cycle_prioritized_warps.end();\n          iter++ ) {\n        // don\'t consider warps that are not yet valid\n        if ( (*iter) == null || (*iter)->done_exit() ) {\n            continue;\n        }\n        while( !warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty() ... && (checked <= issued) && (issued < max_issue) ) {\n         const warp_inst_t *pi = warp(warp_id).ibuffer_next_inst();\n         ...\n         if ( (pi->op == load_op) || (pi->op == store_op) || (pi->op == memory_barrier_op) ) {\n             if( m_mem_out->has_free() ) {\n                 m_shader->issue_warp(*m_mem_out,pi,active_mask,warp_id);\n             }\n         } else {\n             \n         }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\nin the above code, u can see the m_shader->issue_warp() . underneath the issue_warp, it is func_exec_inst, as shown in the following code.\n\ncode\n\n// @@@@@@ shader.cc\n\nvoid shader_core_ctx::func_exec_inst( warp_inst_t &inst )\n{\n    execute_warp_inst_t(inst);\n    if( inst.is_load() || inst.is_store() )\n        inst.generate_mem_accesses();\n}\n\nvoid shader_core_ctx::issue_warp( register_set& pipe_reg_set, const warp_inst_t* next_inst, const active_mask_t &active_mask, unsigned warp_id )\n{\n    warp_inst_t** pipe_reg = pipe_reg_set.get_free();\n    assert(pipe_reg);\n\n    m_warp[warp_id].ibuffer_free();\n    assert(next_inst->valid());\n    **pipe_reg = *next_inst; // static instruction information\n    (*pipe_reg)->issue( active_mask, warp_id, gpu_tot_sim_cycle + gpu_sim_cycle, m_warp[warp_id].get_dynamic_warp_id() ); // dynamic instruction information\n    m_stats->shader_cycle_distro[2+(*pipe_reg)->active_count()]++;\n    func_exec_inst( **pipe_reg );\n    if( next_inst->op == barrier_op ){\n        m_warp[warp_id].store_info_of_last_inst_at_barrier(*pipe_reg);\n        m_barriers.warp_reaches_barrier(m_warp[warp_id].get_cta_id(),warp_id,const_cast<warp_inst_t*> (next_inst));\n\n    }else if( next_inst->op == memory_barrier_op ){\n        m_warp[warp_id].set_membar();\n    }\n\n    updatesimtstack(warp_id,*pipe_reg);\n    m_scoreboard->reserveregisters(*pipe_reg);\n    m_warp[warp_id].set_next_pc(next_inst->pc + next_inst->isize);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n# register hazard\n\nit seems like gpgpu-sim does not use register dependency wakeup, like rob, which contains a register id that if dependency instruction has executed, the result transfer through bypassnet work and wakeup pending instructions.\n\nin gpgpu-sim, in the writeback stage, it will release its target registers in scoreboard.\n\ncode\n\n\n\n// @@@@@@ shader.cc\nvoid shader_core_ctx::writeback()\n{\n    warp_inst_t** preg = m_pipeline_reg[ex_wb].get_ready();\n    warp_inst_t* pipe_reg = (preg==null)? null:*preg;\n    while( preg and !pipe_reg->empty()) {\n        m_scoreboard->releaseregisters( pipe_reg );\n        warp_inst_complete(*pipe_reg);\n    }\n}\n\n// @@@@@@ scoreboard.cc\n// release target registers for an instruction\nvoid scoreboard::releaseregisters(const class warp_inst_t *inst) \n{\n    for( unsigned r=0; r < 4; r++) {\n        if(inst->out[r] > 0) {\n            releaseregister(inst->warp_id(), inst->out[r]);\n        }\n    }\n}\n\n// it can be seen that in this function, there is no ready or pending status. only bookkeeping.\n// unmark register as write-pending\nvoid scoreboard::releaseregister(unsigned wid, unsigned regnum) \n{\n      reg_table[wid].erase(regnum);\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\nso the scoreboard will maintains all pending registers. once it is written back, the register will be released.\n\nif another instruction is about to issue, it will check whether the register it reads or writes matches with any of the registes.\n\ncode\n\n\n\n/** \n * checks to see if registers used by an instruction are reserved in the scoreboard\n *  \n * @return \n * true if waw or raw hazard (no war since in-order issue)\n **/ \nbool scoreboard::checkcollision( unsigned wid, const class inst_t *inst ) const\n{\n\t// get list of all input and output registers\n\tstd::set<int> inst_regs;\n\n\t// from 0 to 3\n\tif(inst->out[0] > 0) inst_regs.insert(inst->out[0]);\n\n\t// from 0 to 3\n\tif(inst->in[0] > 0) inst_regs.insert(inst->in[0]);\n\n\tif(inst->pred > 0) inst_regs.insert(inst->pred);\n\tif(inst->ar1 > 0) inst_regs.insert(inst->ar1);\n\tif(inst->ar2 > 0) inst_regs.insert(inst->ar2);\n\n\t// check for collision, get the intersection of reserved registers and instruction registers\n\tstd::set<int>::const_iterator it2;\n\tfor ( it2=inst_regs.begin() ; it2 != inst_regs.end(); it2++ )\n\t\tif(reg_table[wid].find(*it2) != reg_table[wid].end()) {\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n# function gpu mode\n\nfunction mode is described in previous blog.',charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"Understanding GPGPU-SIM 3 How is the simulation started",frontmatter:{title:"Understanding GPGPU-SIM 3 How is the simulation started",date:"2024-08-13T00:00:00.000Z",permalink:"/pages/458727/",tags:[null]},regularPath:"/03.gpu/14.gpgpusim.html",relativePath:"03.gpu/14.gpgpusim.md",key:"v-9e671962",path:"/pages/458727/",headers:[{level:3,title:"How is the simulation started?",slug:"how-is-the-simulation-started",normalizedTitle:"how is the simulation started?",charIndex:2}],headersStr:"How is the simulation started?",content:'# How is the simulation started?\n\n# From Binary File\n\nCode\n\n   0x000000000040171f <+13>:    callq  0x400ad0 <__cudaRegisterFatBinary@plt>\n=> 0x0000000000401724 <+18>:    mov    %rax,0x2029f5(%rip)        # 0x604120 <_ZL20__cudaFatCubinHandle>\n\n\n1\n2\n\nCode\n\n// @@@@@@ cuda_runtime_api.cc\nvoid** CUDARTAPI __cudaRegisterFatBinary( void *fatCubin ) {\n\tCUctx_st *context = GPGPUSim_Context();\n}\n\nstatic CUctx_st* GPGPUSim_Context()\n{\n\tstatic CUctx_st *the_context = NULL;\n\tif( the_context == NULL ) {\n\t\t_cuda_device_id *the_gpu = GPGPUSim_Init();\n\t\tthe_context = new CUctx_st(the_gpu);\n\t}\n\treturn the_context;\n}\n\nclass _cuda_device_id *GPGPUSim_Init()\n{\n\tstatic _cuda_device_id *the_device = NULL;\n\tif( !the_device ) {\n\t\tgpgpu_sim *the_gpu = gpgpu_ptx_sim_init_perf();\n\n\t\tcudaDeviceProp *prop = (cudaDeviceProp *) calloc(sizeof(cudaDeviceProp),1);\n\t\t...\n\t\tthe_gpu->set_prop(prop);\n\t\tthe_device = new _cuda_device_id(the_gpu);\n\t}\n\tstart_sim_thread(1);\n\treturn the_device;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\nGPGPUSim_Init() in cuda_runtime will call function from gpgpusim_entrypoint.cc.\n\nIt conductthe initialization and create gpgpu_sim and also stream_manager\n\n * gpu config\n * opcode latency config\n\nThis is the creation time of gpu simulation.\n\nNotice that function start_sim_thread. It starts the simulation thread.\n\nCode\n\n// @@@@@@ gpgpusim_entrypoint.cc gpgpu_ptx_sim_init_perf\ngpgpu_sim *gpgpu_ptx_sim_init_perf()\n{\n\n   read_sim_environment_variables();\n   read_parser_environment_variables();\n   option_parser_t opp = option_parser_create();\n\n   icnt_reg_options(opp);\n   g_the_gpu_config.reg_options(opp); // register GPU microrachitecture options\n   ptx_reg_options(opp);\n   ptx_opcocde_latency_options(opp);\n   option_parser_cmdline(opp, sg_argc, sg_argv); // parse configuration options\n   g_the_gpu_config.convert_byte_string();\n   fprintf(stdout, "GPGPU-Sim: Configuration options:\\n\\n");\n   option_parser_print(opp, stdout);\n\n   g_the_gpu_config.init();\n   g_the_gpu = new gpgpu_sim(g_the_gpu_config);\n   g_stream_manager = new stream_manager(g_the_gpu,g_cuda_launch_blocking);\n\n   g_simulation_starttime = time((time_t *)NULL);\n\n   sem_init(&g_sim_signal_start,0,0);\n   sem_init(&g_sim_signal_finish,0,0);\n   sem_init(&g_sim_signal_exit,0,0);\n\n   return g_the_gpu;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\ncuda_runtime_api.cc\n\nCode\n\n// @@@@@@ gpgpusim_entrypoint.cc\nvoid start_sim_thread(int api)\n{\n    if( g_sim_done ) {\n        g_sim_done = false;\n\t...\n\tpthread_create(&g_simulation_thread,NULL,gpgpu_sim_thread_concurrent,NULL);\n\n    }\n}\n\n\nvoid *gpgpu_sim_thread_concurrent(void*)\n{\n    // concurrent kernel execution simulation thread\n    do {\n\t...\n        while( g_stream_manager->empty() && !g_sim_done )\n            ;\n\t...\n        pthread_mutex_lock(&g_sim_lock);\n        g_sim_active = true;\n        pthread_mutex_unlock(&g_sim_lock);\n        bool active = false;\n        bool sim_cycles = false;\n        g_the_gpu->init();\n        do {\n            // check if a kernel has completed\n            // launch operation on device if one is pending and can be run\n\n            // Need to break this loop when a kernel completes. This was a\n            // source of non-deterministic behaviour in GPGPU-Sim (bug 147).\n            // If another stream operation is available, g_the_gpu remains active,\n            // causing this loop to not break. If the next operation happens to be\n            // another kernel, the gpu is not re-initialized and the inter-kernel\n            // behaviour may be incorrect. Check that a kernel has finished and\n            // no other kernel is currently running.\n            // !!!!!! This will check whether the operation is done and whether gpu is active\n            if(g_stream_manager->operation(&sim_cycles) && !g_the_gpu->active())\n                break;\n\n            //functional simulation\n            if( g_the_gpu->is_functional_sim()) {\n                kernel_info_t * kernel = g_the_gpu->get_functional_kernel();\n                gpgpu_cuda_ptx_sim_main_func(*kernel);\n                g_the_gpu->finish_functional_sim(kernel);\n            }\n\n            // !!!!!! This is the most essential part of event-driven function\n            // The gpu event is cycled().\n            //performance simulation\n            if( g_the_gpu->active() ) {\n                g_the_gpu->cycle();\n                sim_cycles = true;\n                g_the_gpu->deadlock_check();\n            }else {\n\t\tg_the_gpu->cycle();\n                if(g_the_gpu->cycle_insn_cta_max_hit()){\n                    g_stream_manager->stop_all_running_kernels();\n                    g_sim_done = true;\n                    break_limit = true;\n                }\n            }\n\n            active=g_the_gpu->active() || !g_stream_manager->empty_protected();\n\n        } while( active && !g_sim_done);\n\t...\n        pthread_mutex_lock(&g_sim_lock);\n        g_sim_active = false;\n        pthread_mutex_unlock(&g_sim_lock);\n    } while( !g_sim_done );\n    printf("GPGPU-Sim: *** simulation thread exiting ***\\n");\n    fflush(stdout);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n\n\nThe stream_manager operation function used above is not for drive each cycle of the simulation.\nInstead, it will call stream_operation do_operation to launch kernel on GPU.\nBased on different type of operation, if just copy from * to * it will copy, if it is kernel launch.\nIt will launch according to function mode or detail mode.\nBut, as to launch, it just put kenerl onto gpu-sim m_running_kernels queue.\nIt does not run it.\n\ng_the_gpu->cycle() drives the gpu simulation.\n\nCode\n\n// @@@@@@ stream_manger.cc\nbool stream_manager::operation( bool * sim)\n{\n    bool check=check_finished_kernel();\n    ...\n    stream_operation op =front();\n    if(!op.do_operation( m_gpu )) //not ready to execute\n    {\n       ...\n    }\n    ...\n    return check;\n}\n\nbool stream_operation::do_operation( gpgpu_sim *gpu )\n{\n    case stream_prefetch_host_to_device:\n    ...\n    case stream_memcpy_device_to_device:\n    ...\n    case stream_kernel_launch:\n        if( m_sim_mode ) { //Functional Sim\n            gpu->set_cache_config(m_kernel->name());\n            gpu->functional_launch( m_kernel );\n        }\n        else { //Performance Sim\n            if( gpu->can_start_kernel() && m_kernel->m_launch_latency == 0) {\n                gpu->set_cache_config(m_kernel->name());\n                gpu->launch( m_kernel );\n\t        gpu->getGmmu()->log_kernel_info(m_kernel->get_uid(), gpu_sim_cycle + gpu_tot_sim_cycle, false);\n                if(sim_prof_enable) {\n\t           kernel_stats* k_s = new kernel_stats(cur_cycle, m_stream->get_uid(), m_kernel->get_uid());\n\t           sim_prof[cur_cycle].push_back(k_s);\n\t        }\t\t\t\n            }\n            else {\n                return false;    \n            }\n        }\n        break;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\nCode\n\n// @@@@@@ gpu-sim.h/cc\nvoid gpgpu_sim::launch( kernel_info_t *kinfo )\n{\n   unsigned cta_size = kinfo->threads_per_cta();\n   if ( cta_size > m_shader_config->n_thread_per_shader ) {\n      abort();\n   }\n\n   unsigned n=0;\n   for(n=0; n < m_running_kernels.size(); n++ ) {\n\t// If previous kernel is already done or not empty yet, replace\n\t// !! There might be bug that if earlist kenerl is done, it will insert kernel to the earilist position\n\t// !! which might break first-in-first-out priority\n       if( (NULL==m_running_kernels[n]) || m_running_kernels[n]->done() ) {\n           m_running_kernels[n] = kinfo;\n           break;\n       }\n   }\n}\n\nvoid functional_launch(kernel_info_t * k) {\n     m_functional_sim = true;\n     m_functional_sim_kernel = k;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n',normalizedContent:'# how is the simulation started?\n\n# from binary file\n\ncode\n\n   0x000000000040171f <+13>:    callq  0x400ad0 <__cudaregisterfatbinary@plt>\n=> 0x0000000000401724 <+18>:    mov    %rax,0x2029f5(%rip)        # 0x604120 <_zl20__cudafatcubinhandle>\n\n\n1\n2\n\ncode\n\n// @@@@@@ cuda_runtime_api.cc\nvoid** cudartapi __cudaregisterfatbinary( void *fatcubin ) {\n\tcuctx_st *context = gpgpusim_context();\n}\n\nstatic cuctx_st* gpgpusim_context()\n{\n\tstatic cuctx_st *the_context = null;\n\tif( the_context == null ) {\n\t\t_cuda_device_id *the_gpu = gpgpusim_init();\n\t\tthe_context = new cuctx_st(the_gpu);\n\t}\n\treturn the_context;\n}\n\nclass _cuda_device_id *gpgpusim_init()\n{\n\tstatic _cuda_device_id *the_device = null;\n\tif( !the_device ) {\n\t\tgpgpu_sim *the_gpu = gpgpu_ptx_sim_init_perf();\n\n\t\tcudadeviceprop *prop = (cudadeviceprop *) calloc(sizeof(cudadeviceprop),1);\n\t\t...\n\t\tthe_gpu->set_prop(prop);\n\t\tthe_device = new _cuda_device_id(the_gpu);\n\t}\n\tstart_sim_thread(1);\n\treturn the_device;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\ngpgpusim_init() in cuda_runtime will call function from gpgpusim_entrypoint.cc.\n\nit conductthe initialization and create gpgpu_sim and also stream_manager\n\n * gpu config\n * opcode latency config\n\nthis is the creation time of gpu simulation.\n\nnotice that function start_sim_thread. it starts the simulation thread.\n\ncode\n\n// @@@@@@ gpgpusim_entrypoint.cc gpgpu_ptx_sim_init_perf\ngpgpu_sim *gpgpu_ptx_sim_init_perf()\n{\n\n   read_sim_environment_variables();\n   read_parser_environment_variables();\n   option_parser_t opp = option_parser_create();\n\n   icnt_reg_options(opp);\n   g_the_gpu_config.reg_options(opp); // register gpu microrachitecture options\n   ptx_reg_options(opp);\n   ptx_opcocde_latency_options(opp);\n   option_parser_cmdline(opp, sg_argc, sg_argv); // parse configuration options\n   g_the_gpu_config.convert_byte_string();\n   fprintf(stdout, "gpgpu-sim: configuration options:\\n\\n");\n   option_parser_print(opp, stdout);\n\n   g_the_gpu_config.init();\n   g_the_gpu = new gpgpu_sim(g_the_gpu_config);\n   g_stream_manager = new stream_manager(g_the_gpu,g_cuda_launch_blocking);\n\n   g_simulation_starttime = time((time_t *)null);\n\n   sem_init(&g_sim_signal_start,0,0);\n   sem_init(&g_sim_signal_finish,0,0);\n   sem_init(&g_sim_signal_exit,0,0);\n\n   return g_the_gpu;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\ncuda_runtime_api.cc\n\ncode\n\n// @@@@@@ gpgpusim_entrypoint.cc\nvoid start_sim_thread(int api)\n{\n    if( g_sim_done ) {\n        g_sim_done = false;\n\t...\n\tpthread_create(&g_simulation_thread,null,gpgpu_sim_thread_concurrent,null);\n\n    }\n}\n\n\nvoid *gpgpu_sim_thread_concurrent(void*)\n{\n    // concurrent kernel execution simulation thread\n    do {\n\t...\n        while( g_stream_manager->empty() && !g_sim_done )\n            ;\n\t...\n        pthread_mutex_lock(&g_sim_lock);\n        g_sim_active = true;\n        pthread_mutex_unlock(&g_sim_lock);\n        bool active = false;\n        bool sim_cycles = false;\n        g_the_gpu->init();\n        do {\n            // check if a kernel has completed\n            // launch operation on device if one is pending and can be run\n\n            // need to break this loop when a kernel completes. this was a\n            // source of non-deterministic behaviour in gpgpu-sim (bug 147).\n            // if another stream operation is available, g_the_gpu remains active,\n            // causing this loop to not break. if the next operation happens to be\n            // another kernel, the gpu is not re-initialized and the inter-kernel\n            // behaviour may be incorrect. check that a kernel has finished and\n            // no other kernel is currently running.\n            // !!!!!! this will check whether the operation is done and whether gpu is active\n            if(g_stream_manager->operation(&sim_cycles) && !g_the_gpu->active())\n                break;\n\n            //functional simulation\n            if( g_the_gpu->is_functional_sim()) {\n                kernel_info_t * kernel = g_the_gpu->get_functional_kernel();\n                gpgpu_cuda_ptx_sim_main_func(*kernel);\n                g_the_gpu->finish_functional_sim(kernel);\n            }\n\n            // !!!!!! this is the most essential part of event-driven function\n            // the gpu event is cycled().\n            //performance simulation\n            if( g_the_gpu->active() ) {\n                g_the_gpu->cycle();\n                sim_cycles = true;\n                g_the_gpu->deadlock_check();\n            }else {\n\t\tg_the_gpu->cycle();\n                if(g_the_gpu->cycle_insn_cta_max_hit()){\n                    g_stream_manager->stop_all_running_kernels();\n                    g_sim_done = true;\n                    break_limit = true;\n                }\n            }\n\n            active=g_the_gpu->active() || !g_stream_manager->empty_protected();\n\n        } while( active && !g_sim_done);\n\t...\n        pthread_mutex_lock(&g_sim_lock);\n        g_sim_active = false;\n        pthread_mutex_unlock(&g_sim_lock);\n    } while( !g_sim_done );\n    printf("gpgpu-sim: *** simulation thread exiting ***\\n");\n    fflush(stdout);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n\n\nthe stream_manager operation function used above is not for drive each cycle of the simulation.\ninstead, it will call stream_operation do_operation to launch kernel on gpu.\nbased on different type of operation, if just copy from * to * it will copy, if it is kernel launch.\nit will launch according to function mode or detail mode.\nbut, as to launch, it just put kenerl onto gpu-sim m_running_kernels queue.\nit does not run it.\n\ng_the_gpu->cycle() drives the gpu simulation.\n\ncode\n\n// @@@@@@ stream_manger.cc\nbool stream_manager::operation( bool * sim)\n{\n    bool check=check_finished_kernel();\n    ...\n    stream_operation op =front();\n    if(!op.do_operation( m_gpu )) //not ready to execute\n    {\n       ...\n    }\n    ...\n    return check;\n}\n\nbool stream_operation::do_operation( gpgpu_sim *gpu )\n{\n    case stream_prefetch_host_to_device:\n    ...\n    case stream_memcpy_device_to_device:\n    ...\n    case stream_kernel_launch:\n        if( m_sim_mode ) { //functional sim\n            gpu->set_cache_config(m_kernel->name());\n            gpu->functional_launch( m_kernel );\n        }\n        else { //performance sim\n            if( gpu->can_start_kernel() && m_kernel->m_launch_latency == 0) {\n                gpu->set_cache_config(m_kernel->name());\n                gpu->launch( m_kernel );\n\t        gpu->getgmmu()->log_kernel_info(m_kernel->get_uid(), gpu_sim_cycle + gpu_tot_sim_cycle, false);\n                if(sim_prof_enable) {\n\t           kernel_stats* k_s = new kernel_stats(cur_cycle, m_stream->get_uid(), m_kernel->get_uid());\n\t           sim_prof[cur_cycle].push_back(k_s);\n\t        }\t\t\t\n            }\n            else {\n                return false;    \n            }\n        }\n        break;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\ncode\n\n// @@@@@@ gpu-sim.h/cc\nvoid gpgpu_sim::launch( kernel_info_t *kinfo )\n{\n   unsigned cta_size = kinfo->threads_per_cta();\n   if ( cta_size > m_shader_config->n_thread_per_shader ) {\n      abort();\n   }\n\n   unsigned n=0;\n   for(n=0; n < m_running_kernels.size(); n++ ) {\n\t// if previous kernel is already done or not empty yet, replace\n\t// !! there might be bug that if earlist kenerl is done, it will insert kernel to the earilist position\n\t// !! which might break first-in-first-out priority\n       if( (null==m_running_kernels[n]) || m_running_kernels[n]->done() ) {\n           m_running_kernels[n] = kinfo;\n           break;\n       }\n   }\n}\n\nvoid functional_launch(kernel_info_t * k) {\n     m_functional_sim = true;\n     m_functional_sim_kernel = k;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n',charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"Understanding GPGPU-SIM 4 Microarchitecture",frontmatter:{title:"Understanding GPGPU-SIM 4 Microarchitecture",date:"2024-08-14T00:00:00.000Z",permalink:"/pages/45872/",tags:[null]},regularPath:"/03.gpu/15.gpgpusim.html",relativePath:"03.gpu/15.gpgpusim.md",key:"v-1e4fb111",path:"/pages/45872/",headers:[{level:3,title:"What is the microarchitecture of GPGPU-SIM?",slug:"what-is-the-microarchitecture-of-gpgpu-sim",normalizedTitle:"what is the microarchitecture of gpgpu-sim?",charIndex:2},{level:3,title:"configuration file",slug:"configuration-file",normalizedTitle:"configuration file",charIndex:5670}],headersStr:"What is the microarchitecture of GPGPU-SIM? configuration file",content:"# What is the microarchitecture of GPGPU-SIM?\n\n\n\n# shader_core_ctx\n\nThis should be the streaming multiprocessors that exeutes at warp level and shares the same L1 cache.\n\nCode\n\n// @@@@@@ shader.cc\nshader_core_ctx::shader_core_ctx( class gpgpu_sim *gpu, \n                                  class simt_core_cluster *cluster,\n                                  unsigned shader_id,\n                                  unsigned tpc_id,\n                                  const struct shader_core_config *config,\n                                  const struct memory_config *mem_config,\n                                  shader_core_stats *stats,\n\t\t\t\t  class gpgpu_new_stats *new_stats )\n   : core_t( gpu, NULL, config->warp_size, config->n_thread_per_shader ),\n     m_barriers( this, config->max_warps_per_shader, config->max_cta_per_core, config->max_barriers_per_cta, config->warp_size ),\n     m_dynamic_warp_id(0)\n{\n    ...\n    m_L1I = new read_only_cache( name,m_config->m_L1I_config,m_sid,get_shader_instruction_cache_id(),m_icnt,IN_L1I_MISS_QUEUE);\n    ...\n    m_warp.resize(m_config->max_warps_per_shader, shd_warp_t(this, warp_size));\n    m_scoreboard = new Scoreboard(m_sid, m_config->max_warps_per_shader);\n}\n\n// different stages:\n{\n    void decode();\n    \n    void issue();\n    friend class scheduler_unit; //this is needed to use private issue warp.\n    friend class TwoLevelScheduler;\n    friend class LooseRoundRobbinScheduler;\n    void issue_warp( register_set& warp, const warp_inst_t *pI, const active_mask_t &active_mask, unsigned warp_id );\n    void func_exec_inst( warp_inst_t &inst );\n\n     // Returns numbers of addresses in translated_addrs\n    unsigned translate_local_memaddr( address_type localaddr, unsigned tid, unsigned num_shader, unsigned datasize, new_addr_type* translated_addrs );\n\n    void read_operands();\n    \n    void execute();\n    \n    void writeback();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n# simt_core_cluster\n\nThis should be at GPU level.\n\nCode\n\n// @@@@@@ shader.cc\nclass simt_core_cluster {\npublic:\n    simt_core_cluster( class gpgpu_sim *gpu, \n                       unsigned cluster_id, \n                       const struct shader_core_config *config, \n                       const struct memory_config *mem_config,\n                       shader_core_stats *stats,\n                       memory_stats_t *mstats,\n\t\t       class gpgpu_new_stats *new_stats ) {\n\n  }\n}\n\nsimt_core_cluster::simt_core_cluster( class gpgpu_sim *gpu, \n                                      unsigned cluster_id, \n                                      const struct shader_core_config *config, \n                                      const struct memory_config *mem_config,\n                                      shader_core_stats *stats, \n                                      class memory_stats_t *mstats,\n\t\t\t\t      class gpgpu_new_stats *new_stats )\n{\n    m_config = config;\n    m_cta_issue_next_core=m_config->n_simt_cores_per_cluster-1; // this causes first launch to use hw cta 0\n    m_cluster_id=cluster_id;\n    m_gpu = gpu;\n    m_stats = stats;\n    m_memory_stats = mstats;\n    \n    m_new_stats = new_stats;\n\n    m_core = new shader_core_ctx*[ config->n_simt_cores_per_cluster ];\n    for( unsigned i=0; i < config->n_simt_cores_per_cluster; i++ ) {\n        unsigned sid = m_config->cid_to_sid(i,m_cluster_id);\n        m_core[i] = new shader_core_ctx(gpu,this,sid,m_cluster_id,config,mem_config,stats, new_stats);\n        m_core_sim_order.push_back(i); \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n# how thread block is issued to stream multiprocessor?\n\ngpgpu-sim cycle() function\n\nCode\n\n// @@@@@@ gpgpu_sim\nvoid gpgpu_sim::cycle()\n{\n    ...\n    m_cluster[i]->icnt_cycle();\n    ...\n    m_memory_partition_unit[i]->dram_cycle();\n    ...\n    m_memory_sub_partition[i]->cache_cycle(gpu_sim_cycle+gpu_tot_sim_cycle);\n    ...\n    icnt_transfer();\n    ...\n    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {\n        if (m_cluster[i]->get_not_completed() || get_more_cta_left() ) {\n            m_cluster[i]->core_cycle();\n            *active_sms+=m_cluster[i]->get_n_active_sms();\n         }\n    }\n    issue_block2core();\n}\n\nvoid gpgpu_sim::issue_block2core()\n{\n    unsigned last_issued = m_last_cluster_issue; \n    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {\n        unsigned idx = (i + last_issued + 1) % m_shader_config->n_simt_clusters;\n        unsigned num = m_cluster[idx]->issue_block2core();\n        if( num ) {\n            m_last_cluster_issue=idx;\n            m_total_cta_launched += num;\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\nsimt_core_cluster issue block to stream multiprocessr.\n\nCode\n\n// @@@@@@ shader.cc\nunsigned simt_core_cluster::issue_block2core()\n{\n    unsigned num_blocks_issued=0;\n    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) {\n        unsigned core = (i+m_cta_issue_next_core+1)%m_config->n_simt_cores_per_cluster;\n\n        kernel_info_t * kernel;\n\n        if(m_config->gpgpu_concurrent_kernel_sm) {//concurrent kernel on sm \n            //always select latest issued kernel\n            kernel_info_t *k = m_gpu->select_kernel();\n            kernel = k;\n        }\n\t...\n\n        if( m_gpu->kernel_more_cta_left(kernel) && \n            m_core[core]->can_issue_1block(*kernel)) {\n            m_core[core]->issue_block2core(*kernel);\n            ...\n        }\n    }\n    return num_blocks_issued;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# configuration file\n\n./config/GeForceGTX1080Ti/gpgpusim.config\n\ngpgpu_n_clusters means the number of stream multiprocessing cores\n\nCode\n\n# high level architecture configuration\n-gpgpu_n_clusters 28\n\n\n\n1\n2\n3\n\n\nHow does gpgpu-sim configure the number of cores inside the SM? I didn't find any configuration for that.",normalizedContent:"# what is the microarchitecture of gpgpu-sim?\n\n\n\n# shader_core_ctx\n\nthis should be the streaming multiprocessors that exeutes at warp level and shares the same l1 cache.\n\ncode\n\n// @@@@@@ shader.cc\nshader_core_ctx::shader_core_ctx( class gpgpu_sim *gpu, \n                                  class simt_core_cluster *cluster,\n                                  unsigned shader_id,\n                                  unsigned tpc_id,\n                                  const struct shader_core_config *config,\n                                  const struct memory_config *mem_config,\n                                  shader_core_stats *stats,\n\t\t\t\t  class gpgpu_new_stats *new_stats )\n   : core_t( gpu, null, config->warp_size, config->n_thread_per_shader ),\n     m_barriers( this, config->max_warps_per_shader, config->max_cta_per_core, config->max_barriers_per_cta, config->warp_size ),\n     m_dynamic_warp_id(0)\n{\n    ...\n    m_l1i = new read_only_cache( name,m_config->m_l1i_config,m_sid,get_shader_instruction_cache_id(),m_icnt,in_l1i_miss_queue);\n    ...\n    m_warp.resize(m_config->max_warps_per_shader, shd_warp_t(this, warp_size));\n    m_scoreboard = new scoreboard(m_sid, m_config->max_warps_per_shader);\n}\n\n// different stages:\n{\n    void decode();\n    \n    void issue();\n    friend class scheduler_unit; //this is needed to use private issue warp.\n    friend class twolevelscheduler;\n    friend class looseroundrobbinscheduler;\n    void issue_warp( register_set& warp, const warp_inst_t *pi, const active_mask_t &active_mask, unsigned warp_id );\n    void func_exec_inst( warp_inst_t &inst );\n\n     // returns numbers of addresses in translated_addrs\n    unsigned translate_local_memaddr( address_type localaddr, unsigned tid, unsigned num_shader, unsigned datasize, new_addr_type* translated_addrs );\n\n    void read_operands();\n    \n    void execute();\n    \n    void writeback();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n# simt_core_cluster\n\nthis should be at gpu level.\n\ncode\n\n// @@@@@@ shader.cc\nclass simt_core_cluster {\npublic:\n    simt_core_cluster( class gpgpu_sim *gpu, \n                       unsigned cluster_id, \n                       const struct shader_core_config *config, \n                       const struct memory_config *mem_config,\n                       shader_core_stats *stats,\n                       memory_stats_t *mstats,\n\t\t       class gpgpu_new_stats *new_stats ) {\n\n  }\n}\n\nsimt_core_cluster::simt_core_cluster( class gpgpu_sim *gpu, \n                                      unsigned cluster_id, \n                                      const struct shader_core_config *config, \n                                      const struct memory_config *mem_config,\n                                      shader_core_stats *stats, \n                                      class memory_stats_t *mstats,\n\t\t\t\t      class gpgpu_new_stats *new_stats )\n{\n    m_config = config;\n    m_cta_issue_next_core=m_config->n_simt_cores_per_cluster-1; // this causes first launch to use hw cta 0\n    m_cluster_id=cluster_id;\n    m_gpu = gpu;\n    m_stats = stats;\n    m_memory_stats = mstats;\n    \n    m_new_stats = new_stats;\n\n    m_core = new shader_core_ctx*[ config->n_simt_cores_per_cluster ];\n    for( unsigned i=0; i < config->n_simt_cores_per_cluster; i++ ) {\n        unsigned sid = m_config->cid_to_sid(i,m_cluster_id);\n        m_core[i] = new shader_core_ctx(gpu,this,sid,m_cluster_id,config,mem_config,stats, new_stats);\n        m_core_sim_order.push_back(i); \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n# how thread block is issued to stream multiprocessor?\n\ngpgpu-sim cycle() function\n\ncode\n\n// @@@@@@ gpgpu_sim\nvoid gpgpu_sim::cycle()\n{\n    ...\n    m_cluster[i]->icnt_cycle();\n    ...\n    m_memory_partition_unit[i]->dram_cycle();\n    ...\n    m_memory_sub_partition[i]->cache_cycle(gpu_sim_cycle+gpu_tot_sim_cycle);\n    ...\n    icnt_transfer();\n    ...\n    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {\n        if (m_cluster[i]->get_not_completed() || get_more_cta_left() ) {\n            m_cluster[i]->core_cycle();\n            *active_sms+=m_cluster[i]->get_n_active_sms();\n         }\n    }\n    issue_block2core();\n}\n\nvoid gpgpu_sim::issue_block2core()\n{\n    unsigned last_issued = m_last_cluster_issue; \n    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {\n        unsigned idx = (i + last_issued + 1) % m_shader_config->n_simt_clusters;\n        unsigned num = m_cluster[idx]->issue_block2core();\n        if( num ) {\n            m_last_cluster_issue=idx;\n            m_total_cta_launched += num;\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\nsimt_core_cluster issue block to stream multiprocessr.\n\ncode\n\n// @@@@@@ shader.cc\nunsigned simt_core_cluster::issue_block2core()\n{\n    unsigned num_blocks_issued=0;\n    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) {\n        unsigned core = (i+m_cta_issue_next_core+1)%m_config->n_simt_cores_per_cluster;\n\n        kernel_info_t * kernel;\n\n        if(m_config->gpgpu_concurrent_kernel_sm) {//concurrent kernel on sm \n            //always select latest issued kernel\n            kernel_info_t *k = m_gpu->select_kernel();\n            kernel = k;\n        }\n\t...\n\n        if( m_gpu->kernel_more_cta_left(kernel) && \n            m_core[core]->can_issue_1block(*kernel)) {\n            m_core[core]->issue_block2core(*kernel);\n            ...\n        }\n    }\n    return num_blocks_issued;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# configuration file\n\n./config/geforcegtx1080ti/gpgpusim.config\n\ngpgpu_n_clusters means the number of stream multiprocessing cores\n\ncode\n\n# high level architecture configuration\n-gpgpu_n_clusters 28\n\n\n\n1\n2\n3\n\n\nhow does gpgpu-sim configure the number of cores inside the sm? i didn't find any configuration for that.",charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"Warp Related Memory Optimization",frontmatter:{title:"Warp Related Memory Optimization",date:"2024-08-15T00:00:00.000Z",permalink:"/pages/45873/",tags:[null]},regularPath:"/03.gpu/17.warp_mem.html",relativePath:"03.gpu/17.warp_mem.md",key:"v-c25059fe",path:"/pages/45873/",headers:[{level:3,title:"1. [215] CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization",slug:"_1-215-cudadma-optimizing-gpu-memory-bandwidth-via-warp-specialization",normalizedTitle:"1. [215] cudadma: optimizing gpu memory bandwidth via warp specialization",charIndex:1}],headersStr:"1. [215] CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization",content:" 1. [215] CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization\n 2. \n\n----------------------------------------\n\n\n# 1. [215] CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization\n\nAPI:\n\n * One synchronization point corresponds to the data having been consumed and the buffer standing empty awaiting the next transfer.\n   The compute threads indicate this status using a non-blocking call to start_async_dma()\n * The DMA threads wait to begin this transfer using the blocking call wait_for_dma_start()\n * DMA warps indicate that a transfer is complete using a non-blocking call to finish_async_dma()\n * The compute warps wait for a transfer to complete using a blocking call to wait_for_dma_finish()\n * The DMA-side calls are usually abstracted behind execute_dma()",normalizedContent:" 1. [215] cudadma: optimizing gpu memory bandwidth via warp specialization\n 2. \n\n----------------------------------------\n\n\n# 1. [215] cudadma: optimizing gpu memory bandwidth via warp specialization\n\napi:\n\n * one synchronization point corresponds to the data having been consumed and the buffer standing empty awaiting the next transfer.\n   the compute threads indicate this status using a non-blocking call to start_async_dma()\n * the dma threads wait to begin this transfer using the blocking call wait_for_dma_start()\n * dma warps indicate that a transfer is complete using a non-blocking call to finish_async_dma()\n * the compute warps wait for a transfer to complete using a blocking call to wait_for_dma_finish()\n * the dma-side calls are usually abstracted behind execute_dma()",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Understanding GPGPU-SIM 5  Memory Interface",frontmatter:{title:"Understanding GPGPU-SIM 5  Memory Interface",date:"2024-08-15T00:00:00.000Z",permalink:"/pages/45874/",tags:[null]},regularPath:"/03.gpu/16.gpgpusim.html",relativePath:"03.gpu/16.gpgpusim.md",key:"v-e85a225a",path:"/pages/45874/",headers:[{level:3,title:"GPGPU-sim Memory Interface",slug:"gpgpu-sim-memory-interface",normalizedTitle:"gpgpu-sim memory interface",charIndex:2}],headersStr:"GPGPU-sim Memory Interface",content:"# GPGPU-sim Memory Interface\n\n# Response Phase from Interconnect to SM\n\nsimt_core_cluster is at GPU level, it can fetch response into m_response_fifo.\n\nicnt_pop, if return non null ptr, it is a memory fetch repsonse, push it into m_response_fifo.\n\nCode\n\n// @@@@@@ shader.cc\n\nvoid simt_core_cluster::icnt_cycle()\n{\n    // pop from upward queue (GMMU to CU) of cluster and push it to the one in core (SM/CU)\n    if ( !m_gmmu_cu_queue.empty() ) {\n      mem_fetch *mf = m_gmmu_cu_queue.front();\n      ...\n      m_core[cid]->accept_access_response(mf);\n    } \n    \n    // pop it from the downward queue (CU to GMMU) of the core (SM/CU) and push it to the one in cluster (TPC)\n    for (unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++) {\n       if (!m_core[i]->empty_cu_gmmu_queue()){\n          mem_fetch *mf = m_core[i]->front_cu_gmmu_queue();\n          ...\n          m_cu_gmmu_queue.push_front(mf);\n       }\n    }\n\n    // Forward response from GPU response fifo into shader core (SM) response fifo\n    if( !m_response_fifo.empty() ) {\n        mem_fetch *mf = m_response_fifo.front();\n        unsigned cid = m_config->sid_to_cid(mf->get_sid());\n        ...\n            // data response\n            if( !m_core[cid]->ldst_unit_response_buffer_full() ) {\n                m_response_fifo.pop_front();\n                // GPU ---\x3e SM\n                m_core[cid]->accept_ldst_unit_response(mf);\n            }\n        }\n    }\n\n    // Accept Response from Interconnect Network\n    if( m_response_fifo.size() < m_config->n_simt_ejection_buffer_size ) {\n        mem_fetch *mf = (mem_fetch*) ::icnt_pop(m_cluster_id);\n        if (!mf) \n            return;\n\n        // The packet size varies depending on the type of request: \n        // - For read request and atomic request, the packet contains the data \n        // - For write-ack, the packet only has control metadata\n        ...\n        m_stats->m_incoming_traffic_stats->record_traffic(mf, packet_size); \n        mf->set_status(IN_CLUSTER_TO_SHADER_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);\n        ...\n        // Interconnect to GPU\n        m_response_fifo.push_back(mf);\n        m_stats->n_mem_to_simt[m_cluster_id] += mf->get_num_flits(false);\n    } \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\nThe flow below is from SM to ldst unit and then to L1 cache.\n\nThe L1 cache is included in ldst unit.\n\nCode\n\n\nvoid shader_core_ctx::accept_ldst_unit_response(mem_fetch * mf) \n{\n   m_ldst_unit->fill(mf);\n}\n\nvoid ldst_unit::fill( mem_fetch *mf )\n{\n    mf->set_status(IN_SHADER_LDST_RESPONSE_FIFO,gpu_sim_cycle+gpu_tot_sim_cycle);\n    m_response_fifo.push_back(mf);\n}\n\nvoid ldst_unit::cycle()\n{\n   writeback();\n   ...\n   if( !m_response_fifo.empty() ) {\n       mem_fetch *mf = m_response_fifo.front();\n       ...\n    \t   if( mf->get_type() == WRITE_ACK || ( m_config->gpgpu_perfect_mem && mf->get_is_write() )) {\n               m_core->store_ack(mf);\n               m_response_fifo.pop_front();\n\n               if ( m_gpu->get_global_memory()->is_page_managed(mf->get_mem_access().get_addr(), mf->get_mem_access().get_size()) ) {\n                    m_gpu->getGmmu()->reserve_pages_remove(mf->get_mem_access().get_addr(), mf->get_mem_access().get_uid());\n               }\n               ...\n           } else {\n              ...\n              if (m_L1D->fill_port_free()) {\n                   m_L1D->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);\n                   m_response_fifo.pop_front();\n              }\n           }\n       }\n   }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\nAt this time the response is fed into L1 Cache, which is shared in SM.\n\nBut how is it responsed to core?\n\n# Response Phase From L1 Cache to Wrap execution\n\nCode\n\n/// @@@@@@ gpu-cache.cc\n/// Interface for response from lower memory level (model bandwidth restictions in caller)\nvoid baseline_cache::fill(mem_fetch *mf, unsigned time){\n    ...\n    if ( m_config.m_alloc_policy == ON_MISS )\n        m_tag_array->fill(e->second.m_cache_index,time);\n    else if ( m_config.m_alloc_policy == ON_FILL )\n        m_tag_array->fill(e->second.m_block_addr,time);\n\n    m_mshrs.mark_ready(e->second.m_block_addr, has_atomic);\n    ...\n}\n\n/// Accept a new cache fill response: mark entry ready for processing\nvoid mshr_table::mark_ready( new_addr_type block_addr, bool &has_atomic ){\n    ...\n    m_current_response.push_back( block_addr );\n    ...\n}\n\n/// Returns next ready access\nmem_fetch *mshr_table::next_access(){\n    ...\n    new_addr_type block_addr = m_current_response.front();\n    ...\n    mem_fetch *result = m_data[block_addr].m_list.front();\n    return result;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\ncache wrapped the next_access, when cache->next_access() is called, it will call mshr next_access().\n\nwriteback() function()\n\n 1. m_next_wb store next_writeback_function, it could be hit in cache or just get response from interconnect\\\n    * scoreboard relase register\n    * m_core->warp_inst_complete\n 2. update m_next_wb from MSHR in L1Cache\n\nCode\n\nvoid ldst_unit::writeback()\n{\n    // process next instruction that is going to writeback\n    if( !m_next_wb.empty() ) {\n        if( m_operand_collector->writeback(m_next_wb) ) {\n            bool insn_completed = false; \n            for( unsigned r=0; r < 4; r++ ) {\n                if( m_next_wb.out[r] > 0 ) {\n\t\t    ...\n\t\t    else { // shared \n                        m_scoreboard->releaseRegister( m_next_wb.warp_id(), m_next_wb.out[r] );\n                        insn_completed = true; \n                    }\n                }\n            }\n            if( insn_completed ) {\n                m_core->warp_inst_complete(m_next_wb);\n            }\n            m_next_wb.clear();\n            m_last_inst_gpu_sim_cycle = gpu_sim_cycle;\n            m_last_inst_gpu_tot_sim_cycle = gpu_tot_sim_cycle;\n        }\n    }\n\n\n    for( unsigned c = 0; m_next_wb.empty() && (c < m_num_writeback_clients); c++ ) {\n        case 4: \n            if( m_L1D && m_L1D->access_ready() ) {\n                mem_fetch *mf = m_L1D->next_access();\n                m_next_wb = mf->get_inst();\n\n                if ( m_gpu->get_global_memory()->is_page_managed(mf->get_mem_access().get_addr(), mf->get_mem_access().get_size()) ) { \n                    m_gpu->getGmmu()->reserve_pages_remove(mf->get_mem_access().get_addr(), mf->get_mem_access().get_uid());\n                }\n\t\t \n                delete mf;\n                serviced_client = next_client; \n            }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\nm_core->warp_inst_complete is simple:\n\nCode\n\nvoid shader_core_ctx::warp_inst_complete(const warp_inst_t &inst)\n{\n  ...\n  m_gpu->gpu_sim_insn += inst.active_count();\n  inst.completed(gpu_tot_sim_cycle + gpu_sim_cycle);\n  ...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# How is memory request generated from warp inst?\n\n 1. warp_inst will generate memory access, for address of each thread in the warp, it will be push into access_q.\n    When the warp is issued, it will call generate_mem_access to generate this m_access_q.\n\nCode\n\n// @@@@@@ abstract_hardware_mode.cc\nvoid warp_inst_t::generate_mem_accesses()\n{\n    if( cache_block_size ) {\n\t...\n        for( unsigned thread=0; thread < m_config->warp_size; thread++ ) {\n            new_addr_type addr = m_per_scalar_thread[thread].memreqaddr[0];\n            unsigned block_address = line_size_based_tag_func(addr,cache_block_size);\n            accesses[block_address].set(thread);\n        }\n        for( a=accesses.begin(); a != accesses.end(); ++a ) \n            m_accessq.push_back( mem_access_t(access_type,a->first,cache_block_size,is_write,a->second,byte_mask) );\n}\n\n// The above function is called by:\nvoid shader_core_ctx::func_exec_inst( warp_inst_t &inst )\n{\n    execute_warp_inst_t(inst);\n    if( inst.is_load() || inst.is_store() )\n        inst.generate_mem_accesses();\n}\n\nvoid shader_core_ctx::issue_warp( register_set& pipe_reg_set, const warp_inst_t* next_inst, const active_mask_t &active_mask, unsigned warp_id )\n{\n    warp_inst_t** pipe_reg = pipe_reg_set.get_free();\n    ...\n    m_warp[warp_id].ibuffer_free();\n    ...\n    **pipe_reg = *next_inst; // static instruction information\n    (*pipe_reg)->issue( active_mask, warp_id, gpu_tot_sim_cycle + gpu_sim_cycle, m_warp[warp_id].get_dynamic_warp_id() ); // dynamic instruction information\n    m_stats->shader_cycle_distro[2+(*pipe_reg)->active_count()]++;\n    func_exec_inst( **pipe_reg );\n    ...\n    updateSIMTStack(warp_id,*pipe_reg);\n    m_scoreboard->reserveRegisters(*pipe_reg);\n    m_warp[warp_id].set_next_pc(next_inst->pc + next_inst->isize);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n 2. ldst_unit::memory_cycle\n    In this memory_cycle, it will invoke each access request inside inst.\n    The access will be converted into memory request, sending out downstream to L1 cache.\n    \n    This means that if each thread access a different block in cache, step 1) will generate 32 request inside the m_access_q.\n    Then memory_cycle will at least needs 32 cycle to allocate mem_fetch request for each req.\n\nCode\n\n// shader.cc\nbool ldst_unit::memory_cycle( warp_inst_t &inst, mem_stage_stall_type &stall_reason, mem_stage_access_type &access_type )\n{\n   if ( !inst.accessq_empty() ) {\n       \tconst mem_access_t &access = inst.accessq_front();\n        if( bypassL1D ) {\n           // bypass L1 cache\n           unsigned control_size = inst.is_store() ? WRITE_PACKET_SIZE : READ_PACKET_SIZE;\n           unsigned size = access.get_size() + control_size;\n           if( m_icnt->full(size, inst.is_store() || inst.isatomic()) ) {\n               stall_cond = ICNT_RC_FAIL;\n           } else {\n               mem_fetch *mf = m_mf_allocator->alloc(inst,access);\n               m_icnt->push(mf);\n\n\t       inst.accessq_pop_front();\n               //inst.clear_active( access.get_warp_mask() );\n               if( inst.is_load() ) { \n                  for( unsigned r=0; r < 4; r++) \n                      if(inst.out[r] > 0) \n                          assert( m_pending_writes[inst.warp_id()][inst.out[r]] > 0 );\n               } else if( inst.is_store() ) \n                  m_core->inc_store_req( inst.warp_id() );\n           }\n       } else {\n           stall_cond = process_memory_access_queue(m_L1D,inst);\n       }\n   }\n}\n\nmem_stage_stall_type ldst_unit::process_memory_access_queue( cache_t *cache, warp_inst_t &inst )\n{\n    ...\n    mem_fetch *mf = m_mf_allocator->alloc(inst,inst.accessq_front());\n    enum cache_request_status status = cache->access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);\n    return process_cache_access( cache, mf->get_addr(), inst, events, mf, status );\n}\n\nmem_fetch *alloc( new_addr_type addr, mem_access_type type, unsigned size, bool wr ) const \n{\n    mem_access_t access( type, addr, size, wr );\n    mem_fetch *mf = new mem_fetch( access, \n    \t\t       NULL,\n    \t\t       wr?WRITE_PACKET_SIZE:READ_PACKET_SIZE, \n    \t\t       -1, \n    \t\t       m_core_id, \n    \t\t       m_cluster_id,\n    \t\t       m_memory_config );\n    \treturn mf;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n",normalizedContent:"# gpgpu-sim memory interface\n\n# response phase from interconnect to sm\n\nsimt_core_cluster is at gpu level, it can fetch response into m_response_fifo.\n\nicnt_pop, if return non null ptr, it is a memory fetch repsonse, push it into m_response_fifo.\n\ncode\n\n// @@@@@@ shader.cc\n\nvoid simt_core_cluster::icnt_cycle()\n{\n    // pop from upward queue (gmmu to cu) of cluster and push it to the one in core (sm/cu)\n    if ( !m_gmmu_cu_queue.empty() ) {\n      mem_fetch *mf = m_gmmu_cu_queue.front();\n      ...\n      m_core[cid]->accept_access_response(mf);\n    } \n    \n    // pop it from the downward queue (cu to gmmu) of the core (sm/cu) and push it to the one in cluster (tpc)\n    for (unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++) {\n       if (!m_core[i]->empty_cu_gmmu_queue()){\n          mem_fetch *mf = m_core[i]->front_cu_gmmu_queue();\n          ...\n          m_cu_gmmu_queue.push_front(mf);\n       }\n    }\n\n    // forward response from gpu response fifo into shader core (sm) response fifo\n    if( !m_response_fifo.empty() ) {\n        mem_fetch *mf = m_response_fifo.front();\n        unsigned cid = m_config->sid_to_cid(mf->get_sid());\n        ...\n            // data response\n            if( !m_core[cid]->ldst_unit_response_buffer_full() ) {\n                m_response_fifo.pop_front();\n                // gpu ---\x3e sm\n                m_core[cid]->accept_ldst_unit_response(mf);\n            }\n        }\n    }\n\n    // accept response from interconnect network\n    if( m_response_fifo.size() < m_config->n_simt_ejection_buffer_size ) {\n        mem_fetch *mf = (mem_fetch*) ::icnt_pop(m_cluster_id);\n        if (!mf) \n            return;\n\n        // the packet size varies depending on the type of request: \n        // - for read request and atomic request, the packet contains the data \n        // - for write-ack, the packet only has control metadata\n        ...\n        m_stats->m_incoming_traffic_stats->record_traffic(mf, packet_size); \n        mf->set_status(in_cluster_to_shader_queue,gpu_sim_cycle+gpu_tot_sim_cycle);\n        ...\n        // interconnect to gpu\n        m_response_fifo.push_back(mf);\n        m_stats->n_mem_to_simt[m_cluster_id] += mf->get_num_flits(false);\n    } \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\nthe flow below is from sm to ldst unit and then to l1 cache.\n\nthe l1 cache is included in ldst unit.\n\ncode\n\n\nvoid shader_core_ctx::accept_ldst_unit_response(mem_fetch * mf) \n{\n   m_ldst_unit->fill(mf);\n}\n\nvoid ldst_unit::fill( mem_fetch *mf )\n{\n    mf->set_status(in_shader_ldst_response_fifo,gpu_sim_cycle+gpu_tot_sim_cycle);\n    m_response_fifo.push_back(mf);\n}\n\nvoid ldst_unit::cycle()\n{\n   writeback();\n   ...\n   if( !m_response_fifo.empty() ) {\n       mem_fetch *mf = m_response_fifo.front();\n       ...\n    \t   if( mf->get_type() == write_ack || ( m_config->gpgpu_perfect_mem && mf->get_is_write() )) {\n               m_core->store_ack(mf);\n               m_response_fifo.pop_front();\n\n               if ( m_gpu->get_global_memory()->is_page_managed(mf->get_mem_access().get_addr(), mf->get_mem_access().get_size()) ) {\n                    m_gpu->getgmmu()->reserve_pages_remove(mf->get_mem_access().get_addr(), mf->get_mem_access().get_uid());\n               }\n               ...\n           } else {\n              ...\n              if (m_l1d->fill_port_free()) {\n                   m_l1d->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);\n                   m_response_fifo.pop_front();\n              }\n           }\n       }\n   }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\nat this time the response is fed into l1 cache, which is shared in sm.\n\nbut how is it responsed to core?\n\n# response phase from l1 cache to wrap execution\n\ncode\n\n/// @@@@@@ gpu-cache.cc\n/// interface for response from lower memory level (model bandwidth restictions in caller)\nvoid baseline_cache::fill(mem_fetch *mf, unsigned time){\n    ...\n    if ( m_config.m_alloc_policy == on_miss )\n        m_tag_array->fill(e->second.m_cache_index,time);\n    else if ( m_config.m_alloc_policy == on_fill )\n        m_tag_array->fill(e->second.m_block_addr,time);\n\n    m_mshrs.mark_ready(e->second.m_block_addr, has_atomic);\n    ...\n}\n\n/// accept a new cache fill response: mark entry ready for processing\nvoid mshr_table::mark_ready( new_addr_type block_addr, bool &has_atomic ){\n    ...\n    m_current_response.push_back( block_addr );\n    ...\n}\n\n/// returns next ready access\nmem_fetch *mshr_table::next_access(){\n    ...\n    new_addr_type block_addr = m_current_response.front();\n    ...\n    mem_fetch *result = m_data[block_addr].m_list.front();\n    return result;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\ncache wrapped the next_access, when cache->next_access() is called, it will call mshr next_access().\n\nwriteback() function()\n\n 1. m_next_wb store next_writeback_function, it could be hit in cache or just get response from interconnect\\\n    * scoreboard relase register\n    * m_core->warp_inst_complete\n 2. update m_next_wb from mshr in l1cache\n\ncode\n\nvoid ldst_unit::writeback()\n{\n    // process next instruction that is going to writeback\n    if( !m_next_wb.empty() ) {\n        if( m_operand_collector->writeback(m_next_wb) ) {\n            bool insn_completed = false; \n            for( unsigned r=0; r < 4; r++ ) {\n                if( m_next_wb.out[r] > 0 ) {\n\t\t    ...\n\t\t    else { // shared \n                        m_scoreboard->releaseregister( m_next_wb.warp_id(), m_next_wb.out[r] );\n                        insn_completed = true; \n                    }\n                }\n            }\n            if( insn_completed ) {\n                m_core->warp_inst_complete(m_next_wb);\n            }\n            m_next_wb.clear();\n            m_last_inst_gpu_sim_cycle = gpu_sim_cycle;\n            m_last_inst_gpu_tot_sim_cycle = gpu_tot_sim_cycle;\n        }\n    }\n\n\n    for( unsigned c = 0; m_next_wb.empty() && (c < m_num_writeback_clients); c++ ) {\n        case 4: \n            if( m_l1d && m_l1d->access_ready() ) {\n                mem_fetch *mf = m_l1d->next_access();\n                m_next_wb = mf->get_inst();\n\n                if ( m_gpu->get_global_memory()->is_page_managed(mf->get_mem_access().get_addr(), mf->get_mem_access().get_size()) ) { \n                    m_gpu->getgmmu()->reserve_pages_remove(mf->get_mem_access().get_addr(), mf->get_mem_access().get_uid());\n                }\n\t\t \n                delete mf;\n                serviced_client = next_client; \n            }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\nm_core->warp_inst_complete is simple:\n\ncode\n\nvoid shader_core_ctx::warp_inst_complete(const warp_inst_t &inst)\n{\n  ...\n  m_gpu->gpu_sim_insn += inst.active_count();\n  inst.completed(gpu_tot_sim_cycle + gpu_sim_cycle);\n  ...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# how is memory request generated from warp inst?\n\n 1. warp_inst will generate memory access, for address of each thread in the warp, it will be push into access_q.\n    when the warp is issued, it will call generate_mem_access to generate this m_access_q.\n\ncode\n\n// @@@@@@ abstract_hardware_mode.cc\nvoid warp_inst_t::generate_mem_accesses()\n{\n    if( cache_block_size ) {\n\t...\n        for( unsigned thread=0; thread < m_config->warp_size; thread++ ) {\n            new_addr_type addr = m_per_scalar_thread[thread].memreqaddr[0];\n            unsigned block_address = line_size_based_tag_func(addr,cache_block_size);\n            accesses[block_address].set(thread);\n        }\n        for( a=accesses.begin(); a != accesses.end(); ++a ) \n            m_accessq.push_back( mem_access_t(access_type,a->first,cache_block_size,is_write,a->second,byte_mask) );\n}\n\n// the above function is called by:\nvoid shader_core_ctx::func_exec_inst( warp_inst_t &inst )\n{\n    execute_warp_inst_t(inst);\n    if( inst.is_load() || inst.is_store() )\n        inst.generate_mem_accesses();\n}\n\nvoid shader_core_ctx::issue_warp( register_set& pipe_reg_set, const warp_inst_t* next_inst, const active_mask_t &active_mask, unsigned warp_id )\n{\n    warp_inst_t** pipe_reg = pipe_reg_set.get_free();\n    ...\n    m_warp[warp_id].ibuffer_free();\n    ...\n    **pipe_reg = *next_inst; // static instruction information\n    (*pipe_reg)->issue( active_mask, warp_id, gpu_tot_sim_cycle + gpu_sim_cycle, m_warp[warp_id].get_dynamic_warp_id() ); // dynamic instruction information\n    m_stats->shader_cycle_distro[2+(*pipe_reg)->active_count()]++;\n    func_exec_inst( **pipe_reg );\n    ...\n    updatesimtstack(warp_id,*pipe_reg);\n    m_scoreboard->reserveregisters(*pipe_reg);\n    m_warp[warp_id].set_next_pc(next_inst->pc + next_inst->isize);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n 2. ldst_unit::memory_cycle\n    in this memory_cycle, it will invoke each access request inside inst.\n    the access will be converted into memory request, sending out downstream to l1 cache.\n    \n    this means that if each thread access a different block in cache, step 1) will generate 32 request inside the m_access_q.\n    then memory_cycle will at least needs 32 cycle to allocate mem_fetch request for each req.\n\ncode\n\n// shader.cc\nbool ldst_unit::memory_cycle( warp_inst_t &inst, mem_stage_stall_type &stall_reason, mem_stage_access_type &access_type )\n{\n   if ( !inst.accessq_empty() ) {\n       \tconst mem_access_t &access = inst.accessq_front();\n        if( bypassl1d ) {\n           // bypass l1 cache\n           unsigned control_size = inst.is_store() ? write_packet_size : read_packet_size;\n           unsigned size = access.get_size() + control_size;\n           if( m_icnt->full(size, inst.is_store() || inst.isatomic()) ) {\n               stall_cond = icnt_rc_fail;\n           } else {\n               mem_fetch *mf = m_mf_allocator->alloc(inst,access);\n               m_icnt->push(mf);\n\n\t       inst.accessq_pop_front();\n               //inst.clear_active( access.get_warp_mask() );\n               if( inst.is_load() ) { \n                  for( unsigned r=0; r < 4; r++) \n                      if(inst.out[r] > 0) \n                          assert( m_pending_writes[inst.warp_id()][inst.out[r]] > 0 );\n               } else if( inst.is_store() ) \n                  m_core->inc_store_req( inst.warp_id() );\n           }\n       } else {\n           stall_cond = process_memory_access_queue(m_l1d,inst);\n       }\n   }\n}\n\nmem_stage_stall_type ldst_unit::process_memory_access_queue( cache_t *cache, warp_inst_t &inst )\n{\n    ...\n    mem_fetch *mf = m_mf_allocator->alloc(inst,inst.accessq_front());\n    enum cache_request_status status = cache->access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);\n    return process_cache_access( cache, mf->get_addr(), inst, events, mf, status );\n}\n\nmem_fetch *alloc( new_addr_type addr, mem_access_type type, unsigned size, bool wr ) const \n{\n    mem_access_t access( type, addr, size, wr );\n    mem_fetch *mf = new mem_fetch( access, \n    \t\t       null,\n    \t\t       wr?write_packet_size:read_packet_size, \n    \t\t       -1, \n    \t\t       m_core_id, \n    \t\t       m_cluster_id,\n    \t\t       m_memory_config );\n    \treturn mf;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n",charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU Cache Coherency",frontmatter:{title:"GPU Cache Coherency",date:"2024-08-21T00:00:00.000Z",permalink:"/pages/45875/",tags:[null]},regularPath:"/03.gpu/18.gpucoherency.html",relativePath:"03.gpu/18.gpucoherency.md",key:"v-798f0c72",path:"/pages/45875/",headers:[{level:3,title:"1. [213] Cache Coherence for GPU Architectures",slug:"_1-213-cache-coherence-for-gpu-architectures",normalizedTitle:"1. [213] cache coherence for gpu architectures",charIndex:1}],headersStr:"1. [213] Cache Coherence for GPU Architectures",content:" 1. [213] Cache Coherence for GPU Architectures\n\n----------------------------------------\n\n\n# 1. [213] Cache Coherence for GPU Architectures\n\n# Another Design in CPU LLC\n\nLibrary Cache Coherence (LCC), that implements sequential consistency on CMPs by stalling writes to cache blocks until they have been self-invalidated by all sharers.\n\nLibrary Cache Coherence (LCC) [34, 54] is a time-based hardware coherence proposal that stores timestamps in a directory structure and delays stores to unexpired blocks to enforce sequential consistency on CMPs.\nThe TC-Strong implementation of the TC framework is similar to LCC as both enforce write atomicity by stalling writes at the shared last level cache.\n\n\n\n# Background\n\n\n\nWe propose TC-Weak and a novel time-based memory fence mechanism to eliminate all write-stalling, improve performance, and reduce interconnect traffic compared to TC-Strong.\n\n * We find that the stalling of writes in TC-Strong causes poor performance on a GPU.\n * We also show that unlike for CPU applications [34, 54], the fixed timestamp prediction proposed by LCC is not suited for GPU applications.\n\n# GPU Memory System\n\nBoth thread-private and global memory are stored in off-chip GDDR DRAM and cached in the multi-level cache hierarchy, however only global memory requires coherence.\n\nMemory accesses to the same cache block from different threads within a wavefront are merged into a single wide access by the Coalescing Unit.\nA memory instruction generates one memory access for every unique cache line accessed by the wavefront.\nAll requests are handled in FIFO order by the in-order memory stage of a GPU core.\n🙇 Writes to the same word by multiple scalar threads in a single wavefront do not have a defined behaviour [46]; only one write will succeed.\n\n# GPU Cache Hierarchy\n\nThe GPU cache hierarchy consists of per-core private L1 data caches and a shared L2 cache.\nThe L1 caches are not coherent.\nThey follow a write-evict [46] (write-purge [24]), write no-allocate caching policy.\nThe L2 caches are writeback with write-allocate.\n\nMemory accesses generated by the coalescing unit in each GPU core are passed, one per cycle, to the per-core MSHR table.\nThe MSHR table combines read accesses to the same cache line from different wavefronts to ensure only a single read access per-cache line per-GPU core is outstanding.\n\nWrites are not combined and, since they write-through, any number of write requests to the same cache line from a GPU core may be outstanding.\n\nPoint-to-point ordering in the interconnection network, L2 cache controllers and off-chip DRAM channels ensures that multiple outstanding writes from the same wavefront to the same address complete in program order.\n\nThis is another situation different from 🙇. It is different code in a program that write to the same address.\n\n# Atomic Operation\n\nAtomic Operation. Read-modify-write atomic operations are performed at each memory partition by an Atomic Operation Unit.\n\n# Consistency and Coherence\n\nA cache coherence protocol performs the following three duties [3].\n\n * It propagates newly written values to all privately cached copies.\n * It informs the writing thread or processor when a write has been completed and is visible to all threads and processors.\n * Lastly, a coherence protocol may ensure write atomicity [3], i.e., a value from a write is logically seen by all threads at once.\n\nWrite atomicity is commonly enforced in write-invalidate coherence protocols by requiring that all other copies of a cache block are invalidated before a write is completed.\n\nMemory consistency models may [4, 19, 57, 59] or may not [2, 19, 53] require write atomicity.\n\n# Directory Protocols\n\n# MESI\n\nfour-state coherence protocol with writeback L1 and L2 caches The write-allocate policy at L1 requires that write data be buffered until proper coherence permission has been obtained.\nThis requires the addition of area and complexity to buffer stores in each GPU core.\n\n# GPU-VI\n\ntwo-state coherence protocol GPU-VI implements write-through, no write-allocate L1 caches.\nIt requires that any write completing at the L2 invalidate all L1 copies.\nA write to a shared cache line cannot complete until the L2 controller has sent invalidation requests and received acknowledgments from all sharers.\n\nTwo Optimizations:\n\n * First, it writes data directly to the L1 cache on a write hit before receiving an acknowledgement, eliminating the area and complexity overheads of buffering stores.\n * Second, it treats loads to L1 blocks with pending writes as misses. This reduces stalling at the cache controller while maintaining write atomicity.\n\n# Challenges\n\n# Coherence Traffic\n\nThese overheads consist of recall traffic due to:\n\n * directory evictions,\n * false sharing invalidation traffic,\n * invalidation traffic due to inter-kernel communication.\n\nAn effective way to reduce coherence traffic is to selectively disable coherence for data regions that do not require it.\n\n# Storage Requirements\n\nIn a CPU-like coherence implementation [18] with enough storage to handle the worst case number of memory accesses (one memory request per thread), a directory protocol would require an impractical on-chip buffer as large as 28% of the total GPU L2 cache for tracking coherence requests.\n\n# Protocal Complexity\n\nstable states & Transient coherent states\n\n# Temporal Coherence\n\n\n\n\n\n# TC-strong Coherence\n\n\n\n# TC-weak Coherence\n\nTC-Weak relaxes the write atomicity of TC-Strong. As we show in Section 8.3, doing so improves performance by 28% and lowers interconnect traffic by 26% compared to TC-Strong.\n\nTC-Strong and LCC enforce coherence across all data by stalling writes.\nTC-Weak uses the insight that GPU applications may contain large amounts of data which does not require coherence and is unnecessarily penalized by write-stalling.\nBy relaxing write-atomicity, TC-Weak eliminates write-stalling and shifts any potential stalling to explicit memory fence operations.\n\nMajor two benefits：\n\n * First, it eliminates expensive stalling at the shared L2 cache controllers, which affects all cores and wavefronts, and shifts it to scheduling of individual wavefronts at memory fences.\n   A wavefront descheduled due to a memory fence does not affect the performance of other wavefronts.\n * Second, it enforces coherence only when required and specified by the program through memory fences. It implements the RCpc [19] consistency model; a detailed discussion on this is available elsewhere [56].\n\n\n\n# TC Strong:\n\nF1 defers scheduling the wavefront because the wavefront has an outstanding store request.\nWhen S1’s store request reaches the L2 ( 3 ), the L2 stalls it because data’s global timestamp will not expire until time T=30.\nAt T=30, C2 self-invalidates data ( 4 ), and the L2 processes S1’s store ( 5 ).\nThe fence instruction completes when C1 receives the acknowledgment for S1’s request ( 6 ).\n\n# TC Weak:\n\nThe write response returns with the global timestamp of the L2 cache line at the time of the write. The returned global timestamp is the guaranteed time by which the write will become visible to all cores in the system.\nThis is because by this time all cores will have invalidated their privately cached stale copies.\n\nIn this case, the value returned is 30 and corresponds to C2’s initially cached copy.\nThe L2 does not stall the write and sends back an acknowledgment with the GWCT, which updates the C1’s GWCT entry for this wavefront.\nAfter C1 receives the acknowledgment ( 4’ ), no memory requests are outstanding.\n\nComparing Figure 6(c) to 6(b) shows that TC-Weak performs better than TC-Strong because it only stalls at explicit memory fence operations.\nThis ensures that writes to data that does not require coherence has minimal impact.\n\nTC-Weak tracks the global timestamps returned by writes, called Global Write Completion Times (GWCT), for each wave-front.\nA memory fence operation uses this information to deschedule the wavefront sufficiently long enough to guar=antee that all previous writes from the wavefront have become globally visible.\n\n# Lifetime Prediction\n\n * we show that a single lifetime value for all accesses performs well.\n * Moreover, this value is application dependent.\n\na single lifetime prediction value at each L2 cache bank, and adjusts it based on application behaviour. A load obtains its lifetime prediction at the L2 bank.\n\nThe lifetime estimation is based on events local to L2 bank\n\n * L2 Block with unexpired timestamp evicted\n * load request miss in L1 due to expired\n * L2 receive a load request to a valid block with an expired global timestamp\n * store operation writes to an unexpired block at L2",normalizedContent:" 1. [213] cache coherence for gpu architectures\n\n----------------------------------------\n\n\n# 1. [213] cache coherence for gpu architectures\n\n# another design in cpu llc\n\nlibrary cache coherence (lcc), that implements sequential consistency on cmps by stalling writes to cache blocks until they have been self-invalidated by all sharers.\n\nlibrary cache coherence (lcc) [34, 54] is a time-based hardware coherence proposal that stores timestamps in a directory structure and delays stores to unexpired blocks to enforce sequential consistency on cmps.\nthe tc-strong implementation of the tc framework is similar to lcc as both enforce write atomicity by stalling writes at the shared last level cache.\n\n\n\n# background\n\n\n\nwe propose tc-weak and a novel time-based memory fence mechanism to eliminate all write-stalling, improve performance, and reduce interconnect traffic compared to tc-strong.\n\n * we find that the stalling of writes in tc-strong causes poor performance on a gpu.\n * we also show that unlike for cpu applications [34, 54], the fixed timestamp prediction proposed by lcc is not suited for gpu applications.\n\n# gpu memory system\n\nboth thread-private and global memory are stored in off-chip gddr dram and cached in the multi-level cache hierarchy, however only global memory requires coherence.\n\nmemory accesses to the same cache block from different threads within a wavefront are merged into a single wide access by the coalescing unit.\na memory instruction generates one memory access for every unique cache line accessed by the wavefront.\nall requests are handled in fifo order by the in-order memory stage of a gpu core.\n🙇 writes to the same word by multiple scalar threads in a single wavefront do not have a defined behaviour [46]; only one write will succeed.\n\n# gpu cache hierarchy\n\nthe gpu cache hierarchy consists of per-core private l1 data caches and a shared l2 cache.\nthe l1 caches are not coherent.\nthey follow a write-evict [46] (write-purge [24]), write no-allocate caching policy.\nthe l2 caches are writeback with write-allocate.\n\nmemory accesses generated by the coalescing unit in each gpu core are passed, one per cycle, to the per-core mshr table.\nthe mshr table combines read accesses to the same cache line from different wavefronts to ensure only a single read access per-cache line per-gpu core is outstanding.\n\nwrites are not combined and, since they write-through, any number of write requests to the same cache line from a gpu core may be outstanding.\n\npoint-to-point ordering in the interconnection network, l2 cache controllers and off-chip dram channels ensures that multiple outstanding writes from the same wavefront to the same address complete in program order.\n\nthis is another situation different from 🙇. it is different code in a program that write to the same address.\n\n# atomic operation\n\natomic operation. read-modify-write atomic operations are performed at each memory partition by an atomic operation unit.\n\n# consistency and coherence\n\na cache coherence protocol performs the following three duties [3].\n\n * it propagates newly written values to all privately cached copies.\n * it informs the writing thread or processor when a write has been completed and is visible to all threads and processors.\n * lastly, a coherence protocol may ensure write atomicity [3], i.e., a value from a write is logically seen by all threads at once.\n\nwrite atomicity is commonly enforced in write-invalidate coherence protocols by requiring that all other copies of a cache block are invalidated before a write is completed.\n\nmemory consistency models may [4, 19, 57, 59] or may not [2, 19, 53] require write atomicity.\n\n# directory protocols\n\n# mesi\n\nfour-state coherence protocol with writeback l1 and l2 caches the write-allocate policy at l1 requires that write data be buffered until proper coherence permission has been obtained.\nthis requires the addition of area and complexity to buffer stores in each gpu core.\n\n# gpu-vi\n\ntwo-state coherence protocol gpu-vi implements write-through, no write-allocate l1 caches.\nit requires that any write completing at the l2 invalidate all l1 copies.\na write to a shared cache line cannot complete until the l2 controller has sent invalidation requests and received acknowledgments from all sharers.\n\ntwo optimizations:\n\n * first, it writes data directly to the l1 cache on a write hit before receiving an acknowledgement, eliminating the area and complexity overheads of buffering stores.\n * second, it treats loads to l1 blocks with pending writes as misses. this reduces stalling at the cache controller while maintaining write atomicity.\n\n# challenges\n\n# coherence traffic\n\nthese overheads consist of recall traffic due to:\n\n * directory evictions,\n * false sharing invalidation traffic,\n * invalidation traffic due to inter-kernel communication.\n\nan effective way to reduce coherence traffic is to selectively disable coherence for data regions that do not require it.\n\n# storage requirements\n\nin a cpu-like coherence implementation [18] with enough storage to handle the worst case number of memory accesses (one memory request per thread), a directory protocol would require an impractical on-chip buffer as large as 28% of the total gpu l2 cache for tracking coherence requests.\n\n# protocal complexity\n\nstable states & transient coherent states\n\n# temporal coherence\n\n\n\n\n\n# tc-strong coherence\n\n\n\n# tc-weak coherence\n\ntc-weak relaxes the write atomicity of tc-strong. as we show in section 8.3, doing so improves performance by 28% and lowers interconnect traffic by 26% compared to tc-strong.\n\ntc-strong and lcc enforce coherence across all data by stalling writes.\ntc-weak uses the insight that gpu applications may contain large amounts of data which does not require coherence and is unnecessarily penalized by write-stalling.\nby relaxing write-atomicity, tc-weak eliminates write-stalling and shifts any potential stalling to explicit memory fence operations.\n\nmajor two benefits：\n\n * first, it eliminates expensive stalling at the shared l2 cache controllers, which affects all cores and wavefronts, and shifts it to scheduling of individual wavefronts at memory fences.\n   a wavefront descheduled due to a memory fence does not affect the performance of other wavefronts.\n * second, it enforces coherence only when required and specified by the program through memory fences. it implements the rcpc [19] consistency model; a detailed discussion on this is available elsewhere [56].\n\n\n\n# tc strong:\n\nf1 defers scheduling the wavefront because the wavefront has an outstanding store request.\nwhen s1’s store request reaches the l2 ( 3 ), the l2 stalls it because data’s global timestamp will not expire until time t=30.\nat t=30, c2 self-invalidates data ( 4 ), and the l2 processes s1’s store ( 5 ).\nthe fence instruction completes when c1 receives the acknowledgment for s1’s request ( 6 ).\n\n# tc weak:\n\nthe write response returns with the global timestamp of the l2 cache line at the time of the write. the returned global timestamp is the guaranteed time by which the write will become visible to all cores in the system.\nthis is because by this time all cores will have invalidated their privately cached stale copies.\n\nin this case, the value returned is 30 and corresponds to c2’s initially cached copy.\nthe l2 does not stall the write and sends back an acknowledgment with the gwct, which updates the c1’s gwct entry for this wavefront.\nafter c1 receives the acknowledgment ( 4’ ), no memory requests are outstanding.\n\ncomparing figure 6(c) to 6(b) shows that tc-weak performs better than tc-strong because it only stalls at explicit memory fence operations.\nthis ensures that writes to data that does not require coherence has minimal impact.\n\ntc-weak tracks the global timestamps returned by writes, called global write completion times (gwct), for each wave-front.\na memory fence operation uses this information to deschedule the wavefront sufficiently long enough to guar=antee that all previous writes from the wavefront have become globally visible.\n\n# lifetime prediction\n\n * we show that a single lifetime value for all accesses performs well.\n * moreover, this value is application dependent.\n\na single lifetime prediction value at each l2 cache bank, and adjusts it based on application behaviour. a load obtains its lifetime prediction at the l2 bank.\n\nthe lifetime estimation is based on events local to l2 bank\n\n * l2 block with unexpired timestamp evicted\n * load request miss in l1 due to expired\n * l2 receive a load request to a valid block with an expired global timestamp\n * store operation writes to an unexpired block at l2",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU Cache & Memory Hirerarchy",frontmatter:{title:"GPU Cache & Memory Hirerarchy",date:"2024-08-25T00:00:00.000Z",permalink:"/pages/45876/",tags:[null]},regularPath:"/03.gpu/19.gpu_cache_mem.html",relativePath:"03.gpu/19.gpu_cache_mem.md",key:"v-53bdf925",path:"/pages/45876/",headers:[{level:2,title:"1. Dissecting GPU Memory Hierarchy through Microbenchmarking",slug:"_1-dissecting-gpu-memory-hierarchy-through-microbenchmarking",normalizedTitle:"1. dissecting gpu memory hierarchy through microbenchmarking",charIndex:501},{level:3,title:"Parameter",slug:"parameter",normalizedTitle:"parameter",charIndex:630},{level:3,title:"L1 Data Cache",slug:"l1-data-cache",normalizedTitle:"l1 data cache",charIndex:650},{level:3,title:"L2 Data Cache",slug:"l2-data-cache",normalizedTitle:"l2 data cache",charIndex:1732},{level:3,title:"Global Memory",slug:"global-memory",normalizedTitle:"global memory",charIndex:2413},{level:3,title:"Global Memory Throughput",slug:"global-memory-throughput",normalizedTitle:"global memory throughput",charIndex:2527},{level:3,title:"Global Memory Latency",slug:"global-memory-latency",normalizedTitle:"global memory latency",charIndex:2985},{level:3,title:"Shared Memory",slug:"shared-memory",normalizedTitle:"shared memory",charIndex:367},{level:3,title:"Shared Memory Throughput",slug:"shared-memory-throughput",normalizedTitle:"shared memory throughput",charIndex:6078},{level:3,title:"Shared Memory Latency",slug:"shared-memory-latency",normalizedTitle:"shared memory latency",charIndex:7502},{level:3,title:"Conclusion",slug:"conclusion",normalizedTitle:"conclusion",charIndex:9161},{level:2,title:"4. Dissecting the NVidia Turing T4 GPU via Microbenchmarking",slug:"_4-dissecting-the-nvidia-turing-t4-gpu-via-microbenchmarking",normalizedTitle:"4. dissecting the nvidia turing t4 gpu via microbenchmarking",charIndex:10028},{level:3,title:"Result",slug:"result",normalizedTitle:"result",charIndex:10093},{level:3,title:"Shared Memory Latency",slug:"shared-memory-latency-2",normalizedTitle:"shared memory latency",charIndex:7502},{level:3,title:"Bandwidth",slug:"bandwidth",normalizedTitle:"bandwidth",charIndex:10136},{level:2,title:"4. Benchmarking the GPU memory at the warp level",slug:"_4-benchmarking-the-gpu-memory-at-the-warp-level",normalizedTitle:"4. benchmarking the gpu memory at the warp level",charIndex:10194},{level:3,title:"Local Memory",slug:"local-memory",normalizedTitle:"local memory",charIndex:10617},{level:3,title:"Shared Memory",slug:"shared-memory-2",normalizedTitle:"shared memory",charIndex:367},{level:2,title:"Constant Memory",slug:"constant-memory",normalizedTitle:"constant memory",charIndex:11600},{level:3,title:"Global Memory",slug:"global-memory-2",normalizedTitle:"global memory",charIndex:2413},{level:2,title:"5. Exploring Modern GPU Memory System Design Challenges through Accurate Modeling",slug:"_5-exploring-modern-gpu-memory-system-design-challenges-through-accurate-modeling",normalizedTitle:"5. exploring modern gpu memory system design challenges through accurate modeling",charIndex:12761},{level:3,title:"Memory Coalescer",slug:"memory-coalescer",normalizedTitle:"memory coalescer",charIndex:12857},{level:3,title:"L2 Cache",slug:"l2-cache",normalizedTitle:"l2 cache",charIndex:13369},{level:2,title:"Streaming Throughput-oriented L1 Cache",slug:"streaming-throughput-oriented-l1-cache",normalizedTitle:"streaming throughput-oriented l1 cache",charIndex:14169},{level:2,title:"If it is a hit to a reserved sector (i.e. the status is pending), it sets its corresponding warp bit in the merging mask (64 bits for 64 warps).\\\nWhen the pending request comes back, it allocates a cache line/sector in the data block and sets the allocated block index in the table.\\\nThen, the merged warps access the sector, on a cycle-by-cyle basis.",slug:"if-it-is-a-hit-to-a-reserved-sector-i-e-the-status-is-pending-it-sets-its-corresponding-warp-bit-in-the-merging-mask-64-bits-for-64-warps-when-the-pending-request-comes-back-it-allocates-a-cache-line-sector-in-the-data-block-and-sets-the-allocated-block-index-in-the-table-then-the-merged-warps-access-the-sector-on-a-cycle-by-cyle-basis",normalizedTitle:"if it is a hit to a reserved sector (i.e. the status is pending), it sets its corresponding warp bit in the merging mask (64 bits for 64 warps).<br>\nwhen the pending request comes back, it allocates a cache line/sector in the data block and sets the allocated block index in the table.<br>\nthen, the merged warps access the sector, on a cycle-by-cyle basis.",charIndex:null},{level:2,title:"6. OSM: Off-Chip Shared Memory for GPUs",slug:"_6-osm-off-chip-shared-memory-for-gpus",normalizedTitle:"6. osm: off-chip shared memory for gpus",charIndex:15426},{level:2,title:"7. Demystifying GPU UVM Cost with Deep Runtime and Workload Analysis",slug:"_7-demystifying-gpu-uvm-cost-with-deep-runtime-and-workload-analysis",normalizedTitle:"7. demystifying gpu uvm cost with deep runtime and workload analysis",charIndex:15934},{level:3,title:"Main Idea in Short",slug:"main-idea-in-short",normalizedTitle:"main idea in short",charIndex:16007},{level:3,title:"Motivation",slug:"motivation",normalizedTitle:"motivation",charIndex:16402},{level:3,title:"Cost of Demand Paging",slug:"cost-of-demand-paging",normalizedTitle:"cost of demand paging",charIndex:16987},{level:3,title:"Prefetching Challenges and Page-level GPU Access Patterns",slug:"prefetching-challenges-and-page-level-gpu-access-patterns",normalizedTitle:"prefetching challenges and page-level gpu access patterns",charIndex:22733},{level:3,title:"Discussion and Conclusions",slug:"discussion-and-conclusions",normalizedTitle:"discussion and conclusions",charIndex:27196}],headersStr:"1. Dissecting GPU Memory Hierarchy through Microbenchmarking Parameter L1 Data Cache L2 Data Cache Global Memory Global Memory Throughput Global Memory Latency Shared Memory Shared Memory Throughput Shared Memory Latency Conclusion 4. Dissecting the NVidia Turing T4 GPU via Microbenchmarking Result Shared Memory Latency Bandwidth 4. Benchmarking the GPU memory at the warp level Local Memory Shared Memory Constant Memory Global Memory 5. Exploring Modern GPU Memory System Design Challenges through Accurate Modeling Memory Coalescer L2 Cache Streaming Throughput-oriented L1 Cache If it is a hit to a reserved sector (i.e. the status is pending), it sets its corresponding warp bit in the merging mask (64 bits for 64 warps).\\\nWhen the pending request comes back, it allocates a cache line/sector in the data block and sets the allocated block index in the table.\\\nThen, the merged warps access the sector, on a cycle-by-cyle basis. 6. OSM: Off-Chip Shared Memory for GPUs 7. Demystifying GPU UVM Cost with Deep Runtime and Workload Analysis Main Idea in Short Motivation Cost of Demand Paging Prefetching Challenges and Page-level GPU Access Patterns Discussion and Conclusions",content:" 1. [248] Dissecting GPU Memory Hierarchy through Microbenchmarking\n 2. [75] Benchmarking the Memory Hierarchy of Modern GPUs\n 3. [18] Benchmarking the GPU memory at the warp level\n 4. [90] Dissecting the NVidia Turing T4 GPU via Microbenchmarking\n 5. [38] Exploring Modern GPU Memory System Design Challenges through Accurate Modeling 👍 👍 👍\n 6. [9] OSM: Off-Chip Shared Memory for GPUs\n 7. [10] Demystifying GPU UVM Cost with Deep Runtime and Workload Analysis 👍 👍 👍\n\nTags: #dissecting gpu\n\n\n# 1. Dissecting GPU Memory Hierarchy through Microbenchmarking\n\nA paper in 2015, profile memory in Fermi, Kepler and Maxwell\n\n\n\n\n# Parameter\n\n\n\n\n\n\n\n\n# L1 Data Cache\n\nOn the Fermi and Kepler devices, the L1 data cache and shared memory are physically implemented together.\nOn the Maxwell devices, the L1 data cache is unified with the texture cache.\n\nThe 16 KB L1 cache has 128 cache lines mapped onto four cache ways.\nFor each cache way, 32 cache sets are divided into 8 major sets. Each major set contains 16 cache lines.\n\nThe data mapping is also unconventional.\nThe 12-13th bits in the memory address define the cache way, the 9-11th bits define the major set, and the 0-6th bits define the memory offset inside the cache line.\n\nOne distinctive feature of the Fermi L1 cache is that its replacement policy is not LRU, as pointed out by Meltzer et.al. Among the four cache ways, cache way 2 is three times more likely to be replaced than the other three cache ways.\n\nAnother paper[4] We found that when the L1 data cache saturates, Turing randomly evicts 4 consecutive cache lines (128 B).\nWe observed that once a block of cache lines are evicted, the second scan will cause more cache lines from the same set to be evicted.\n\n\n\n\n# L2 Data Cache\n\n * The replacement policy of the L2 cache is not LRU\n * The L2 cache line size is 32 bytes by observing the memory access pattern of overflowing the cache and visiting array element one by one.\n * The data mapping is sophisticated and not conventional bits-defined\n * a hardware-level pre-fetching mechanism from the DRAM to the L2 data cache on all three platforms.\n   The pre-fetching size is about 2/3 of the L2 cache size and the prefetching is sequential. This is deduced from that if we load an array smaller than 2/3 of the L2 data cache size, there is no cold cache miss patterns.\n   🙋(Maybe they can cover the gap just by prefetching sequential line.)\n\n\n# Global Memory\n\nglobal memory access involves accessing the DRAM, L1 and L2 data caches, TLBs and page tables.\n\n\n# Global Memory Throughput\n\nThe theoretical bandwidth is calculated as fmem * bus width * DDR factor.\n\n\n\nthe throughput of a larger ILP saturates faster.\n\nThe GTX780 has the highest throughput as it benefits from the highest bus width,\nbut its convergence speed is the slowest, i.e., it requires the most memory requests to hide the pipeline latency.\n\nThis could be part of the reason that NVIDIA reduced the bus width back to 256 bits in Maxwell devices.\n\n\n# Global Memory Latency\n\nThe global memory access latency is the whole time accessing a data located in DRAM/L2 or L1 cache, including the latency of page table look-ups.\n\n * very large s1 = 32 MB to construct the TLB/page table miss and cache miss (P5&P6)\n * set s2 = 1 MB to construct the L1 TLB hit but cache miss (P4)\n * After a total of 65 data accesses, 65 data lines are loaded into the cache.\n   We then visit the cached data lines with s1 again for several times, to construct cache hit but TLB miss (P2&P3).\n * set s3 = 1 element and repeatedly load the data in a cache line so that every memory access is a cache hit (P1).\n\n\n\n\n\n\n\n * The Maxwell and Kepler devices have a unique memory access pattern (P6) for page table context switching.\n   When a kernel is launched, only memory page entries of 512 MB are activated.\n   If the thread visits an inactivate page entry, the hardware needs a rather long time to switch between page tables.\n   This phenomena is also reported in [22] as page table “miss”.\n * The Maxwell L1 data cache addressing does not go through the TLBs or page tables.\n   On the GTX980, there is no TLB miss pattern (i.e., P2 and P3) when the L1 data cache is hit.\n   Once the L1 cache is missed, the access latency increases from tens of cycles to hundreds or even thousands of cycles. My comments: But if we look at GTX560Ti in P2, the latency is different with P1. So does this means that in Fermi, the memory request has to go through TLB first, and then access L1 DataCache? This might be the reason that the latency is longer. But this will degrade the performance....\n * The TLBs are off-chip. we infer that the physical memory locations of the L1 TLB and L2 data cache are close.\n   The physical memory locations of the L1 TLB and L2 TLB are also close, which means that the L1/L2 TLB and L2 data cache are shared off-chip by all SMs.\n * The GTX780 generally has the shortest global memory latencies, almost half that of the Fermi, with an access pattern of P2-P5.\n   The page table context switching of the GTX980 is also much more expensive than that of the GTX780.\n\nTo summarize, the Maxwell device has long global memory access latencies for cold cache misses and page table context switching.\nExcept for these rare access patterns, its access latency cycles are close to those of the Kepler device.\nbecause the GTX980 has higher fmem than the GTX780, it actually offers the shortest global memory access time (P2-P4).\n\n\n\n\n# Shared Memory\n\nIn CUDA programming, different CTAs assigned to the same SM have to share the same physical memory space.\nOn the Fermi and Kepler platforms, the shared memory is physically integrated with the L1 cache.\nOn the Maxwell platform, it occupies a separate memory space. Note that the shared memory and L1 cache are separated since Maxwell architecture.\n\nProgrammers move the data into and out of shared memory from global memory before and after arithmetic execution,\nto avoid the frequent occurrence of long global memory access latencies.\n\nWe report a dramatic improvement in performance for the Maxwell device.\n\n\n# Shared Memory Throughput\n\nthe shared memory is organized as 32 memory banks [15].\nThe bank width of the Fermi and Maxwell devices is 4 bytes, while that of the Kepler device is 8 bytes. The theoretical peak throughput of each SM (WSM) is calculated as fcore ∗ Wbank ∗ 32.\n\n\n\nThe achieved throughput per SM is calculated as 2 * fcore * sizeof(int) * (number of active threads per SM) * ILP / (total latency of each SM). Usually a large value of ILP results in less active warps per SM.\nThe peak throughput W0SM denotes the respective maximum throughput of the abovecombinations.\nTwo key factors that affect the throughput are the number of active warps per SM and the ILP level.\n\nThe GTX980 reaches its peak throughput when the CTA size = 256, CTAs per SM = 2 and ILP = 8, i.e., 16 active warps per SM. The peak throughput is 137.41 GB/s, about 83.9% of the theoretical bandwidth. The Maxwell device shows the best use of its shared memory bandwidth, and the Kepler device shows the worst.\n\nGTX980 exhibits similar behavior as GTX780: high ILP is required to achieve high throughput for high SM occupancy.\n\nAccording to Little’s Law, we roughly have: number of active warps * ILP = latency cycles * throughput.\n\nGTX780 sucks in ILP = 1, since its limited 64 warps at most to be scheduled concurrently.\nWe consider this to be the main reason the achieved throughput of the GTX780 is poor compared with its designed value.\n\n\n# Shared Memory Latency\n\nThe shared memory latencies on Fermi, Kepler and Maxwell devices are 50, 47 and 28 cycles, respectively.\n\nFermi and Maxwell devices have the same number of potential bank conflicts because they have the same architecture.\n\nThe shared memory space is divided into 32 banks.\nSuccessive words are allocated to successive banks.\nIf two threads in the same warp access memory spaces in the same bank, a 2-way bank conflict occurs.\n\n\n\n\n\n\n\nFor the Fermi and Kepler devices, where there is a 32-way bank conflict, it takes much longer to access shared memory than regular global memory (TLB hit, cache miss).\nSurprisingly, the effect of a bank conflict on shared memory access latency on the Maxwell device is mild.\nEven the longest shared memory access latency is still at the same level as L1 data cache latency.\n\nIn summary, although the shared memory has very short access latency, it can be rather long if there are many ways of bank conflicts.\nThis is most obvious on the Fermi hardware.\nThe Kepler device tries to solve it by doubling the bank width of shared memory.\nCompared with the Fermi, the Kepler’s 4-byte mode shared memory halves the chance of bank conflict, and the 8-byte mode reduces it further.\n\nHowever, we also find that the Kepler’s shared memory is inefficient in terms of throughput.\nThe Maxwell device has the best shared memory performance.\nWith the same architecture as the Fermi device, the Maxwell hardware shows a 2x size, 2x memory access speedup and achieves the highest throughput.\nMost importantly, the Maxwell device’s shared memory has been optimized to avoid the long latency caused by bank conflicts.\n\n\n# Conclusion\n\nThe memory capacity is significantly enhanced in both Kepler and Maxwell as compared with Fermi.\nThe Kepler device is performance-oriented and incorporates several aggressive elements in its design, such as increasing the bus width of DRAM and doubling the bank width of shared memory.\nThese designs have some side-effects.\nThe theoretical bandwidths of both global memory and shared memory are difficult to saturate, and hardware resources are imbalanced with a low utilization rate.\nThe Maxwell device has a more efficient and conservative design.\nIt has a reduced bus width and bank width, and the on-chip cache architectures are adjusted, including doubling the shared memory size and the read-only data cache size.\nFurthermore, it sharply decreases the shared memory latency caused under bank conflicts.\n\n----------------------------------------\n\n\n# 4. Dissecting the NVidia Turing T4 GPU via Microbenchmarking\n\n\n# Result\n\n\n\n\n\n\n# Shared Memory Latency\n\n\n\n\n# Bandwidth\n\n\n\n----------------------------------------\n\n\n# 4. Benchmarking the GPU memory at the warp level\n\nIn this work, we investigate the data accessing capability of a warp of threads: broadcasting and parallel accessing.\\\n\n * Broadcasting occurs when multiple threads access the same data element, i.e., multiple threads request a single data element (MTSD).\n * We refer the case of multiple threads accessing multiple distinct data elements (MTMD) as parallel accessing.\n\n\n# Local Memory\n\n * For the simple memory access patterns, we should allocate a sufficient small array to guarantee that it is located in registers.\n * For the complex memory access patterns, we should simplify codes to exploit registers. For example, we merge a three-level loop into an one-level loop so that a larger temporal vector can be allocated in registers.\n * \n\n\n# Shared Memory\n\n * Bank conflicts must be avoided by the ways of e.g., data padding.\n * Shared memory supports both broadcasting and parallel accessing.\n * Neither consecutively accessing nor aligned accessing is a must.\n * The latency decreases when the number of threads increase, and thus we should use a sufficiently large thread block.\n * Replacing global memory with shared memory, because the latency of shared memory is smaller than that of global memory.\n * Using shared memory bares an overhead (i.e., buffer allocation and data movement) and reusing data in it is a must for improved performance.\n\n\n# Constant Memory\n\nBut constant memory does not support parallel accessing.\nThat is, constant memory can only be accessed serially when requesting different data elements.\nOn the one hand, constant memory is used to store a small amount of read-only data, which is not sensitive to bandwidth.\nSo parallel accessing is not a must for constant memory.\n\n * Constant memory supports the accessing capability of broadcasting.\n * Constant memory does not support parallel accessing, and satisfies parallel memory requests in a serial manner.\n\n\n# Global Memory\n\n * Global memory supports both broadcasting and parallel accessing.\n * The data types of 4 or 8 bytes can obtain the near upper-bounded bandwidth of global memory, while the data types cannot.\n   So the char data should be coalesced into the char4 type for improved bandwidth.\n * Global memory accesses should be consecutive, but aligned accessing is not necessary for global memory.\n * When memory accessing is non-consecutive, the latency changes with the number of threads, but not with the number of blocks. So we should configure the thread dimensionality.\n\n----------------------------------------\n\n\n# 5. Exploring Modern GPU Memory System Design Challenges through Accurate Modeling\n\n👍 👍 👍\n\n\n# Memory Coalescer\n\nthe eviction granularity of the cache is 128B, indicating that the L1 cache has 128B lines with 32B sectors.\nFurthermore, the coalescer operates across eight threads, i.e. the coalescer tries to coalesce each group of eight threads separately to generate sectored accesses.\n\n\n\nWhen the stride=32, the memory access is converged, and all the threads within the same warp will access the same cache line,\nhowever we receive four read accesses at L1 cache.\n\n8 Thread register 32bit == 32Byte.\n\n\n# L2 Cache\n\nL2 cache applies something similar to write-validate not fetch on write.\\ 😱 However, all the reads received by L2 caches from the coalescer are 32-byte sectored accesses.\nThus, the read access granularity (32 bytes) is different from the write access granularity (one byte).\nTo handle this, the L2 cache applies a different write allocation policy, which we named lazy fetch-on-read, that is a compromise between write-validate and fetch-on-write.\n\nWhen a sector read request is received to a modified sector, it first checks if the sector write-mask is complete, i.e. all the bytes have been written to and the line is fully readable.\nIf so, it reads the sector, otherwise, similar to fetch-on-write, it generates a read request for this sector and merges it with the modified bytes.\n\n\n# Streaming Throughput-oriented L1 Cache\n\n\n\nThe L1 cache in Volta is what NVIDIA is calling a streaming cache [33].\nIt is streaming because the documentation states that it allows unlimited cache misses to be in flight regardless the number of cache lines per cache set [10].\n\nindependent of the number of L1 configured size, the number of MSHRs available are the same, even if more of the on-chip SRAM storage is devoted to shared memory.\n\nWe believe that unified cache is a plain SRAM where sectored data blocks are shared between the L1D and the CUDA shared memory.\nIt can be configured adaptively by the driver as we discussed earlier.\nWe assume that the L1D’s TAG and MSHR merging functionality are combined together in a separate table structure (TAG-MSHR table).\nSince, the filling policy is now ON FILL, we can have more TAG entries and outstanding requests than the assigned L1D cache lines.\n\n\n# If it is a hit to a reserved sector (i.e. the status is pending), it sets its corresponding warp bit in the merging mask (64 bits for 64 warps).\nWhen the pending request comes back, it allocates a cache line/sector in the data block and sets the allocated block index in the table.\nThen, the merged warps access the sector, on a cycle-by-cyle basis.\n\n\n# 6. OSM: Off-Chip Shared Memory for GPUs\n\n\n\nL1-D cache and shared memory use the same 32-bank memory structure (4 KB capacity per bank) as shown in Fig. 4;\nhowever, they have some differences.\nWe can access 32-bit shared memory arrays via a thread-index directly, while for accessing L1-D cache, we should read 128B (four 32B sectors) of the cache block.\nIn addition, L1 cache requires an extra hardware for managing tags and implementing LRU replacement policy.\n\n----------------------------------------\n\n\n# 7. Demystifying GPU UVM Cost with Deep Runtime and Workload Analysis\n\n\n# Main Idea in Short\n\nProfile the GPU UVM with different benchmarks.\n\nFor memory that fits in GPU memory, just prefetch.\n\nFor memory oversubscription, prefetch & eviction has room to improve.\n\nPrefetch and eviction are maintained by hardware.\n\n * accuracy of prefetch\n * accuracy of eviction\n\nThe LRU algorithm is operated on page fault pages.\n\nEven frequently accessed page might be evicted.\n\n\n# Motivation\n\n 1. cumulative data access latency without prefetching generally increases one or more orders of magnitude with UVM in comparison to explicit direct management by programmers\n 2. when all data fits in GPU global memory, prefetching reduces the cost significantly.\n    but the overall time can still be several times higher than the baseline\n 3. once the GPU global memory is oversubscribed, data access latency dramatically increases by another order of magnitude depending on access pattern\n 4. prefetching can aggravate the performance issues after oversubscription.\n\n\n# Cost of Demand Paging\n\nFar Fault\nPaged migration moves data between devices in response to a page fault, maps the page into the faulter’s physical space, and unmaps from the previous location.\n\nRemote Mapping maps the requested data into the requester’s page tables without actually migrating it and accesses it using DMA or a related mechanism.\nRead-only duplication duplicates data at two or more physical devices and maps them locally to each device under the constraint that the data cannot be mutated.\n\nUVM on-demand paging is implemented using GPU hardware and CPU software working in tandem.\nTo integrate with the host OS, the UVM driver is provided as a kernel module for the host OS to extend the virtual memory space and map it to GPU global memory utilizing the host memory layout.\n\n\n\nUVM uses a four-level hierarchy for memory address space: address spaces, virtual address ranges, virtual address blocks, and pages.\n\nIn general, a virtual address space is associated with an application.\nEach address space is composed of “ranges”, each corresponding to an arbitrarily sized memory allocation i.e. cudaMallocManaged() or related allocator.\nA range is broken up into 2MB sequential virtual address blocks, VABlocks.\nVABlocks are page-aligned and are composed of OS pages.\n\n# Flow\n\noperations of the UVM Driver in three groups: pre/post-processing, fault servicing, and fault replay policy.\n\n * pre-processing\n   the driver stores page fault information read from the GPU fault buffer and sorts them locally.\n   Faults are fetched until all the fault pointer queue is empty, the current batch of faults is full, or fault that is not ready is encountered, depending on policy.\n   The default batch size is 256 faults. Per batch, the driver groups page faults based on VABlocks and service the faults.\n\n * Fault Servicing Fault servicing involves memory allocation, updating page tables, data transfer, and possibly issuing one or more fault replays or other operations, subject ot the fault replay policy.\n\n\n\n# Cost Overview\n\nPre/post processing is shown to be negligible in cost, but functionally important for the fault servicing and replay implementation.\n\nPre-processing first gathers faults from the device, performs basic bookkeeping and logical checks, and sorts them into the appropriate VABlock bins.\nNVIDIA documentation indicates that the driver uses a circular device-side queue to store a fault pointer when a fault occurs [15].\nThe host can read these pointers, which subsequently point to locations in the global GPU fault buffer that contain the full fault information.\nThe driver will generally read at least a full batch from the queue during every pass and cache the faults on the host to avoid having to make multiple remote updates to the queue.\nFaults may not be immediately available in the GPU fault buffer due to the asynchronicity.\nThus the driver may need to poll the buffer until the appropriate “ready” field is marked true or may be able to begin processing on previously fetched faults.\nSorting cost for batches is roughly constant due to the nature of sorting and the relatively small size of batches.\n\n\n\n# Sevice Cost Breakdown\n\nFault servicing is a multi-step process that includes:\n\n * allocating physical space\n * zeroing out GPU pages\n * migrating data from the source to the destination\n * mapping pages and permissions, and a number of other tasks.\n\n# Physical Memory Allocation\n\nPhysical memory allocation accounts for a large but variable quantity of service cost.\nThe UVM driver uses a physical memory allocator to track physical allocations on the GPU.\nAllocation is performed by calling into the main NVIDIA driver.\n\nThe allocator over-allocates memory to cache it, knowing that the cost of each call is quite high.\nThis over-allocation and caching causes the allocation cost to remain relatively constant and negligible at large sizes.\nThis cost is actually contained within the greater “Migrate Pages” category, but is separated here as it is responsible for the “constant” dominating transfer cost within UVM at small sizes.\n\n# Page Migration\n\nPage migration involves:\n\n * permission checking and updates\n * memory allocation and zeroing of newly-allocated memory\n * copying data from the source location to staging locations\n * eventually issuing GPU instructions to copy data from the staging location to the final destination. Once data is staged on the destination device, page duplication would be broken and unmapped from source locations.\n   The UVM driver initiates the memory copy command, and notifies the GPU to actually perform the data copy using DMA.\n\n# Mapping Data\n\nMapping data includes updating the local and remote page tables and issuing appropriate memory barriers to ensure consistency on the GPU.\nWhile updating the GPU page tables is part of the cost here, the importance of this step is in bookkeeping and ensuring data consistency and integrity.\n\nImportant Insights\n\n * First, the number of VABlocks in a batch has a great impact on service time.\n * Second, the batch size affects the cost and the optimal size depends on application access patterns and data requirement. The appropriate tuning of batch size may differ on a per-application basis, and would be an interesting area of future study.\n\n# Replay Policy Cost\n\nReplayable faults do not block the faulting GPU compute unit, which can continue running non-faulting warps until a replay command is received [19].\n\nThe replay notification indicates that the original memory access should be tried again.\nNote that a single fault may need to be replayed multiple times due to hardware fault capacity limitations or software policy.\n\n * Block Policy\n * Batch Policy\n * Batch Flush Policy\n * Once Policy\n\n\n\n\n# Prefetching Challenges and Page-level GPU Access Patterns\n\n# Prefetching Design Constraints and the Tree-Based Algorithm\n\nUVM faces the fundamental prefetching challenge of a finite lead time, the requirement to effectively hide latency.\n\n * Lead Time\n   the lead time is more stringent than hardware prefetching due to the greater cost for executing in software, larger data size, and different interconnect characteristics.\n\nUVM prefetcher should try to prefetch a large volume of data where possible to better utilize the H-D interconnect bandwidth, reduce the overall number of faults, and amortize the prefetching cost.\n\n * UVM page prefetcher only has coarse-grain, partial information of page access\n\nParticularly, the only information specific is the address that originated the fault.\nThe driver lacks sufficient information for correlating faults with their generating GPU core/thread.\nPoor prediction from limited information may result in fetching a large amount of unwanted data, wasting H-D bandwidth.\nConsequently, UVM prefetching is left with the best effort option to support common applications.\n\n * Add transfer overhead that could be wasted if prediction is poor\n\nTwo Stage Prefetching\n\n * First stage: promote 4KB to 64KB\n * Second stage: density prefetcher\n\n# Complex GPU Access Patterns\n\nThe driver does not see non-faulting data, and therefore the driver is oblivious to the full access pattern.\n\nThis algorithm largely drops the timing aspect of prefetching.\n\nInstead, it utilizes the information already being tracked about page location to make confidencebased predictions about which data will be used.\n\nIf a specific VABlock is saturated with faults over any length of time, the algorithm will confidently predict that the rest of that VABlock will also be used.\n\nfault coverage\n\nIn summary, for undersubscribed workloads the density prefetcher can range from effective to highly effective in terms of fault coverage, but can be difficult to code against.\n\n# Effectiveness of Tree-Based Algorithm\n\nDensity prefetching ignores the precise ordering of page faults, which is critical for handling faults from many GPU cores simultaneously.\n\n# Game-Changer：Oversubscription\n\n# The Cost of Eviction\n\n# Eviction in UVM\n\nThe eviction mechanism is triggered whenever the driver attempts to allocate memory for a VABlock that does not have memory reserved on the GPU already, e.g. the first page fault.\nEvictions are performed at the VABlock level, mirroring allocation. When evicted, any modified pages are copied back to the host, and the physical memory allocation for the VABlock is released.\n\nThe UVM driver uses least-recently-used eviction.\nThe LRU list is updated when a fault is handled from a VABlock.\n\n\n\n# Direct and Indirect Costs\n\nDirect Costs\n\n * First, the eviction itself has the same components as a device-to-host fault for a VABlock not present on the host.\n   The changed data needs to be migrated, involving data transfer, memory barrier, and page mapping/unmapping.\n * Second, due to the locking scheme in the driver, eviction causes the VABlock faulting path to start over, as the faulting block lock must be dropped while the evicted block lock is held.\n\nIndirect Costs\n\n * Random Access Pattern: increased quantity of evict/map operations for small data sizes.\n * Eviction mechanism can evict data that is still being used.\n   Because the LRU function is only aware of page-faults, it is possible that the “hottest” regions of data may also be the most likely to be evicted.\n   The data would quickly be migrated to the GPU and then never again updated in the LRU list.\n\nPrefetching can fetch data that will not be used prior to eviction.\n\nDelays due to frequent eviction cause larger backlogs of faults in the fault buffer, requiring increased time to flush the fault buffer before replays and accumulating the time spent handling the fault replay policy.\n\n# Total Eviction Costs\n\nIt is important to understand that the impact of eviction is at its greatest when data access is irregular.\n\nThe overhead of eviction mixed with the sheer number of additional faults and evictions from the poor access pattern account for the order-of-magnitude performance loss.\n\nThe overall cost depends on the measure of oversubscription as well as the access pattern itself..\n\nThe worst effect is noticeable when applications cross the threshold where local data no longer fits in-core, and data is evicted prior to being used.\n\n\n# Discussion and Conclusions\n\n# Challenges in Effective Prefetching and Eviction\n\n# Prefetching\n\nDensity prefetching has limitations when eviction is involved, because there is no guarantee that any prefetched data will actually be used.\nWhile a key advantage of density prefetching is its ignorance of precise fault order, it loses a lot of information about spatial locality.\n\nThe primary cost of prefetching is in additional data migration.\n\n# Eviction\n\nEviction is a very difficult problem because it has its own algorithmic component, as well as a dependency on the memory allocation functionality.\\\n\nAlgorithmically, the implementation is still dependent on page fault information, which is insufficient.\nThe granularity of evictions also impacts its performance, which is locked in UVM at the VABlock level.\n\nThis has two key implications:\n\n * Data that is accessed on the GPU but does not cause a page fault because the page is present will not upgrade its location in the LRU list.\n * VABlocks that are fully resident on the GPU will never be upgraded in the LRU list until they are evicted and re-faulted.\n\nAddressing allocation granularity, 2MB blocks may be too coarse for allocations and evictions for irregular applications.\nWhile 4KB granularity is very small, irregular applications may not even have locality at the 4KB granularity.\n\n# Potential Paths for Better Prefetching and Eviction\n\n# Increased fault origin information\n\nThe GPU presently provides quite a bit of information along with a GPU fault, primarily for tracing higher-level information about the origin of a fault.\n\nAnother level of information that offers SM ID, logical thread ID, or related information sufficient to pinpoint a specific area of execution.\n\n# Flexible memory allocation granularity\n\nirregular applications may benefit from a tuneable parameter allowing different sized memory allocations.\n\nOverhead more complex μTLB implementations and highly flexible driver implementation.\n\n# GPU memory access-aware eviction\n\nNVIDIA has included support for multiple-granularity access counters for GPU-level memory access on GPUs since the Volta architecture [27].\n\nThis idea is explored and simulated in Ganguly et al. [4], but has not been explored on a real system.\n\n# Adpative prefetching\n\nThe existing mechanism has demonstrated that it is quite capable depending on the circumstance.\nUsing existing information, the driver could adapt some simple heuristics to adaptively tune prefetching.",normalizedContent:" 1. [248] dissecting gpu memory hierarchy through microbenchmarking\n 2. [75] benchmarking the memory hierarchy of modern gpus\n 3. [18] benchmarking the gpu memory at the warp level\n 4. [90] dissecting the nvidia turing t4 gpu via microbenchmarking\n 5. [38] exploring modern gpu memory system design challenges through accurate modeling 👍 👍 👍\n 6. [9] osm: off-chip shared memory for gpus\n 7. [10] demystifying gpu uvm cost with deep runtime and workload analysis 👍 👍 👍\n\ntags: #dissecting gpu\n\n\n# 1. dissecting gpu memory hierarchy through microbenchmarking\n\na paper in 2015, profile memory in fermi, kepler and maxwell\n\n\n\n\n# parameter\n\n\n\n\n\n\n\n\n# l1 data cache\n\non the fermi and kepler devices, the l1 data cache and shared memory are physically implemented together.\non the maxwell devices, the l1 data cache is unified with the texture cache.\n\nthe 16 kb l1 cache has 128 cache lines mapped onto four cache ways.\nfor each cache way, 32 cache sets are divided into 8 major sets. each major set contains 16 cache lines.\n\nthe data mapping is also unconventional.\nthe 12-13th bits in the memory address define the cache way, the 9-11th bits define the major set, and the 0-6th bits define the memory offset inside the cache line.\n\none distinctive feature of the fermi l1 cache is that its replacement policy is not lru, as pointed out by meltzer et.al. among the four cache ways, cache way 2 is three times more likely to be replaced than the other three cache ways.\n\nanother paper[4] we found that when the l1 data cache saturates, turing randomly evicts 4 consecutive cache lines (128 b).\nwe observed that once a block of cache lines are evicted, the second scan will cause more cache lines from the same set to be evicted.\n\n\n\n\n# l2 data cache\n\n * the replacement policy of the l2 cache is not lru\n * the l2 cache line size is 32 bytes by observing the memory access pattern of overflowing the cache and visiting array element one by one.\n * the data mapping is sophisticated and not conventional bits-defined\n * a hardware-level pre-fetching mechanism from the dram to the l2 data cache on all three platforms.\n   the pre-fetching size is about 2/3 of the l2 cache size and the prefetching is sequential. this is deduced from that if we load an array smaller than 2/3 of the l2 data cache size, there is no cold cache miss patterns.\n   🙋(maybe they can cover the gap just by prefetching sequential line.)\n\n\n# global memory\n\nglobal memory access involves accessing the dram, l1 and l2 data caches, tlbs and page tables.\n\n\n# global memory throughput\n\nthe theoretical bandwidth is calculated as fmem * bus width * ddr factor.\n\n\n\nthe throughput of a larger ilp saturates faster.\n\nthe gtx780 has the highest throughput as it benefits from the highest bus width,\nbut its convergence speed is the slowest, i.e., it requires the most memory requests to hide the pipeline latency.\n\nthis could be part of the reason that nvidia reduced the bus width back to 256 bits in maxwell devices.\n\n\n# global memory latency\n\nthe global memory access latency is the whole time accessing a data located in dram/l2 or l1 cache, including the latency of page table look-ups.\n\n * very large s1 = 32 mb to construct the tlb/page table miss and cache miss (p5&p6)\n * set s2 = 1 mb to construct the l1 tlb hit but cache miss (p4)\n * after a total of 65 data accesses, 65 data lines are loaded into the cache.\n   we then visit the cached data lines with s1 again for several times, to construct cache hit but tlb miss (p2&p3).\n * set s3 = 1 element and repeatedly load the data in a cache line so that every memory access is a cache hit (p1).\n\n\n\n\n\n\n\n * the maxwell and kepler devices have a unique memory access pattern (p6) for page table context switching.\n   when a kernel is launched, only memory page entries of 512 mb are activated.\n   if the thread visits an inactivate page entry, the hardware needs a rather long time to switch between page tables.\n   this phenomena is also reported in [22] as page table “miss”.\n * the maxwell l1 data cache addressing does not go through the tlbs or page tables.\n   on the gtx980, there is no tlb miss pattern (i.e., p2 and p3) when the l1 data cache is hit.\n   once the l1 cache is missed, the access latency increases from tens of cycles to hundreds or even thousands of cycles. my comments: but if we look at gtx560ti in p2, the latency is different with p1. so does this means that in fermi, the memory request has to go through tlb first, and then access l1 datacache? this might be the reason that the latency is longer. but this will degrade the performance....\n * the tlbs are off-chip. we infer that the physical memory locations of the l1 tlb and l2 data cache are close.\n   the physical memory locations of the l1 tlb and l2 tlb are also close, which means that the l1/l2 tlb and l2 data cache are shared off-chip by all sms.\n * the gtx780 generally has the shortest global memory latencies, almost half that of the fermi, with an access pattern of p2-p5.\n   the page table context switching of the gtx980 is also much more expensive than that of the gtx780.\n\nto summarize, the maxwell device has long global memory access latencies for cold cache misses and page table context switching.\nexcept for these rare access patterns, its access latency cycles are close to those of the kepler device.\nbecause the gtx980 has higher fmem than the gtx780, it actually offers the shortest global memory access time (p2-p4).\n\n\n\n\n# shared memory\n\nin cuda programming, different ctas assigned to the same sm have to share the same physical memory space.\non the fermi and kepler platforms, the shared memory is physically integrated with the l1 cache.\non the maxwell platform, it occupies a separate memory space. note that the shared memory and l1 cache are separated since maxwell architecture.\n\nprogrammers move the data into and out of shared memory from global memory before and after arithmetic execution,\nto avoid the frequent occurrence of long global memory access latencies.\n\nwe report a dramatic improvement in performance for the maxwell device.\n\n\n# shared memory throughput\n\nthe shared memory is organized as 32 memory banks [15].\nthe bank width of the fermi and maxwell devices is 4 bytes, while that of the kepler device is 8 bytes. the theoretical peak throughput of each sm (wsm) is calculated as fcore ∗ wbank ∗ 32.\n\n\n\nthe achieved throughput per sm is calculated as 2 * fcore * sizeof(int) * (number of active threads per sm) * ilp / (total latency of each sm). usually a large value of ilp results in less active warps per sm.\nthe peak throughput w0sm denotes the respective maximum throughput of the abovecombinations.\ntwo key factors that affect the throughput are the number of active warps per sm and the ilp level.\n\nthe gtx980 reaches its peak throughput when the cta size = 256, ctas per sm = 2 and ilp = 8, i.e., 16 active warps per sm. the peak throughput is 137.41 gb/s, about 83.9% of the theoretical bandwidth. the maxwell device shows the best use of its shared memory bandwidth, and the kepler device shows the worst.\n\ngtx980 exhibits similar behavior as gtx780: high ilp is required to achieve high throughput for high sm occupancy.\n\naccording to little’s law, we roughly have: number of active warps * ilp = latency cycles * throughput.\n\ngtx780 sucks in ilp = 1, since its limited 64 warps at most to be scheduled concurrently.\nwe consider this to be the main reason the achieved throughput of the gtx780 is poor compared with its designed value.\n\n\n# shared memory latency\n\nthe shared memory latencies on fermi, kepler and maxwell devices are 50, 47 and 28 cycles, respectively.\n\nfermi and maxwell devices have the same number of potential bank conflicts because they have the same architecture.\n\nthe shared memory space is divided into 32 banks.\nsuccessive words are allocated to successive banks.\nif two threads in the same warp access memory spaces in the same bank, a 2-way bank conflict occurs.\n\n\n\n\n\n\n\nfor the fermi and kepler devices, where there is a 32-way bank conflict, it takes much longer to access shared memory than regular global memory (tlb hit, cache miss).\nsurprisingly, the effect of a bank conflict on shared memory access latency on the maxwell device is mild.\neven the longest shared memory access latency is still at the same level as l1 data cache latency.\n\nin summary, although the shared memory has very short access latency, it can be rather long if there are many ways of bank conflicts.\nthis is most obvious on the fermi hardware.\nthe kepler device tries to solve it by doubling the bank width of shared memory.\ncompared with the fermi, the kepler’s 4-byte mode shared memory halves the chance of bank conflict, and the 8-byte mode reduces it further.\n\nhowever, we also find that the kepler’s shared memory is inefficient in terms of throughput.\nthe maxwell device has the best shared memory performance.\nwith the same architecture as the fermi device, the maxwell hardware shows a 2x size, 2x memory access speedup and achieves the highest throughput.\nmost importantly, the maxwell device’s shared memory has been optimized to avoid the long latency caused by bank conflicts.\n\n\n# conclusion\n\nthe memory capacity is significantly enhanced in both kepler and maxwell as compared with fermi.\nthe kepler device is performance-oriented and incorporates several aggressive elements in its design, such as increasing the bus width of dram and doubling the bank width of shared memory.\nthese designs have some side-effects.\nthe theoretical bandwidths of both global memory and shared memory are difficult to saturate, and hardware resources are imbalanced with a low utilization rate.\nthe maxwell device has a more efficient and conservative design.\nit has a reduced bus width and bank width, and the on-chip cache architectures are adjusted, including doubling the shared memory size and the read-only data cache size.\nfurthermore, it sharply decreases the shared memory latency caused under bank conflicts.\n\n----------------------------------------\n\n\n# 4. dissecting the nvidia turing t4 gpu via microbenchmarking\n\n\n# result\n\n\n\n\n\n\n# shared memory latency\n\n\n\n\n# bandwidth\n\n\n\n----------------------------------------\n\n\n# 4. benchmarking the gpu memory at the warp level\n\nin this work, we investigate the data accessing capability of a warp of threads: broadcasting and parallel accessing.\\\n\n * broadcasting occurs when multiple threads access the same data element, i.e., multiple threads request a single data element (mtsd).\n * we refer the case of multiple threads accessing multiple distinct data elements (mtmd) as parallel accessing.\n\n\n# local memory\n\n * for the simple memory access patterns, we should allocate a sufficient small array to guarantee that it is located in registers.\n * for the complex memory access patterns, we should simplify codes to exploit registers. for example, we merge a three-level loop into an one-level loop so that a larger temporal vector can be allocated in registers.\n * \n\n\n# shared memory\n\n * bank conflicts must be avoided by the ways of e.g., data padding.\n * shared memory supports both broadcasting and parallel accessing.\n * neither consecutively accessing nor aligned accessing is a must.\n * the latency decreases when the number of threads increase, and thus we should use a sufficiently large thread block.\n * replacing global memory with shared memory, because the latency of shared memory is smaller than that of global memory.\n * using shared memory bares an overhead (i.e., buffer allocation and data movement) and reusing data in it is a must for improved performance.\n\n\n# constant memory\n\nbut constant memory does not support parallel accessing.\nthat is, constant memory can only be accessed serially when requesting different data elements.\non the one hand, constant memory is used to store a small amount of read-only data, which is not sensitive to bandwidth.\nso parallel accessing is not a must for constant memory.\n\n * constant memory supports the accessing capability of broadcasting.\n * constant memory does not support parallel accessing, and satisfies parallel memory requests in a serial manner.\n\n\n# global memory\n\n * global memory supports both broadcasting and parallel accessing.\n * the data types of 4 or 8 bytes can obtain the near upper-bounded bandwidth of global memory, while the data types cannot.\n   so the char data should be coalesced into the char4 type for improved bandwidth.\n * global memory accesses should be consecutive, but aligned accessing is not necessary for global memory.\n * when memory accessing is non-consecutive, the latency changes with the number of threads, but not with the number of blocks. so we should configure the thread dimensionality.\n\n----------------------------------------\n\n\n# 5. exploring modern gpu memory system design challenges through accurate modeling\n\n👍 👍 👍\n\n\n# memory coalescer\n\nthe eviction granularity of the cache is 128b, indicating that the l1 cache has 128b lines with 32b sectors.\nfurthermore, the coalescer operates across eight threads, i.e. the coalescer tries to coalesce each group of eight threads separately to generate sectored accesses.\n\n\n\nwhen the stride=32, the memory access is converged, and all the threads within the same warp will access the same cache line,\nhowever we receive four read accesses at l1 cache.\n\n8 thread register 32bit == 32byte.\n\n\n# l2 cache\n\nl2 cache applies something similar to write-validate not fetch on write.\\ 😱 however, all the reads received by l2 caches from the coalescer are 32-byte sectored accesses.\nthus, the read access granularity (32 bytes) is different from the write access granularity (one byte).\nto handle this, the l2 cache applies a different write allocation policy, which we named lazy fetch-on-read, that is a compromise between write-validate and fetch-on-write.\n\nwhen a sector read request is received to a modified sector, it first checks if the sector write-mask is complete, i.e. all the bytes have been written to and the line is fully readable.\nif so, it reads the sector, otherwise, similar to fetch-on-write, it generates a read request for this sector and merges it with the modified bytes.\n\n\n# streaming throughput-oriented l1 cache\n\n\n\nthe l1 cache in volta is what nvidia is calling a streaming cache [33].\nit is streaming because the documentation states that it allows unlimited cache misses to be in flight regardless the number of cache lines per cache set [10].\n\nindependent of the number of l1 configured size, the number of mshrs available are the same, even if more of the on-chip sram storage is devoted to shared memory.\n\nwe believe that unified cache is a plain sram where sectored data blocks are shared between the l1d and the cuda shared memory.\nit can be configured adaptively by the driver as we discussed earlier.\nwe assume that the l1d’s tag and mshr merging functionality are combined together in a separate table structure (tag-mshr table).\nsince, the filling policy is now on fill, we can have more tag entries and outstanding requests than the assigned l1d cache lines.\n\n\n# if it is a hit to a reserved sector (i.e. the status is pending), it sets its corresponding warp bit in the merging mask (64 bits for 64 warps).\nwhen the pending request comes back, it allocates a cache line/sector in the data block and sets the allocated block index in the table.\nthen, the merged warps access the sector, on a cycle-by-cyle basis.\n\n\n# 6. osm: off-chip shared memory for gpus\n\n\n\nl1-d cache and shared memory use the same 32-bank memory structure (4 kb capacity per bank) as shown in fig. 4;\nhowever, they have some differences.\nwe can access 32-bit shared memory arrays via a thread-index directly, while for accessing l1-d cache, we should read 128b (four 32b sectors) of the cache block.\nin addition, l1 cache requires an extra hardware for managing tags and implementing lru replacement policy.\n\n----------------------------------------\n\n\n# 7. demystifying gpu uvm cost with deep runtime and workload analysis\n\n\n# main idea in short\n\nprofile the gpu uvm with different benchmarks.\n\nfor memory that fits in gpu memory, just prefetch.\n\nfor memory oversubscription, prefetch & eviction has room to improve.\n\nprefetch and eviction are maintained by hardware.\n\n * accuracy of prefetch\n * accuracy of eviction\n\nthe lru algorithm is operated on page fault pages.\n\neven frequently accessed page might be evicted.\n\n\n# motivation\n\n 1. cumulative data access latency without prefetching generally increases one or more orders of magnitude with uvm in comparison to explicit direct management by programmers\n 2. when all data fits in gpu global memory, prefetching reduces the cost significantly.\n    but the overall time can still be several times higher than the baseline\n 3. once the gpu global memory is oversubscribed, data access latency dramatically increases by another order of magnitude depending on access pattern\n 4. prefetching can aggravate the performance issues after oversubscription.\n\n\n# cost of demand paging\n\nfar fault\npaged migration moves data between devices in response to a page fault, maps the page into the faulter’s physical space, and unmaps from the previous location.\n\nremote mapping maps the requested data into the requester’s page tables without actually migrating it and accesses it using dma or a related mechanism.\nread-only duplication duplicates data at two or more physical devices and maps them locally to each device under the constraint that the data cannot be mutated.\n\nuvm on-demand paging is implemented using gpu hardware and cpu software working in tandem.\nto integrate with the host os, the uvm driver is provided as a kernel module for the host os to extend the virtual memory space and map it to gpu global memory utilizing the host memory layout.\n\n\n\nuvm uses a four-level hierarchy for memory address space: address spaces, virtual address ranges, virtual address blocks, and pages.\n\nin general, a virtual address space is associated with an application.\neach address space is composed of “ranges”, each corresponding to an arbitrarily sized memory allocation i.e. cudamallocmanaged() or related allocator.\na range is broken up into 2mb sequential virtual address blocks, vablocks.\nvablocks are page-aligned and are composed of os pages.\n\n# flow\n\noperations of the uvm driver in three groups: pre/post-processing, fault servicing, and fault replay policy.\n\n * pre-processing\n   the driver stores page fault information read from the gpu fault buffer and sorts them locally.\n   faults are fetched until all the fault pointer queue is empty, the current batch of faults is full, or fault that is not ready is encountered, depending on policy.\n   the default batch size is 256 faults. per batch, the driver groups page faults based on vablocks and service the faults.\n\n * fault servicing fault servicing involves memory allocation, updating page tables, data transfer, and possibly issuing one or more fault replays or other operations, subject ot the fault replay policy.\n\n\n\n# cost overview\n\npre/post processing is shown to be negligible in cost, but functionally important for the fault servicing and replay implementation.\n\npre-processing first gathers faults from the device, performs basic bookkeeping and logical checks, and sorts them into the appropriate vablock bins.\nnvidia documentation indicates that the driver uses a circular device-side queue to store a fault pointer when a fault occurs [15].\nthe host can read these pointers, which subsequently point to locations in the global gpu fault buffer that contain the full fault information.\nthe driver will generally read at least a full batch from the queue during every pass and cache the faults on the host to avoid having to make multiple remote updates to the queue.\nfaults may not be immediately available in the gpu fault buffer due to the asynchronicity.\nthus the driver may need to poll the buffer until the appropriate “ready” field is marked true or may be able to begin processing on previously fetched faults.\nsorting cost for batches is roughly constant due to the nature of sorting and the relatively small size of batches.\n\n\n\n# sevice cost breakdown\n\nfault servicing is a multi-step process that includes:\n\n * allocating physical space\n * zeroing out gpu pages\n * migrating data from the source to the destination\n * mapping pages and permissions, and a number of other tasks.\n\n# physical memory allocation\n\nphysical memory allocation accounts for a large but variable quantity of service cost.\nthe uvm driver uses a physical memory allocator to track physical allocations on the gpu.\nallocation is performed by calling into the main nvidia driver.\n\nthe allocator over-allocates memory to cache it, knowing that the cost of each call is quite high.\nthis over-allocation and caching causes the allocation cost to remain relatively constant and negligible at large sizes.\nthis cost is actually contained within the greater “migrate pages” category, but is separated here as it is responsible for the “constant” dominating transfer cost within uvm at small sizes.\n\n# page migration\n\npage migration involves:\n\n * permission checking and updates\n * memory allocation and zeroing of newly-allocated memory\n * copying data from the source location to staging locations\n * eventually issuing gpu instructions to copy data from the staging location to the final destination. once data is staged on the destination device, page duplication would be broken and unmapped from source locations.\n   the uvm driver initiates the memory copy command, and notifies the gpu to actually perform the data copy using dma.\n\n# mapping data\n\nmapping data includes updating the local and remote page tables and issuing appropriate memory barriers to ensure consistency on the gpu.\nwhile updating the gpu page tables is part of the cost here, the importance of this step is in bookkeeping and ensuring data consistency and integrity.\n\nimportant insights\n\n * first, the number of vablocks in a batch has a great impact on service time.\n * second, the batch size affects the cost and the optimal size depends on application access patterns and data requirement. the appropriate tuning of batch size may differ on a per-application basis, and would be an interesting area of future study.\n\n# replay policy cost\n\nreplayable faults do not block the faulting gpu compute unit, which can continue running non-faulting warps until a replay command is received [19].\n\nthe replay notification indicates that the original memory access should be tried again.\nnote that a single fault may need to be replayed multiple times due to hardware fault capacity limitations or software policy.\n\n * block policy\n * batch policy\n * batch flush policy\n * once policy\n\n\n\n\n# prefetching challenges and page-level gpu access patterns\n\n# prefetching design constraints and the tree-based algorithm\n\nuvm faces the fundamental prefetching challenge of a finite lead time, the requirement to effectively hide latency.\n\n * lead time\n   the lead time is more stringent than hardware prefetching due to the greater cost for executing in software, larger data size, and different interconnect characteristics.\n\nuvm prefetcher should try to prefetch a large volume of data where possible to better utilize the h-d interconnect bandwidth, reduce the overall number of faults, and amortize the prefetching cost.\n\n * uvm page prefetcher only has coarse-grain, partial information of page access\n\nparticularly, the only information specific is the address that originated the fault.\nthe driver lacks sufficient information for correlating faults with their generating gpu core/thread.\npoor prediction from limited information may result in fetching a large amount of unwanted data, wasting h-d bandwidth.\nconsequently, uvm prefetching is left with the best effort option to support common applications.\n\n * add transfer overhead that could be wasted if prediction is poor\n\ntwo stage prefetching\n\n * first stage: promote 4kb to 64kb\n * second stage: density prefetcher\n\n# complex gpu access patterns\n\nthe driver does not see non-faulting data, and therefore the driver is oblivious to the full access pattern.\n\nthis algorithm largely drops the timing aspect of prefetching.\n\ninstead, it utilizes the information already being tracked about page location to make confidencebased predictions about which data will be used.\n\nif a specific vablock is saturated with faults over any length of time, the algorithm will confidently predict that the rest of that vablock will also be used.\n\nfault coverage\n\nin summary, for undersubscribed workloads the density prefetcher can range from effective to highly effective in terms of fault coverage, but can be difficult to code against.\n\n# effectiveness of tree-based algorithm\n\ndensity prefetching ignores the precise ordering of page faults, which is critical for handling faults from many gpu cores simultaneously.\n\n# game-changer：oversubscription\n\n# the cost of eviction\n\n# eviction in uvm\n\nthe eviction mechanism is triggered whenever the driver attempts to allocate memory for a vablock that does not have memory reserved on the gpu already, e.g. the first page fault.\nevictions are performed at the vablock level, mirroring allocation. when evicted, any modified pages are copied back to the host, and the physical memory allocation for the vablock is released.\n\nthe uvm driver uses least-recently-used eviction.\nthe lru list is updated when a fault is handled from a vablock.\n\n\n\n# direct and indirect costs\n\ndirect costs\n\n * first, the eviction itself has the same components as a device-to-host fault for a vablock not present on the host.\n   the changed data needs to be migrated, involving data transfer, memory barrier, and page mapping/unmapping.\n * second, due to the locking scheme in the driver, eviction causes the vablock faulting path to start over, as the faulting block lock must be dropped while the evicted block lock is held.\n\nindirect costs\n\n * random access pattern: increased quantity of evict/map operations for small data sizes.\n * eviction mechanism can evict data that is still being used.\n   because the lru function is only aware of page-faults, it is possible that the “hottest” regions of data may also be the most likely to be evicted.\n   the data would quickly be migrated to the gpu and then never again updated in the lru list.\n\nprefetching can fetch data that will not be used prior to eviction.\n\ndelays due to frequent eviction cause larger backlogs of faults in the fault buffer, requiring increased time to flush the fault buffer before replays and accumulating the time spent handling the fault replay policy.\n\n# total eviction costs\n\nit is important to understand that the impact of eviction is at its greatest when data access is irregular.\n\nthe overhead of eviction mixed with the sheer number of additional faults and evictions from the poor access pattern account for the order-of-magnitude performance loss.\n\nthe overall cost depends on the measure of oversubscription as well as the access pattern itself..\n\nthe worst effect is noticeable when applications cross the threshold where local data no longer fits in-core, and data is evicted prior to being used.\n\n\n# discussion and conclusions\n\n# challenges in effective prefetching and eviction\n\n# prefetching\n\ndensity prefetching has limitations when eviction is involved, because there is no guarantee that any prefetched data will actually be used.\nwhile a key advantage of density prefetching is its ignorance of precise fault order, it loses a lot of information about spatial locality.\n\nthe primary cost of prefetching is in additional data migration.\n\n# eviction\n\neviction is a very difficult problem because it has its own algorithmic component, as well as a dependency on the memory allocation functionality.\\\n\nalgorithmically, the implementation is still dependent on page fault information, which is insufficient.\nthe granularity of evictions also impacts its performance, which is locked in uvm at the vablock level.\n\nthis has two key implications:\n\n * data that is accessed on the gpu but does not cause a page fault because the page is present will not upgrade its location in the lru list.\n * vablocks that are fully resident on the gpu will never be upgraded in the lru list until they are evicted and re-faulted.\n\naddressing allocation granularity, 2mb blocks may be too coarse for allocations and evictions for irregular applications.\nwhile 4kb granularity is very small, irregular applications may not even have locality at the 4kb granularity.\n\n# potential paths for better prefetching and eviction\n\n# increased fault origin information\n\nthe gpu presently provides quite a bit of information along with a gpu fault, primarily for tracing higher-level information about the origin of a fault.\n\nanother level of information that offers sm id, logical thread id, or related information sufficient to pinpoint a specific area of execution.\n\n# flexible memory allocation granularity\n\nirregular applications may benefit from a tuneable parameter allowing different sized memory allocations.\n\noverhead more complex μtlb implementations and highly flexible driver implementation.\n\n# gpu memory access-aware eviction\n\nnvidia has included support for multiple-granularity access counters for gpu-level memory access on gpus since the volta architecture [27].\n\nthis idea is explored and simulated in ganguly et al. [4], but has not been explored on a real system.\n\n# adpative prefetching\n\nthe existing mechanism has demonstrated that it is quite capable depending on the circumstance.\nusing existing information, the driver could adapt some simple heuristics to adaptively tune prefetching.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU TLB",frontmatter:{title:"GPU TLB",date:"2024-08-26T00:00:00.000Z",permalink:"/pages/45877/",tags:[null]},regularPath:"/03.gpu/20.gpu_tlb.html",relativePath:"03.gpu/20.gpu_tlb.md",key:"v-5552df36",path:"/pages/45877/",headers:[{level:3,title:"1. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring",slug:"_1-dissecting-the-nvidia-volta-gpu-architecture-via-microbenchmaring",normalizedTitle:"1. dissecting the nvidia volta gpu architecture via microbenchmaring",charIndex:1122},{level:3,title:"2. SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs",slug:"_2-snakebyte-a-tlb-design-with-adaptive-and-recursive-page-merging-in-gpus",normalizedTitle:"2. snakebyte: a tlb design with adaptive and recursive page merging in gpus",charIndex:1400},{level:3,title:"4. TunneLs for Bootlegging: Fully Reverse-Engineering GPU TLBs  for Challenging Isolation Guarantees of NVIDIA MIG",slug:"_4-tunnels-for-bootlegging-fully-reverse-engineering-gpu-tlbs-for-challenging-isolation-guarantees-of-nvidia-mig",normalizedTitle:"4. tunnels for bootlegging: fully reverse-engineering gpu tlbs  for challenging isolation guarantees of nvidia mig",charIndex:null},{level:3,title:"Observations",slug:"observations",normalizedTitle:"observations",charIndex:169},{level:3,title:"5. Big data causing big (TLB) problems: taming random memory accesses on the GPU",slug:"_5-big-data-causing-big-tlb-problems-taming-random-memory-accesses-on-the-gpu",normalizedTitle:"5. big data causing big (tlb) problems: taming random memory accesses on the gpu",charIndex:8857},{level:3,title:"6. Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding",slug:"_6-trans-fw-short-circuiting-page-table-walk-in-multi-gpu-systems-via-remote-forwarding",normalizedTitle:"6. trans-fw: short circuiting page table walk in multi-gpu systems via remote forwarding",charIndex:12791},{level:3,title:"9. Improving Multi-Instance GPU Efficiency via Sub-Entry Sharing TLB Design",slug:"_9-improving-multi-instance-gpu-efficiency-via-sub-entry-sharing-tlb-design",normalizedTitle:"9. improving multi-instance gpu efficiency via sub-entry sharing tlb design",charIndex:15470},{level:2,title:"Please Notice that in the above picture, for each subpage, it has a physical address. Thus it is not physically consecutive. Physical page address could be randomly located.",slug:"please-notice-that-in-the-above-picture-for-each-subpage-it-has-a-physical-address-thus-it-is-not-physically-consecutive-physical-page-address-could-be-randomly-located",normalizedTitle:"please notice that in the above picture, for each subpage, it has a physical address. thus it is not physically consecutive. physical page address could be randomly located.",charIndex:16539},{level:3,title:"11. MASK: Redesigning the GPU Memory Hierarchy to Support Multi-Application Concurrency",slug:"_11-mask-redesigning-the-gpu-memory-hierarchy-to-support-multi-application-concurrency",normalizedTitle:"11. mask: redesigning the gpu memory hierarchy to support multi-application concurrency",charIndex:16717}],headersStr:"1. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring 2. SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs 4. TunneLs for Bootlegging: Fully Reverse-Engineering GPU TLBs  for Challenging Isolation Guarantees of NVIDIA MIG Observations 5. Big data causing big (TLB) problems: taming random memory accesses on the GPU 6. Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding 9. Improving Multi-Instance GPU Efficiency via Sub-Entry Sharing TLB Design Please Notice that in the above picture, for each subpage, it has a physical address. Thus it is not physically consecutive. Physical page address could be randomly located. 11. MASK: Redesigning the GPU Memory Hierarchy to Support Multi-Application Concurrency",content:" 1.  [90] Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring\n 2.  [6] SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs\n 3.  [117] Observations and Opportunities in Architecting Shared Virtual Memory for Heterogeneous Systems 👍 👍 👍 👍 👴\n 4.  [2023] TunneLs for Bootlegging: Fully Reverse-Engineering GPU TLBs for Challenging Isolation Guarantees of NVIDIA MIG 👍 👍 👍\n 5.  [31] Big data causing big (TLB) problems: taming random memory accesses on the GPU 👍 👍 👍\n 6.  [248] Dissecting GPU Memory Hierarchy through Microbenchmarking\n 7.  [2023 HPCA] Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding\n 8.  [2020 PACT] Enhancing Address Translations in Throughput Processors via Compression 🌚\n 9.  [2024 MICRO] Improving Multi-Instance GPU Efficiency via Sub-Entry Sharing TLB Design\n 10. [137 MICRO] Mosaic: A GPU Memory Manager with Application-Transparent Support for Multiple Page Sizes\n 11. [109 ASPLOS] MASK: Redesigning the GPU Memory Hierarchy to Support Multi-Application Concurrency\n\n----------------------------------------\n\n\n\n\n\n\n# 1. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring\n\nOn Volta and on all other architectures we examined:\n\n * the L1 data cache is indexed by virtual addresses;\n * the L2 data cache is indexed by physical addresses\n\n----------------------------------------\n\n\n# 2. SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs\n\n# Idea\n\nSnakeByte allows multiple equal-sized pages coalescing into a page table entry (PTE).\nIt records the validity of pages to be merged using a bit vector, and few bits are annexed to indicate the size of merged pages.\n\n# TLB & PTW & GMMU\n\nDeparting from conventional paging schemes of CPUs that heavily rely on operating systems, hardware-based GPU memory management units (GMMUs) are essential to effectively separate device memory management from host CPUs.\nOtherwise, GPUs require the frequent intervention of OS to handle page table walks (PTWs) and TLB misses, which significantly penalize the GPU performance.\n\nObservations:\n\n * GPU workloads demand a large number of TLB entries (e.g., 32K to 256K entries) to handle sizable working sets, but conventional TLBs cannot provide sufficient coverage.\n * GPU workloads have variable ranges of page contiguity.\n\n\n\n# Paper Idea\n\nIf contiguity exists, valid bits are accordingly set in the bit vector. When all pages in the page group are allocated with contiguity (i.e., all valid bits set), the first PTE of the page group called base PTE is promoted to be further coalesced into a larger page group.\n\n# Address Translation\n\nAn L1 TLB is private to a streaming multiprocessor (SM), and an L2 TLB is shared among SMs [41], [42].\nOn a last-level TLB miss, a request is sent to a centralized GMMU [18], [41], [42] to walk through page tables, and the GMMU concurrently handles multiple PTW requests (e.g., 8-16 PTWs).\nTo amortize the latency cost of PTWs, GPUs employ page walk caches that store recently used translations at different levels of page tables.\nImportantly, the GMMU execution has to be independent of host-side operations unlike the conventional paging schemes of CPUs that heavily rely on operating systems.\nOtherwise, GPUs involve frequent OS interventions, which significantly penalize the GPU performance [44], [54].\n\nThis observation is the primary motivation of SnakeByte that can flexibly manage variable-sized page groups and maximize TLB reach.\n\n\n\n\n\nWhen eight 4KB pages are allocated with contiguity, the page group is promoted to be coalesced into the next level of page group.\n\nAt the new page allocation, SnakeByte checks the contiguity of the new PTE with others in the page group.\n\n# Simulation\n\n * By recursively coalescing PTEs, SnakeByte inevitably loses fine-grained controls on the A/D bits for individual pages.\n   SnakeByte adds 8-bit access and dirty fields to a TLB entry to trace A/D states within a page group.\n * GPUs have long shootdown delays (4.2us).\n * The TLB hierarchy consists of a private L1 TLB per SM, a shared L2 TLB, and miss status holding registers (MSHRs).\n   An MSHR in an L1 TLB merges up to 16 misses.\n * 16 page table walkers can concurrently access four-level page tables, and a page walk cache per page table level stores up to 16 recently used translations.\n * When a new page is allocated, a sequential page prefetcher allocates 16 consecutive pages (total 64KB) at a time.\n * To analyze the effect of page migration latency [9], [55], we add a 20us latency overhead for each 4KB page fault [55] with 8.48GB/s bandwidth for a 64KB prefetcher [18].\n\n----------------------------------------\n\n\n# 4. TunneLs for Bootlegging: Fully Reverse-Engineering GPU TLBs for Challenging Isolation Guarantees of NVIDIA MIG\n\nHowever, we surprisingly find that MIG does not partitation the last-level TLB, which is shared by all the compute units in a GPU.\n\n# UVM-Managed Pages\n\nA module in the NVIDIA driver is in charge of allocating UVM-managed pages.\nThe cudaMallocManaged() function registers a virtual address subspace for UVM use.\nThe UVM module allocates pages when the GPU accesses addresses in the registered address space.\nAlthough three page sizes are supported (see Figure 1), we find that UVM actually only allocates 64KB and 2MB pages.\n\nUVM starts with allocating 64KB pages, but it will merge the 64KB pages within a 2MB page into the 2MB page if the residency reaches certain conditions.\nFor example, if the first 17 or more 64KB pages in a 2MB page are present on GPU, the page table entries for these 64KB pages will be purged and replaced with a 2MB entry;\nbut if just the first 16 64KB pages are used, the merging operation will not be triggered.\nWe find that some other residency patterns with less than 17 pages can also trigger the merging.\nFor instance, if every other 64KB page is used (i.e., 16 ones as there are 32 64KB pages in a 2MB page), the merging will also happen.\n\n\n\n# TLB Sub-Entries\n\nIn [26], Nayak et al. claim that NVIDIA GPUs enforce TLB coalescing, which combines 16 address translations to occupy just one TLB entry if the corresponding virtual page numbers are consecutive and the mapped physical page frames are also contiguous.\nHowever, the results of our experiments do not agree with this claim.\n\nWe notice that address translations reside in one L2-uTLB or L3-uTLB entry as long as the virtual base addresses of the corresponding pages are:\n\n * within the same 1MB-aligned address range if the pages are 64KB\n * within the same 32MB-aligned range if the pages are 2MB.\n\nThis observation disproves the existence of dynamically coalescing TLB entries and explains why we separate the base addresses by 0x100000 (i.e., 1MB) and 0x2000000 (i.e., 32MB) when using sequences of 64KB and 2MB pages respectively to perform the above-mentioned tasks.\n\nInstead of TLB coalescing, we conjecture that there are 16 sub-entries in one L2-uTLB or L3-uTLB entry,\nand they have a one-to-one mapping relationship with the address translations for 16 pages of size 64KB or 2MB located in the same 1MB- or 32MB-aligned range.\nIf any sub-entry encounters an eviction, the rest of them are also invalidated.\nInterestingly, we find that the entries of L1-iTLB and L1-dTLB do not have such sub-entries.\n\nInclusivity and Exclusivity.\nWe find that the L2-uTLB is neither inclusive nor exclusive in all the inspected GPUs. The same is also true for the L3-uTLB.\n\nReinsertion. We find that an L2-uTLB hit is reinserted into the L1 and an L3-uTLB hit is also reinserted into the L2 and L1.\n\n\n\n\n# Observations\n\nWe observe that the execution of the infinite loop on all the tested GPUs is not affected after modifying the address translation for the code page, which implies that (at least) the L1 TLB of a GPU is split into an L1-iTLB and an L1-dTLB.\n\nFrom [6]\n\nwe can infer that the L1-iTLB of these GPUs has 16 entries and is fully-associative (otherwise, the smallest 𝑁 evicting the target address translation should differ from 16 occasionally).\n\nExchanging the above roles played by code and data pages, we can learn that the L1-dTLB of all these GPUs also has 16 entries and is fully-associative.\n\nWe find that the L1-iTLB and the L1-dTLB are private to each SM in Turing GPUs (e.g., RTX 2080), but they are shared between the two SMs of each TPC in Ampere GPUs (e.g., RTX 3080 and A100).\n\nThis observation indicates that at least two levels of unified TLBs, which we call L2-uTLB and L3-uTLB.\n\nL2-uTLB is 8-way set-associative in all the tested GPUs.\n\nThese observations lead to the conclusion that the L3-uTLB in MIG-supported GPUs is still 8-way set-associative and it is (physically or just logically) split into two slices; and each slice has an 8-entry victim buffer shared by all the TLB sets in the slice.\n\nL3-uTLB is shared by all the SMs.\n\n\n# 5. Big data causing big (TLB) problems: taming random memory accesses on the GPU\n\nIf the data accesses are irregular, like hash table accesses or random sampling, the GPU performance can suffer.\nEspecially when scaling such accesses beyond 2GB of data, a performance decrease of an order of magnitude is encountered.\nThis is paper analyzes the source of the slowdown through extensive micro-benchmarking, attributing the root cause to the Translation Lookaside Buffer (TLB).\n\n# Introduction\n\nGPU data larger than 2GB, which, in some cases, may result in a ≈13.3x runtime decrease.\nwe identified the Translation Lookaside Buffer (TLB) as the source of this slowdown, where TLB misses cost hundreds of cycles per memory access.\n\n * NVIDIA Kepler [15]\n * NVIDIA Pascal [16]\n\nthe P100 shows a significantly better performance than the K80, as it has a newer hardware architecture.\nHowever, the slowdown for memory accesses >2GB is still significant with factors of 4.3x for random sampling and 3.3x for grouping.\n\n# Benchmark\n\n# Virtual Memory\n\nThe reasons why GPU use virtual address\n(1) Isolation: The indirection controls a program’s memory accesses and, thus, keeps it from disallowed memory accesses to internal device data or to data of other applications using the same GPU.\n(2) Fragmentation: Memory fragmentation can be hidden with virtual pages, allowing a large consecutive region of virtual memory to be scattered across many positions in physical memory.\nThis can also increase memory bandwidth if physical memory is scattered to multiple memory chips, which then can be accessed in parallel.\n\n# Benchmark\n\npointer chasing with stride distance\n\n\n\nEvery stride size smaller than the page size behaves like (1/2)*X: showing lower cycle counts but experiences the first TLB miss at the same position.\n\n# Observation\n\n\n\n\n\n# Summary\n\n(1) We found three levels of TLBs for the K80 and two levels for the P100.\n(2) For both GPUs, the different TLB levels apparently use different page sizes, where the L1 TLB uses a small page size and the L2/L3 TLB use a 16x larger page size.\n(3) Compared to K80, the P100 always has 16x larger pages.\n(4) For data larger than 2GB, the K80 has a total delay of 241 cycles, while the P100 only has a 119 cycle delay.\n\n# Plausibility and Validation\n\n * First, the sizes of the L1 TLB (16 entries) and L2 TLB (65 entries) for Kepler GPUs (K80).\n * We can confirm this for the L1 TLB, while the K80 already uses 2MB pages for the L2 and L3 TLB (as shown by [10]).\n * Third, every TPC has its own L1 TLB and every GPC has its own L2 TLB, while the L3 TLB is shared for all SMs.\n * Fourth, we can see a significant performance drop in our investigated database operations when we access more data than ≈2GB.\n   Even with different page sizes for both GPUs, we can pinpoint the problem to the L3 TLB on the K80 and the L2 TLB on the P100.\n   We can even identify the L2 TLB boundary on the K80, where performance problems start at ≈130MB.\n * Fifth, in [6], the performance of a grouping operator on Kepler GPUs was improved by reducing the number of threads to <1000 instead of multiple thousands for data accesses beyond 2GB.\n   With our results, we can explain that this is benefinicial because each thread can load one page translation in the L3 TLB (1032 entries).\n   The page translations stay in the TLB.\n\n# Argument for Unconventional Properties\n\ntwo unconventional results:\n(1) TLB entry numbers not being the power of two\n(2) different page sizes for different TLB levels.\n\nWe evaluated the allocation size and found that the smaller page size is always used for allocations (128KB on K80, 2MB on P100).\nOne possible explanation for the apparently larger page sizes in the L2/L3 TLB could be a static pre-fetching algorithm, which always loads 16 contiguous pages when a TLB miss occurs.\nThis would result in one TLB miss and 15 TLB hits, when using the small page size as traversal stride.\n\n\n# 6. Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding\n\n\n\n# Address Translation Flow\n\n 1. The memory requests generated by the same wavefront are first coalesced by the GPU memory coalescing unit.\n 2. the L1 data cache and the L1 TLB perform lookups in parallel in a virtually indexed physically tagged (VIPT) TLB-cache design.\n 3. Upon L1 TLB misses, the L1 Miss Status Holding Register (MSHR) is first checked to filter out repetitive requests, and the outstanding requests are forwarded to the L2 TLB for lookup.\n 4. Translations that miss in the L2 TLB and L2 MSHR are sent to the local PT-walk in the GMMU.\n\nBecause there is limited number of PT-walk threads, L2 TLB misses may not be served immediately.\\\n\n 1. these translation requests will be stored in the PW-queue and wait for available PT-walk threads.\n 2. During the page table walking, the translation is first checked in the PW-cache;\n 3. if it misses the PW-cache, the GPU local page table is accessed, which can be expensive and involves multiple memory accesses ( 5 ).\n 4. If the page walk fails, a far fault is propagated to the GMMU and kept in a structure called GPU Fault Buffer [6], [7].\n 5. Each time a far fault arises, the GMMU sends an alert to the UMV-driver.\n 6. Upon the receipt of a far fault, the UVM driver fetches the fault information and caches them on the host side.\n 7. The cached page faults are processed in batch granularity (the batch size is 256 [53]).\n 8. Per batch, the UVM-driver initiates threads to perform page table walks using the centralized page table, initiates data transfer, and updates the GPU local page tables [7].\n 9. The translation request is replayed after the far fault is resolved.\n\n# Hardware Handled Far Faults\n\nThis is the flow of hardware-acclerated Far faults handling\n\n 1. When a far fault is generated, it is sent to the host and then handled by host MMU.\n 2. Specifically, upon receiving a translation request, the host MMU first performs a host MMU TLB lookup.\n 3. If the translation misses in the TLB, the request waits in the host MMU PW-queue for PT-walk\n 4. The PT-walk process in the host MMU is similar to the GPU local PT-walk, including host MMU PW-cache lookup\n 5. host MMU PT-walk for PW-cache misses\n 6. host MMU TLB update\n\nthe address translation overhead of software is 4.5× higher than that of hardware in 32 GPUs.\n\nthree major latencies in the baseline address translation:\n\n 1. waiting time for available PT-walk threads in the PW-queue\n 2. additionally memory accesses after PW-cache misses\n 3. handling far faults caused by page sharing among multiple GPUs\n\n----------------------------------------\n\n\n# 9. Improving Multi-Instance GPU Efficiency via Sub-Entry Sharing TLB Design\n\nSpecifically, in the L2 and L3 TLBs, an entry comprises 16 sub-entries, each directly corresponding to the address translation of 16 sequential 64 KB pages within a contiguous 1 MB-aligned segment.\n\nBy compressing multiple translations into a single TLB entry, the TLB can manage more data with fewer entries, thereby reducing hardware overhead, while improving TLB efficiency and boosting overall performance.\n\nthese GPUs organize their L2 and L3 TLB entries differently to increase TLB reach [60].\nSpecifically, each of these entries contains 16 sub-entries, which directly map to the address translations for 16 pages.\nThese pages can be either 64 KB or 2 MB in size, and all of them fall within an aligned range of either 1 MB or 32 MB in size, respectively.\nThat means each sub-entry in a TLB entry has a one-to-one relationship with a single page.\nNote that, in the sub-entry setting, if any TLB entry is evicted, all the 16 sub-entries associated with that TLB entry are zeroed.\n\n\n\n\n# Please Notice that in the above picture, for each subpage, it has a physical address. Thus it is not physically consecutive. Physical page address could be randomly located.\n\n\n# 11. MASK: Redesigning the GPU Memory Hierarchy to Support Multi-Application Concurrency\n\n# Main Idea\n\nContention of shard TLB leads to frequent misses in the shared translation lookaside buffer (TLB), where a single miss can induce long-latency stalls for hundreds of threads.\nAs a result, the GPU often cannot schedule enough threads to successfully hide the stalls, which diminishes system throughput and becomes a first-order performance concern.\n\n# Contributions\n\n(1) a token-based technique to reduce TLB contention\n(2) a bypassing mechanism to improve the effectiveness of cached address translations\n(3) an application-aware memory scheduling scheme to reduce the interference between address translation and data requests.\n\n\n\n",normalizedContent:" 1.  [90] dissecting the nvidia volta gpu architecture via microbenchmaring\n 2.  [6] snakebyte: a tlb design with adaptive and recursive page merging in gpus\n 3.  [117] observations and opportunities in architecting shared virtual memory for heterogeneous systems 👍 👍 👍 👍 👴\n 4.  [2023] tunnels for bootlegging: fully reverse-engineering gpu tlbs for challenging isolation guarantees of nvidia mig 👍 👍 👍\n 5.  [31] big data causing big (tlb) problems: taming random memory accesses on the gpu 👍 👍 👍\n 6.  [248] dissecting gpu memory hierarchy through microbenchmarking\n 7.  [2023 hpca] trans-fw: short circuiting page table walk in multi-gpu systems via remote forwarding\n 8.  [2020 pact] enhancing address translations in throughput processors via compression 🌚\n 9.  [2024 micro] improving multi-instance gpu efficiency via sub-entry sharing tlb design\n 10. [137 micro] mosaic: a gpu memory manager with application-transparent support for multiple page sizes\n 11. [109 asplos] mask: redesigning the gpu memory hierarchy to support multi-application concurrency\n\n----------------------------------------\n\n\n\n\n\n\n# 1. dissecting the nvidia volta gpu architecture via microbenchmaring\n\non volta and on all other architectures we examined:\n\n * the l1 data cache is indexed by virtual addresses;\n * the l2 data cache is indexed by physical addresses\n\n----------------------------------------\n\n\n# 2. snakebyte: a tlb design with adaptive and recursive page merging in gpus\n\n# idea\n\nsnakebyte allows multiple equal-sized pages coalescing into a page table entry (pte).\nit records the validity of pages to be merged using a bit vector, and few bits are annexed to indicate the size of merged pages.\n\n# tlb & ptw & gmmu\n\ndeparting from conventional paging schemes of cpus that heavily rely on operating systems, hardware-based gpu memory management units (gmmus) are essential to effectively separate device memory management from host cpus.\notherwise, gpus require the frequent intervention of os to handle page table walks (ptws) and tlb misses, which significantly penalize the gpu performance.\n\nobservations:\n\n * gpu workloads demand a large number of tlb entries (e.g., 32k to 256k entries) to handle sizable working sets, but conventional tlbs cannot provide sufficient coverage.\n * gpu workloads have variable ranges of page contiguity.\n\n\n\n# paper idea\n\nif contiguity exists, valid bits are accordingly set in the bit vector. when all pages in the page group are allocated with contiguity (i.e., all valid bits set), the first pte of the page group called base pte is promoted to be further coalesced into a larger page group.\n\n# address translation\n\nan l1 tlb is private to a streaming multiprocessor (sm), and an l2 tlb is shared among sms [41], [42].\non a last-level tlb miss, a request is sent to a centralized gmmu [18], [41], [42] to walk through page tables, and the gmmu concurrently handles multiple ptw requests (e.g., 8-16 ptws).\nto amortize the latency cost of ptws, gpus employ page walk caches that store recently used translations at different levels of page tables.\nimportantly, the gmmu execution has to be independent of host-side operations unlike the conventional paging schemes of cpus that heavily rely on operating systems.\notherwise, gpus involve frequent os interventions, which significantly penalize the gpu performance [44], [54].\n\nthis observation is the primary motivation of snakebyte that can flexibly manage variable-sized page groups and maximize tlb reach.\n\n\n\n\n\nwhen eight 4kb pages are allocated with contiguity, the page group is promoted to be coalesced into the next level of page group.\n\nat the new page allocation, snakebyte checks the contiguity of the new pte with others in the page group.\n\n# simulation\n\n * by recursively coalescing ptes, snakebyte inevitably loses fine-grained controls on the a/d bits for individual pages.\n   snakebyte adds 8-bit access and dirty fields to a tlb entry to trace a/d states within a page group.\n * gpus have long shootdown delays (4.2us).\n * the tlb hierarchy consists of a private l1 tlb per sm, a shared l2 tlb, and miss status holding registers (mshrs).\n   an mshr in an l1 tlb merges up to 16 misses.\n * 16 page table walkers can concurrently access four-level page tables, and a page walk cache per page table level stores up to 16 recently used translations.\n * when a new page is allocated, a sequential page prefetcher allocates 16 consecutive pages (total 64kb) at a time.\n * to analyze the effect of page migration latency [9], [55], we add a 20us latency overhead for each 4kb page fault [55] with 8.48gb/s bandwidth for a 64kb prefetcher [18].\n\n----------------------------------------\n\n\n# 4. tunnels for bootlegging: fully reverse-engineering gpu tlbs for challenging isolation guarantees of nvidia mig\n\nhowever, we surprisingly find that mig does not partitation the last-level tlb, which is shared by all the compute units in a gpu.\n\n# uvm-managed pages\n\na module in the nvidia driver is in charge of allocating uvm-managed pages.\nthe cudamallocmanaged() function registers a virtual address subspace for uvm use.\nthe uvm module allocates pages when the gpu accesses addresses in the registered address space.\nalthough three page sizes are supported (see figure 1), we find that uvm actually only allocates 64kb and 2mb pages.\n\nuvm starts with allocating 64kb pages, but it will merge the 64kb pages within a 2mb page into the 2mb page if the residency reaches certain conditions.\nfor example, if the first 17 or more 64kb pages in a 2mb page are present on gpu, the page table entries for these 64kb pages will be purged and replaced with a 2mb entry;\nbut if just the first 16 64kb pages are used, the merging operation will not be triggered.\nwe find that some other residency patterns with less than 17 pages can also trigger the merging.\nfor instance, if every other 64kb page is used (i.e., 16 ones as there are 32 64kb pages in a 2mb page), the merging will also happen.\n\n\n\n# tlb sub-entries\n\nin [26], nayak et al. claim that nvidia gpus enforce tlb coalescing, which combines 16 address translations to occupy just one tlb entry if the corresponding virtual page numbers are consecutive and the mapped physical page frames are also contiguous.\nhowever, the results of our experiments do not agree with this claim.\n\nwe notice that address translations reside in one l2-utlb or l3-utlb entry as long as the virtual base addresses of the corresponding pages are:\n\n * within the same 1mb-aligned address range if the pages are 64kb\n * within the same 32mb-aligned range if the pages are 2mb.\n\nthis observation disproves the existence of dynamically coalescing tlb entries and explains why we separate the base addresses by 0x100000 (i.e., 1mb) and 0x2000000 (i.e., 32mb) when using sequences of 64kb and 2mb pages respectively to perform the above-mentioned tasks.\n\ninstead of tlb coalescing, we conjecture that there are 16 sub-entries in one l2-utlb or l3-utlb entry,\nand they have a one-to-one mapping relationship with the address translations for 16 pages of size 64kb or 2mb located in the same 1mb- or 32mb-aligned range.\nif any sub-entry encounters an eviction, the rest of them are also invalidated.\ninterestingly, we find that the entries of l1-itlb and l1-dtlb do not have such sub-entries.\n\ninclusivity and exclusivity.\nwe find that the l2-utlb is neither inclusive nor exclusive in all the inspected gpus. the same is also true for the l3-utlb.\n\nreinsertion. we find that an l2-utlb hit is reinserted into the l1 and an l3-utlb hit is also reinserted into the l2 and l1.\n\n\n\n\n# observations\n\nwe observe that the execution of the infinite loop on all the tested gpus is not affected after modifying the address translation for the code page, which implies that (at least) the l1 tlb of a gpu is split into an l1-itlb and an l1-dtlb.\n\nfrom [6]\n\nwe can infer that the l1-itlb of these gpus has 16 entries and is fully-associative (otherwise, the smallest 𝑁 evicting the target address translation should differ from 16 occasionally).\n\nexchanging the above roles played by code and data pages, we can learn that the l1-dtlb of all these gpus also has 16 entries and is fully-associative.\n\nwe find that the l1-itlb and the l1-dtlb are private to each sm in turing gpus (e.g., rtx 2080), but they are shared between the two sms of each tpc in ampere gpus (e.g., rtx 3080 and a100).\n\nthis observation indicates that at least two levels of unified tlbs, which we call l2-utlb and l3-utlb.\n\nl2-utlb is 8-way set-associative in all the tested gpus.\n\nthese observations lead to the conclusion that the l3-utlb in mig-supported gpus is still 8-way set-associative and it is (physically or just logically) split into two slices; and each slice has an 8-entry victim buffer shared by all the tlb sets in the slice.\n\nl3-utlb is shared by all the sms.\n\n\n# 5. big data causing big (tlb) problems: taming random memory accesses on the gpu\n\nif the data accesses are irregular, like hash table accesses or random sampling, the gpu performance can suffer.\nespecially when scaling such accesses beyond 2gb of data, a performance decrease of an order of magnitude is encountered.\nthis is paper analyzes the source of the slowdown through extensive micro-benchmarking, attributing the root cause to the translation lookaside buffer (tlb).\n\n# introduction\n\ngpu data larger than 2gb, which, in some cases, may result in a ≈13.3x runtime decrease.\nwe identified the translation lookaside buffer (tlb) as the source of this slowdown, where tlb misses cost hundreds of cycles per memory access.\n\n * nvidia kepler [15]\n * nvidia pascal [16]\n\nthe p100 shows a significantly better performance than the k80, as it has a newer hardware architecture.\nhowever, the slowdown for memory accesses >2gb is still significant with factors of 4.3x for random sampling and 3.3x for grouping.\n\n# benchmark\n\n# virtual memory\n\nthe reasons why gpu use virtual address\n(1) isolation: the indirection controls a program’s memory accesses and, thus, keeps it from disallowed memory accesses to internal device data or to data of other applications using the same gpu.\n(2) fragmentation: memory fragmentation can be hidden with virtual pages, allowing a large consecutive region of virtual memory to be scattered across many positions in physical memory.\nthis can also increase memory bandwidth if physical memory is scattered to multiple memory chips, which then can be accessed in parallel.\n\n# benchmark\n\npointer chasing with stride distance\n\n\n\nevery stride size smaller than the page size behaves like (1/2)*x: showing lower cycle counts but experiences the first tlb miss at the same position.\n\n# observation\n\n\n\n\n\n# summary\n\n(1) we found three levels of tlbs for the k80 and two levels for the p100.\n(2) for both gpus, the different tlb levels apparently use different page sizes, where the l1 tlb uses a small page size and the l2/l3 tlb use a 16x larger page size.\n(3) compared to k80, the p100 always has 16x larger pages.\n(4) for data larger than 2gb, the k80 has a total delay of 241 cycles, while the p100 only has a 119 cycle delay.\n\n# plausibility and validation\n\n * first, the sizes of the l1 tlb (16 entries) and l2 tlb (65 entries) for kepler gpus (k80).\n * we can confirm this for the l1 tlb, while the k80 already uses 2mb pages for the l2 and l3 tlb (as shown by [10]).\n * third, every tpc has its own l1 tlb and every gpc has its own l2 tlb, while the l3 tlb is shared for all sms.\n * fourth, we can see a significant performance drop in our investigated database operations when we access more data than ≈2gb.\n   even with different page sizes for both gpus, we can pinpoint the problem to the l3 tlb on the k80 and the l2 tlb on the p100.\n   we can even identify the l2 tlb boundary on the k80, where performance problems start at ≈130mb.\n * fifth, in [6], the performance of a grouping operator on kepler gpus was improved by reducing the number of threads to <1000 instead of multiple thousands for data accesses beyond 2gb.\n   with our results, we can explain that this is benefinicial because each thread can load one page translation in the l3 tlb (1032 entries).\n   the page translations stay in the tlb.\n\n# argument for unconventional properties\n\ntwo unconventional results:\n(1) tlb entry numbers not being the power of two\n(2) different page sizes for different tlb levels.\n\nwe evaluated the allocation size and found that the smaller page size is always used for allocations (128kb on k80, 2mb on p100).\none possible explanation for the apparently larger page sizes in the l2/l3 tlb could be a static pre-fetching algorithm, which always loads 16 contiguous pages when a tlb miss occurs.\nthis would result in one tlb miss and 15 tlb hits, when using the small page size as traversal stride.\n\n\n# 6. trans-fw: short circuiting page table walk in multi-gpu systems via remote forwarding\n\n\n\n# address translation flow\n\n 1. the memory requests generated by the same wavefront are first coalesced by the gpu memory coalescing unit.\n 2. the l1 data cache and the l1 tlb perform lookups in parallel in a virtually indexed physically tagged (vipt) tlb-cache design.\n 3. upon l1 tlb misses, the l1 miss status holding register (mshr) is first checked to filter out repetitive requests, and the outstanding requests are forwarded to the l2 tlb for lookup.\n 4. translations that miss in the l2 tlb and l2 mshr are sent to the local pt-walk in the gmmu.\n\nbecause there is limited number of pt-walk threads, l2 tlb misses may not be served immediately.\\\n\n 1. these translation requests will be stored in the pw-queue and wait for available pt-walk threads.\n 2. during the page table walking, the translation is first checked in the pw-cache;\n 3. if it misses the pw-cache, the gpu local page table is accessed, which can be expensive and involves multiple memory accesses ( 5 ).\n 4. if the page walk fails, a far fault is propagated to the gmmu and kept in a structure called gpu fault buffer [6], [7].\n 5. each time a far fault arises, the gmmu sends an alert to the umv-driver.\n 6. upon the receipt of a far fault, the uvm driver fetches the fault information and caches them on the host side.\n 7. the cached page faults are processed in batch granularity (the batch size is 256 [53]).\n 8. per batch, the uvm-driver initiates threads to perform page table walks using the centralized page table, initiates data transfer, and updates the gpu local page tables [7].\n 9. the translation request is replayed after the far fault is resolved.\n\n# hardware handled far faults\n\nthis is the flow of hardware-acclerated far faults handling\n\n 1. when a far fault is generated, it is sent to the host and then handled by host mmu.\n 2. specifically, upon receiving a translation request, the host mmu first performs a host mmu tlb lookup.\n 3. if the translation misses in the tlb, the request waits in the host mmu pw-queue for pt-walk\n 4. the pt-walk process in the host mmu is similar to the gpu local pt-walk, including host mmu pw-cache lookup\n 5. host mmu pt-walk for pw-cache misses\n 6. host mmu tlb update\n\nthe address translation overhead of software is 4.5× higher than that of hardware in 32 gpus.\n\nthree major latencies in the baseline address translation:\n\n 1. waiting time for available pt-walk threads in the pw-queue\n 2. additionally memory accesses after pw-cache misses\n 3. handling far faults caused by page sharing among multiple gpus\n\n----------------------------------------\n\n\n# 9. improving multi-instance gpu efficiency via sub-entry sharing tlb design\n\nspecifically, in the l2 and l3 tlbs, an entry comprises 16 sub-entries, each directly corresponding to the address translation of 16 sequential 64 kb pages within a contiguous 1 mb-aligned segment.\n\nby compressing multiple translations into a single tlb entry, the tlb can manage more data with fewer entries, thereby reducing hardware overhead, while improving tlb efficiency and boosting overall performance.\n\nthese gpus organize their l2 and l3 tlb entries differently to increase tlb reach [60].\nspecifically, each of these entries contains 16 sub-entries, which directly map to the address translations for 16 pages.\nthese pages can be either 64 kb or 2 mb in size, and all of them fall within an aligned range of either 1 mb or 32 mb in size, respectively.\nthat means each sub-entry in a tlb entry has a one-to-one relationship with a single page.\nnote that, in the sub-entry setting, if any tlb entry is evicted, all the 16 sub-entries associated with that tlb entry are zeroed.\n\n\n\n\n# please notice that in the above picture, for each subpage, it has a physical address. thus it is not physically consecutive. physical page address could be randomly located.\n\n\n# 11. mask: redesigning the gpu memory hierarchy to support multi-application concurrency\n\n# main idea\n\ncontention of shard tlb leads to frequent misses in the shared translation lookaside buffer (tlb), where a single miss can induce long-latency stalls for hundreds of threads.\nas a result, the gpu often cannot schedule enough threads to successfully hide the stalls, which diminishes system throughput and becomes a first-order performance concern.\n\n# contributions\n\n(1) a token-based technique to reduce tlb contention\n(2) a bypassing mechanism to improve the effectiveness of cached address translations\n(3) an application-aware memory scheduling scheme to reduce the interference between address translation and data requests.\n\n\n\n",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU Page Table Walk",frontmatter:{title:"GPU Page Table Walk",date:"2024-08-29T00:00:00.000Z",permalink:"/pages/45878/",tags:[null]},regularPath:"/03.gpu/21.gpu_ptw.html",relativePath:"03.gpu/21.gpu_ptw.md",key:"v-4301e3b6",path:"/pages/45878/",headers:[{level:2,title:"1. Observations and Opportunities in Architecting Shared Virtual Memory for Heterogeneous Systems",slug:"_1-observations-and-opportunities-in-architecting-shared-virtual-memory-for-heterogeneous-systems",normalizedTitle:"1. observations and opportunities in architecting shared virtual memory for heterogeneous systems",charIndex:340},{level:3,title:"Contributions",slug:"contributions",normalizedTitle:"contributions",charIndex:442},{level:3,title:"Background",slug:"background",normalizedTitle:"background",charIndex:1084},{level:3,title:"Address Translation",slug:"address-translation",normalizedTitle:"address translation",charIndex:1635},{level:3,title:"Page Fault",slug:"page-fault",normalizedTitle:"page fault",charIndex:3270},{level:3,title:"Important Analysis",slug:"important-analysis",normalizedTitle:"important analysis",charIndex:4441},{level:3,title:"Concluding Remarks",slug:"concluding-remarks",normalizedTitle:"concluding remarks",charIndex:5223},{level:2,title:"2. Sheduling Page Table Walks for Irregular GPU Applications [2018]",slug:"_2-sheduling-page-table-walks-for-irregular-gpu-applications-2018",normalizedTitle:"2. sheduling page table walks for irregular gpu applications [2018]",charIndex:6811},{level:3,title:"Background",slug:"background-2",normalizedTitle:"background",charIndex:1084},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:8286},{level:3,title:"Idea",slug:"idea",normalizedTitle:"idea",charIndex:9419},{level:2,title:"3. Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding",slug:"_3-trans-fw-short-circuiting-page-table-walk-in-multi-gpu-systems-via-remote-forwarding",normalizedTitle:"3. trans-fw: short circuiting page table walk in multi-gpu systems via remote forwarding",charIndex:9474},{level:3,title:"Background",slug:"background-3",normalizedTitle:"background",charIndex:1084},{level:3,title:"Address Translation",slug:"address-translation-2",normalizedTitle:"address translation",charIndex:1635},{level:3,title:"Handle Far Fault",slug:"handle-far-fault",normalizedTitle:"handle far fault",charIndex:11979},{level:3,title:"Simulation Result Analysis",slug:"simulation-result-analysis",normalizedTitle:"simulation result analysis",charIndex:12765}],headersStr:"1. Observations and Opportunities in Architecting Shared Virtual Memory for Heterogeneous Systems Contributions Background Address Translation Page Fault Important Analysis Concluding Remarks 2. Sheduling Page Table Walks for Irregular GPU Applications [2018] Background Introduction Idea 3. Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding Background Address Translation Handle Far Fault Simulation Result Analysis",content:" 1. [117 AMD] Observations and Opportunities in Architecting Shared Virtual Memory for Heterogeneous Systems 👍 👍 👍 👍 👴\n 2. [68 AMD] Sheduling Page Table Walks for Irregular GPU Applications\n 3. [7 HPCA] Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding\n\n----------------------------------------\n\n\n# 1. Observations and Opportunities in Architecting Shared Virtual Memory for Heterogeneous Systems\n\n\n# Contributions\n\n(1) servicing a TLB miss from the GPU can be an order of magnitude slower than that from the CPU and consequently it is imperative to enable many concurrent TLB misses to hide this larger latency;\n(2) divergence in memory accesses impacts the GPU’s address translation more than the rest of the memory hierarchy, and research in designing address translation mechanisms tolerant to this effect is imperative;\n(3) page faults from the GPU are considerably slower than that from the CPU and software-hardware co-design is essential for efficient implementation of page faults from throughput-oriented accelerators like GPUs.\n\n\n# Background\n\nThe IOMMU resides in the processor’s northbridge complex.\nThe IOMMU can access the same x86-64 page table structures used by processes running on the CPU.\nThis enables the accelerator to share the same set of page tables (and thus the same virtual address space) as the processes running on the CPU via the IOMMU.\nA software driver in the OS executing on the CPU manages the IOMMU.\nThe runtime software, in coordination with the OS driver, sets up the IOMMU to enable accelerators access to the same virtual address spaces of the CPU.\n\n\n# Address Translation\n\nThe GPU has its own TLB hierarchy that caches recently used address translations.\nOn a GPU TLB miss, a translation request is sent as an ATS (Address Translation Service [17]) request packet over the PCIe®-based [27] internal interconnect to the IOMMU.\nThis interconnect carries PCIe® packets but latency and bandwidth are not necessarily constrained by PCIe®’s electrical specifications.\nThe IOMMU has its own TLB hierarchy which is checked first;\non a miss there, a hardware page table walker in the IOMMU checks the page table.\nATS requests are tagged with a process address space identifier (PASID) and the IOMMU maintains a table that matches PASIDs to page table base physical addresses.\nOnce the address is successfully translated, the IOMMU sends an ATS response to the GPU.\nThe protocol and packet formats for ATS requests and responses are part of the PCIe® standard specification and are the same across all accelerators.\n\nThe PCIe®’s ATS protocol enables devices (and accelerators) to prefetch translation requests for up to eight contiguous virtual address pages in a single ATS response from the IOMMU.\nBy default, the GPU in our system allows the prefetch value to the maximum setting of eight.\n\nComparison with CPUs: In the CPU, per-core Memory management Units (MMUs) are responsible for address translations.\nIn contrast, the IOMMU services requests from all accelerators.\nUnlike the CPU’s MMU, the IOMMU is not tightly integrated with CPU’s data cache hierarchy.\nThe data caches may contain the most up-to-date translations but the cached copies cannot be directly accessed by accelerators.\n\n\n# Page Fault\n\nIf the IOMMU’s page table walker fails to find the desired translation in the page table, it sends an ATS response to the GPU notifying it of this failure.\nThis in turn corresponds to a page fault.\nIn response, the GPU sends another request to the IOMMU called a Peripheral Page Request (PPR).\nThe IOMMU places this request in a memory-mapped queue and raises an interrupt on the CPU.\nMultiple PPR requests can be queued before the CPU is interrupted.\nThe OS must have a suitable IOMMU driver to process this interrupt and the queued PPR requests.\nIn Linux, while in an interrupt context, the driver pulls PPR requests from the queue and places them in a work-queue for later processing.\nPresumably this design decision was made to minimize the time spent executing in an interrupt context, where lower priority interrupts would be disabled.\nAt a later time, an OS worker-thread calls back into the driver to process page fault requests in the work-queue.\nOnce the requests are serviced, the driver notifies the IOMMU.\nIn turn, the IOMMU notifies the GPU.\nThe GPU then sends another ATS request to retry the translation for the original faulting address.\n\n\n# Important Analysis\n\nWe divide the time to handle a GPU page fault into three major parts:\n(1) “initialization”, the latency for the OS driver to read the fault requests from the PPR queue and pre-process it;\n(2) “processing”, the latency to find a physical page and update the page table;\n(3) “schedule”, the time between initialization and processing of a page fault request. We observe that only a small fraction of the time is spent in actually processing the work to service a page fault.\nThe OS’s scheduling delay introduced by the asynchronous handling of GPU page faults is the primary contributor to the latency. This suggests that page faults from the GPU can be handled more efficiently by modifying the OS driver to handle the faults synchronously whenever possible.\n\n\n# Concluding Remarks\n\nObservations and opportunities:\n\n 1. Latency of servicing a TLB miss is significantly higher on a GPU than on a CPU (~25).\n 2. Increasing the number of concurrent page table walks supported by the hardware is key to supporting diverse heterogeneous applications.\n 3. Half of the programs we studied suffer performance degradation from GPU address translation overheads.\n 4. Larger pages are effective in reducing TLB misses. Heterogeneous software and hardware should enhance support for larger page sizes.\n 5. Divergence in memory accesses impacts address translation overhead more than cache and DRAM latency. Research into divergence-tolerant address translation mechanisms for throughput-oriented accelerators is important.\n 6. Prefetching address translations can degrade performance for programs with poor locality. Applicationdependent translation prefetching is desirable.\n\nObservations and opportunities:\n\n 1. The latency to service a page fault from the GPU can be significantly higher than from the CPU.\n 2. Enhancements into system software to handle page faults synchronously can reduce this latency.\n 3. Software-hardware co-design is needed service a large number of concurrent faults from the GPU/accelerators.\n 4. It is imperative to scale CPU performance and resources to scale the GPU page fault servicing.\n 5. Future heterogeneous applications can reduce their physical memory footprints through the use of on-demand page faults from the GPU, although current applications may need to be re-written.\n\n\n\n----------------------------------------\n\n\n# 2. Sheduling Page Table Walks for Irregular GPU Applications [2018]\n\n * better forward progress is achieved by prioritizing translation requests from the instructions that require less work to service their address translation needs.\n * batching walk requests originating from the same SIMD instruction could reduce unnecessary stalls.\n\n\n# Background\n\nreal hardware demonstrated that such divergent memory accesses can slow down an irregular GPU application by up to 3.7-4× due to address translation overheads alone [5].\nThe study found that the negative impact of divergence could be greater on address translation than on the caches.\n\nDue to the lack of sufficient spatial locality in such irregular applications, these requests often miss in TLBs, each generating a page table walk request.\n\nwe show that the order in which page table walk requests are serviced is also critical.\n\nFirst, the number of page table walks generated due to the execution of a single SIMD memory instruction can vary widely based on how many distinct pages the instruction accesses and the TLB hits/misses it generates. a completely divergent SIMD instruction can generate page table walk requests equal to the number of workitems in the wavefront (here, 64).\nSecond, each page walk may itself need anywhere between one to four memory requests to complete.\nThis happens due to hits/misses in page walk caches (PWCs) that store recently-used upperlevel entries of four-level page tables (d\n\n\n# Introduction\n\n\n\n 1. An address translation request is generated when executing a SIMD memory instruction (load/store).\n 2. A coalescer merges multiple requests to the same page (e.g., 4KB) generated by the same SIMD instruction.\n 3. The coalesced translation request looks up the GPU’s L1 TLB and then the GPU’s shared L2 (if L1 misses).\n 4. On a miss in the GPU’s L2 TLB, the request is sent to the IOMMU.\n 5. Upon arrival at the IOMMU, the request looks up the IOMMU’s TLBs. An IOMMU typically supports multiple independent page table walkers (e.g., 8-16) to concurrently service multiple page table walk requests (TLB misses).\n 6. On a miss, the request queues up as a page walk request in the IOMMU buffer.\n 7. When an IOMMU’s page table walker becomes free, it typically selects a pending request from the IOMMU buffer in FCFS order.\n 8. The page table walker first performs a PWC (page table walk caches) lookup and then completes the walk of the page table, generating one to four memory accesses.\n 9. On finishing a walk, the desired translation is returned to the TLBs and ultimately to the SIMD unit that requested it.\n\n\n# Idea\n\n\n\n\n\n----------------------------------------\n\n\n# 3. Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding\n\n\n\n\n# Background\n\nNote that, each GPU has its own local memory and local page table.\n\nA GPU Memory Management Unit (GMMU) handles local page table walks.\nThe GMMU comprises:\\\n\n * a page walk queue (PW-queue) to buffer the translation requests waiting for available page walk threads\n * a page walk cache (PW-cache) that holds the recent translations to reduce the number of memory accesses of page table walks\n * multi-threaded page table walk (PT-walk) that handles multiple translation requests concurrently.\n\nThe UVM-driver on the CPU side is responsible for coordinating all GPU far faults.\nThe UVM driver manages a centralized page table in the host memory, which holds all valid and up-to-date address translations for all GPUs and in which GPU/CPU the physical addresses are located.\n\n\n# Address Translation\n\n * The memory requests generated by the same wavefront are first coalesced by the GPU memory coalescing unit.\n * L1 data cache and the L1 TLB perform lookups in parallel in a virtually indexed physically tagged (VIPT) TLB-cache design.\n * Upon L1 TLB misses, the L1 Miss Status Holding Register (MSHR) is first checked to filter out repetitive requests, and the outstanding requests are forwarded to the L2 TLB for lookup.\n * Translations that miss in the L2 TLB and L2 MSHR are sent to the local PT-walk in the GMMU.\n\nBecause there is limited number of PT-walk threads, L2 TLB misses may not be served immediately.\nAs a result, these translation requests will be stored in the PW-queue and wait for available PT-walk threads.\\\n\n * During the page table walking, the translation is first checked in the PW-cache\n * if it misses the PW-cache, the GPU local page table is accessed, which can be expensive and involves multiple memory accesses.\n * If the page walk fails, a far fault is propagated to the GMMU and kept in a structure called GPU Fault Buffer [6], [7].\n * Each time a far fault arises, the GMMU sends an alert to the UMV-driver.\n * Upon the receipt of a far fault, the UVM driver fetches the fault information and caches them on the host side.\n * The cached page faults are processed in batch granularity (the batch size is 256 [53]).\n * Per batch, the UVM-driver initiates threads to perform page table walks using the centralized page table, initiates data transfer, and updates the GPU local page tables [7].\n * The translation request is replayed after the far fault is resolved.\n\n\n# Handle Far Fault\n\n# Hardware\n\nthere has been an increasing trend for multi-GPUs to leverage hardware (e.g., host MMU/IOMMU2 [43], [44]) to accelerate the far fault handling.\n\n * The process within each GPU is identical to the driver handled page faults.\n * When a far fault is generated, it is sent to the host and then handled by host MMU.\n * Specifically, upon receiving a translation request, the host MMU first performs a host MMU TLB lookup.\n * If the translation misses in the TLB, the request waits in the host MMU PW-queue for PT-walk.\n * The PT-walk process in the host MMU is similar to the GPU local PT-walk, including\n   * host MMU PW-cache lookup\n   * host MMU PT-walk for PW-cache misses\n   * host MMU TLB update\n\n\n\n# Software\n\nUMV Driver\n\n# Simulator Configuration\n\n\n\n\n# Simulation Result Analysis\n\nFirst, handling local page faults causes significant overhead and accounts for 86.1% of the L2 TLB miss latency.\nThe local page faults latency can be further breakdown into\\\n\n * waiting in the shared host MMU PW-queue\n * missing the host MMU PW-cache\n * migrating page to local memory CPU-GPU interconnection and request replayed when page fault is resolved. Second, 25.0% of the latency is caused by requests waiting for the available page table walk thread in the PW-queue.\n   The PW-queue queuing latency in shared host MMU is generally longer than that in the GMMU for most applications.\n   Specifically, the average queuing latencies are 4.1% and 20.9% for the GMMU PW-queue and the host MMU PW-queue, respectively.\n   This is because the host MMU handles page faults generated from all GPUs, encountering severer contention than local GPUs.\n\nRoom for improvement: We study the performance gains when we i) adopt infinite PW-cache in both GPU and host MMU ii) employ infinite page table walking threads in both GPU and host MMU iii) eliminate all GPU local page faults.",normalizedContent:" 1. [117 amd] observations and opportunities in architecting shared virtual memory for heterogeneous systems 👍 👍 👍 👍 👴\n 2. [68 amd] sheduling page table walks for irregular gpu applications\n 3. [7 hpca] trans-fw: short circuiting page table walk in multi-gpu systems via remote forwarding\n\n----------------------------------------\n\n\n# 1. observations and opportunities in architecting shared virtual memory for heterogeneous systems\n\n\n# contributions\n\n(1) servicing a tlb miss from the gpu can be an order of magnitude slower than that from the cpu and consequently it is imperative to enable many concurrent tlb misses to hide this larger latency;\n(2) divergence in memory accesses impacts the gpu’s address translation more than the rest of the memory hierarchy, and research in designing address translation mechanisms tolerant to this effect is imperative;\n(3) page faults from the gpu are considerably slower than that from the cpu and software-hardware co-design is essential for efficient implementation of page faults from throughput-oriented accelerators like gpus.\n\n\n# background\n\nthe iommu resides in the processor’s northbridge complex.\nthe iommu can access the same x86-64 page table structures used by processes running on the cpu.\nthis enables the accelerator to share the same set of page tables (and thus the same virtual address space) as the processes running on the cpu via the iommu.\na software driver in the os executing on the cpu manages the iommu.\nthe runtime software, in coordination with the os driver, sets up the iommu to enable accelerators access to the same virtual address spaces of the cpu.\n\n\n# address translation\n\nthe gpu has its own tlb hierarchy that caches recently used address translations.\non a gpu tlb miss, a translation request is sent as an ats (address translation service [17]) request packet over the pcie®-based [27] internal interconnect to the iommu.\nthis interconnect carries pcie® packets but latency and bandwidth are not necessarily constrained by pcie®’s electrical specifications.\nthe iommu has its own tlb hierarchy which is checked first;\non a miss there, a hardware page table walker in the iommu checks the page table.\nats requests are tagged with a process address space identifier (pasid) and the iommu maintains a table that matches pasids to page table base physical addresses.\nonce the address is successfully translated, the iommu sends an ats response to the gpu.\nthe protocol and packet formats for ats requests and responses are part of the pcie® standard specification and are the same across all accelerators.\n\nthe pcie®’s ats protocol enables devices (and accelerators) to prefetch translation requests for up to eight contiguous virtual address pages in a single ats response from the iommu.\nby default, the gpu in our system allows the prefetch value to the maximum setting of eight.\n\ncomparison with cpus: in the cpu, per-core memory management units (mmus) are responsible for address translations.\nin contrast, the iommu services requests from all accelerators.\nunlike the cpu’s mmu, the iommu is not tightly integrated with cpu’s data cache hierarchy.\nthe data caches may contain the most up-to-date translations but the cached copies cannot be directly accessed by accelerators.\n\n\n# page fault\n\nif the iommu’s page table walker fails to find the desired translation in the page table, it sends an ats response to the gpu notifying it of this failure.\nthis in turn corresponds to a page fault.\nin response, the gpu sends another request to the iommu called a peripheral page request (ppr).\nthe iommu places this request in a memory-mapped queue and raises an interrupt on the cpu.\nmultiple ppr requests can be queued before the cpu is interrupted.\nthe os must have a suitable iommu driver to process this interrupt and the queued ppr requests.\nin linux, while in an interrupt context, the driver pulls ppr requests from the queue and places them in a work-queue for later processing.\npresumably this design decision was made to minimize the time spent executing in an interrupt context, where lower priority interrupts would be disabled.\nat a later time, an os worker-thread calls back into the driver to process page fault requests in the work-queue.\nonce the requests are serviced, the driver notifies the iommu.\nin turn, the iommu notifies the gpu.\nthe gpu then sends another ats request to retry the translation for the original faulting address.\n\n\n# important analysis\n\nwe divide the time to handle a gpu page fault into three major parts:\n(1) “initialization”, the latency for the os driver to read the fault requests from the ppr queue and pre-process it;\n(2) “processing”, the latency to find a physical page and update the page table;\n(3) “schedule”, the time between initialization and processing of a page fault request. we observe that only a small fraction of the time is spent in actually processing the work to service a page fault.\nthe os’s scheduling delay introduced by the asynchronous handling of gpu page faults is the primary contributor to the latency. this suggests that page faults from the gpu can be handled more efficiently by modifying the os driver to handle the faults synchronously whenever possible.\n\n\n# concluding remarks\n\nobservations and opportunities:\n\n 1. latency of servicing a tlb miss is significantly higher on a gpu than on a cpu (~25).\n 2. increasing the number of concurrent page table walks supported by the hardware is key to supporting diverse heterogeneous applications.\n 3. half of the programs we studied suffer performance degradation from gpu address translation overheads.\n 4. larger pages are effective in reducing tlb misses. heterogeneous software and hardware should enhance support for larger page sizes.\n 5. divergence in memory accesses impacts address translation overhead more than cache and dram latency. research into divergence-tolerant address translation mechanisms for throughput-oriented accelerators is important.\n 6. prefetching address translations can degrade performance for programs with poor locality. applicationdependent translation prefetching is desirable.\n\nobservations and opportunities:\n\n 1. the latency to service a page fault from the gpu can be significantly higher than from the cpu.\n 2. enhancements into system software to handle page faults synchronously can reduce this latency.\n 3. software-hardware co-design is needed service a large number of concurrent faults from the gpu/accelerators.\n 4. it is imperative to scale cpu performance and resources to scale the gpu page fault servicing.\n 5. future heterogeneous applications can reduce their physical memory footprints through the use of on-demand page faults from the gpu, although current applications may need to be re-written.\n\n\n\n----------------------------------------\n\n\n# 2. sheduling page table walks for irregular gpu applications [2018]\n\n * better forward progress is achieved by prioritizing translation requests from the instructions that require less work to service their address translation needs.\n * batching walk requests originating from the same simd instruction could reduce unnecessary stalls.\n\n\n# background\n\nreal hardware demonstrated that such divergent memory accesses can slow down an irregular gpu application by up to 3.7-4× due to address translation overheads alone [5].\nthe study found that the negative impact of divergence could be greater on address translation than on the caches.\n\ndue to the lack of sufficient spatial locality in such irregular applications, these requests often miss in tlbs, each generating a page table walk request.\n\nwe show that the order in which page table walk requests are serviced is also critical.\n\nfirst, the number of page table walks generated due to the execution of a single simd memory instruction can vary widely based on how many distinct pages the instruction accesses and the tlb hits/misses it generates. a completely divergent simd instruction can generate page table walk requests equal to the number of workitems in the wavefront (here, 64).\nsecond, each page walk may itself need anywhere between one to four memory requests to complete.\nthis happens due to hits/misses in page walk caches (pwcs) that store recently-used upperlevel entries of four-level page tables (d\n\n\n# introduction\n\n\n\n 1. an address translation request is generated when executing a simd memory instruction (load/store).\n 2. a coalescer merges multiple requests to the same page (e.g., 4kb) generated by the same simd instruction.\n 3. the coalesced translation request looks up the gpu’s l1 tlb and then the gpu’s shared l2 (if l1 misses).\n 4. on a miss in the gpu’s l2 tlb, the request is sent to the iommu.\n 5. upon arrival at the iommu, the request looks up the iommu’s tlbs. an iommu typically supports multiple independent page table walkers (e.g., 8-16) to concurrently service multiple page table walk requests (tlb misses).\n 6. on a miss, the request queues up as a page walk request in the iommu buffer.\n 7. when an iommu’s page table walker becomes free, it typically selects a pending request from the iommu buffer in fcfs order.\n 8. the page table walker first performs a pwc (page table walk caches) lookup and then completes the walk of the page table, generating one to four memory accesses.\n 9. on finishing a walk, the desired translation is returned to the tlbs and ultimately to the simd unit that requested it.\n\n\n# idea\n\n\n\n\n\n----------------------------------------\n\n\n# 3. trans-fw: short circuiting page table walk in multi-gpu systems via remote forwarding\n\n\n\n\n# background\n\nnote that, each gpu has its own local memory and local page table.\n\na gpu memory management unit (gmmu) handles local page table walks.\nthe gmmu comprises:\\\n\n * a page walk queue (pw-queue) to buffer the translation requests waiting for available page walk threads\n * a page walk cache (pw-cache) that holds the recent translations to reduce the number of memory accesses of page table walks\n * multi-threaded page table walk (pt-walk) that handles multiple translation requests concurrently.\n\nthe uvm-driver on the cpu side is responsible for coordinating all gpu far faults.\nthe uvm driver manages a centralized page table in the host memory, which holds all valid and up-to-date address translations for all gpus and in which gpu/cpu the physical addresses are located.\n\n\n# address translation\n\n * the memory requests generated by the same wavefront are first coalesced by the gpu memory coalescing unit.\n * l1 data cache and the l1 tlb perform lookups in parallel in a virtually indexed physically tagged (vipt) tlb-cache design.\n * upon l1 tlb misses, the l1 miss status holding register (mshr) is first checked to filter out repetitive requests, and the outstanding requests are forwarded to the l2 tlb for lookup.\n * translations that miss in the l2 tlb and l2 mshr are sent to the local pt-walk in the gmmu.\n\nbecause there is limited number of pt-walk threads, l2 tlb misses may not be served immediately.\nas a result, these translation requests will be stored in the pw-queue and wait for available pt-walk threads.\\\n\n * during the page table walking, the translation is first checked in the pw-cache\n * if it misses the pw-cache, the gpu local page table is accessed, which can be expensive and involves multiple memory accesses.\n * if the page walk fails, a far fault is propagated to the gmmu and kept in a structure called gpu fault buffer [6], [7].\n * each time a far fault arises, the gmmu sends an alert to the umv-driver.\n * upon the receipt of a far fault, the uvm driver fetches the fault information and caches them on the host side.\n * the cached page faults are processed in batch granularity (the batch size is 256 [53]).\n * per batch, the uvm-driver initiates threads to perform page table walks using the centralized page table, initiates data transfer, and updates the gpu local page tables [7].\n * the translation request is replayed after the far fault is resolved.\n\n\n# handle far fault\n\n# hardware\n\nthere has been an increasing trend for multi-gpus to leverage hardware (e.g., host mmu/iommu2 [43], [44]) to accelerate the far fault handling.\n\n * the process within each gpu is identical to the driver handled page faults.\n * when a far fault is generated, it is sent to the host and then handled by host mmu.\n * specifically, upon receiving a translation request, the host mmu first performs a host mmu tlb lookup.\n * if the translation misses in the tlb, the request waits in the host mmu pw-queue for pt-walk.\n * the pt-walk process in the host mmu is similar to the gpu local pt-walk, including\n   * host mmu pw-cache lookup\n   * host mmu pt-walk for pw-cache misses\n   * host mmu tlb update\n\n\n\n# software\n\numv driver\n\n# simulator configuration\n\n\n\n\n# simulation result analysis\n\nfirst, handling local page faults causes significant overhead and accounts for 86.1% of the l2 tlb miss latency.\nthe local page faults latency can be further breakdown into\\\n\n * waiting in the shared host mmu pw-queue\n * missing the host mmu pw-cache\n * migrating page to local memory cpu-gpu interconnection and request replayed when page fault is resolved. second, 25.0% of the latency is caused by requests waiting for the available page table walk thread in the pw-queue.\n   the pw-queue queuing latency in shared host mmu is generally longer than that in the gmmu for most applications.\n   specifically, the average queuing latencies are 4.1% and 20.9% for the gmmu pw-queue and the host mmu pw-queue, respectively.\n   this is because the host mmu handles page faults generated from all gpus, encountering severer contention than local gpus.\n\nroom for improvement: we study the performance gains when we i) adopt infinite pw-cache in both gpu and host mmu ii) employ infinite page table walking threads in both gpu and host mmu iii) eliminate all gpu local page faults.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU Cache's Papers",frontmatter:{title:"GPU Cache's Papers",date:"2024-09-06T00:00:00.000Z",permalink:"/pages/45879/",tags:[null]},regularPath:"/03.gpu/22.gpu_cache_paper.html",relativePath:"03.gpu/22.gpu_cache_paper.md",key:"v-dcff77f6",path:"/pages/45879/",headers:[{level:2,title:"1. Adaptive Memory-Side Last-Level GPU Chacing",slug:"_1-adaptive-memory-side-last-level-gpu-chacing",normalizedTitle:"1. adaptive memory-side last-level gpu chacing",charIndex:249},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:300},{level:2,title:"2. Understanding the Tradeoffs between Software-Managed vs. Hardware-Managed Caches in GPUs",slug:"_2-understanding-the-tradeoffs-between-software-managed-vs-hardware-managed-caches-in-gpus",normalizedTitle:"2. understanding the tradeoffs between software-managed vs. hardware-managed caches in gpus",charIndex:2620},{level:3,title:"Main Idea",slug:"main-idea-2",normalizedTitle:"main idea",charIndex:2397},{level:3,title:"Study",slug:"study",normalizedTitle:"study",charIndex:3799},{level:2,title:"3. Locality-Driven Dynamic GPU Cache Bypassing",slug:"_3-locality-driven-dynamic-gpu-cache-bypassing",normalizedTitle:"3. locality-driven dynamic gpu cache bypassing",charIndex:4879},{level:3,title:"Contribution",slug:"contribution",normalizedTitle:"contribution",charIndex:4930},{level:3,title:"Introduction",slug:"introduction-2",normalizedTitle:"introduction",charIndex:300},{level:3,title:"Two paths: cache and bypass",slug:"two-paths-cache-and-bypass",normalizedTitle:"two paths: cache and bypass",charIndex:5498},{level:3,title:"Categorize application according to friendly to cache",slug:"categorize-application-according-to-friendly-to-cache",normalizedTitle:"categorize application according to friendly to cache",charIndex:6154}],headersStr:"1. Adaptive Memory-Side Last-Level GPU Chacing Introduction 2. Understanding the Tradeoffs between Software-Managed vs. Hardware-Managed Caches in GPUs Main Idea Study 3. Locality-Driven Dynamic GPU Cache Bypassing Contribution Introduction Two paths: cache and bypass Categorize application according to friendly to cache",content:" 1. [22] Adaptive Memory-Side Last-Level GPU Chacing\n 2. [42] Understaning the Tradeoffs between Software-Managed vs. Hardware-Managed Caches in GPUs\n 3. [88] Locality-Driven Dynamic GPU Cache Bypassing\n\n----------------------------------------\n\n\n# 1. Adaptive Memory-Side Last-Level GPU Chacing\n\n\n# Introduction\n\nGPUs typically feature a two-level on-chip cache hierarchy in which the first-level caches are private to each SM while the last-level cache (LLC) is a shared memory-side cache that is partitioned into equally-sized slices and accessed via the NoC.\n\nIn fact, we find that GPU workloads have large read-only shared data footprints. For such sharing-intensive workloads, multiple SMs experience a bandwidth bottleneck when they serialize on accesses to the same shared cache line.\n\nWhile this is a practical solution for a limited number of cores in a CPU, it does not scale to a large number of SMs due to limitations in scaling GPU die size.\n\nShared LLCs incur a performance bottleneck for workloads that frequently access data shared by multiple SMs.\n\nA shared memory-side LLC consists of multiple slices each caching a specific memory partition, i.e., a specific address range of the entire memory space is served by a particular memory controller.\n\nAs a result, a shared cache line appears in a single LLC slice, which leads to a severe performance bottleneck if multiple SMs concurrently access the same shared data.\n\nWe find that GPU applications with high degrees of read-only data sharing significantly benefit from a private LLC organization.\n\nTo that end, this paper proposes adaptive memory-side caching to dynamically choose between a shared or private memory-side LLC.\n\nThese observations suggest an opportunity to improve performance by dynamically adapting a memory-side LLC to the needs of an application’s sharing behavior.\n\nadaptive memory-side caching to dynamically choose between a shared or private memory-side LLC.\nSelecting a shared versus private LLC is done using a lightweight performance model.\n\nBy default, the GPU assumes a shared LLC.\n\nProfiling information is periodically collected to predict LLC miss rate and bandwidth under a private LLC organization while executing under a shared LLC. If deemed beneficial, the LLC is adapted to a private cache.\n\nThe LLC reverts back to a shared organization periodically and when a new kernel gets launched.\n\n# Main Idea\n\nPrivate LLC could have replicate data, but waste memory space.\n\nShared LLC could access shared data a lot, introducing bottleneck.\n\nSwitch between those two modes.\n\n\n\n----------------------------------------\n\n\n# 2. Understanding the Tradeoffs between Software-Managed vs. Hardware-Managed Caches in GPUs\n\nOn one hand, the kernels utilizing the L1 caches may support higher degrees of thread-level parallelism, offer more opportunities for data to be allocated in registers, and sometimes result in lower dynamic instruction counts.\n\nOn the other hand, the applications utilizing shared memory enable more coalesced accesses and tend to achieve higher degrees of memory-level parallelism.\n\n\n# Main Idea\n\nEven if a matrix totally fits into cache, the L1 D-cache version is surprisingly much slower (43.8%) than the shared-memory version.\n\nOur results show that for most applications, the GPU kernels utilizing shared memory deliver significantly higher performance than those leveraging L1 D-caches.\nThe fundamental reasons are MLP and coalescing.\n\nFor a few benchmarks for which the L1 D-cache versions have higher performance, the performance impact is mainly due to improved thread-level parallelism (TLP) and allocating more data to registers.\n\nOverall, rather than cache hit rates, the subtle factors including MLP, coalescing, and TLP often have more profound performance impacts.\n\n\n\n\n# Study\n\nthe register usage is the limiting factor on how many TBs can run concurrently on an SM.\n\n\n\n(a) Why is the D-cache version slightly slower than the shared-memory version even with a perfect cache? Since the array access ‘A[a+WA*ty+k]’ of the D-cache version cannot be coalesced into a single cache access, it suffers from additional pipeline stalls even though all accesses hit in cache.\n\n(b) With a realistic cache, why is the D-cache version much slower? We found that performance is not determined by the total number of cache misses.\nInstead it depends more on how these cache-misses overlap with each other, i.e., the degrees of memory-level parallelism (MLP).\n\nIn cache-version, the cache-misses overlap is bad, as seen in the figure.\n\n> In other words, in cache version, each iteration it has a cache miss. In shared-memory version, in the first two load-to-sharememory, there is large MLP. Cache-version is like a man eat food every day. Sharedmemory-version is like a man only eat food in first ten years of his life.\n\n----------------------------------------\n\n\n# 3. Locality-Driven Dynamic GPU Cache Bypassing\n\n\n# Contribution\n\nFor area and energy efficiency, we propose to decouple the tag and data stores of the existing L1 D-cache and integrate the locality filtering capability into the tag store through simple and cost-effective hardware extensions.\n\n\n# Introduction\n\nMore importantly, large number of the incoming memory requests with no or low reuse may evict cache lines with high reuse,resulting in cache pollution. In this case, even advanced cache replacement policies (e.g. RRIP [14] and SHiP [29]) are ineffective to address such contention problems on GPUs [24].\n\n\n# Two paths: cache and bypass\n\n\n\n 1. For cacheable accesses, the first path, which sends the memory requests into L1 D-cache and is labeled as ‘L1 D-path’, is used.\n    Considering the small number of cache lines and MSHR entries, these resources can be quickly occupied if all memory requests are diverted into L1 D-Path.\n 2. The second path is for un-cacheable accesses, such as global memory accesses in NVIDIA’s Kepler architecture (not cached in Kepler’s L1).\n    It diverts the memory requests to bypass the L1 D-cache (labeled as ‘Bypass Path’ in Figure 1) and directly sends requests through an interconnect into the next level memory hierarchy.\n\n\n# Categorize application according to friendly to cache\n\n\n\nNew tag store entry now contains a Reference Count (RC) field and a Position field.\nThe RC field (6-bit) holds the reference frequency (reuse) accumulated for the address.\nThe Position field (2-bit) connects a tag store entry with a data line in the data store using a pointer to record the data line’s position.\n\nAs shown in below figure, if RC is over thresholds, install the block into cache. Or else, just bypass.",normalizedContent:" 1. [22] adaptive memory-side last-level gpu chacing\n 2. [42] understaning the tradeoffs between software-managed vs. hardware-managed caches in gpus\n 3. [88] locality-driven dynamic gpu cache bypassing\n\n----------------------------------------\n\n\n# 1. adaptive memory-side last-level gpu chacing\n\n\n# introduction\n\ngpus typically feature a two-level on-chip cache hierarchy in which the first-level caches are private to each sm while the last-level cache (llc) is a shared memory-side cache that is partitioned into equally-sized slices and accessed via the noc.\n\nin fact, we find that gpu workloads have large read-only shared data footprints. for such sharing-intensive workloads, multiple sms experience a bandwidth bottleneck when they serialize on accesses to the same shared cache line.\n\nwhile this is a practical solution for a limited number of cores in a cpu, it does not scale to a large number of sms due to limitations in scaling gpu die size.\n\nshared llcs incur a performance bottleneck for workloads that frequently access data shared by multiple sms.\n\na shared memory-side llc consists of multiple slices each caching a specific memory partition, i.e., a specific address range of the entire memory space is served by a particular memory controller.\n\nas a result, a shared cache line appears in a single llc slice, which leads to a severe performance bottleneck if multiple sms concurrently access the same shared data.\n\nwe find that gpu applications with high degrees of read-only data sharing significantly benefit from a private llc organization.\n\nto that end, this paper proposes adaptive memory-side caching to dynamically choose between a shared or private memory-side llc.\n\nthese observations suggest an opportunity to improve performance by dynamically adapting a memory-side llc to the needs of an application’s sharing behavior.\n\nadaptive memory-side caching to dynamically choose between a shared or private memory-side llc.\nselecting a shared versus private llc is done using a lightweight performance model.\n\nby default, the gpu assumes a shared llc.\n\nprofiling information is periodically collected to predict llc miss rate and bandwidth under a private llc organization while executing under a shared llc. if deemed beneficial, the llc is adapted to a private cache.\n\nthe llc reverts back to a shared organization periodically and when a new kernel gets launched.\n\n# main idea\n\nprivate llc could have replicate data, but waste memory space.\n\nshared llc could access shared data a lot, introducing bottleneck.\n\nswitch between those two modes.\n\n\n\n----------------------------------------\n\n\n# 2. understanding the tradeoffs between software-managed vs. hardware-managed caches in gpus\n\non one hand, the kernels utilizing the l1 caches may support higher degrees of thread-level parallelism, offer more opportunities for data to be allocated in registers, and sometimes result in lower dynamic instruction counts.\n\non the other hand, the applications utilizing shared memory enable more coalesced accesses and tend to achieve higher degrees of memory-level parallelism.\n\n\n# main idea\n\neven if a matrix totally fits into cache, the l1 d-cache version is surprisingly much slower (43.8%) than the shared-memory version.\n\nour results show that for most applications, the gpu kernels utilizing shared memory deliver significantly higher performance than those leveraging l1 d-caches.\nthe fundamental reasons are mlp and coalescing.\n\nfor a few benchmarks for which the l1 d-cache versions have higher performance, the performance impact is mainly due to improved thread-level parallelism (tlp) and allocating more data to registers.\n\noverall, rather than cache hit rates, the subtle factors including mlp, coalescing, and tlp often have more profound performance impacts.\n\n\n\n\n# study\n\nthe register usage is the limiting factor on how many tbs can run concurrently on an sm.\n\n\n\n(a) why is the d-cache version slightly slower than the shared-memory version even with a perfect cache? since the array access ‘a[a+wa*ty+k]’ of the d-cache version cannot be coalesced into a single cache access, it suffers from additional pipeline stalls even though all accesses hit in cache.\n\n(b) with a realistic cache, why is the d-cache version much slower? we found that performance is not determined by the total number of cache misses.\ninstead it depends more on how these cache-misses overlap with each other, i.e., the degrees of memory-level parallelism (mlp).\n\nin cache-version, the cache-misses overlap is bad, as seen in the figure.\n\n> in other words, in cache version, each iteration it has a cache miss. in shared-memory version, in the first two load-to-sharememory, there is large mlp. cache-version is like a man eat food every day. sharedmemory-version is like a man only eat food in first ten years of his life.\n\n----------------------------------------\n\n\n# 3. locality-driven dynamic gpu cache bypassing\n\n\n# contribution\n\nfor area and energy efficiency, we propose to decouple the tag and data stores of the existing l1 d-cache and integrate the locality filtering capability into the tag store through simple and cost-effective hardware extensions.\n\n\n# introduction\n\nmore importantly, large number of the incoming memory requests with no or low reuse may evict cache lines with high reuse,resulting in cache pollution. in this case, even advanced cache replacement policies (e.g. rrip [14] and ship [29]) are ineffective to address such contention problems on gpus [24].\n\n\n# two paths: cache and bypass\n\n\n\n 1. for cacheable accesses, the first path, which sends the memory requests into l1 d-cache and is labeled as ‘l1 d-path’, is used.\n    considering the small number of cache lines and mshr entries, these resources can be quickly occupied if all memory requests are diverted into l1 d-path.\n 2. the second path is for un-cacheable accesses, such as global memory accesses in nvidia’s kepler architecture (not cached in kepler’s l1).\n    it diverts the memory requests to bypass the l1 d-cache (labeled as ‘bypass path’ in figure 1) and directly sends requests through an interconnect into the next level memory hierarchy.\n\n\n# categorize application according to friendly to cache\n\n\n\nnew tag store entry now contains a reference count (rc) field and a position field.\nthe rc field (6-bit) holds the reference frequency (reuse) accumulated for the address.\nthe position field (2-bit) connects a tag store entry with a data line in the data store using a pointer to record the data line’s position.\n\nas shown in below figure, if rc is over thresholds, install the block into cache. or else, just bypass.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU Unified Memory Innovations",frontmatter:{title:"GPU Unified Memory Innovations",date:"2024-10-29T00:00:00.000Z",permalink:"/pages/45882/",tags:[null]},regularPath:"/03.gpu/24.gpu_novel_um.html",relativePath:"03.gpu/24.gpu_novel_um.md",key:"v-3690e719",path:"/pages/45882/",headers:[{level:2,title:"1.[2023] Evaluating Unified Memory Performance in HIP",slug:"_1-2023-evaluating-unified-memory-performance-in-hip",normalizedTitle:"1.[2023] evaluating unified memory performance in hip",charIndex:693},{level:3,title:"Result Analysis",slug:"result-analysis",normalizedTitle:"result analysis",charIndex:1971},{level:2,title:"2.[97] Unlocking Bandwidth for GPUs in CC-NUMA Systems",slug:"_2-97-unlocking-bandwidth-for-gpus-in-cc-numa-systems",normalizedTitle:"2.[97] unlocking bandwidth for gpus in cc-numa systems",charIndex:2825},{level:3,title:"Main Idea in Short",slug:"main-idea-in-short",normalizedTitle:"main idea in short",charIndex:2903},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:3359},{level:3,title:"Contribution",slug:"contribution",normalizedTitle:"contribution",charIndex:3378},{level:3,title:"Interesting Experiment",slug:"interesting-experiment",normalizedTitle:"interesting experiment",charIndex:4139},{level:3,title:"Interesting Finding",slug:"interesting-finding",normalizedTitle:"interesting finding",charIndex:4496},{level:3,title:"TLB",slug:"tlb",normalizedTitle:"tlb",charIndex:3106},{level:3,title:"3. [ASPLOS] DeepUM: Tensor Migration and Prefetching in Unified Memory",slug:"_3-asplos-deepum-tensor-migration-and-prefetching-in-unified-memory",normalizedTitle:"3. [asplos] deepum: tensor migration and prefetching in unified memory",charIndex:114},{level:3,title:"4. [ASPLOS] Capuchin: Tensor-based GPU Memory Management for Deep Learning",slug:"_4-asplos-capuchin-tensor-based-gpu-memory-management-for-deep-learning",normalizedTitle:"4. [asplos] capuchin: tensor-based gpu memory management for deep learning",charIndex:186},{level:3,title:"Framework",slug:"framework",normalizedTitle:"framework",charIndex:10268},{level:3,title:"5. PUMP Profiling-free Unified Memory Prefetcher for Large DNN Model Support",slug:"_5-pump-profiling-free-unified-memory-prefetcher-for-large-dnn-model-support",normalizedTitle:"5. pump profiling-free unified memory prefetcher for large dnn model support",charIndex:262},{level:3,title:"6. [MICRO] G10 Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations",slug:"_6-micro-g10-enabling-an-efficient-unified-gpu-memory-and-storage-architecture-with-smart-tensor-migrations",normalizedTitle:"6. [micro] g10 enabling an efficient unified gpu memory and storage architecture with smart tensor migrations",charIndex:340}],headersStr:"1.[2023] Evaluating Unified Memory Performance in HIP Result Analysis 2.[97] Unlocking Bandwidth for GPUs in CC-NUMA Systems Main Idea in Short Introduction Contribution Interesting Experiment Interesting Finding TLB 3. [ASPLOS] DeepUM: Tensor Migration and Prefetching in Unified Memory 4. [ASPLOS] Capuchin: Tensor-based GPU Memory Management for Deep Learning Framework 5. PUMP Profiling-free Unified Memory Prefetcher for Large DNN Model Support 6. [MICRO] G10 Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations",content:" 1. [2023] Evaluating Unified Memory Performance in HIP\n 2. [97] Unlocking Bandwidth for GPUs in CC-NUMA Systems\n 3. [ASPLOS] DeepUM: Tensor Migration and Prefetching in Unified Memory\n 4. [ASPLOS] Capuchin: Tensor-based GPU Memory Management for Deep Learning\n 5. PUMP Profiling-free Unified Memory Prefetcher for Large DNN Model Support\n 6. [MICRO] G10 Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations\n\n----------------------------------------\n\nNot Read\n\n 1. MegTaiChi: Dynamic Tensor-based Memory Management Optimization for DNN Training\n 2. [14] Efficient GPU Memory Management for Nonlinear DNNs\n\n----------------------------------------\n\n\n# 1.[2023] Evaluating Unified Memory Performance in HIP\n\nUM only works on recent AMD GPUs, including Vega10 and MI100.\nThere are two flavors of the support: XNACK-enabled and XNACK-disabled.\\\n\n * In the XNACK-enabled mode,a GPU can handle retry of a memory access after page-faults, which enables mapping and migrating data on demand, as well as memory overcommitment.\n * In the XNACK-disabled mode, all memory must be resident and mapped in GPU page tables when the GPU is executing application code.\n   The XNACK-enabled mode only has experimental support.\n\nThe experimental results show that the performance of the applications using UM is closely related to data transfer size and memory accesses of a kernel. Compared to “UM”, prefetching memory as a memory usage hint leads to significant data transfers between the host and device.\n\nCompared to “UM”, prefetching memory as a memory usage hint leads to significant data transfers between the host and device.\n\n * “UM-hint” and “UM” indicate unified memory with and without memory usage hints, respectively.\n * “ZeroCopy” uses zero-copy buffers for data migration.\n * “PageableCopy” copies data from pageable host memory to device memory\n * “PageLockedCopy” transfers data from page-locked host memory to device memory\n\n\n\n\n# Result Analysis\n\nThe result shows that the stall rate is highly sensitive to the increase of memory size in UM.\n\n\n\nThe decrease of the kernel execution time ranges from approximately 1.1X to 2.8X with respect to the vector length for the three optimization techniques.、 However, the execution time is still approximately 1.4X to 74.8X longer than that of the kernel that takes the copy-then-execute approach.\n\nIn [28], the authors present 32 open-source UM benchmarks in CUDA and evaluate their performance on an NVIDIA Pascal GPU.\nThey find that across the benchmarks the performance of the UM benchmarks is on average 34.2% slower compared with the benchmarks without UM due to the cost of page fault handling\n\n> [28] UVMBench: A Comprehensive Benchmark Suite for Researching Unified Virtual Memory in GPU\n\n----------------------------------------\n\n\n# 2.[97] Unlocking Bandwidth for GPUs in CC-NUMA Systems\n\nNvidia with umich\n\n\n# Main Idea in Short\n\n * Mainly focus on how many pages that covers the page-fault pages should be migrated.\n * Prefetching with upgraded range, which balance the prefetching and also reduce the number of TLB shootdowns\n * TLB shootdown is estimated at 100 cycles\n * Memory Oversubscription and Eviction is not considered.\n * Page Migration Threshold accustomed to each workload is complex. And not worth it. It is better to just migrate on first touch.\n\n\n# Introduction\n\n\n\n\n# Contribution\n\n * Counter-based metrics to determine when to migrate pages from the CPU to GPU are insufficient for finding an optimal migration policy to exploit GPU memory bandwidth.\n   In streaming workloads, where each page may be accessed only a few times, waiting for N accesses to occur before migrating a page will actually limit the number of accesses that occur after migration, reducing the efficacy of the page migration operation.\n\n 2. TLB shootdown and refill overhead can significantly degrade the performance of any page migration policy for GPUs.\n    We show that combining reactive migration with virtual address locality information to aggressively prefetch pages can mitigate much of this overhead, resulting in increased GPU throughput.\n\n\n# Interesting Experiment\n\nPerformance comparson of DDR and GDDR Experiments\n\n\n\nThis choice is motivated by our observation that the performance of some GPU compute workloads would degrade by as much as 66% if the traditional GDDR memory on a GPU were replaced with standard DDR memory, as seen in Figure 2.\n\n\n\nStill confused about the following Figure.\n\n\n\n\n# Interesting Finding\n\n# Clustered Page\n\nPage Accessing is clusted by memory arranges.\nPart of continuous virtual address is hot.\nThis clustering is key to range expansion because it suggests that if a page is identified for migration, then other neighboring pages in the virtual address space are likely to have a similar number of total touches.\n\n# Threshold to trigger page migration\n\n\n\na first touch policy (threshold-1) requires no tracking information and can be trivially implemented by migrating a page the first time the GPU translates an address for the page.\n\nConsidering the performance differential seen across thresholds, we believe the overhead of implementing the necessary hardware counters to track all pages within a system to differentiate their access counts is not worth the improvement over a vastly simpler first-touch migration policy.\n\n\n# TLB\n\nThe runtime system also must be cognizant that performing TLB invalidations (an integral part of page migration) on a GPU does not just halt a single processor, but thousands of compute pipelines that may be accessing these pages through a large shared TLB structure.\nThis shared TLB structure makes page migrations between a CPU and GPU potentially much more costly (in terms of the opportunity cost of lost execution throughput) than in CPU-only systems.\n\nRecent papers have provided proposals about how to efficiently implement general purpose TLBs that are, or could be, optimized for a GPU’s needs [28]–[30].\nOthers have recently looked at improving TLB reach by exploiting locality within the virtual to physical memory remapping, or avoiding this layer completely [31]–[33].\nFinally, Gerofi et al. [34] recently examined TLB performance of the Xeon Phi for applications with large footprints, while McCurdy et al. [35] investigated the effect of superpages and TLB coverage for HPC applications in the context of CPUs.\n\n\n\n----------------------------------------\n\n\n# 3. [ASPLOS] DeepUM: Tensor Migration and Prefetching in Unified Memory\n\n\n\nDeepUM automatically prefetches data using correlation prefetching.\n\nTwo correlation table records the history of kenrel execution and page access patterns during training of prefetching.\n\nDeepUM’s correlation tables record the history of the kernel executions and their page accesses during the training phase of a DNN.\nIt prefetches pages based on the information in the correlation tables by predicting which kernel will execute next.\nWhile traditional correlation prefetching uses a single table to store history and records the relationship between the CPU cache lines, DeepUM correlation prefetching uses two different table structures.\n\nAssign each kernel with a kernel ID and maitian kernel ID with its page access history.\n\n----------------------------------------\n\n\n# 4. [ASPLOS] Capuchin: Tensor-based GPU Memory Management for Deep Learning\n\n👍 👍 👍 👍 👍\n\nThe key feature of Capuchin is that it makes memory management decisions based on dynamic tensor access pattern tracked at runtime.\nThis design is motivated by the observation that the access pattern to tensors is regular during training iterations.\nBased on the identified patterns, one can exploit the total memory optimization space and offer the fine-grain and flexible control of when and how to perform memory optimization techniques.\n\n# Introduction\n\nBERT 768 hiddenlayers 73GB memory int training.\nV100 32GB on-board memory and P100 16GB on-board memory.\n\nFeature maps are produced in the forward propagation and used again in the backward propagation.\nMajor deep learning frameworks such as Tensorflow , MXNet and Pytorch usually maintain these feature maps in GPU memory until they are no longer needed in backward propagation computation.\nHowever, there is usually a large gap between two accesses to the same feature map in forward and backward propagation, which incurs high memory consumption to store the intermediate results.\n\nTo reduce memory consumption:\n\n * swapping\n * recomputting\n\nKey observaions:\n\n * we believe that dynamically tracking fine-grained tensor accesses is a fundamental and general technique that enables effective memory management optimizations.\n   This paper demonstrates that this essential idea can be implemented efficiently on top of major deep learning frameworks.\n * The training process is composed of millions of iterations with clear boundaries, and the tensor accesses have regular and repeated access patterns across iterations.\n   This means that analyzing the timing and tensor access patterns can easily reveal the memory optimization opportunities with concrete guidance\n\n# Background\n\nForward propagation\n\ninput feature maps, current layer’s weights and bias produce the output feature maps which become the next layer’s input data.\nThe forward propagation concludes with the calculation of loss by comparing the output with ground truth label at the output layer.\n\nBackward propagation\n\nThe backward propagation starts from the output layer and reversely traverses layers to optimize the weights and bias.\n\nMemory Usage\n\n * feature maps:output in the forward propagation\n * gradient maps:output in the backward propagtaion\n * convolution workspace: extra memory space needed by convolution algorithm.\n\nModel weights consume very small amount of memory and are usually persistent in GPU memory to be continuously updated.\n\nthe latter two are temporary memory usage which can be released immediately after current computations are finished.\n\nThe feature maps are needed in both forward and backward propagation. However, there exists a large time gap between the two usage points for computations in forward and backward phase.\n\nDeap learning framework execution modes\n\n * eager mode: dynamic graph\n * graph mode: static graph\n\n\n# Framework\n\n\n\n\n\nThe idea is clear, simple and classic.\n\nProfile based on tensor reuse pattern.\n\nBased on history information choose early prefetch or recompute.\n\n----------------------------------------\n\n\n# 5. PUMP Profiling-free Unified Memory Prefetcher for Large DNN Model Support\n\nYear: 2022\n\n👍 👍 👍 👍 👍\n\nThis paper is published with source code. Real Stuff!\n\nPUMP exploits GPU asynchronous execution for prefetch; that is, there exists a delay between the time that CPU launches a kernel and the time the kernel executes in GPU.\n\nPUMP extracts memory blocks accessed by the kernel when launching and swaps these blocks into GPU memory.\n\n\n\nThe idea behind this paper is also simple and classic.\n\nBefore launch the kernel, launch the prefetch cudaEvent.\n\nThey exploit dynamic linker of linux, with LD_PRELOAD, they could call their version of cuda wrapper.\n\n----------------------------------------\n\n\n# 6. [MICRO] G10 Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations\n\n👍 👍 👍 👍 👍\n\nPaper with Code\n\n\n\nKey Observations: tensors are not active for long time.\n\n\n\nMain Idea: insert pre-prefetch and evict function in CUDA code.",normalizedContent:" 1. [2023] evaluating unified memory performance in hip\n 2. [97] unlocking bandwidth for gpus in cc-numa systems\n 3. [asplos] deepum: tensor migration and prefetching in unified memory\n 4. [asplos] capuchin: tensor-based gpu memory management for deep learning\n 5. pump profiling-free unified memory prefetcher for large dnn model support\n 6. [micro] g10 enabling an efficient unified gpu memory and storage architecture with smart tensor migrations\n\n----------------------------------------\n\nnot read\n\n 1. megtaichi: dynamic tensor-based memory management optimization for dnn training\n 2. [14] efficient gpu memory management for nonlinear dnns\n\n----------------------------------------\n\n\n# 1.[2023] evaluating unified memory performance in hip\n\num only works on recent amd gpus, including vega10 and mi100.\nthere are two flavors of the support: xnack-enabled and xnack-disabled.\\\n\n * in the xnack-enabled mode,a gpu can handle retry of a memory access after page-faults, which enables mapping and migrating data on demand, as well as memory overcommitment.\n * in the xnack-disabled mode, all memory must be resident and mapped in gpu page tables when the gpu is executing application code.\n   the xnack-enabled mode only has experimental support.\n\nthe experimental results show that the performance of the applications using um is closely related to data transfer size and memory accesses of a kernel. compared to “um”, prefetching memory as a memory usage hint leads to significant data transfers between the host and device.\n\ncompared to “um”, prefetching memory as a memory usage hint leads to significant data transfers between the host and device.\n\n * “um-hint” and “um” indicate unified memory with and without memory usage hints, respectively.\n * “zerocopy” uses zero-copy buffers for data migration.\n * “pageablecopy” copies data from pageable host memory to device memory\n * “pagelockedcopy” transfers data from page-locked host memory to device memory\n\n\n\n\n# result analysis\n\nthe result shows that the stall rate is highly sensitive to the increase of memory size in um.\n\n\n\nthe decrease of the kernel execution time ranges from approximately 1.1x to 2.8x with respect to the vector length for the three optimization techniques.、 however, the execution time is still approximately 1.4x to 74.8x longer than that of the kernel that takes the copy-then-execute approach.\n\nin [28], the authors present 32 open-source um benchmarks in cuda and evaluate their performance on an nvidia pascal gpu.\nthey find that across the benchmarks the performance of the um benchmarks is on average 34.2% slower compared with the benchmarks without um due to the cost of page fault handling\n\n> [28] uvmbench: a comprehensive benchmark suite for researching unified virtual memory in gpu\n\n----------------------------------------\n\n\n# 2.[97] unlocking bandwidth for gpus in cc-numa systems\n\nnvidia with umich\n\n\n# main idea in short\n\n * mainly focus on how many pages that covers the page-fault pages should be migrated.\n * prefetching with upgraded range, which balance the prefetching and also reduce the number of tlb shootdowns\n * tlb shootdown is estimated at 100 cycles\n * memory oversubscription and eviction is not considered.\n * page migration threshold accustomed to each workload is complex. and not worth it. it is better to just migrate on first touch.\n\n\n# introduction\n\n\n\n\n# contribution\n\n * counter-based metrics to determine when to migrate pages from the cpu to gpu are insufficient for finding an optimal migration policy to exploit gpu memory bandwidth.\n   in streaming workloads, where each page may be accessed only a few times, waiting for n accesses to occur before migrating a page will actually limit the number of accesses that occur after migration, reducing the efficacy of the page migration operation.\n\n 2. tlb shootdown and refill overhead can significantly degrade the performance of any page migration policy for gpus.\n    we show that combining reactive migration with virtual address locality information to aggressively prefetch pages can mitigate much of this overhead, resulting in increased gpu throughput.\n\n\n# interesting experiment\n\nperformance comparson of ddr and gddr experiments\n\n\n\nthis choice is motivated by our observation that the performance of some gpu compute workloads would degrade by as much as 66% if the traditional gddr memory on a gpu were replaced with standard ddr memory, as seen in figure 2.\n\n\n\nstill confused about the following figure.\n\n\n\n\n# interesting finding\n\n# clustered page\n\npage accessing is clusted by memory arranges.\npart of continuous virtual address is hot.\nthis clustering is key to range expansion because it suggests that if a page is identified for migration, then other neighboring pages in the virtual address space are likely to have a similar number of total touches.\n\n# threshold to trigger page migration\n\n\n\na first touch policy (threshold-1) requires no tracking information and can be trivially implemented by migrating a page the first time the gpu translates an address for the page.\n\nconsidering the performance differential seen across thresholds, we believe the overhead of implementing the necessary hardware counters to track all pages within a system to differentiate their access counts is not worth the improvement over a vastly simpler first-touch migration policy.\n\n\n# tlb\n\nthe runtime system also must be cognizant that performing tlb invalidations (an integral part of page migration) on a gpu does not just halt a single processor, but thousands of compute pipelines that may be accessing these pages through a large shared tlb structure.\nthis shared tlb structure makes page migrations between a cpu and gpu potentially much more costly (in terms of the opportunity cost of lost execution throughput) than in cpu-only systems.\n\nrecent papers have provided proposals about how to efficiently implement general purpose tlbs that are, or could be, optimized for a gpu’s needs [28]–[30].\nothers have recently looked at improving tlb reach by exploiting locality within the virtual to physical memory remapping, or avoiding this layer completely [31]–[33].\nfinally, gerofi et al. [34] recently examined tlb performance of the xeon phi for applications with large footprints, while mccurdy et al. [35] investigated the effect of superpages and tlb coverage for hpc applications in the context of cpus.\n\n\n\n----------------------------------------\n\n\n# 3. [asplos] deepum: tensor migration and prefetching in unified memory\n\n\n\ndeepum automatically prefetches data using correlation prefetching.\n\ntwo correlation table records the history of kenrel execution and page access patterns during training of prefetching.\n\ndeepum’s correlation tables record the history of the kernel executions and their page accesses during the training phase of a dnn.\nit prefetches pages based on the information in the correlation tables by predicting which kernel will execute next.\nwhile traditional correlation prefetching uses a single table to store history and records the relationship between the cpu cache lines, deepum correlation prefetching uses two different table structures.\n\nassign each kernel with a kernel id and maitian kernel id with its page access history.\n\n----------------------------------------\n\n\n# 4. [asplos] capuchin: tensor-based gpu memory management for deep learning\n\n👍 👍 👍 👍 👍\n\nthe key feature of capuchin is that it makes memory management decisions based on dynamic tensor access pattern tracked at runtime.\nthis design is motivated by the observation that the access pattern to tensors is regular during training iterations.\nbased on the identified patterns, one can exploit the total memory optimization space and offer the fine-grain and flexible control of when and how to perform memory optimization techniques.\n\n# introduction\n\nbert 768 hiddenlayers 73gb memory int training.\nv100 32gb on-board memory and p100 16gb on-board memory.\n\nfeature maps are produced in the forward propagation and used again in the backward propagation.\nmajor deep learning frameworks such as tensorflow , mxnet and pytorch usually maintain these feature maps in gpu memory until they are no longer needed in backward propagation computation.\nhowever, there is usually a large gap between two accesses to the same feature map in forward and backward propagation, which incurs high memory consumption to store the intermediate results.\n\nto reduce memory consumption:\n\n * swapping\n * recomputting\n\nkey observaions:\n\n * we believe that dynamically tracking fine-grained tensor accesses is a fundamental and general technique that enables effective memory management optimizations.\n   this paper demonstrates that this essential idea can be implemented efficiently on top of major deep learning frameworks.\n * the training process is composed of millions of iterations with clear boundaries, and the tensor accesses have regular and repeated access patterns across iterations.\n   this means that analyzing the timing and tensor access patterns can easily reveal the memory optimization opportunities with concrete guidance\n\n# background\n\nforward propagation\n\ninput feature maps, current layer’s weights and bias produce the output feature maps which become the next layer’s input data.\nthe forward propagation concludes with the calculation of loss by comparing the output with ground truth label at the output layer.\n\nbackward propagation\n\nthe backward propagation starts from the output layer and reversely traverses layers to optimize the weights and bias.\n\nmemory usage\n\n * feature maps:output in the forward propagation\n * gradient maps:output in the backward propagtaion\n * convolution workspace: extra memory space needed by convolution algorithm.\n\nmodel weights consume very small amount of memory and are usually persistent in gpu memory to be continuously updated.\n\nthe latter two are temporary memory usage which can be released immediately after current computations are finished.\n\nthe feature maps are needed in both forward and backward propagation. however, there exists a large time gap between the two usage points for computations in forward and backward phase.\n\ndeap learning framework execution modes\n\n * eager mode: dynamic graph\n * graph mode: static graph\n\n\n# framework\n\n\n\n\n\nthe idea is clear, simple and classic.\n\nprofile based on tensor reuse pattern.\n\nbased on history information choose early prefetch or recompute.\n\n----------------------------------------\n\n\n# 5. pump profiling-free unified memory prefetcher for large dnn model support\n\nyear: 2022\n\n👍 👍 👍 👍 👍\n\nthis paper is published with source code. real stuff!\n\npump exploits gpu asynchronous execution for prefetch; that is, there exists a delay between the time that cpu launches a kernel and the time the kernel executes in gpu.\n\npump extracts memory blocks accessed by the kernel when launching and swaps these blocks into gpu memory.\n\n\n\nthe idea behind this paper is also simple and classic.\n\nbefore launch the kernel, launch the prefetch cudaevent.\n\nthey exploit dynamic linker of linux, with ld_preload, they could call their version of cuda wrapper.\n\n----------------------------------------\n\n\n# 6. [micro] g10 enabling an efficient unified gpu memory and storage architecture with smart tensor migrations\n\n👍 👍 👍 👍 👍\n\npaper with code\n\n\n\nkey observations: tensors are not active for long time.\n\n\n\nmain idea: insert pre-prefetch and evict function in cuda code.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU WARP Mangement Papers",frontmatter:{title:"GPU WARP Mangement Papers",date:"2024-09-06T00:00:00.000Z",permalink:"/pages/45880/",tags:[null]},regularPath:"/03.gpu/23.gpu_warp_paper.html",relativePath:"03.gpu/23.gpu_warp_paper.md",key:"v-6f6994e3",path:"/pages/45880/",headers:[{level:2,title:"1. Improving GPU Perfromance via Large Warps and Two-Level Warp Scheduling",slug:"_1-improving-gpu-perfromance-via-large-warps-and-two-level-warp-scheduling",normalizedTitle:"1. improving gpu perfromance via large warps and two-level warp scheduling",charIndex:302},{level:3,title:"Main Idea in Short",slug:"main-idea-in-short",normalizedTitle:"main idea in short",charIndex:381},{level:3,title:"Framework",slug:"framework",normalizedTitle:"framework",charIndex:746},{level:3,title:"MISC",slug:"misc",normalizedTitle:"misc",charIndex:762},{level:3,title:"Core Pipeline",slug:"core-pipeline",normalizedTitle:"core pipeline",charIndex:1384},{level:2,title:"2. Improving GPGPU Resource Utilization Through Alternative Thread Blocking Scheduling",slug:"_2-improving-gpgpu-resource-utilization-through-alternative-thread-blocking-scheduling",normalizedTitle:"2. improving gpgpu resource utilization through alternative thread blocking scheduling",charIndex:1446},{level:3,title:"Main Idea in Short",slug:"main-idea-in-short-2",normalizedTitle:"main idea in short",charIndex:381},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:1712},{level:3,title:"Two Different scheduling CTA policy for different workloads",slug:"two-different-scheduling-cta-policy-for-different-workloads",normalizedTitle:"two different scheduling cta policy for different workloads",charIndex:2657},{level:3,title:"Observation",slug:"observation",normalizedTitle:"observation",charIndex:3187},{level:3,title:"Method",slug:"method",normalizedTitle:"method",charIndex:4005},{level:3,title:"Framework",slug:"framework-2",normalizedTitle:"framework",charIndex:746},{level:2,title:"3. Exploiting Inter-Warp Heterogeneity to Improve GPGPU Performance",slug:"_3-exploiting-inter-warp-heterogeneity-to-improve-gpgpu-performance",normalizedTitle:"3. exploiting inter-warp heterogeneity to improve gpgpu performance",charIndex:6547},{level:3,title:"Main Idea in Short",slug:"main-idea-in-short-3",normalizedTitle:"main idea in short",charIndex:381},{level:3,title:"Observation",slug:"observation-2",normalizedTitle:"observation",charIndex:3187},{level:3,title:"Components",slug:"components",normalizedTitle:"components",charIndex:7259}],headersStr:"1. Improving GPU Perfromance via Large Warps and Two-Level Warp Scheduling Main Idea in Short Framework MISC Core Pipeline 2. Improving GPGPU Resource Utilization Through Alternative Thread Blocking Scheduling Main Idea in Short Introduction Two Different scheduling CTA policy for different workloads Observation Method Framework 3. Exploiting Inter-Warp Heterogeneity to Improve GPGPU Performance Main Idea in Short Observation Components",content:" 1. [530 MICRO] Improving GPU Perfromance via Large Warps and Two-Level Warp Scheduling\n 2. [219] Improving GPGPU Resource Utilization Through Alternative Thread Blocking Scheduling\n 3. [94] Exploiting Inter-Warp Heterogeneity to Improve GPGPU Performance\n\n----------------------------------------\n\n\n# 1. Improving GPU Perfromance via Large Warps and Two-Level Warp Scheduling\n\n\n# Main Idea in Short\n\n 1. Solve issue of branch divergence by managing large wrap and create diverged sub-warp from large warp\n 2. Two-level warp scheduling. If all warps are scheduled together, they might get stuck by memory access request at the same time.\n    Thus they group 32 warps into 4 fetch groups. Group0 is prioritized first and then following warps.\n\n\n# Framework\n\n\n\n\n# MISC\n\n# Branch Divergence\n\nMaintainance of branch divergence is well illustrated in the paper.\n\n\n\n * Since a warp can only have a single active PC at any given time, when branch divergence occurs, one path must be chosen first and the other is pushed on a divergence stack associated with the warp so that it can be executed later.\n * The divergence stack is also used to bring the warp back together once the divergent paths have been executed and all threads have reached a control flow merge (CFM) point.\n * A divergence stack entry consists of three fields: a re-convergence PC, an active mask, and an execute PC.\n\n\n# Core Pipeline\n\n\n\n----------------------------------------\n\n\n# 2. Improving GPGPU Resource Utilization Through Alternative Thread Blocking Scheduling\n\n\n# Main Idea in Short\n\nInteraction between thread block scheduler and wrap scheduler for different characteristics of workloads\n\n * resource contention\n * inter-CTA locality\n\n\n# Introduction\n\nTwo level of schedulers within a GPGPU:\n\n * a warp (or a wavefront) scheduler to determine which warp is executed\n * a thread block or CTA scheduler to assign CTAs to cores\n\nBy default, the current CTA scheduler in hardware assigns the maximum number of CTAs to each core.\nThe maximum number of CTAs depends on the resources used by each thread and the upper limit is determined the architecture (e.g., 8 CTAs in the Tesla architecture that we evaluate).\n\nAssigning the maximum number of CTAs does not necessarily result in maximum performance as additional CTAs degrade performance by likely creating resource contention.\n\n> Other's work Cache Conscious Wavefront Scheduling (CCWS) [29] proposes a warp scheduler that tracks L1 cache accesses to throttle the number of warps scheduled. Dynamic CTA scheduling (DYNCTA) [16] attempts to allocate the optimal number of CTAs to each core based on the application characteristics.\n\n\n# Two Different scheduling CTA policy for different workloads\n\nFor workloads where the maximum number of CTAs does not maximize performance, we leverage a greedy warp scheduler [29] to propose a lazy CTA scheduling (LCS) where the maximum number of CTAs allocated to each core is reduced to avoid resource contention and performance degradation.\n\nIn addition, to exploit inter-CTA locality, we propose block CTA scheduling (in conjunction with an appropriate block-aware warp scheduling) to improve performance and efficiency.\n\n\n# Observation\n\n * Type I : Increased Performance\n * Type II : Increased Performance and Saturate\n\nContention of L1 and increasing L2 miss rate can leads to degrade performance\n\n * Type III : Decreased Performance\n * Type IV : Increase then Decrease\n\nCore Activity\n\n * IDLE\n   There are no available warps that can be issued. This can occur when there are not sufficient warps (and CTAs) assigned to the core.\n * MEM_STALL\n   Most of the warps in the core are stalled waiting for data reply from memory while other warps have no valid instruction to issue.\n * CORE_STALL\n   The core pipeline is stalled and no warp can be issued.\n   While some of the warps in the core might be stalled waiting for data from memory, other warps are stalled because of core/pipeline resource contention (e.g., lack of MSHR entries).\n\n\n\n\n# Method\n\nWe analyzed the behavior of the CTA scheduler through instrumentation.\nIn the source code of the workloads, we used the PTX register %smid to determine which SM each CTA was assigned to.\n\n\n# Framework\n\n#\n\nLazy CTA scheduling (LCS) that reduces the maximum number of CTAs that can be assigned to each core to improve performance and energy efficiency.\nBlock CTA scheduling (BCS) where sequential CTA blocks are assigned to the same core to improve inter-CTA cache locality and an appropriate warp scheduler that exploits such locality.\n\n# LCS\n\nIn comparison, LCS only requires a single measurement during the execution of the first thread block and based on the data collected, the number of thread blocks allocated to the core is adjusted.\n\n\n\nIt is simple.\nDuring the monitor phase, the number of instructions issued (inst) for each thread block x is measured. The monitor phase continues until the first thread block finishes execution.\n\n\n\nIn Figure7 (b), Tnew = floor(10/4) = 3\n\n# BCS\n\nwe focus on a block of size 2 CTAs. BCS is not applicable to workloads with one-dimensional CTAs as there is little inter-CTA L1 locality.\n\n# Increasing Efficiency of GPGPUs: mixed Concurrent Kernel Execution (mCKE)\n\nSimilar to prior work [2], the unused resource (e.g., register file, shared memory) can be powergated to improve energy-efficiency with the reduced number of thread blocks allocated to each core with LCS.\nIn addition, the underutilized resources within a core provide opportunity for concurrent execution of different kernels on the same core, which we refer to as mixed concurrent kernel execution (mCKE).\n\nThe main goal of CKE is to efficiently utilize the GPU by overlapping kernel execution.\n\nIn the baseline CKE, each core can be stalled at different point in time while waiting for the response from the memory and result in the core being idle for significant amount of time.\nHowever, by interleaving the kernels on the same core with mCKE, the memory latency can be hidden (or overlapped) with other kernel execution and effectively improve overall performance.\nThis is similar to the benefits of two-level warp scheduling [20] where the memory accesses from the warps within a thread block are not necessarily schedule together but partitioned into different fetch groups.\n\n> [20] V. Narasiman et al. Improving GPU Performance via Large Warps and Two-Level Warp Scheduling. In International Symposium on Microarchitecture (MICRO), pages 308–317, Porto Alegre, Brazil, 2011.\n\n----------------------------------------\n\n\n# 3. Exploiting Inter-Warp Heterogeneity to Improve GPGPU Performance\n\n\n# Main Idea in Short\n\nFor almost-all-miss warp, bypass their request to reduce memory & cache contention.\n\n\n# Observation\n\nthree new observations:\n\n * GPGPU warps exhibit heterogeneous memory divergence behavior at the shared cache: some warps have most of their requests hit in the cache (high cache utility), while other warps see most of their request miss (low cache utility).\n * a warp retains the same divergence behavior for long periods of execution\n * due to high memory level parallelism, requests going to the shared cache can incur queuing delays as large as hundreds of cycles, exacerbating the effects of memory divergence.\n\n\n# Components\n\n * a cache bypassing mechanism that exploits the latency tolerance of low cache utility warps to both alleviate queuing delay and increase the hit rate for high cache utility warps\n * a cache insertion policy that prevents data from high cache utility warps from being prematurely evicted\n * a memory controller that prioritizes the few requests received from high cache utility warps to minimize stall time.\n\n\n\n",normalizedContent:" 1. [530 micro] improving gpu perfromance via large warps and two-level warp scheduling\n 2. [219] improving gpgpu resource utilization through alternative thread blocking scheduling\n 3. [94] exploiting inter-warp heterogeneity to improve gpgpu performance\n\n----------------------------------------\n\n\n# 1. improving gpu perfromance via large warps and two-level warp scheduling\n\n\n# main idea in short\n\n 1. solve issue of branch divergence by managing large wrap and create diverged sub-warp from large warp\n 2. two-level warp scheduling. if all warps are scheduled together, they might get stuck by memory access request at the same time.\n    thus they group 32 warps into 4 fetch groups. group0 is prioritized first and then following warps.\n\n\n# framework\n\n\n\n\n# misc\n\n# branch divergence\n\nmaintainance of branch divergence is well illustrated in the paper.\n\n\n\n * since a warp can only have a single active pc at any given time, when branch divergence occurs, one path must be chosen first and the other is pushed on a divergence stack associated with the warp so that it can be executed later.\n * the divergence stack is also used to bring the warp back together once the divergent paths have been executed and all threads have reached a control flow merge (cfm) point.\n * a divergence stack entry consists of three fields: a re-convergence pc, an active mask, and an execute pc.\n\n\n# core pipeline\n\n\n\n----------------------------------------\n\n\n# 2. improving gpgpu resource utilization through alternative thread blocking scheduling\n\n\n# main idea in short\n\ninteraction between thread block scheduler and wrap scheduler for different characteristics of workloads\n\n * resource contention\n * inter-cta locality\n\n\n# introduction\n\ntwo level of schedulers within a gpgpu:\n\n * a warp (or a wavefront) scheduler to determine which warp is executed\n * a thread block or cta scheduler to assign ctas to cores\n\nby default, the current cta scheduler in hardware assigns the maximum number of ctas to each core.\nthe maximum number of ctas depends on the resources used by each thread and the upper limit is determined the architecture (e.g., 8 ctas in the tesla architecture that we evaluate).\n\nassigning the maximum number of ctas does not necessarily result in maximum performance as additional ctas degrade performance by likely creating resource contention.\n\n> other's work cache conscious wavefront scheduling (ccws) [29] proposes a warp scheduler that tracks l1 cache accesses to throttle the number of warps scheduled. dynamic cta scheduling (dyncta) [16] attempts to allocate the optimal number of ctas to each core based on the application characteristics.\n\n\n# two different scheduling cta policy for different workloads\n\nfor workloads where the maximum number of ctas does not maximize performance, we leverage a greedy warp scheduler [29] to propose a lazy cta scheduling (lcs) where the maximum number of ctas allocated to each core is reduced to avoid resource contention and performance degradation.\n\nin addition, to exploit inter-cta locality, we propose block cta scheduling (in conjunction with an appropriate block-aware warp scheduling) to improve performance and efficiency.\n\n\n# observation\n\n * type i : increased performance\n * type ii : increased performance and saturate\n\ncontention of l1 and increasing l2 miss rate can leads to degrade performance\n\n * type iii : decreased performance\n * type iv : increase then decrease\n\ncore activity\n\n * idle\n   there are no available warps that can be issued. this can occur when there are not sufficient warps (and ctas) assigned to the core.\n * mem_stall\n   most of the warps in the core are stalled waiting for data reply from memory while other warps have no valid instruction to issue.\n * core_stall\n   the core pipeline is stalled and no warp can be issued.\n   while some of the warps in the core might be stalled waiting for data from memory, other warps are stalled because of core/pipeline resource contention (e.g., lack of mshr entries).\n\n\n\n\n# method\n\nwe analyzed the behavior of the cta scheduler through instrumentation.\nin the source code of the workloads, we used the ptx register %smid to determine which sm each cta was assigned to.\n\n\n# framework\n\n#\n\nlazy cta scheduling (lcs) that reduces the maximum number of ctas that can be assigned to each core to improve performance and energy efficiency.\nblock cta scheduling (bcs) where sequential cta blocks are assigned to the same core to improve inter-cta cache locality and an appropriate warp scheduler that exploits such locality.\n\n# lcs\n\nin comparison, lcs only requires a single measurement during the execution of the first thread block and based on the data collected, the number of thread blocks allocated to the core is adjusted.\n\n\n\nit is simple.\nduring the monitor phase, the number of instructions issued (inst) for each thread block x is measured. the monitor phase continues until the first thread block finishes execution.\n\n\n\nin figure7 (b), tnew = floor(10/4) = 3\n\n# bcs\n\nwe focus on a block of size 2 ctas. bcs is not applicable to workloads with one-dimensional ctas as there is little inter-cta l1 locality.\n\n# increasing efficiency of gpgpus: mixed concurrent kernel execution (mcke)\n\nsimilar to prior work [2], the unused resource (e.g., register file, shared memory) can be powergated to improve energy-efficiency with the reduced number of thread blocks allocated to each core with lcs.\nin addition, the underutilized resources within a core provide opportunity for concurrent execution of different kernels on the same core, which we refer to as mixed concurrent kernel execution (mcke).\n\nthe main goal of cke is to efficiently utilize the gpu by overlapping kernel execution.\n\nin the baseline cke, each core can be stalled at different point in time while waiting for the response from the memory and result in the core being idle for significant amount of time.\nhowever, by interleaving the kernels on the same core with mcke, the memory latency can be hidden (or overlapped) with other kernel execution and effectively improve overall performance.\nthis is similar to the benefits of two-level warp scheduling [20] where the memory accesses from the warps within a thread block are not necessarily schedule together but partitioned into different fetch groups.\n\n> [20] v. narasiman et al. improving gpu performance via large warps and two-level warp scheduling. in international symposium on microarchitecture (micro), pages 308–317, porto alegre, brazil, 2011.\n\n----------------------------------------\n\n\n# 3. exploiting inter-warp heterogeneity to improve gpgpu performance\n\n\n# main idea in short\n\nfor almost-all-miss warp, bypass their request to reduce memory & cache contention.\n\n\n# observation\n\nthree new observations:\n\n * gpgpu warps exhibit heterogeneous memory divergence behavior at the shared cache: some warps have most of their requests hit in the cache (high cache utility), while other warps see most of their request miss (low cache utility).\n * a warp retains the same divergence behavior for long periods of execution\n * due to high memory level parallelism, requests going to the shared cache can incur queuing delays as large as hundreds of cycles, exacerbating the effects of memory divergence.\n\n\n# components\n\n * a cache bypassing mechanism that exploits the latency tolerance of low cache utility warps to both alleviate queuing delay and increase the hit rate for high cache utility warps\n * a cache insertion policy that prevents data from high cache utility warps from being prematurely evicted\n * a memory controller that prioritizes the few requests received from high cache utility warps to minimize stall time.\n\n\n\n",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU Memory Layout Papers",frontmatter:{title:"GPU Memory Layout Papers",date:"2024-09-06T00:00:00.000Z",permalink:"/pages/45881/",tags:[null]},regularPath:"/03.gpu/24.gpu_memory_layout.html",relativePath:"03.gpu/24.gpu_memory_layout.md",key:"v-eff2fe36",path:"/pages/45881/",headers:[{level:2,title:"1. Static Cost Estimation for Data Layout Selection on GPUs",slug:"_1-static-cost-estimation-for-data-layout-selection-on-gpus",normalizedTitle:"1. static cost estimation for data layout selection on gpus",charIndex:186},{level:3,title:"Background",slug:"background",normalizedTitle:"background",charIndex:732},{level:3,title:"Framework",slug:"framework",normalizedTitle:"framework",charIndex:1545},{level:2,title:"2. Managing DRAM Latency Divergence in Irregular GPGPU Applications",slug:"_2-managing-dram-latency-divergence-in-irregular-gpgpu-applications",normalizedTitle:"2. managing dram latency divergence in irregular gpgpu applications",charIndex:3242}],headersStr:"1. Static Cost Estimation for Data Layout Selection on GPUs Background Framework 2. Managing DRAM Latency Divergence in Irregular GPGPU Applications",content:" 1. [3] Static Cost Estimation for Data Layout Selection on GPUs\n 2. [118] Managing DRAM Latency Divergence in Irregular GPGPU Applications\n\n----------------------------------------\n\n\n# 1. Static Cost Estimation for Data Layout Selection on GPUs\n\non the CPU, using Array-of-Struct (AoS) would be more efficient in general, because using AoS would encourage spatial locality.\nin old GPU models before the L1 and L2 cache were introduced, Struct-of-Array (SoA) would usually be preferred since it would achieve better coalescing of memory access.\nThe GPU L1 and L2 cache brought back the opportunity of cache reuse, which means merging fields stored in discrete arrays in SoA into a single AoS might lead to better performance.\n\n\n\n\n# Background\n\nAccessing one field will also bring the adjacent fields into the cache.\nOn the other hand, SoA may not utilize the cache well, as accessing x[i] will not result in loading y[i] into the cache since x[i] and y[i] are unlikely to be in the same cache line.\n\non the GPU, using SoA will be more likely to result in more coalesced memory accesses, all threads in the same warp are accessing the same field.\nfor different logical data points simultaneously, using SoA will lead to fewer memory transactions as the same fields are declared in adjacent memory locations.\n\nAoS might lead to worse coalesced accesses, as declaring different fields in the same struct will waste memory bandwidth when accessing only a single field. Thus, SoA was preferred on the GPU before the GPU cache was introduced.\n\n\n\n\n\n\n# Framework\n\n 1. Block Size, Grid Size, Number of Blocks per SM, and Thread ID:\n    For a GPU kernel, the block size is the number of threads inside each CUDA thread block, and the grid size is the number of CUDA thread blocks in total.\n    The number of blocks per SM is the maximum number of CUDA thread blocks each SM can have when executing the kernel, and it is bounded by\n\n * the GPU architecture\n * the amount of resources each block is requesting. For each thread, the thread ID (tid) is its one-dimensional rank inside the global thread pool.\n\n 2. Structure Size The structure size of a structure is the number of bytes of the structure including the padding bytes.\n\n 3. Array Index\n\n 4. Stride\n    For a global access to data field x, the stride between a pair of adjacent threads in the same warp is the difference between the two memory addresses of the field x these two threads are accessing.\n    The value of the stride depends on the difference of the array indices accessed between adjacent threads, as well as the structure size of the structure field x belongs to.\n    Strides can be different for different pairs of adjacent threads.\n\n 5. Instruction Distance\n    We define the L1 and L2 instruction distance between two memory access instructions I1 and I2, as the number of unique memory locations accessed by all threads sharing the L1 or L2 cache between I1 and I2.\\\n\n> It is like memory footprint?\n\nThey estimate the best memory layout based on cost function.\n\n * Estimating the Cost Coefficient\n * Estimating the Number of Blocks per SM\n * Estimating the Cache Hit Ratio\n * Estimating the Cost\n * Handling Dynamic Loop Lengths\n\n----------------------------------------\n\n\n# 2. Managing DRAM Latency Divergence in Irregular GPGPU Applications\n\n * we propose memory scheduling mechanisms that avoid inter-warp interference in the DRAM system to reduce the average memory stall latency experienced by warps.\n * we reduce latency divergence through mechanisms that coordinate scheduling decisions across multiple independent memory channels.\n * we show that carefully orchestrating the memory scheduling policy can achieve low average latency for warps, without compromising bandwidth utilization.\n\nIrregular Workloads:\n\n\n\nMemory Divergence:\n\nFollowing Figure shows that 56% of loads (the black bar) issued by irregular programs result in more than one memory request and that on average each load generates 5.9 memory requests after coalescing (benchmarks and evaluation techniques explained in Section V).",normalizedContent:" 1. [3] static cost estimation for data layout selection on gpus\n 2. [118] managing dram latency divergence in irregular gpgpu applications\n\n----------------------------------------\n\n\n# 1. static cost estimation for data layout selection on gpus\n\non the cpu, using array-of-struct (aos) would be more efficient in general, because using aos would encourage spatial locality.\nin old gpu models before the l1 and l2 cache were introduced, struct-of-array (soa) would usually be preferred since it would achieve better coalescing of memory access.\nthe gpu l1 and l2 cache brought back the opportunity of cache reuse, which means merging fields stored in discrete arrays in soa into a single aos might lead to better performance.\n\n\n\n\n# background\n\naccessing one field will also bring the adjacent fields into the cache.\non the other hand, soa may not utilize the cache well, as accessing x[i] will not result in loading y[i] into the cache since x[i] and y[i] are unlikely to be in the same cache line.\n\non the gpu, using soa will be more likely to result in more coalesced memory accesses, all threads in the same warp are accessing the same field.\nfor different logical data points simultaneously, using soa will lead to fewer memory transactions as the same fields are declared in adjacent memory locations.\n\naos might lead to worse coalesced accesses, as declaring different fields in the same struct will waste memory bandwidth when accessing only a single field. thus, soa was preferred on the gpu before the gpu cache was introduced.\n\n\n\n\n\n\n# framework\n\n 1. block size, grid size, number of blocks per sm, and thread id:\n    for a gpu kernel, the block size is the number of threads inside each cuda thread block, and the grid size is the number of cuda thread blocks in total.\n    the number of blocks per sm is the maximum number of cuda thread blocks each sm can have when executing the kernel, and it is bounded by\n\n * the gpu architecture\n * the amount of resources each block is requesting. for each thread, the thread id (tid) is its one-dimensional rank inside the global thread pool.\n\n 2. structure size the structure size of a structure is the number of bytes of the structure including the padding bytes.\n\n 3. array index\n\n 4. stride\n    for a global access to data field x, the stride between a pair of adjacent threads in the same warp is the difference between the two memory addresses of the field x these two threads are accessing.\n    the value of the stride depends on the difference of the array indices accessed between adjacent threads, as well as the structure size of the structure field x belongs to.\n    strides can be different for different pairs of adjacent threads.\n\n 5. instruction distance\n    we define the l1 and l2 instruction distance between two memory access instructions i1 and i2, as the number of unique memory locations accessed by all threads sharing the l1 or l2 cache between i1 and i2.\\\n\n> it is like memory footprint?\n\nthey estimate the best memory layout based on cost function.\n\n * estimating the cost coefficient\n * estimating the number of blocks per sm\n * estimating the cache hit ratio\n * estimating the cost\n * handling dynamic loop lengths\n\n----------------------------------------\n\n\n# 2. managing dram latency divergence in irregular gpgpu applications\n\n * we propose memory scheduling mechanisms that avoid inter-warp interference in the dram system to reduce the average memory stall latency experienced by warps.\n * we reduce latency divergence through mechanisms that coordinate scheduling decisions across multiple independent memory channels.\n * we show that carefully orchestrating the memory scheduling policy can achieve low average latency for warps, without compromising bandwidth utilization.\n\nirregular workloads:\n\n\n\nmemory divergence:\n\nfollowing figure shows that 56% of loads (the black bar) issued by irregular programs result in more than one memory request and that on average each load generates 5.9 memory requests after coalescing (benchmarks and evaluation techniques explained in section v).",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU MultiTask",frontmatter:{title:"GPU MultiTask",date:"2024-09-08T00:00:00.000Z",permalink:"/pages/45883/",tags:[null]},regularPath:"/03.gpu/25.gpu_multitask.html",relativePath:"03.gpu/25.gpu_multitask.md",key:"v-27253dc5",path:"/pages/45883/",headersStr:null,content:" 1. [203 ASPLOS] Chimera Collaborative Rreemption for Multitasking on a Shared GPU 👍 👍 👍\n\n 2. [207 ASPLOS] Dynamic Resource Management for Efficient Utilization of Multitasking GPUs",normalizedContent:" 1. [203 asplos] chimera collaborative rreemption for multitasking on a shared gpu 👍 👍 👍\n\n 2. [207 asplos] dynamic resource management for efficient utilization of multitasking gpus",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU Training Notes",frontmatter:{title:"GPU Training Notes",date:"2024-09-08T00:00:00.000Z",permalink:"/pages/45884/",tags:[null]},regularPath:"/03.gpu/26.gpu_cuda_training.html",relativePath:"03.gpu/26.gpu_cuda_training.md",key:"v-70c0f165",path:"/pages/45884/",headers:[{level:2,title:"Notes from Youtube Link",slug:"notes-from-youtube-link",normalizedTitle:"notes from youtube link",charIndex:2},{level:2,title:"1. Shared Memory",slug:"_1-shared-memory",normalizedTitle:"1. shared memory",charIndex:30},{level:3,title:"Stencil Kernel",slug:"stencil-kernel",normalizedTitle:"stencil kernel",charIndex:183},{level:3,title:"__syncthreads()",slug:"syncthreads",normalizedTitle:"__syncthreads()",charIndex:281},{level:3,title:"Sync within blocks or throughout blocks: Cooperative Groups",slug:"sync-within-blocks-or-throughout-blocks-cooperative-groups",normalizedTitle:"sync within blocks or throughout blocks: cooperative groups",charIndex:399},{level:2,title:"5. Atomics Reductions",slug:"_5-atomics-reductions",normalizedTitle:"5. atomics reductions",charIndex:507},{level:2,title:"6. Managed Memory",slug:"_6-managed-memory",normalizedTitle:"6. managed memory",charIndex:602},{level:2,title:"7. Concurrency",slug:"_7-concurrency",normalizedTitle:"7. concurrency",charIndex:1360},{level:3,title:"Pin Memory",slug:"pin-memory",normalizedTitle:"pin memory",charIndex:1379},{level:3,title:"Streams",slug:"streams",normalizedTitle:"streams",charIndex:1469},{level:3,title:"CUDA Event",slug:"cuda-event",normalizedTitle:"cuda event",charIndex:1761},{level:3,title:"Concurrent Kernels",slug:"concurrent-kernels",normalizedTitle:"concurrent kernels",charIndex:1798},{level:3,title:"CUDA Graph",slug:"cuda-graph",normalizedTitle:"cuda graph",charIndex:2044},{level:2,title:"8. GPU Performance Analysis",slug:"_8-gpu-performance-analysis",normalizedTitle:"8. gpu performance analysis",charIndex:2101},{level:3,title:"TOP-Level",slug:"top-level",normalizedTitle:"top-level",charIndex:2133},{level:3,title:"Example",slug:"example",normalizedTitle:"example",charIndex:2328},{level:2,title:"Compile",slug:"compile",normalizedTitle:"compile",charIndex:2458},{level:3,title:"NVIDIA Nsight System",slug:"nvidia-nsight-system",normalizedTitle:"nvidia nsight system",charIndex:2542},{level:3,title:"NVIDIA Compute",slug:"nvidia-compute",normalizedTitle:"nvidia compute",charIndex:2658},{level:2,title:"9. Cooperative Groups",slug:"_9-cooperative-groups",normalizedTitle:"9. cooperative groups",charIndex:2831},{level:3,title:"Block Level Sync",slug:"block-level-sync",normalizedTitle:"block level sync",charIndex:2857},{level:3,title:"Coalesced Group & Grid Wide Sync",slug:"coalesced-group-grid-wide-sync",normalizedTitle:"coalesced group &amp; grid wide sync",charIndex:null},{level:3,title:"Cooperative Launch Kernel",slug:"cooperative-launch-kernel",normalizedTitle:"cooperative launch kernel",charIndex:3159},{level:3,title:"Persistent Kernel",slug:"persistent-kernel",normalizedTitle:"persistent kernel",charIndex:3447},{level:3,title:"Coalesced Group",slug:"coalesced-group",normalizedTitle:"coalesced group",charIndex:2880},{level:2,title:"10 Multithreading and CUDA Concurrency",slug:"_10-multithreading-and-cuda-concurrency",normalizedTitle:"10 multithreading and cuda concurrency",charIndex:3733},{level:2,title:"11 CUDA Multi Process Service",slug:"_11-cuda-multi-process-service",normalizedTitle:"11 cuda multi process service",charIndex:3818},{level:2,title:"12 CUDA Debugging",slug:"_12-cuda-debugging",normalizedTitle:"12 cuda debugging",charIndex:3894},{level:3,title:"CUDA Error Codes",slug:"cuda-error-codes",normalizedTitle:"cuda error codes",charIndex:3916},{level:3,title:"Compute Sanitizer Tool",slug:"compute-sanitizer-tool",normalizedTitle:"compute sanitizer tool",charIndex:3939},{level:3,title:"GDB",slug:"gdb",normalizedTitle:"gdb",charIndex:4040},{level:2,title:"11 CUDA Multi Process Service",slug:"_11-cuda-multi-process-service-2",normalizedTitle:"11 cuda multi process service",charIndex:3818},{level:3,title:"Simple Overscription",slug:"simple-overscription",normalizedTitle:"simple overscription",charIndex:4879},{level:2,title:"Multi Process Service",slug:"multi-process-service",normalizedTitle:"multi process service",charIndex:3826},{level:2,title:"Multi-Instance GPU(MIG)",slug:"multi-instance-gpu-mig",normalizedTitle:"multi-instance gpu(mig)",charIndex:5480}],headersStr:"Notes from Youtube Link 1. Shared Memory Stencil Kernel __syncthreads() Sync within blocks or throughout blocks: Cooperative Groups 5. Atomics Reductions 6. Managed Memory 7. Concurrency Pin Memory Streams CUDA Event Concurrent Kernels CUDA Graph 8. GPU Performance Analysis TOP-Level Example Compile NVIDIA Nsight System NVIDIA Compute 9. Cooperative Groups Block Level Sync Coalesced Group & Grid Wide Sync Cooperative Launch Kernel Persistent Kernel Coalesced Group 10 Multithreading and CUDA Concurrency 11 CUDA Multi Process Service 12 CUDA Debugging CUDA Error Codes Compute Sanitizer Tool GDB 11 CUDA Multi Process Service Simple Overscription Multi Process Service Multi-Instance GPU(MIG)",content:"# Notes from Youtube Link\n\n\n# 1. Shared Memory\n\n * logicall per block resource\n * on-chip memory fast\n * shared by threads in block, but non-visible to threads in other blocks.\n\n\n\n\n# Stencil Kernel\n\nFirst load the green cells.\nLoad the boundary cells using boundary threads.\n\n\n\n\n# __syncthreads()\n\nExecution Barrier\n\nsynchronize all threads within a block.\n\nNot grid level.\n\n48KB hardware limit\n\n\n# Sync within blocks or throughout blocks: Cooperative Groups\n\n\n\n----------------------------------------\n\n\n# 5. Atomics Reductions\n\nClassic Sweep Reduction\n\n\n\n----------------------------------------\n\n\n# 6. Managed Memory\n\nManaged Memory does not promise performance.\nIt only paves ways for software programmer. For example, deepcopy.\n\nWe could restore the performance by using cudaMemPrefetchAsync\n\n\n\nUM cannot do better than expertly written manual data movement, in most cases\n\n * Unified Memory: Page-faulting\n * ATS: Nvidia with Power9. ATS service allows GPU to access CPU (Malloc) Memory\n   Only works in Power9, not for X86.\n * HMM: Nvidia is working on HMM to allow similar with ATS.\n\n\n\ncudaDeviceSynchronize() function is necessary\n\nAfter cudaLaunch kernel, CPU can execute immedidately, which might read data that has not been processed by GPU yet.\nThus, synchronize to wait GPU finishing the processing.\n\n----------------------------------------\n\n\n# 7. Concurrency\n\n\n# Pin Memory\n\n * Statically allocated in Physical Memory.\n * Stay out of paging system.\n\n\n# Streams\n\n * Sequential in Stream\n * No order among Streams\n\n\n\nhost Code could also be put into streams.\n\nMigration(unified memory) call could be more expensive than memcopy.\n\n * memcopy is handled by hardware engine\n * unfied memory operate at page level and needs update of page tables.\n\n\n# CUDA Event\n\nMost used in timing.\n\n\n# Concurrent Kernels\n\nLess efficient than saturating the device with a single kernel.\n\nScheduler might launch blocks from one kernel as much as much possible, try to exhaust the GPU.\nIf any resource is totally token, other kenel cannot launch.\n\n\n# CUDA Graph\n\n----------------------------------------\n\n\n# 8. GPU Performance Analysis\n\n\n# TOP-Level\n\n * Memory Bound\n * Compute Bound\n * Latency Bound\n   * view the issue from idle time of scheduler\n   * view the issue that the workloads shoule be in memory bound or compute bound\n\n\n# Example\n\nIf SM Utilization is low, it indicates that there might be latency problem since GPU cannot schedule enough threads.\n\n\n# Compile\n\nnvcc -o t5 t5.cu -arch=sm_70 -lineinfo\n\nnsys profile --stats=true ./t5\n\n\n# NVIDIA Nsight System\n\nMainly for CPU and GPU. Initial timeline, find where to optimize. nsys-ui -> open *.qdrep\n\n\n# NVIDIA Compute\n\nTargeting at Kernel ncu-cli --page details -f -o t5.profout ./t5\n\nwe could find bottleneck from source code.\n\n\n\n----------------------------------------\n\n\n# 9. Cooperative Groups\n\n\n# Block Level Sync\n\n\n\n\n# Coalesced Group & Grid Wide Sync\n\n\n\n\n\nSync is not just execution barrier but also visualability barrier.\n\n * This reduce utilize shuffle.\n   Do not need shared memory in this case. Only thread-inter communication is necessary.\n\n * This reduce utilize shared memory and sync.\n\n\n# Cooperative Launch Kernel\n\nDeadlock might be introduced.\nIf too many threads in kernel, SM is full of threads waiting in grid.sync(). They are waiting in the queue and idle thread cannot be scheduled.\nThus, deadlock happens.\n\nuse this cudaOccupancyMaxActiveBlocksPerMultiprocessor.\n\n\n\n\n# Persistent Kernel\n\nPersistent kernel could keep state in register and shared memory.\n\nWithout persistent kernel, kernel communation might need to shuttle state into global memory.\n\n\n# Coalesced Group\n\nforce sync of group: active.group().\n\n\n\n----------------------------------------\n\n\n# 10 Multithreading and CUDA Concurrency\n\n----------------------------------------\n\n\n# 11 CUDA Multi Process Service\n\n----------------------------------------\n\n\n# 12 CUDA Debugging\n\n\n# CUDA Error Codes\n\n\n\n\n# Compute Sanitizer Tool\n\n * Synchronous Error VS Asynchronous Error\n * Sticky VS Non-Sticky Error\n\n\n# GDB\n\n * -g debug host code\n * -G debug device code\n * -arch=\n\nMake sure the kernel is launched.\n\nnsys profile --stats=true ./my_exe\n\n\n\nswith to another thread: cuda thread(100)\n\n// break at specific line\nb file:linenumber\n\n// set condition\n// hit break point 1 when block id == 2\ncondition 1 b==2\n\n\n// step, not just one step, even step into the function\ns\n\n// print out\np s[0]\n\n// print array of 8 elements\np s[0]@8\n\n// change to another block\ncuda block 1\ncuda thread 2\n\ninfo cuda device\n\nhelp info cuda\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n\nWhen debug CUDA code, notice that the thread that you are current debugging, is asynchronous with other threads.\n\n\n\n/usr/loca/cuda/include/driver_types.h\n\nThe Defined MACRO of errors.\n\n----------------------------------------\n\n\n# 11 CUDA Multi Process Service\n\n\n# Simple Overscription\n\nIf the compute resource stays the same, we only divide the compute task by 1/4 and the number of task increase by 4 times,\n\nin each kernel, even in the waveform, it seems like four kernels are running parallelly.\n\nIn face, only one kernel is makeing progress. 1/4 workload and 4 kernel switching, the cost of time will stays or even longer, considering overload.\n\n\n\nContext overhead ~300MB\n\n\n\n\n# Multi Process Service\n\n\n\n\n\nPlease notice that if the kernel is too small, using multi rank with multi process might not be optimial, considering the overhead of kernel launching.\n\n\n# Multi-Instance GPU(MIG)\n\nFrom A100, MIG is supported.\n\nMPS does not guarantee quality of service.\n\nMIG support isolation, guarantee the quality of service.\n\n\n\n",normalizedContent:"# notes from youtube link\n\n\n# 1. shared memory\n\n * logicall per block resource\n * on-chip memory fast\n * shared by threads in block, but non-visible to threads in other blocks.\n\n\n\n\n# stencil kernel\n\nfirst load the green cells.\nload the boundary cells using boundary threads.\n\n\n\n\n# __syncthreads()\n\nexecution barrier\n\nsynchronize all threads within a block.\n\nnot grid level.\n\n48kb hardware limit\n\n\n# sync within blocks or throughout blocks: cooperative groups\n\n\n\n----------------------------------------\n\n\n# 5. atomics reductions\n\nclassic sweep reduction\n\n\n\n----------------------------------------\n\n\n# 6. managed memory\n\nmanaged memory does not promise performance.\nit only paves ways for software programmer. for example, deepcopy.\n\nwe could restore the performance by using cudamemprefetchasync\n\n\n\num cannot do better than expertly written manual data movement, in most cases\n\n * unified memory: page-faulting\n * ats: nvidia with power9. ats service allows gpu to access cpu (malloc) memory\n   only works in power9, not for x86.\n * hmm: nvidia is working on hmm to allow similar with ats.\n\n\n\ncudadevicesynchronize() function is necessary\n\nafter cudalaunch kernel, cpu can execute immedidately, which might read data that has not been processed by gpu yet.\nthus, synchronize to wait gpu finishing the processing.\n\n----------------------------------------\n\n\n# 7. concurrency\n\n\n# pin memory\n\n * statically allocated in physical memory.\n * stay out of paging system.\n\n\n# streams\n\n * sequential in stream\n * no order among streams\n\n\n\nhost code could also be put into streams.\n\nmigration(unified memory) call could be more expensive than memcopy.\n\n * memcopy is handled by hardware engine\n * unfied memory operate at page level and needs update of page tables.\n\n\n# cuda event\n\nmost used in timing.\n\n\n# concurrent kernels\n\nless efficient than saturating the device with a single kernel.\n\nscheduler might launch blocks from one kernel as much as much possible, try to exhaust the gpu.\nif any resource is totally token, other kenel cannot launch.\n\n\n# cuda graph\n\n----------------------------------------\n\n\n# 8. gpu performance analysis\n\n\n# top-level\n\n * memory bound\n * compute bound\n * latency bound\n   * view the issue from idle time of scheduler\n   * view the issue that the workloads shoule be in memory bound or compute bound\n\n\n# example\n\nif sm utilization is low, it indicates that there might be latency problem since gpu cannot schedule enough threads.\n\n\n# compile\n\nnvcc -o t5 t5.cu -arch=sm_70 -lineinfo\n\nnsys profile --stats=true ./t5\n\n\n# nvidia nsight system\n\nmainly for cpu and gpu. initial timeline, find where to optimize. nsys-ui -> open *.qdrep\n\n\n# nvidia compute\n\ntargeting at kernel ncu-cli --page details -f -o t5.profout ./t5\n\nwe could find bottleneck from source code.\n\n\n\n----------------------------------------\n\n\n# 9. cooperative groups\n\n\n# block level sync\n\n\n\n\n# coalesced group & grid wide sync\n\n\n\n\n\nsync is not just execution barrier but also visualability barrier.\n\n * this reduce utilize shuffle.\n   do not need shared memory in this case. only thread-inter communication is necessary.\n\n * this reduce utilize shared memory and sync.\n\n\n# cooperative launch kernel\n\ndeadlock might be introduced.\nif too many threads in kernel, sm is full of threads waiting in grid.sync(). they are waiting in the queue and idle thread cannot be scheduled.\nthus, deadlock happens.\n\nuse this cudaoccupancymaxactiveblockspermultiprocessor.\n\n\n\n\n# persistent kernel\n\npersistent kernel could keep state in register and shared memory.\n\nwithout persistent kernel, kernel communation might need to shuttle state into global memory.\n\n\n# coalesced group\n\nforce sync of group: active.group().\n\n\n\n----------------------------------------\n\n\n# 10 multithreading and cuda concurrency\n\n----------------------------------------\n\n\n# 11 cuda multi process service\n\n----------------------------------------\n\n\n# 12 cuda debugging\n\n\n# cuda error codes\n\n\n\n\n# compute sanitizer tool\n\n * synchronous error vs asynchronous error\n * sticky vs non-sticky error\n\n\n# gdb\n\n * -g debug host code\n * -g debug device code\n * -arch=\n\nmake sure the kernel is launched.\n\nnsys profile --stats=true ./my_exe\n\n\n\nswith to another thread: cuda thread(100)\n\n// break at specific line\nb file:linenumber\n\n// set condition\n// hit break point 1 when block id == 2\ncondition 1 b==2\n\n\n// step, not just one step, even step into the function\ns\n\n// print out\np s[0]\n\n// print array of 8 elements\np s[0]@8\n\n// change to another block\ncuda block 1\ncuda thread 2\n\ninfo cuda device\n\nhelp info cuda\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n\nwhen debug cuda code, notice that the thread that you are current debugging, is asynchronous with other threads.\n\n\n\n/usr/loca/cuda/include/driver_types.h\n\nthe defined macro of errors.\n\n----------------------------------------\n\n\n# 11 cuda multi process service\n\n\n# simple overscription\n\nif the compute resource stays the same, we only divide the compute task by 1/4 and the number of task increase by 4 times,\n\nin each kernel, even in the waveform, it seems like four kernels are running parallelly.\n\nin face, only one kernel is makeing progress. 1/4 workload and 4 kernel switching, the cost of time will stays or even longer, considering overload.\n\n\n\ncontext overhead ~300mb\n\n\n\n\n# multi process service\n\n\n\n\n\nplease notice that if the kernel is too small, using multi rank with multi process might not be optimial, considering the overhead of kernel launching.\n\n\n# multi-instance gpu(mig)\n\nfrom a100, mig is supported.\n\nmps does not guarantee quality of service.\n\nmig support isolation, guarantee the quality of service.\n\n\n\n",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU Paper with Code",frontmatter:{title:"GPU Paper with Code",date:"2024-10-30T00:00:00.000Z",permalink:"/pages/45885/",tags:[null]},regularPath:"/03.gpu/27.gpu_paper_code.html",relativePath:"03.gpu/27.gpu_paper_code.md",key:"v-66582056",path:"/pages/45885/",headersStr:null,content:" 1. Parla: A Python Orchestration System for Heterogeneous Architectures\n\nhttps://github.com/ut-parla/Parla.py\n\n 2. Gemini: Enabling Multi-Tenant GPU Sharing Based on Kernel Burst Estimation\n\nhttps://github.com/NTHU-LSALAB/Gemini\n\n 3. EDGE: Event-Driven GPU Execution\n\nhttps://github.com/marialubeznov/event_driven_gpu_execution\n\n 4. CRAC: Checkpoint-Restart Architecture for CUDA Streams and UVM\n\nhttps://github.com/JainTwinkle/CRAC-early-development\n\n 5. PUMP: Profiling-free Unified Memory Prefetcher for Large Model Deep Learning\n\nhttps://github.com/hitqshao/PUMP\n\n 6. ",normalizedContent:" 1. parla: a python orchestration system for heterogeneous architectures\n\nhttps://github.com/ut-parla/parla.py\n\n 2. gemini: enabling multi-tenant gpu sharing based on kernel burst estimation\n\nhttps://github.com/nthu-lsalab/gemini\n\n 3. edge: event-driven gpu execution\n\nhttps://github.com/marialubeznov/event_driven_gpu_execution\n\n 4. crac: checkpoint-restart architecture for cuda streams and uvm\n\nhttps://github.com/jaintwinkle/crac-early-development\n\n 5. pump: profiling-free unified memory prefetcher for large model deep learning\n\nhttps://github.com/hitqshao/pump\n\n 6. ",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU Driver & Runtime & Compliation",frontmatter:{title:"GPU Driver & Runtime & Compliation",date:"2025-01-17T00:00:00.000Z",permalink:"/pages/45886/",tags:[null]},regularPath:"/03.gpu/28.gpu_runtime.html",relativePath:"03.gpu/28.gpu_runtime.md",key:"v-70c8fe25",path:"/pages/45886/",headersStr:null,content:" 1.  Low Overhead Instruction Latency Characterization for NVIDIA GPGPUs\n 2.  Guardian: Safe GPU Sharing in Multi-Tenant Environments\n 3.  Strategies for Protecting Intellectual Property when Using CUDA Applications on Graphics Processing Units\n 4.  GPU Acceleration in Unikernels Using Cricket GPU Virtualization\n 5.  Verified instruction-level energy consumption measurement for NVIDIA GPUs\n 6.  Operating Systems Challenges for GPU Resource Management 👍\n 7.  PTask: operating system abstractions to manage GPUs as compute devices\n 8.  https://insujang.github.io/2017-04-27/gpu-architecture-overview/\n 9.  https://insujang.github.io/2017-06-23/gpu-resource-management/\n 10. https://slideplayer.com/slide/16161216/\n 11. https://slideplayer.com/slide/16136303/\n 12. Gdev: First-Class GPU Resource Management in the Operating System\n 13. GPUfs: The Case for Operating System Services on GPUs\n 14. EDGE: Event-Driven GPU Execution\n 15. An Approach to Estimate Power Consumption of a CUDA Kernel\n 16. Dissecting the CUDA scheduling hierarchy: a Performance and Predictability Perspective :+1：\n 17. Implementing Open-Source CUDA Runtime 👍 👍\n 18. rCUDA: Reducing the number of GPU-based accelerators in high performance clusters 👍 👍\n 19. Cudagrind: Memory-Usage Checking for CUDA 👍\n 20. Throughput Oriented Analytical Models for Performance Estimation on Programmable Accelerators 👍 👍\n 21. Novel methodologies for predictable CPU-to-GPU command offloading 👍 👍\n 22. Overview and comparison of OpenCL and CUDA technology for GPGPU 👍 👍\n 23. Real-Time GPU Resource Management with Loadable Kernel Modules :+1：\n 24. Cudagrind: Memory-Usage Checking for CUDA 👍",normalizedContent:" 1.  low overhead instruction latency characterization for nvidia gpgpus\n 2.  guardian: safe gpu sharing in multi-tenant environments\n 3.  strategies for protecting intellectual property when using cuda applications on graphics processing units\n 4.  gpu acceleration in unikernels using cricket gpu virtualization\n 5.  verified instruction-level energy consumption measurement for nvidia gpus\n 6.  operating systems challenges for gpu resource management 👍\n 7.  ptask: operating system abstractions to manage gpus as compute devices\n 8.  https://insujang.github.io/2017-04-27/gpu-architecture-overview/\n 9.  https://insujang.github.io/2017-06-23/gpu-resource-management/\n 10. https://slideplayer.com/slide/16161216/\n 11. https://slideplayer.com/slide/16136303/\n 12. gdev: first-class gpu resource management in the operating system\n 13. gpufs: the case for operating system services on gpus\n 14. edge: event-driven gpu execution\n 15. an approach to estimate power consumption of a cuda kernel\n 16. dissecting the cuda scheduling hierarchy: a performance and predictability perspective :+1：\n 17. implementing open-source cuda runtime 👍 👍\n 18. rcuda: reducing the number of gpu-based accelerators in high performance clusters 👍 👍\n 19. cudagrind: memory-usage checking for cuda 👍\n 20. throughput oriented analytical models for performance estimation on programmable accelerators 👍 👍\n 21. novel methodologies for predictable cpu-to-gpu command offloading 👍 👍\n 22. overview and comparison of opencl and cuda technology for gpgpu 👍 👍\n 23. real-time gpu resource management with loadable kernel modules :+1：\n 24. cudagrind: memory-usage checking for cuda 👍",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Accel-Sim Simulator",frontmatter:{title:"Accel-Sim Simulator",date:"2024-11-10T00:00:00.000Z",permalink:"/pages/45887/",tags:[null]},regularPath:"/03.gpu/29.accel_sim.html",relativePath:"03.gpu/29.accel_sim.md",key:"v-8a6d3836",path:"/pages/45887/",headersStr:null,content:'# env\n\n# Install Software\n\nubuntu 20.4\n\nsudo apt-get install -y wget build-essential bison zlib1g-dev flex libglu1-mesa-dev libssl-dev libxml2-dev libboost-all-dev git g++ vim python-setuptools python-dev build-essential xutils-dev\n\npip3 install pyyaml plotly psutil\n\nIn case you meet bug: release/cuda-sim/Makefile.makedepend: No such file or directory\n\nsudo apt install xutils-dev\n\n# Install CUDA\n\nwget http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda_11.0.2_450.51.05_linux.run\n\nsudo sh cuda_11.0.2_450.51.05_linux.run\n\n# Install driver\n\nsudo ubuntu-drivers devices\n\nsudo apt install nvidia-driver-535\n\nnvidia-smi\n\n\n\nnvcc --version\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2020 NVIDIA Corporation\nBuilt on Thu_Jun_11_22:26:38_PDT_2020\nCuda compilation tools, release 11.0, V11.0.194\nBuild cuda_11.0_bu.TC445_37.28540450_0\n\n# Install cuDNN\n\nwget https://developer.download.nvidia.com/compute/cudnn/9.0.0/local_installers/cudnn-local-repo-ubuntu2004-9.0.0_1.0-1_amd64.deb\n\nsudo dpkg -i cudnn-local-repo-ubuntu2004-9.0.0_1.0-1_amd64.deb\n\nsudo cp /var/cudnn-local-repo-ubuntu2004-9.0.0/cudnn-*-keyring.gpg /usr/share/keyrings/\n\nsudo apt-get update\n\nsudo apt-get -y install cudnn-cuda-12\n\n# GPU-APP-Collection\n\ngit clone https://github.com/accel-sim/gpu-app-collection.git\n\nmake -C ./gpu-app-collection/src data\n\ngpu app support compile of sm_10. compute_10, remove the versions below 50.\n\nfind ./gpu-app-collection/src/cuda/GPU_Microbenchmark/* -name "Makefile"|xargs sed -i "s/sm_30/sm_50/g"\n\nfind ./gpu-app-collection/src/cuda/GPU_Microbenchmark/* -name "Makefile"|xargs sed -i "s/compute_30/compute_50/g"\n\n./accel\n\n-----./accel-sim-framework\n\n-----./gpu-app-collection/src/cuda\n\n-----./gpu-app-collection/bin/11.0/release/BINARY\n\nin ./gpu-app-collection/src, you could use:\n\n * make lonestargpu-2.0\n * make GPU_Microbenchmark\n * make DeepBench\n * make rodinia-2.0-ft\n * make parboil\n * make shoc\n * make ispass-2009\n * make polybench\n\n# .bashrc\n\nexport PATH=/usr/local/cuda-11.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n\nexport LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64\n\nexport CUDA_INSTALL_PATH=/usr/local/cuda-11.0\n\nexport PATH=$CUDA_INSTALL_PATH/bin:$PATH export GPUAPPS_ROOT=/home/qishao/Project/gpu_simulator/accel/gpu-app-collection\n\n# Collect and run trace:\n\n./util/tracer_nvbit/run_hw_trace.py -B polybench -D 0\n\n./gpu-simulator/bin/release/accel-sim.out -trace ./hw_run/traces/device-0/11.0/nw-rodinia-2.0-ft/128_10___data_result_128_10_txt/traces/kernelslist.g -config ./gpu-simulator/gpgpu-sim/configs/tested-cfgs/SM7_QV100/gpgpusim.config -config ./gpu-simulator/configs/tested-cfgs/SM7_QV100/trace.config\n\n# Pytorch docker\n\n# Install pytorch gpu docker\n\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed \'s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n\nsudo apt-get update\n\nsudo apt-get install -y nvidia-container-toolkit\n\nsudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker\n\n# run docker\n\nsudo docker run --gpus all -it --rm -v pwd:/workspace -e LOCAL_UID=$(id -u $USER) -e LOCAL_GID=$(id -g $USER) pinto0309/pytorch-build:11.3.0-cudnn8-devel-ubuntu20.04 bash\n\n# Success Fully Pytorch Compiled\n\nconda install python 3.12 version\n\nThis works for "239d87af5e5fecec452214a5e6e5e204b51c0597"\n\n\n\n# Build with CUDA and debug\n\nDEBUG=1 USE_CUDA=1 USE_ROCM=0 python setup.py develop\n\n# Enable vscode python attach c++ debug\n\necho 0| sudo tee /proc/sys/kernel/yama/ptrace_scope sudo setcap cap_sys_ptrace=eip /usr/bin/gdb\n\nlaunch.json\n\n{\n    // Use IntelliSense to learn about possible attributes.\n    // Hover to view descriptions of existing attributes.\n    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n    "version": "0.2.0",\n    "configurations": [\n        {\n            "name": "Python Debugger: Current File",\n            "type": "debugpy",\n            "request": "launch",\n            "program": "${file}",\n            "console": "integratedTerminal",\n            "justMyCode": false,\n            "env": {\n                "PYTORCH_JIT": "0",\n                "TORCH_COMPILE_DEBUG": "1",\n                "PYTORCH_DEBUG": "1",\n                "PYTORCH_LOGS": "1",\n                "TORCH_SHOW_CPP_STACKTRACES": "1",\n                "PYTHONPATH": "/home/***/Project/learn_pytorch/pytorch/build/:${PYTHONPATH}",\n                "LD_LIBRARY_PATH": "/home/***/Project/learn_pytorch/pytorch/build/lib:/home/***/anaconda3/envs/py311/lib:/usr/local/cuda-11.8/targets/x86_64-linux/lib/:${env:LD_LIBRARY_PATH}"\n            },\n            "envFile": "${workspaceFolder}/.env"    \n        },\n        {\n            "name": "TorchC++ Debugger",\n            "type": "cppdbg",\n            "request": "attach",\n            "program": "/home/***/anaconda3/envs/py311/bin/python",\n            "processId": "${command:pickProcess}",\n            "MIMode": "gdb",\n            "setupCommands": [\n                {\n                    "description": "Enable pretty-printing for gdb",\n                    "text": "-enable-pretty-printing",\n                    "ignoreFailures": true\n                }\n            ]\n        \n        }\n\n    ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n',normalizedContent:'# env\n\n# install software\n\nubuntu 20.4\n\nsudo apt-get install -y wget build-essential bison zlib1g-dev flex libglu1-mesa-dev libssl-dev libxml2-dev libboost-all-dev git g++ vim python-setuptools python-dev build-essential xutils-dev\n\npip3 install pyyaml plotly psutil\n\nin case you meet bug: release/cuda-sim/makefile.makedepend: no such file or directory\n\nsudo apt install xutils-dev\n\n# install cuda\n\nwget http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda_11.0.2_450.51.05_linux.run\n\nsudo sh cuda_11.0.2_450.51.05_linux.run\n\n# install driver\n\nsudo ubuntu-drivers devices\n\nsudo apt install nvidia-driver-535\n\nnvidia-smi\n\n\n\nnvcc --version\n\nnvcc: nvidia (r) cuda compiler driver\ncopyright (c) 2005-2020 nvidia corporation\nbuilt on thu_jun_11_22:26:38_pdt_2020\ncuda compilation tools, release 11.0, v11.0.194\nbuild cuda_11.0_bu.tc445_37.28540450_0\n\n# install cudnn\n\nwget https://developer.download.nvidia.com/compute/cudnn/9.0.0/local_installers/cudnn-local-repo-ubuntu2004-9.0.0_1.0-1_amd64.deb\n\nsudo dpkg -i cudnn-local-repo-ubuntu2004-9.0.0_1.0-1_amd64.deb\n\nsudo cp /var/cudnn-local-repo-ubuntu2004-9.0.0/cudnn-*-keyring.gpg /usr/share/keyrings/\n\nsudo apt-get update\n\nsudo apt-get -y install cudnn-cuda-12\n\n# gpu-app-collection\n\ngit clone https://github.com/accel-sim/gpu-app-collection.git\n\nmake -c ./gpu-app-collection/src data\n\ngpu app support compile of sm_10. compute_10, remove the versions below 50.\n\nfind ./gpu-app-collection/src/cuda/gpu_microbenchmark/* -name "makefile"|xargs sed -i "s/sm_30/sm_50/g"\n\nfind ./gpu-app-collection/src/cuda/gpu_microbenchmark/* -name "makefile"|xargs sed -i "s/compute_30/compute_50/g"\n\n./accel\n\n-----./accel-sim-framework\n\n-----./gpu-app-collection/src/cuda\n\n-----./gpu-app-collection/bin/11.0/release/binary\n\nin ./gpu-app-collection/src, you could use:\n\n * make lonestargpu-2.0\n * make gpu_microbenchmark\n * make deepbench\n * make rodinia-2.0-ft\n * make parboil\n * make shoc\n * make ispass-2009\n * make polybench\n\n# .bashrc\n\nexport path=/usr/local/cuda-11.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n\nexport ld_library_path=/usr/local/cuda-11.0/lib64\n\nexport cuda_install_path=/usr/local/cuda-11.0\n\nexport path=$cuda_install_path/bin:$path export gpuapps_root=/home/qishao/project/gpu_simulator/accel/gpu-app-collection\n\n# collect and run trace:\n\n./util/tracer_nvbit/run_hw_trace.py -b polybench -d 0\n\n./gpu-simulator/bin/release/accel-sim.out -trace ./hw_run/traces/device-0/11.0/nw-rodinia-2.0-ft/128_10___data_result_128_10_txt/traces/kernelslist.g -config ./gpu-simulator/gpgpu-sim/configs/tested-cfgs/sm7_qv100/gpgpusim.config -config ./gpu-simulator/configs/tested-cfgs/sm7_qv100/trace.config\n\n# pytorch docker\n\n# install pytorch gpu docker\n\ncurl -fssl https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg && curl -s -l https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed \'s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n\nsudo apt-get update\n\nsudo apt-get install -y nvidia-container-toolkit\n\nsudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker\n\n# run docker\n\nsudo docker run --gpus all -it --rm -v pwd:/workspace -e local_uid=$(id -u $user) -e local_gid=$(id -g $user) pinto0309/pytorch-build:11.3.0-cudnn8-devel-ubuntu20.04 bash\n\n# success fully pytorch compiled\n\nconda install python 3.12 version\n\nthis works for "239d87af5e5fecec452214a5e6e5e204b51c0597"\n\n\n\n# build with cuda and debug\n\ndebug=1 use_cuda=1 use_rocm=0 python setup.py develop\n\n# enable vscode python attach c++ debug\n\necho 0| sudo tee /proc/sys/kernel/yama/ptrace_scope sudo setcap cap_sys_ptrace=eip /usr/bin/gdb\n\nlaunch.json\n\n{\n    // use intellisense to learn about possible attributes.\n    // hover to view descriptions of existing attributes.\n    // for more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n    "version": "0.2.0",\n    "configurations": [\n        {\n            "name": "python debugger: current file",\n            "type": "debugpy",\n            "request": "launch",\n            "program": "${file}",\n            "console": "integratedterminal",\n            "justmycode": false,\n            "env": {\n                "pytorch_jit": "0",\n                "torch_compile_debug": "1",\n                "pytorch_debug": "1",\n                "pytorch_logs": "1",\n                "torch_show_cpp_stacktraces": "1",\n                "pythonpath": "/home/***/project/learn_pytorch/pytorch/build/:${pythonpath}",\n                "ld_library_path": "/home/***/project/learn_pytorch/pytorch/build/lib:/home/***/anaconda3/envs/py311/lib:/usr/local/cuda-11.8/targets/x86_64-linux/lib/:${env:ld_library_path}"\n            },\n            "envfile": "${workspacefolder}/.env"    \n        },\n        {\n            "name": "torchc++ debugger",\n            "type": "cppdbg",\n            "request": "attach",\n            "program": "/home/***/anaconda3/envs/py311/bin/python",\n            "processid": "${command:pickprocess}",\n            "mimode": "gdb",\n            "setupcommands": [\n                {\n                    "description": "enable pretty-printing for gdb",\n                    "text": "-enable-pretty-printing",\n                    "ignorefailures": true\n                }\n            ]\n        \n        }\n\n    ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n',charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"Understanding GPGPU-SIM 6 Memory Space",frontmatter:{title:"Understanding GPGPU-SIM 6 Memory Space",date:"2024-11-20T00:00:00.000Z",permalink:"/pages/45889/",tags:[null]},regularPath:"/03.gpu/30.gpgpusim.html",relativePath:"03.gpu/30.gpgpusim.md",key:"v-feef1d7a",path:"/pages/45889/",headers:[{level:3,title:"GPGPU-sim Memory Allocation",slug:"gpgpu-sim-memory-allocation",normalizedTitle:"gpgpu-sim memory allocation",charIndex:2}],headersStr:"GPGPU-sim Memory Allocation",content:'# GPGPU-sim Memory Allocation\n\n# Create global memory pool\n\nCode\n\n// @@@@@@ ./src/abstract_hardware_model.cc\n\ngpgpu_t::gpgpu_t(const gpgpu_functional_sim_config &config, gpgpu_context *ctx)\n    : m_function_model_config(config) {\n  gpgpu_ctx = ctx;\n  ...\n  m_global_mem = new memory_space_impl<4096>("global",64*1024, config.gddr_size);\n  m_tex_mem = new memory_space_impl<4096>("tex",64*1024);\n  m_surf_mem = new memory_space_impl<4096>("surf",64*1024);\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# Implementation\n\nIn gpu instantiation, the code "memory_space_impl<4096>("global",64*1024,)"\n\n4096 is page set, the variable "m_log2_block_size" will be 12 in this case.\n\nIt will further used to create pageIndex, when read or write operation operates on memory.\n\n * create page index\n * read/write memory_storage with offset\n\nCode\n\n// @@@@@@ src/cuda-sim/memory.h\n\n  // Key will be virtual page address, and mem_storage is corresponding page data\n  typedef mem_map<mem_addr_t, mem_storage<BSIZE> > map_t;\n  map_t m_data;\n\n// @@@@@@ src/cuda-sim/memory.cc\n// write operation\n\ntemplate <unsigned BSIZE>\nvoid memory_space_impl<BSIZE>::write(mem_addr_t addr, size_t length,\n                                     const void *data,\n                                     class ptx_thread_info *thd,\n                                     const ptx_instruction *pI) {\n  mem_addr_t index = addr >> m_log2_block_size;\n\n  if ((addr + length) <= (index + 1) * BSIZE) {\n    // fast route for intra-block access\n    unsigned offset = addr & (BSIZE - 1);\n    unsigned nbytes = length;\n    m_data[index].write(offset, nbytes, (const unsigned char *)data);\n  }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# When will this data be write or read?\n\nIt is implemented in instruction exectuion.\n\nAfter decode address space, when instruction is finally exectued, it read from/write into memory.\n\nCode\n\n// @@@@@@ src/cuda-sim/instruction.cc\n\n// read operation\nvoid ld_exec(const ptx_instruction *pI, ptx_thread_info *thread) {\n  ...\n  memory_space *mem = NULL;\n  decode_space(space, thread, src1, mem, addr);\n  ...\n  mem->read(addr, size / 8, &data.s64);\n}\n\n// store operation\nvoid st_impl(const ptx_instruction *pI, ptx_thread_info *thread) {\n  ...\n  memory_space *mem = NULL;\n  mem->write(addr, size / 8, &ptx_regs[0].s64, thread, pI);\n  ...\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',normalizedContent:'# gpgpu-sim memory allocation\n\n# create global memory pool\n\ncode\n\n// @@@@@@ ./src/abstract_hardware_model.cc\n\ngpgpu_t::gpgpu_t(const gpgpu_functional_sim_config &config, gpgpu_context *ctx)\n    : m_function_model_config(config) {\n  gpgpu_ctx = ctx;\n  ...\n  m_global_mem = new memory_space_impl<4096>("global",64*1024, config.gddr_size);\n  m_tex_mem = new memory_space_impl<4096>("tex",64*1024);\n  m_surf_mem = new memory_space_impl<4096>("surf",64*1024);\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# implementation\n\nin gpu instantiation, the code "memory_space_impl<4096>("global",64*1024,)"\n\n4096 is page set, the variable "m_log2_block_size" will be 12 in this case.\n\nit will further used to create pageindex, when read or write operation operates on memory.\n\n * create page index\n * read/write memory_storage with offset\n\ncode\n\n// @@@@@@ src/cuda-sim/memory.h\n\n  // key will be virtual page address, and mem_storage is corresponding page data\n  typedef mem_map<mem_addr_t, mem_storage<bsize> > map_t;\n  map_t m_data;\n\n// @@@@@@ src/cuda-sim/memory.cc\n// write operation\n\ntemplate <unsigned bsize>\nvoid memory_space_impl<bsize>::write(mem_addr_t addr, size_t length,\n                                     const void *data,\n                                     class ptx_thread_info *thd,\n                                     const ptx_instruction *pi) {\n  mem_addr_t index = addr >> m_log2_block_size;\n\n  if ((addr + length) <= (index + 1) * bsize) {\n    // fast route for intra-block access\n    unsigned offset = addr & (bsize - 1);\n    unsigned nbytes = length;\n    m_data[index].write(offset, nbytes, (const unsigned char *)data);\n  }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# when will this data be write or read?\n\nit is implemented in instruction exectuion.\n\nafter decode address space, when instruction is finally exectued, it read from/write into memory.\n\ncode\n\n// @@@@@@ src/cuda-sim/instruction.cc\n\n// read operation\nvoid ld_exec(const ptx_instruction *pi, ptx_thread_info *thread) {\n  ...\n  memory_space *mem = null;\n  decode_space(space, thread, src1, mem, addr);\n  ...\n  mem->read(addr, size / 8, &data.s64);\n}\n\n// store operation\nvoid st_impl(const ptx_instruction *pi, ptx_thread_info *thread) {\n  ...\n  memory_space *mem = null;\n  mem->write(addr, size / 8, &ptx_regs[0].s64, thread, pi);\n  ...\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"GPU Insturctions",frontmatter:{title:"GPU Insturctions",date:"2025-04-02T00:00:00.000Z",permalink:"/pages/45890/",tags:[null]},regularPath:"/03.gpu/31.gpu_inst.html",relativePath:"03.gpu/31.gpu_inst.md",key:"v-072788bb",path:"/pages/45890/",headers:[{level:2,title:"1. [20] Decoding CUDA Binary",slug:"_1-20-decoding-cuda-binary",normalizedTitle:"1. [20] decoding cuda binary",charIndex:1},{level:3,title:"Compiling Flow",slug:"compiling-flow",normalizedTitle:"compiling flow",charIndex:109},{level:3,title:"Assemble Code",slug:"assemble-code",normalizedTitle:"assemble code",charIndex:427},{level:3,title:"Load/Store Instruction and Control Flow of Divergence",slug:"load-store-instruction-and-control-flow-of-divergence",normalizedTitle:"load/store instruction and control flow of divergence",charIndex:466},{level:3,title:"Compile-Time Scheduling",slug:"compile-time-scheduling",normalizedTitle:"compile-time scheduling",charIndex:526},{level:3,title:"Reverse Engineering Tool",slug:"reverse-engineering-tool",normalizedTitle:"reverse engineering tool",charIndex:2013},{level:3,title:"Instruction Format Basics",slug:"instruction-format-basics",normalizedTitle:"instruction format basics",charIndex:2065},{level:3,title:"Predicate",slug:"predicate",normalizedTitle:"predicate",charIndex:2428}],headersStr:"1. [20] Decoding CUDA Binary Compiling Flow Assemble Code Load/Store Instruction and Control Flow of Divergence Compile-Time Scheduling Reverse Engineering Tool Instruction Format Basics Predicate",content:" 1. [20] Decoding CUDA Binary\n\n----------------------------------------\n\n\n# 1. [20] Decoding CUDA Binary\n\n\n# Compiling Flow\n\nWhen every thread in the warp has reached a re-convergence command - either a .S modifier or a SYNC instruction, depending on the architecture - it will wait until the thread warp reaches the instruction whose address is specified by the SSY instruction, and then return to running in lock-step.\n\n\n\n\n# Assemble Code\n\n64 bits or 128 bits\n\n\n# Load/Store Instruction and Control Flow of Divergence\n\n\n\n\n# Compile-Time Scheduling\n\nAs of Compute Capability 3.0, instruction scheduling is handled by the compiler rather than by the hardware.\n\nOn this architecture every 8−th instruction, rather than being a real instruction, is a set of scheduling codes inserted by the compiler.\n\nThese scheduling codes dictate the minimum number of cycles that the thread must wait between every two consecutive instructions in the following seven instructions in order to satisfy dependence constraints.\n\nStarting with Compute Capability 5.0, NVIDIA moved even more control logic away from the hardware, saving power and space.\n\nThus instruction-level barrier has been added to the scheduling codes generated by the compiler.\n\nThe scheduling codes on Compute Capabilities 5.x and 6.x occur in place of every fourth instruction.\n\nAs of Compute Capability 7.0, they are embedded into each individual instruction, rather than controlling larger blocks of instructions.\n\n# Instruction with Operand\n\n\n\nAlthough instructions are of fixed length, NVIDIA’s instruction sets lack the relative simplicity of a RISC architecture.\n\nIt includes complicated instructions such as multiplication-and-addition, multi-function operation that performs trigonometric functions including sine and cosine, and so on.\n\nAlthough we can make generalizations about which bits are used for which components of the instruction, there are few consistent rules across different instructions.\n\nCheck PSETP, it has 3 source operands.\n\n\n\n\n# Reverse Engineering Tool\n\nnvdisasm and sass2ptx\n\n\n# Instruction Format Basics\n\n# Instruction Length:\n\nNVIDIA GPU instructions are typically 8 or 16 bytes in length (i.e., 64 or 128 bits), depending on the generation and specific instruction.\n\nMost common instructions are encoded in 8 bytes, but certain instructions may require 16 bytes for additional fields (larger immediate values, special modifiers, etc.).\n\n# Predicate Bits:\n\nEach instruction can be conditionally executed based on a predicate register (e.g., @P0 or @!P0).\n\nThe instruction encoding typically reserves a few bits for specifying which predicate is used, whether it’s negated, and whether the instruction updates that predicate or only tests it.\n\n# Opcode and Sub-Op Fields:\n\nA chunk of bits is used to identify the primary operation (e.g., FADD for floating-point add, IMUL for integer multiply, LDG for global memory load, etc.).\n\nSome instructions have “sub-ops” or “specialization bits” that further refine the operation (e.g., specifying data type, rounding mode, or variant of the operation).\n\n# Source and Destination Registers:\n\nSASS instructions typically encode up to four source operands and one or two destinations (though most commonly one destination).\n\nThe register indices (e.g., R0, R1, R2, etc.) appear in dedicated fields.\n\nDepending on the instruction, immediate operands (e.g., a constant offset) may replace a register operand.\n\n# Modifiers and Flags:\n\nMany instructions have bits for modifiers (e.g., .CC to set condition codes, .SAT to enable saturation, etc.).\n\nReuse flags (discussed in your previous question) are also stored in a few bits in the encoding.\n\nAdditional bits might control things like whether a memory operation is cache-specific (.E for eviction policy, .L1 or .L2 usage, etc.), or whether an instruction is uniform across a warp, and so on.\n\n# Scheduling Information:\n\nModern NVIDIA architectures embed scheduling information in the instruction to help the hardware’s instruction scheduler. You might see references to “stall” counts or “read dependency” codes. In short:\n\nA few bits can indicate how many cycles to wait before reading certain registers, or how many cycles to wait before issuing the next instruction.\n\nThis is sometimes referred to as “scheduling” or “scoreboarding” fields.\n\n# Operand Encoding and Immediate Values\n\nRegister Operands: Typically specified by a field that directly encodes the register number (e.g., 7 bits for the register index if up to 128 registers).\n\nImmediate Operands: Some instructions support small inline immediates.\n\nThe immediate field is part of the instruction encoding, using a certain number of bits.\n\nIf the immediate is too large to fit, a 16-byte (128-bit) encoding might be used, or the compiler may materialize the immediate in a register first.\n\nAddressing Modes: Memory instructions (LDG, STG, LDS, etc.) often encode an offset or a base+offset form. Some bits specify how to interpret those, e.g., 8-bit or 20-bit offset, sign extension, scaled by data type size, etc.\n\n# Reuse Flags and the 2-Way Associative CAM\n\nReuse Flags: Each instruction can mark which of its first four source registers should be saved in a small local cache, so that subsequent instructions can reuse them without accessing the main register file.\n\nThese flags are 4 bits in the SASS encoding (one per operand), typically in the lower part of the 8-byte instruction encoding.\n\nThis is a micro-architectural feature that helps reduce register file pressure and bank conflicts.\n\n# Predicate and Condition Code Fields\n\nOne or two bits designate whether an instruction is predicated (e.g., @P0, @!P0, etc.).\n\nAnother small field may specify which predicate register is used (since GPUs can have multiple predicate registers).\n\nSome instructions also set condition codes (e.g., for subsequent instructions to test), which the hardware might encode in a “condition code” sub-field.\n\n# Example Layout (Hypothetical)\n\nBelow is a hypothetical 64-bit (8-byte) SASS instruction breakdown (not official, but a conceptual approximation):\n\n\n\n * Opcode (7 bits): Identifies the core instruction (e.g., FADD, IMUL, LDG).\n * Source Registers (5 bits each): Up to three or four sources, each needing enough bits to address the register file.\n * Destination Register (5 bits): Usually one, possibly two in some instructions (like a multiply-add that writes an extra output).\n * Modifiers / Flags: Several bits for controlling instruction behavior (e.g., rounding modes, type specifiers, etc.).\n * Reuse Flags (4 bits): One bit per operand position, marking which registers to cache.\n * Predicate / Condition Code: Often stored in either the high bits or low bits, depending on generation.\n * Scheduling Info: Usually a small field that the compiler sets to help with instruction issuing/stalling.\n\nDifferent architectures shift these fields around or allocate more/less bits, but the principle remains similar.\n\n\n# Predicate\n\nNVIDIA GPUs use predicate registers and explicit branch instructions (e.g., BRA) to handle conditional logic at the SASS (assembly) level.\n\nIn concert with the hardware’s warp execution model, this mechanism can create control flow divergence when different threads of the same warp take different paths.\n\nBelow is a more detailed explanation of how predicates, branching, and divergence work together.\n\n# The Basic Idea of Predication\n\nA predicate (e.g., P0, P1, etc.) is a 1-bit register that can be set or cleared by a comparison instruction. For example:\n\nPSETP.EQ.U32 P0, PT, R4, RZ, PT;\n\n\n1\n\n\nThis sets predicate P0 to 1 if R4 == 0; otherwise P0 = 0.\n\nPT means “always pass” (no predicate on that comparison itself).\n\nRZ is the “zero register.”\n\nOnce a predicate is set, any subsequent instruction can be predicated—i.e., guarded—by referencing that predicate. For example:\n\n@P0 IADD R5, R5, R6;\n\n\n1\n\n\nReads as: “Perform IADD R5, R5, R6 only if P0 == 1; otherwise do nothing.”\n\nHowever, you can also have a predicated branch:\n\n@P0 BRA 0x210;\n\n\n1\n\n\nMeans: “If P0 == 1, jump to the instruction at address 0x210; if P0 == 0, continue sequentially.”\n\n# Divergence and the Active Mask\n\nA key point about GPU execution is that an entire warp (32 threads on most NVIDIA hardware) executes in lockstep on a single instruction stream. If a branch is taken by some threads but not others, the warp must diverge:\n\n * The hardware splits the warp’s threads into multiple “subsets,” one subset that takes the branch and another that doesn’t.\n * The warp serially executes each subset’s path, with the other subset of threads masked out (inactive).\n\nAt the end, the warp reconverges at a known instruction (e.g., the instruction pointed to by SSY).\n\nThis is how a single warp can handle different control flow paths for its 32 threads.\n\n# The Role of SSY (Set SYnchronization)\n\nWhen the compiler emits:\n\nSSY 0x238\n@P0 BRA 0x210\n...\nSYNC\n\n\n1\n2\n3\n4\n\n\nSSY 0x238 instructs the hardware to push a reconvergence point (address 0x238) onto the hardware’s “divergence stack.”\n\nThe next branch—@P0 BRA 0x210—can cause divergence: some threads branch, others continue.\n\nAfter each subset of threads has finished, the hardware automatically goes to 0x238 (the SSY target) to reconverge the warp, so all threads proceed together again.\n\n# Using Predicates to Create Conditional Branches\n\nExample Flow Let’s say your high-level code is:\n\nif (tid != 0) {\n    sum += 4;\n}\n\n\n1\n2\n3\n\n\nThe compiler might produce:\n\n// 1) Compare tid != 0, store result in P0\nPSETP.NE.U32 P0, PT, R9, RZ, PT;  // if (R9 != 0) P0=1; else P0=0\n\n// 2) Set the reconvergence point after the IF block\nSSY targetAddr\n\n// 3) Predicated branch: jump if P0=1\n@P0 BRA insideIf\n\n// (fallthrough path: if P0=0, skip the IF block)\nBRA endIf   // or a SYNC, depending on code structure\n\ninsideIf:\n IADD32I R8, R8, 4;   // sum += 4\n SYNC\n\nendIf:\n// warp reconverges here\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\nHere, the hardware uses P0 to decide which threads branch and which do not. The warp may need multiple passes if some threads are P0=1 and others are P0=0.\n\n# (tid) and (tid % 4): Why You Don’t Always See Explicit Instructions\n\nIn high-level CUDA (C/C++), you write:\n\nint tid = threadIdx.x;\nif (tid != 0) ...\nif (tid % 4 == 0) ...\n\n\n1\n2\n3\n\n\nBut the final SASS might not show a direct “integer modulo” or “compare to zero” instruction. Instead:\n\n * Thread ID is Already in a Register\n   \n   * At the start of the kernel, the compiler loads threadIdx.x into a register (R9 or something).\n   * That code might be hoisted well above the snippet you’re looking at.\n\n * Modulo 4 = Bitwise Test\n   \n   * tid % 4 == 0 is the same as (tid & 3) == 0.\n   * The compiler can use a single bitwise AND or LOP3 (logical operation) to check the two least significant bits. Then it sets a predicate based on the result.\n   * So you’ll see something like LOP3.LUT P1, R9, RZ, ... 0x... or I2I P1, R9, AND, 3 or some variant that directly sets P1.\n\n * Predicate is Checked at Branch\n   \n   * Instead of a standalone branch if (tid != 0), the hardware does PSETP.NE.U32 P0, R9, RZ followed by @P0 BRA label.\n   * This merges “compare” + “branch” logic with the warp’s active mask concept.\n\nHence, you rarely see an explicit “%4” machine instruction or a standalone CMP R9 != 0 or “TID instruction.”\n\nThe predicate logic folds these checks into specialized SASS instructions that set P0 or P1, and then uses predicated branching or predicated instructions.\n\n# Putting It All Together\n\nPredicate Computation A special compare (e.g., PSETP) sets P0 based on (tid != 0), (tid & 3) == 0, etc. Control Divergence If some threads in the warp have P0=1 and others have P0=0, the warp diverges when it hits @P0 BRA somewhere.\n\nReconvergence: An SSY target before the branch and a SYNC (or matching BRA) after the branch help the hardware manage warp subsets and eventually bring them back together at the same program counter.\n\nIn short, predicates let each thread in a warp conditionally execute code. If different threads disagree on the condition, the warp temporarily serializes the different paths but eventually merges (reconverges) again at an SSY target. This is how GPUs handle “if” statements, loops, etc., across thousands of parallel threads.",normalizedContent:" 1. [20] decoding cuda binary\n\n----------------------------------------\n\n\n# 1. [20] decoding cuda binary\n\n\n# compiling flow\n\nwhen every thread in the warp has reached a re-convergence command - either a .s modifier or a sync instruction, depending on the architecture - it will wait until the thread warp reaches the instruction whose address is specified by the ssy instruction, and then return to running in lock-step.\n\n\n\n\n# assemble code\n\n64 bits or 128 bits\n\n\n# load/store instruction and control flow of divergence\n\n\n\n\n# compile-time scheduling\n\nas of compute capability 3.0, instruction scheduling is handled by the compiler rather than by the hardware.\n\non this architecture every 8−th instruction, rather than being a real instruction, is a set of scheduling codes inserted by the compiler.\n\nthese scheduling codes dictate the minimum number of cycles that the thread must wait between every two consecutive instructions in the following seven instructions in order to satisfy dependence constraints.\n\nstarting with compute capability 5.0, nvidia moved even more control logic away from the hardware, saving power and space.\n\nthus instruction-level barrier has been added to the scheduling codes generated by the compiler.\n\nthe scheduling codes on compute capabilities 5.x and 6.x occur in place of every fourth instruction.\n\nas of compute capability 7.0, they are embedded into each individual instruction, rather than controlling larger blocks of instructions.\n\n# instruction with operand\n\n\n\nalthough instructions are of fixed length, nvidia’s instruction sets lack the relative simplicity of a risc architecture.\n\nit includes complicated instructions such as multiplication-and-addition, multi-function operation that performs trigonometric functions including sine and cosine, and so on.\n\nalthough we can make generalizations about which bits are used for which components of the instruction, there are few consistent rules across different instructions.\n\ncheck psetp, it has 3 source operands.\n\n\n\n\n# reverse engineering tool\n\nnvdisasm and sass2ptx\n\n\n# instruction format basics\n\n# instruction length:\n\nnvidia gpu instructions are typically 8 or 16 bytes in length (i.e., 64 or 128 bits), depending on the generation and specific instruction.\n\nmost common instructions are encoded in 8 bytes, but certain instructions may require 16 bytes for additional fields (larger immediate values, special modifiers, etc.).\n\n# predicate bits:\n\neach instruction can be conditionally executed based on a predicate register (e.g., @p0 or @!p0).\n\nthe instruction encoding typically reserves a few bits for specifying which predicate is used, whether it’s negated, and whether the instruction updates that predicate or only tests it.\n\n# opcode and sub-op fields:\n\na chunk of bits is used to identify the primary operation (e.g., fadd for floating-point add, imul for integer multiply, ldg for global memory load, etc.).\n\nsome instructions have “sub-ops” or “specialization bits” that further refine the operation (e.g., specifying data type, rounding mode, or variant of the operation).\n\n# source and destination registers:\n\nsass instructions typically encode up to four source operands and one or two destinations (though most commonly one destination).\n\nthe register indices (e.g., r0, r1, r2, etc.) appear in dedicated fields.\n\ndepending on the instruction, immediate operands (e.g., a constant offset) may replace a register operand.\n\n# modifiers and flags:\n\nmany instructions have bits for modifiers (e.g., .cc to set condition codes, .sat to enable saturation, etc.).\n\nreuse flags (discussed in your previous question) are also stored in a few bits in the encoding.\n\nadditional bits might control things like whether a memory operation is cache-specific (.e for eviction policy, .l1 or .l2 usage, etc.), or whether an instruction is uniform across a warp, and so on.\n\n# scheduling information:\n\nmodern nvidia architectures embed scheduling information in the instruction to help the hardware’s instruction scheduler. you might see references to “stall” counts or “read dependency” codes. in short:\n\na few bits can indicate how many cycles to wait before reading certain registers, or how many cycles to wait before issuing the next instruction.\n\nthis is sometimes referred to as “scheduling” or “scoreboarding” fields.\n\n# operand encoding and immediate values\n\nregister operands: typically specified by a field that directly encodes the register number (e.g., 7 bits for the register index if up to 128 registers).\n\nimmediate operands: some instructions support small inline immediates.\n\nthe immediate field is part of the instruction encoding, using a certain number of bits.\n\nif the immediate is too large to fit, a 16-byte (128-bit) encoding might be used, or the compiler may materialize the immediate in a register first.\n\naddressing modes: memory instructions (ldg, stg, lds, etc.) often encode an offset or a base+offset form. some bits specify how to interpret those, e.g., 8-bit or 20-bit offset, sign extension, scaled by data type size, etc.\n\n# reuse flags and the 2-way associative cam\n\nreuse flags: each instruction can mark which of its first four source registers should be saved in a small local cache, so that subsequent instructions can reuse them without accessing the main register file.\n\nthese flags are 4 bits in the sass encoding (one per operand), typically in the lower part of the 8-byte instruction encoding.\n\nthis is a micro-architectural feature that helps reduce register file pressure and bank conflicts.\n\n# predicate and condition code fields\n\none or two bits designate whether an instruction is predicated (e.g., @p0, @!p0, etc.).\n\nanother small field may specify which predicate register is used (since gpus can have multiple predicate registers).\n\nsome instructions also set condition codes (e.g., for subsequent instructions to test), which the hardware might encode in a “condition code” sub-field.\n\n# example layout (hypothetical)\n\nbelow is a hypothetical 64-bit (8-byte) sass instruction breakdown (not official, but a conceptual approximation):\n\n\n\n * opcode (7 bits): identifies the core instruction (e.g., fadd, imul, ldg).\n * source registers (5 bits each): up to three or four sources, each needing enough bits to address the register file.\n * destination register (5 bits): usually one, possibly two in some instructions (like a multiply-add that writes an extra output).\n * modifiers / flags: several bits for controlling instruction behavior (e.g., rounding modes, type specifiers, etc.).\n * reuse flags (4 bits): one bit per operand position, marking which registers to cache.\n * predicate / condition code: often stored in either the high bits or low bits, depending on generation.\n * scheduling info: usually a small field that the compiler sets to help with instruction issuing/stalling.\n\ndifferent architectures shift these fields around or allocate more/less bits, but the principle remains similar.\n\n\n# predicate\n\nnvidia gpus use predicate registers and explicit branch instructions (e.g., bra) to handle conditional logic at the sass (assembly) level.\n\nin concert with the hardware’s warp execution model, this mechanism can create control flow divergence when different threads of the same warp take different paths.\n\nbelow is a more detailed explanation of how predicates, branching, and divergence work together.\n\n# the basic idea of predication\n\na predicate (e.g., p0, p1, etc.) is a 1-bit register that can be set or cleared by a comparison instruction. for example:\n\npsetp.eq.u32 p0, pt, r4, rz, pt;\n\n\n1\n\n\nthis sets predicate p0 to 1 if r4 == 0; otherwise p0 = 0.\n\npt means “always pass” (no predicate on that comparison itself).\n\nrz is the “zero register.”\n\nonce a predicate is set, any subsequent instruction can be predicated—i.e., guarded—by referencing that predicate. for example:\n\n@p0 iadd r5, r5, r6;\n\n\n1\n\n\nreads as: “perform iadd r5, r5, r6 only if p0 == 1; otherwise do nothing.”\n\nhowever, you can also have a predicated branch:\n\n@p0 bra 0x210;\n\n\n1\n\n\nmeans: “if p0 == 1, jump to the instruction at address 0x210; if p0 == 0, continue sequentially.”\n\n# divergence and the active mask\n\na key point about gpu execution is that an entire warp (32 threads on most nvidia hardware) executes in lockstep on a single instruction stream. if a branch is taken by some threads but not others, the warp must diverge:\n\n * the hardware splits the warp’s threads into multiple “subsets,” one subset that takes the branch and another that doesn’t.\n * the warp serially executes each subset’s path, with the other subset of threads masked out (inactive).\n\nat the end, the warp reconverges at a known instruction (e.g., the instruction pointed to by ssy).\n\nthis is how a single warp can handle different control flow paths for its 32 threads.\n\n# the role of ssy (set synchronization)\n\nwhen the compiler emits:\n\nssy 0x238\n@p0 bra 0x210\n...\nsync\n\n\n1\n2\n3\n4\n\n\nssy 0x238 instructs the hardware to push a reconvergence point (address 0x238) onto the hardware’s “divergence stack.”\n\nthe next branch—@p0 bra 0x210—can cause divergence: some threads branch, others continue.\n\nafter each subset of threads has finished, the hardware automatically goes to 0x238 (the ssy target) to reconverge the warp, so all threads proceed together again.\n\n# using predicates to create conditional branches\n\nexample flow let’s say your high-level code is:\n\nif (tid != 0) {\n    sum += 4;\n}\n\n\n1\n2\n3\n\n\nthe compiler might produce:\n\n// 1) compare tid != 0, store result in p0\npsetp.ne.u32 p0, pt, r9, rz, pt;  // if (r9 != 0) p0=1; else p0=0\n\n// 2) set the reconvergence point after the if block\nssy targetaddr\n\n// 3) predicated branch: jump if p0=1\n@p0 bra insideif\n\n// (fallthrough path: if p0=0, skip the if block)\nbra endif   // or a sync, depending on code structure\n\ninsideif:\n iadd32i r8, r8, 4;   // sum += 4\n sync\n\nendif:\n// warp reconverges here\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\nhere, the hardware uses p0 to decide which threads branch and which do not. the warp may need multiple passes if some threads are p0=1 and others are p0=0.\n\n# (tid) and (tid % 4): why you don’t always see explicit instructions\n\nin high-level cuda (c/c++), you write:\n\nint tid = threadidx.x;\nif (tid != 0) ...\nif (tid % 4 == 0) ...\n\n\n1\n2\n3\n\n\nbut the final sass might not show a direct “integer modulo” or “compare to zero” instruction. instead:\n\n * thread id is already in a register\n   \n   * at the start of the kernel, the compiler loads threadidx.x into a register (r9 or something).\n   * that code might be hoisted well above the snippet you’re looking at.\n\n * modulo 4 = bitwise test\n   \n   * tid % 4 == 0 is the same as (tid & 3) == 0.\n   * the compiler can use a single bitwise and or lop3 (logical operation) to check the two least significant bits. then it sets a predicate based on the result.\n   * so you’ll see something like lop3.lut p1, r9, rz, ... 0x... or i2i p1, r9, and, 3 or some variant that directly sets p1.\n\n * predicate is checked at branch\n   \n   * instead of a standalone branch if (tid != 0), the hardware does psetp.ne.u32 p0, r9, rz followed by @p0 bra label.\n   * this merges “compare” + “branch” logic with the warp’s active mask concept.\n\nhence, you rarely see an explicit “%4” machine instruction or a standalone cmp r9 != 0 or “tid instruction.”\n\nthe predicate logic folds these checks into specialized sass instructions that set p0 or p1, and then uses predicated branching or predicated instructions.\n\n# putting it all together\n\npredicate computation a special compare (e.g., psetp) sets p0 based on (tid != 0), (tid & 3) == 0, etc. control divergence if some threads in the warp have p0=1 and others have p0=0, the warp diverges when it hits @p0 bra somewhere.\n\nreconvergence: an ssy target before the branch and a sync (or matching bra) after the branch help the hardware manage warp subsets and eventually bring them back together at the same program counter.\n\nin short, predicates let each thread in a warp conditionally execute code. if different threads disagree on the condition, the warp temporarily serializes the different paths but eventually merges (reconverges) again at an ssy target. this is how gpus handle “if” statements, loops, etc., across thousands of parallel threads.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"checkpoint",frontmatter:{title:"checkpoint",date:"2023-11-09T15:32:49.000Z",permalink:"/pages/cc7035/",tags:[null]},regularPath:"/04.cpu/01.checkpoint.html",relativePath:"04.cpu/01.checkpoint.md",key:"v-121d90a9",path:"/pages/cc7035/",headersStr:null,content:"An Analysis of a Resource Efficient Checkpoint Architecture [Intel]\n\nThe main part that I like is the discussion\n\n(1) Using map table checkpoints. Map table checkpoints are created periodically either at every branch or every few cycles [Leibholz and Razdan 1997; Yeager 1996]. On a misprediction, the checkpoint corresponding to the mispredicted branch is restored. The number of checkpoints limits the number of unresolved branches allowed in the instruction window.\n\n(2) Using the retirement map table (RMAP). In this scheme, a retirement map table [Hinton et al. 2001] is used in addition to the frontend map table. Each ROB entry also has the rename map for its corresponding instruction. Once a misprediction is resolved, the mispredicted branch is allowed to reach the head of the ROB at which time the retirement map table will have the correct map table corresponding to the mispredicted branch. At this point, the retirement map table is copied to the frontend map table, after which renaming can start. Since all instructions prior to the mispredicted branch must be retired before renaming can start, this scheme can lead to significant delays if long latency operations prior to the mispredicted branch stall retirement.\n\n(3) Using the retirement map table and the ROB (RMAP+WALK). This scheme is an optimization on the scheme above. Instead of waiting for the mispredicted branch to reach the head of the ROB, we start with the current retirement map table and pro-actively walk from the head of the ROB toward the mispredicted branch, incorporating the rename information of each ROB entry. This allows renaming of correct path instructions to commence without waiting for all instructions prior to the mispredicted branch to retire. (4) Using the frontend map table and a history buffer (HBMAP+WALK). In this scheme, a history buffer is used to store overwritten maps of each instruction. On a branch misprediction, we start with the current frontend map table. We pro-actively walk from the current tail of the ROB (i.e., the most recently allocated instruction) toward the mispredicted branch, incorporating the overwritten maps of each instruction. Depending on whether the mispredicted branch is closer to the ROB head or ROB tail, RMAP + WALK, or HBMAP + WALK will perform better.\n\nIn short:\n\n1. checkpoint generated at the moment of decoding branch instruction. Recover at detection of missprediction.\n\n2. use retire map table(RMAP). wait the commit of missprediction instruction, then rewrite the RAT(remap alias table) with RMAP\n\n3. RMAP + WALK. Restart from the moment that miss prediction is detected, copy the RMAP into RAT and then modify RMAP with ROB remapping, until we get to the miss predicted intruction.\n\n4. HBMAP + WALK. Start from frontend RAT(FRAT), use history buffer to recover the overwritten register relation.\n\nRMAP+WALK utilize commited RMAP, thus it walks from the head of ROB (oldest) to the branch instruction. HBMAP+WALK utilize frontend RAT, thus it walks from the end of ROB (youngest) to the branch instruction.",normalizedContent:"an analysis of a resource efficient checkpoint architecture [intel]\n\nthe main part that i like is the discussion\n\n(1) using map table checkpoints. map table checkpoints are created periodically either at every branch or every few cycles [leibholz and razdan 1997; yeager 1996]. on a misprediction, the checkpoint corresponding to the mispredicted branch is restored. the number of checkpoints limits the number of unresolved branches allowed in the instruction window.\n\n(2) using the retirement map table (rmap). in this scheme, a retirement map table [hinton et al. 2001] is used in addition to the frontend map table. each rob entry also has the rename map for its corresponding instruction. once a misprediction is resolved, the mispredicted branch is allowed to reach the head of the rob at which time the retirement map table will have the correct map table corresponding to the mispredicted branch. at this point, the retirement map table is copied to the frontend map table, after which renaming can start. since all instructions prior to the mispredicted branch must be retired before renaming can start, this scheme can lead to significant delays if long latency operations prior to the mispredicted branch stall retirement.\n\n(3) using the retirement map table and the rob (rmap+walk). this scheme is an optimization on the scheme above. instead of waiting for the mispredicted branch to reach the head of the rob, we start with the current retirement map table and pro-actively walk from the head of the rob toward the mispredicted branch, incorporating the rename information of each rob entry. this allows renaming of correct path instructions to commence without waiting for all instructions prior to the mispredicted branch to retire. (4) using the frontend map table and a history buffer (hbmap+walk). in this scheme, a history buffer is used to store overwritten maps of each instruction. on a branch misprediction, we start with the current frontend map table. we pro-actively walk from the current tail of the rob (i.e., the most recently allocated instruction) toward the mispredicted branch, incorporating the overwritten maps of each instruction. depending on whether the mispredicted branch is closer to the rob head or rob tail, rmap + walk, or hbmap + walk will perform better.\n\nin short:\n\n1. checkpoint generated at the moment of decoding branch instruction. recover at detection of missprediction.\n\n2. use retire map table(rmap). wait the commit of missprediction instruction, then rewrite the rat(remap alias table) with rmap\n\n3. rmap + walk. restart from the moment that miss prediction is detected, copy the rmap into rat and then modify rmap with rob remapping, until we get to the miss predicted intruction.\n\n4. hbmap + walk. start from frontend rat(frat), use history buffer to recover the overwritten register relation.\n\nrmap+walk utilize commited rmap, thus it walks from the head of rob (oldest) to the branch instruction. hbmap+walk utilize frontend rat, thus it walks from the end of rob (youngest) to the branch instruction.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"load store unit",frontmatter:{title:"load store unit",date:"2024-07-15T15:32:49.000Z",permalink:"/pages/cc7037/",tags:[null]},regularPath:"/04.cpu/03.loadstore.html",relativePath:"04.cpu/03.loadstore.md",key:"v-891a4db6",path:"/pages/cc7037/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Trend",frontmatter:{title:"Trend",date:"2024-07-15T15:32:49.000Z",permalink:"/pages/cc7036/",tags:[null]},regularPath:"/04.cpu/02.trend.html",relativePath:"04.cpu/02.trend.md",key:"v-23804245",path:"/pages/cc7036/",headersStr:null,content:"Some Good Paper pending to be read\n\n 1. Microarchitectural comparison and in-core modeling of state-of-the-art CPUs: Grace, Sapphire Rapids, and Genoa\n 2. [128] Summarizing CPU and GPU Design Trends with Product Data\n 3. [41 Year:2001] Microarchitectural Innovations: Boosting Microprocessor Performance Beyond Semiconductor Technology Scaling\n 4. [4 Year:2024] Fifty Years of ISCA: A data-driven retrospective on key trends 👍 👍 👍 👍 👍\n 5. [18 Year:2018] Trends in Processor Architecture\n 6. [306 Year: 2022] Compute Trends Across Three Eras of Machine Learning\n 7. [23 Year: 2020] Rebasing Instruction Prefetching An Industry Perspective",normalizedContent:"some good paper pending to be read\n\n 1. microarchitectural comparison and in-core modeling of state-of-the-art cpus: grace, sapphire rapids, and genoa\n 2. [128] summarizing cpu and gpu design trends with product data\n 3. [41 year:2001] microarchitectural innovations: boosting microprocessor performance beyond semiconductor technology scaling\n 4. [4 year:2024] fifty years of isca: a data-driven retrospective on key trends 👍 👍 👍 👍 👍\n 5. [18 year:2018] trends in processor architecture\n 6. [306 year: 2022] compute trends across three eras of machine learning\n 7. [23 year: 2020] rebasing instruction prefetching an industry perspective",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"cache & bank structure",frontmatter:{title:"cache & bank structure",date:"2024-07-16T15:32:49.000Z",permalink:"/pages/cc7038/",tags:[null]},regularPath:"/04.cpu/05.cache%20structure.html",relativePath:"04.cpu/05.cache structure.md",key:"v-64b7ef30",path:"/pages/cc7038/",headers:[{level:3,title:"Cache",slug:"cache",normalizedTitle:"cache",charIndex:2},{level:3,title:"Timing",slug:"timing",normalizedTitle:"timing",charIndex:4764},{level:3,title:"References",slug:"references",normalizedTitle:"references",charIndex:5151}],headersStr:"Cache Timing References",content:"# Cache\n\n# Structure\n\n[1]\n\n[1]\n\n * Bank - A memory structure that consists of a data and a tag array. A cache is typically split into multiple banks and CACTI assumes enough bandwidth so that these banks can be accessed simultaneously. The network topology that interconnects these banks can vary depending on the cache model.\n * Sub-arrays - A data or tag array is divided into a number of sub-arrays to reduce the delay due to wordline and bitline. Unlike banks, at any given time, these sub-arrays support only one single access. The total number of sub-arrays in a cache is equal to the product of Ndwl and Ndbl.\n * Mat - A group of four sub-arrays (2x2) that share a common central predecoder. CACTI’s exhaustive search starts from a minimum of at least one mat.\n * Sub-bank - In a typical cache, a cache block is scattered across multiple sub-arrays to improve the reliability of a cache. Irrespective of the cache organization, CACTI assumes that every cache block in a cache is distributed across an entire row of mats and the row number corresponding to a particular block is determined based on the block address. Each row (of mats) in an array is referred to as a sub-bank.\n\n# why do we needs bank\n\n[2] Support for more than one access to a memory structure at the same time can be provided by adding more ports to each memory cell.\n\nMultiporting a cache makes the cache capable of handling as many read or write requests as ports of that type.\n\nBut multiporting a cache makes each bit cell much bigger and so the overall area of the memory array can increase enormously for large number of ports. The extra length spanned by the cache also adds directly to the access time and power consumed by the cache.\n\nAnother way of supporting simultaneous multiple accesses to a cache is by banking with fully independent banks.\n\nEach bank does not share address and data.\n\nBanking also adds the decoding overhead of routing the appropriate address to the right bank and detecting collisions.\n\n[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n# How to supply 64Byte at each time\n\nEach sub-array outputs 128 bits, all 4 sub-array in the mat output 512bit together. [3]\n\n[4]\n\nA bank consists of Nsubbanks of identical sub-banks that are activated sequentially with each access. In turn, each sub-bank contains a number of identical mats.\n\nA mat is a self-contained, compact memory array composed of four identical sub-arrays, with Nrows rows and Ncols columns, and accompanying predecoding logic, with each sub-array being a two-dimensional matrix of memory cells and associated peripheral circuitry.\n\nEach mat holds a portion of a word in one of its four sub-arrays; during cache access, all mats in a sub-bank are activated to form the whole word.\n\nBy whole word, he means a cacheline.\n\nH-tree distribution networks are often used to deliver address and data to mats.\n\nIn the following figure, each sub-array outputs 8 bits(1 byte), all mats in a subbank makesup 64 bytes.\n\n\n\n[5]\n\n\n\nwhy is there 10 bit in the above figure?\n\nParity and Error Correcting Codes(ECC).\n\n\n\n[5] discussed the same flow.\n\nConsidering a number of input ports and a different/same number of banks, the controller detects the desired bank accesses, arbitrates eventual bank conflicts, and generates the required crossbar switches selection signals.\n\nAt the highest level the address space is split across identical banks, four in this example, with each bank having its own address and data bus, thus allowing for concurrent bank accesses.\n\nEach bank is composed of identical sub-banks, again four in this example, with only one being active per access.\n\nFurther, each sub-bank is partitioned into multiple mats that simultaneously provide parts of the required data (cache block in a cache data array).\n\nFinally, each mat is composed of four identical sub-arrays that share predecoding/decoding logic and peripheral circuitry, and which again deliver together the requested data.\n\nAn H-tree routing distribution network is used to drive addresses and data to/from the banks, and also to/from every mat inside a bank.\n\n# How to index into bank\n\n[6]In particular, the cache index, consisting of line number (LN) and line offset (LO), is divided into two portions: bank-internal address (BA) and bank index (BI). BA selects a cache word or tag within a 1-port-memory-cell bank, and BI selects the respective bank within data/instruction or tag memory. BI uses the lower rank bits to assure that consecutive cache-lines and words within the same line are interleaved and located in different banks.\n\n\n\n[7]\n\n\n\n\n\n# conclusion\n\nIn summary, mat is the mininum unit to provide data. and all mat in a subbank provides whole cache line.\n\nBank can only be accessed serially. Multi bank can provide mutiple request parallely.\n\n\n# Timing\n\n[1] CACTI models the delay/power/area of eight major cache components: decoder, wordline, bitline, senseamp, comparator, multiplexor, output driver, and inter-bank wires. The wordline and bitline delays are two of the most significant components of the access time. The wordline and bitline delays are quadratic functions of the width and height of each array, respectively.\n\n\n# References\n\n[1] CACTI 6.0: A Tool to Understand Large Caches\n\n[2] CACTI 3.0: An Integrated Cache Timing, Power, and Area Model\n\n[3] Flexicache: Highly Reliable and Low Power Cache\n\n[4] Best Memory Architecture Exploration under Parameters Variations accelerated with Machine Learning\n\n[5] A Shared Polyhedral Cache for 3D Wide-I/O Multi-core Computing Platforms\n\n[6] 4-Port Unified Data/Instruction Cache Design with Distributed Crossbar and Interleaved Cache-Line Words\n\n[7] Unified Data/Instruction Cache with Bank-Based Multi-Port Architecture",normalizedContent:"# cache\n\n# structure\n\n[1]\n\n[1]\n\n * bank - a memory structure that consists of a data and a tag array. a cache is typically split into multiple banks and cacti assumes enough bandwidth so that these banks can be accessed simultaneously. the network topology that interconnects these banks can vary depending on the cache model.\n * sub-arrays - a data or tag array is divided into a number of sub-arrays to reduce the delay due to wordline and bitline. unlike banks, at any given time, these sub-arrays support only one single access. the total number of sub-arrays in a cache is equal to the product of ndwl and ndbl.\n * mat - a group of four sub-arrays (2x2) that share a common central predecoder. cacti’s exhaustive search starts from a minimum of at least one mat.\n * sub-bank - in a typical cache, a cache block is scattered across multiple sub-arrays to improve the reliability of a cache. irrespective of the cache organization, cacti assumes that every cache block in a cache is distributed across an entire row of mats and the row number corresponding to a particular block is determined based on the block address. each row (of mats) in an array is referred to as a sub-bank.\n\n# why do we needs bank\n\n[2] support for more than one access to a memory structure at the same time can be provided by adding more ports to each memory cell.\n\nmultiporting a cache makes the cache capable of handling as many read or write requests as ports of that type.\n\nbut multiporting a cache makes each bit cell much bigger and so the overall area of the memory array can increase enormously for large number of ports. the extra length spanned by the cache also adds directly to the access time and power consumed by the cache.\n\nanother way of supporting simultaneous multiple accesses to a cache is by banking with fully independent banks.\n\neach bank does not share address and data.\n\nbanking also adds the decoding overhead of routing the appropriate address to the right bank and detecting collisions.\n\n[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n# how to supply 64byte at each time\n\neach sub-array outputs 128 bits, all 4 sub-array in the mat output 512bit together. [3]\n\n[4]\n\na bank consists of nsubbanks of identical sub-banks that are activated sequentially with each access. in turn, each sub-bank contains a number of identical mats.\n\na mat is a self-contained, compact memory array composed of four identical sub-arrays, with nrows rows and ncols columns, and accompanying predecoding logic, with each sub-array being a two-dimensional matrix of memory cells and associated peripheral circuitry.\n\neach mat holds a portion of a word in one of its four sub-arrays; during cache access, all mats in a sub-bank are activated to form the whole word.\n\nby whole word, he means a cacheline.\n\nh-tree distribution networks are often used to deliver address and data to mats.\n\nin the following figure, each sub-array outputs 8 bits(1 byte), all mats in a subbank makesup 64 bytes.\n\n\n\n[5]\n\n\n\nwhy is there 10 bit in the above figure?\n\nparity and error correcting codes(ecc).\n\n\n\n[5] discussed the same flow.\n\nconsidering a number of input ports and a different/same number of banks, the controller detects the desired bank accesses, arbitrates eventual bank conflicts, and generates the required crossbar switches selection signals.\n\nat the highest level the address space is split across identical banks, four in this example, with each bank having its own address and data bus, thus allowing for concurrent bank accesses.\n\neach bank is composed of identical sub-banks, again four in this example, with only one being active per access.\n\nfurther, each sub-bank is partitioned into multiple mats that simultaneously provide parts of the required data (cache block in a cache data array).\n\nfinally, each mat is composed of four identical sub-arrays that share predecoding/decoding logic and peripheral circuitry, and which again deliver together the requested data.\n\nan h-tree routing distribution network is used to drive addresses and data to/from the banks, and also to/from every mat inside a bank.\n\n# how to index into bank\n\n[6]in particular, the cache index, consisting of line number (ln) and line offset (lo), is divided into two portions: bank-internal address (ba) and bank index (bi). ba selects a cache word or tag within a 1-port-memory-cell bank, and bi selects the respective bank within data/instruction or tag memory. bi uses the lower rank bits to assure that consecutive cache-lines and words within the same line are interleaved and located in different banks.\n\n\n\n[7]\n\n\n\n\n\n# conclusion\n\nin summary, mat is the mininum unit to provide data. and all mat in a subbank provides whole cache line.\n\nbank can only be accessed serially. multi bank can provide mutiple request parallely.\n\n\n# timing\n\n[1] cacti models the delay/power/area of eight major cache components: decoder, wordline, bitline, senseamp, comparator, multiplexor, output driver, and inter-bank wires. the wordline and bitline delays are two of the most significant components of the access time. the wordline and bitline delays are quadratic functions of the width and height of each array, respectively.\n\n\n# references\n\n[1] cacti 6.0: a tool to understand large caches\n\n[2] cacti 3.0: an integrated cache timing, power, and area model\n\n[3] flexicache: highly reliable and low power cache\n\n[4] best memory architecture exploration under parameters variations accelerated with machine learning\n\n[5] a shared polyhedral cache for 3d wide-i/o multi-core computing platforms\n\n[6] 4-port unified data/instruction cache design with distributed crossbar and interleaved cache-line words\n\n[7] unified data/instruction cache with bank-based multi-port architecture",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"cache timing",frontmatter:{title:"cache timing",date:"2024-07-16T15:32:49.000Z",permalink:"/pages/cc7039/",tags:[null]},regularPath:"/04.cpu/06.cache%20timing.html",relativePath:"04.cpu/06.cache timing.md",key:"v-90037b60",path:"/pages/cc7039/",headers:[{level:3,title:"Analysis",slug:"analysis",normalizedTitle:"analysis",charIndex:2618},{level:3,title:"An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches [[2]](#2)",slug:"an-adaptive-non-uniform-cache-structure-for-wire-delay-dominated-on-chip-caches-2",normalizedTitle:'an adaptive, non-uniform cache structure for wire-delay dominated on-chip caches <a href="#2">[2]</a>',charIndex:null},{level:3,title:"References",slug:"references",normalizedTitle:"references",charIndex:6629}],headersStr:"Analysis An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches [[2]](#2) References",content:"Non-Uniform Cache Architecture (NUCA). The idea is to split the cache into a large number of banks and employ an on-chip network for communication. Now the access time of the cache is a function of distance between the bank and cache controller rather than the latency of the slowest subbank.\n\n * The access latency of a cache depends on delays in the decoders, word-lines, bit-lines, and drivers.\n * Decoder and driver delay components of a cache go up with increase in the number of sub-banks.\n * On the other hand, word line or bit line delay components reduce with decrease in subbank size in the vertical and horizontal directions, respectively.\n * large caches in future processors will have an additional overhead of network delay. Due to the growing disparity between wire and transistor delay, this factor will continue to dominate with technology improvements. T\n\n[1]\n\nCACTI divides the total access time of the cache into seven main components (decoder, wordline, bitline, senseamp, comparator, multiplexer-driver, and output driver).\n\n# senseamp and comparator delay\n\nOf these, senseamp and comparator delay is almost constant across different cache sizes and its contribution to the total access time reduces with increase in cache size.\n\n# multiplexer\n\nThe mux-driver delay component consists of delay due to multiplexer logic (to select the appropriate line) and driver delay to route the control signal to the output driver.\n\nThe latter part is proportional to the size of the cache.\n\n# Decoder\n\nThe decoder part of the cache consists of a single centralized pre-decoder and a separate decoder for each subarray.\n\nThe output from the pre-decoder is fed to the final decoder stage to drive the appropriate wordline.\n\nThus, the decoder delay component is the sum of time to route address bits to the central predecoder, time to route the output of the pre-decoder to the finalstage decoder, and the logic delay associated with predecoder, decoder, and driver circuits. Thus, decoder delay depends on both cache size and subarray count.\n\n# wordline bit delay\n\nThe wordline delay of data/tag array is proportional to the length of the array and the bit line delay is proportional to the height of the array.\n\nThese two delay values can be tweaked by adjusting the aspect ratio of the sub-array.\n\nTo bring down the delay values of both these components the cache is split into a number of sub-arrays.\n\nBut, with an increased number of sub-arrays, the latency to send signals to and from the central pre-decoder increases.\n\nThus, there exists a trade-off between the sub-array size and the wire length.\n\n\n# Analysis\n\n\n\nbank access latency increases exponentially with a decrease in bank count value. This is because as the size of the cache increases, the decoder delay component (that includes wiring delay) increases significantly. Also, the bank access time saturates beyond a bank count value of 512 (bank size of 64KB).\n\nBeyond this point, the latency is primarily due to logic delay associated with each stage, which is constant across different cache sizes.\n\nThe average (uncontended) network latency plotted in the graph is obtained by calculating the access time to each indvidual bank and averaging them against the total bank count value. This value depends on both the bank size and total number of banks.\n\n> Please notice, in the following statement, hop latency means the hop latency for each hop. There is another variable hop counts. The network latency is determined by hop latency * hop count.\n\nIt can be observed that the average latency first goes down with an increase in bank count and then starts increasing for large bank count values. If the bank size is extremely large, the hop latency dominates the total access time and hence the network latency is very high.\n\nIdeally, the network latency should go down with an increase in bank count. But, dividing the bank into half only reduces the area of data and tag arrays.\n\nOther constant area overheads associated with each bank will remain the same and hence the reduction in hop latency is less than half its original value.\n\nFor very large bank count values, the reduction in hop latency is usually less than the increase in hop count to reach a destination, leading to high average network latencies. The only exception is a change in bank count value from 1024 to 2048 – because hop latencies are rounded up to the next integer value, a doubling in bank count results in halving the vertical hop latency.\n\nThus, finding the optimal bank count value is critical to achieving the least possible access latency.\n\n\n# An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches [2]\n\nCitation over 1000.\n\nData residing in the part of a large cache close to the processor could be accessed much faster than data that reside physically farther from the processor. For example, the closest bank in a 16-megabyte, on-chip L2 cache built in a 50-nanometer process technology could be accessed in 4 cycles, while an access to the farthest bank might take 47 cycles.\n\n\n\n\n\n\n\n\n\n * Simple mapping: each column of banks in the cache becomes a bank set, and all banks within that column comprise the set-associative ways. Thus, the cache may be searched for a line by first selecting the bank column, selecting the set within the column, and finally performing a tag match on banks within that column of the cache. The two drawbacks of this scheme are that\n   \n   * the number of rows may not correspond to the number of desired ways in each bank set\n   * latencies to access all bank sets are not the same; some bank sets will be faster than others, since some rows are closer to the cache controller than others.\n\n * fair mapping policy, which addresses both problems of the simple mapping policy at the cost of additional complexity.\n\n * shared mapping policy, attempts to provide fastest-bank access to all bank sets by sharing the closest banks among multiple bank sets.\n\nThe policies we explore for D-NUCA consist of four major components: (1) Mapping: simple or shared.\n\n(2) Search: multicast, incremental, or combination. We restrict the combined policies such that a block set is partitioned into just two groups, which may then each vary in size (number of blocks) and the method of access (incremental or multicast).\n\n(3) Promotion: described by promotion distance, measured in cache banks, and promotion trigger, measured in number of hits to a bank before a promotion occurs.\n\n(4) Insertion: identifies the location to place an incoming block and what to do with the block it replaces (zero copy or one copy policies).\n\n\n# References\n\n[1] The Effect of Interconnect Design on the Performance of Large L2 Caches\n\n[2] An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches",normalizedContent:"non-uniform cache architecture (nuca). the idea is to split the cache into a large number of banks and employ an on-chip network for communication. now the access time of the cache is a function of distance between the bank and cache controller rather than the latency of the slowest subbank.\n\n * the access latency of a cache depends on delays in the decoders, word-lines, bit-lines, and drivers.\n * decoder and driver delay components of a cache go up with increase in the number of sub-banks.\n * on the other hand, word line or bit line delay components reduce with decrease in subbank size in the vertical and horizontal directions, respectively.\n * large caches in future processors will have an additional overhead of network delay. due to the growing disparity between wire and transistor delay, this factor will continue to dominate with technology improvements. t\n\n[1]\n\ncacti divides the total access time of the cache into seven main components (decoder, wordline, bitline, senseamp, comparator, multiplexer-driver, and output driver).\n\n# senseamp and comparator delay\n\nof these, senseamp and comparator delay is almost constant across different cache sizes and its contribution to the total access time reduces with increase in cache size.\n\n# multiplexer\n\nthe mux-driver delay component consists of delay due to multiplexer logic (to select the appropriate line) and driver delay to route the control signal to the output driver.\n\nthe latter part is proportional to the size of the cache.\n\n# decoder\n\nthe decoder part of the cache consists of a single centralized pre-decoder and a separate decoder for each subarray.\n\nthe output from the pre-decoder is fed to the final decoder stage to drive the appropriate wordline.\n\nthus, the decoder delay component is the sum of time to route address bits to the central predecoder, time to route the output of the pre-decoder to the finalstage decoder, and the logic delay associated with predecoder, decoder, and driver circuits. thus, decoder delay depends on both cache size and subarray count.\n\n# wordline bit delay\n\nthe wordline delay of data/tag array is proportional to the length of the array and the bit line delay is proportional to the height of the array.\n\nthese two delay values can be tweaked by adjusting the aspect ratio of the sub-array.\n\nto bring down the delay values of both these components the cache is split into a number of sub-arrays.\n\nbut, with an increased number of sub-arrays, the latency to send signals to and from the central pre-decoder increases.\n\nthus, there exists a trade-off between the sub-array size and the wire length.\n\n\n# analysis\n\n\n\nbank access latency increases exponentially with a decrease in bank count value. this is because as the size of the cache increases, the decoder delay component (that includes wiring delay) increases significantly. also, the bank access time saturates beyond a bank count value of 512 (bank size of 64kb).\n\nbeyond this point, the latency is primarily due to logic delay associated with each stage, which is constant across different cache sizes.\n\nthe average (uncontended) network latency plotted in the graph is obtained by calculating the access time to each indvidual bank and averaging them against the total bank count value. this value depends on both the bank size and total number of banks.\n\n> please notice, in the following statement, hop latency means the hop latency for each hop. there is another variable hop counts. the network latency is determined by hop latency * hop count.\n\nit can be observed that the average latency first goes down with an increase in bank count and then starts increasing for large bank count values. if the bank size is extremely large, the hop latency dominates the total access time and hence the network latency is very high.\n\nideally, the network latency should go down with an increase in bank count. but, dividing the bank into half only reduces the area of data and tag arrays.\n\nother constant area overheads associated with each bank will remain the same and hence the reduction in hop latency is less than half its original value.\n\nfor very large bank count values, the reduction in hop latency is usually less than the increase in hop count to reach a destination, leading to high average network latencies. the only exception is a change in bank count value from 1024 to 2048 – because hop latencies are rounded up to the next integer value, a doubling in bank count results in halving the vertical hop latency.\n\nthus, finding the optimal bank count value is critical to achieving the least possible access latency.\n\n\n# an adaptive, non-uniform cache structure for wire-delay dominated on-chip caches [2]\n\ncitation over 1000.\n\ndata residing in the part of a large cache close to the processor could be accessed much faster than data that reside physically farther from the processor. for example, the closest bank in a 16-megabyte, on-chip l2 cache built in a 50-nanometer process technology could be accessed in 4 cycles, while an access to the farthest bank might take 47 cycles.\n\n\n\n\n\n\n\n\n\n * simple mapping: each column of banks in the cache becomes a bank set, and all banks within that column comprise the set-associative ways. thus, the cache may be searched for a line by first selecting the bank column, selecting the set within the column, and finally performing a tag match on banks within that column of the cache. the two drawbacks of this scheme are that\n   \n   * the number of rows may not correspond to the number of desired ways in each bank set\n   * latencies to access all bank sets are not the same; some bank sets will be faster than others, since some rows are closer to the cache controller than others.\n\n * fair mapping policy, which addresses both problems of the simple mapping policy at the cost of additional complexity.\n\n * shared mapping policy, attempts to provide fastest-bank access to all bank sets by sharing the closest banks among multiple bank sets.\n\nthe policies we explore for d-nuca consist of four major components: (1) mapping: simple or shared.\n\n(2) search: multicast, incremental, or combination. we restrict the combined policies such that a block set is partitioned into just two groups, which may then each vary in size (number of blocks) and the method of access (incremental or multicast).\n\n(3) promotion: described by promotion distance, measured in cache banks, and promotion trigger, measured in number of hits to a bank before a promotion occurs.\n\n(4) insertion: identifies the location to place an incoming block and what to do with the block it replaces (zero copy or one copy policies).\n\n\n# references\n\n[1] the effect of interconnect design on the performance of large l2 caches\n\n[2] an adaptive, non-uniform cache structure for wire-delay dominated on-chip caches",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"cache timing",frontmatter:{title:"cache timing",date:"2024-07-20T15:32:49.000Z",permalink:"/pages/cc7040/",tags:[null]},regularPath:"/04.cpu/07.register%20file.html",relativePath:"04.cpu/07.register file.md",key:"v-7ddc0f42",path:"/pages/cc7040/",headers:[{level:3,title:"1. Revisiting Wide Superscalar Microarchitecture",slug:"_1-revisiting-wide-superscalar-microarchitecture",normalizedTitle:"1. revisiting wide superscalar microarchitecture",charIndex:191},{level:3,title:"2. Banked Multiported Register Files for High-Frequency Superscalar Microprocessors",slug:"_2-banked-multiported-register-files-for-high-frequency-superscalar-microprocessors",normalizedTitle:"2. banked multiported register files for high-frequency superscalar microprocessors",charIndex:1521}],headersStr:"1. Revisiting Wide Superscalar Microarchitecture 2. Banked Multiported Register Files for High-Frequency Superscalar Microprocessors",content:" 1. [4] Revisiting Wide Superscalar Microarchitecture\n 2. [196] Banked Multiported Register Files for High-Frequency Superscalar Microprocessors\n\n----------------------------------------\n\n\n# 1. Revisiting Wide Superscalar Microarchitecture\n\nMust Read\n\nImpact 👍 👍 👍 👍\n\nUnderstand ☺️ ☺️\n\nThis paper discussed the chanllening issue of wide superscalar microarchitecture might trigger.\n\nAlso from circuit perspective.\n\nContributions\n\n * This study shows that considering wide issue instead of narrow issue clusters has a dramatic impact on the performance of Mod-N, one of the simplest steering policy.\n * We find that, with wide issue clusters, if the instruction window is large enough and considering a realistic intercluster delay, the optimal value of N is much larger than three, typically several tens.\n * We argue that the instruction window and the issue width can be augmented by combining clustering and register write specialization.\n * we propose two independent and orthogonal energy optimizations exploiting loops.\n   * The first optimization we propose is a mechanism to detect redundant microops producing the same result on every iteration and to remove these micro-ops from the execution core.\n   * The second optimization focuses on saving energy consumed by load microops.\n * We also test this mechanism with a configuration which emulates what happens for very large instruction footprint applications.\n\nChapter 2 state of the art is so well-written.\n\n\n\n----------------------------------------\n\n\n# 2. Banked Multiported Register Files for High-Frequency Superscalar Microprocessors\n\nImpact 👍 👍 👍\n\nUnderstand ☺️\n\nContribution\n\n * banked register file with much simpler and faster control logic while only slightly increasing the number of ports per bank\n * We present area, delay, and energy numbers extracted from layouts of the banked register file\n\n\n\n\n\n\n\n\n\n\n\n\n\n#banks/#reads/#writes/bypass/sharing\n\n\n\nConclusion\n\nArea\n\n * As the number of local ports per bank is reduced, area drops dramatically\n\n * Compared to the baseline design, the designs with four banks are around one quarter the size, and the designs with eight banks are around one third the size.\n\n * Apart from the reduction in storage cell size, designs with smaller numbers of ports per bank have significantly less address decoder area than the highly multiported designs. Each bank has fewer decoders with narrower addresses.\n\n * Designs with two read ports per bank are not much larger than designs with a single read port per bank given that the single read port must connect to all global read ports whereas each of the two read ports only connects to half of the global read ports.\n\n * For the fully ported storage cell designs, using hierarchical bitlines cuts delay by 8–17%.\n\n * The lesser-ported bank designs have a slightly greater reduction in delay, at around 25% faster for the two read, two write port case.\n\n * we found the (8/2/2/y/y) configuration to perform well for this design point, and we chose this as our center point in perturbing other parameters.\n\n * Reducing the number of banks to four (4/2/2/y/y), lowers performance by another 3–4%.\n\n * We can also see that moving from 1 to 2 write ports (8/2/1/y/y, 8/2/2/y/y) improves performance by more than 4% but having more than 2 write ports per bank (8/2/4/y/y and 8/2/8/y/y) adds only another 0.3%.\n\n * This is expected given that average IPCs are rarely above 2, and some instructions do not write registers.\n\nRemarks\n\nThis paper discussed in detail the circuit design and tradeoff of multi-bank register file and bypass network.\n\nThe area break down of multi bank and multi port.",normalizedContent:" 1. [4] revisiting wide superscalar microarchitecture\n 2. [196] banked multiported register files for high-frequency superscalar microprocessors\n\n----------------------------------------\n\n\n# 1. revisiting wide superscalar microarchitecture\n\nmust read\n\nimpact 👍 👍 👍 👍\n\nunderstand ☺️ ☺️\n\nthis paper discussed the chanllening issue of wide superscalar microarchitecture might trigger.\n\nalso from circuit perspective.\n\ncontributions\n\n * this study shows that considering wide issue instead of narrow issue clusters has a dramatic impact on the performance of mod-n, one of the simplest steering policy.\n * we find that, with wide issue clusters, if the instruction window is large enough and considering a realistic intercluster delay, the optimal value of n is much larger than three, typically several tens.\n * we argue that the instruction window and the issue width can be augmented by combining clustering and register write specialization.\n * we propose two independent and orthogonal energy optimizations exploiting loops.\n   * the first optimization we propose is a mechanism to detect redundant microops producing the same result on every iteration and to remove these micro-ops from the execution core.\n   * the second optimization focuses on saving energy consumed by load microops.\n * we also test this mechanism with a configuration which emulates what happens for very large instruction footprint applications.\n\nchapter 2 state of the art is so well-written.\n\n\n\n----------------------------------------\n\n\n# 2. banked multiported register files for high-frequency superscalar microprocessors\n\nimpact 👍 👍 👍\n\nunderstand ☺️\n\ncontribution\n\n * banked register file with much simpler and faster control logic while only slightly increasing the number of ports per bank\n * we present area, delay, and energy numbers extracted from layouts of the banked register file\n\n\n\n\n\n\n\n\n\n\n\n\n\n#banks/#reads/#writes/bypass/sharing\n\n\n\nconclusion\n\narea\n\n * as the number of local ports per bank is reduced, area drops dramatically\n\n * compared to the baseline design, the designs with four banks are around one quarter the size, and the designs with eight banks are around one third the size.\n\n * apart from the reduction in storage cell size, designs with smaller numbers of ports per bank have significantly less address decoder area than the highly multiported designs. each bank has fewer decoders with narrower addresses.\n\n * designs with two read ports per bank are not much larger than designs with a single read port per bank given that the single read port must connect to all global read ports whereas each of the two read ports only connects to half of the global read ports.\n\n * for the fully ported storage cell designs, using hierarchical bitlines cuts delay by 8–17%.\n\n * the lesser-ported bank designs have a slightly greater reduction in delay, at around 25% faster for the two read, two write port case.\n\n * we found the (8/2/2/y/y) configuration to perform well for this design point, and we chose this as our center point in perturbing other parameters.\n\n * reducing the number of banks to four (4/2/2/y/y), lowers performance by another 3–4%.\n\n * we can also see that moving from 1 to 2 write ports (8/2/1/y/y, 8/2/2/y/y) improves performance by more than 4% but having more than 2 write ports per bank (8/2/4/y/y and 8/2/8/y/y) adds only another 0.3%.\n\n * this is expected given that average ipcs are rarely above 2, and some instructions do not write registers.\n\nremarks\n\nthis paper discussed in detail the circuit design and tradeoff of multi-bank register file and bypass network.\n\nthe area break down of multi bank and multi port.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"two-test-1",frontmatter:{title:"two-test-1",date:"2022-07-18T17:23:23.000Z",permalink:"/pages/f07697/",tags:[null]},regularPath:"/04.cpu/1234.markdown.html",relativePath:"04.cpu/1234.markdown.md",key:"v-c1b2e1c6",path:"/pages/f07697/",headers:[{level:2,title:"为什么要使用 Markdown?",slug:"为什么要使用-markdown",normalizedTitle:"为什么要使用 markdown?",charIndex:429},{level:2,title:"Markdown 相关软件推荐",slug:"markdown-相关软件推荐",normalizedTitle:"markdown 相关软件推荐",charIndex:1004},{level:2,title:"1. 标题&目录",slug:"_1-标题-目录",normalizedTitle:"1. 标题&amp;目录",charIndex:null},{level:3,title:"1.1 标题",slug:"_1-1-标题",normalizedTitle:"1.1 标题",charIndex:1269},{level:3,title:"1.2 目录",slug:"_1-2-目录",normalizedTitle:"1.2 目录",charIndex:1470},{level:2,title:"2. 斜体&粗体",slug:"_2-斜体-粗体",normalizedTitle:"2. 斜体&amp;粗体",charIndex:null},{level:3,title:"2.1 斜体",slug:"_2-1-斜体",normalizedTitle:"2.1 斜体",charIndex:1662},{level:3,title:"2.2 粗体",slug:"_2-2-粗体",normalizedTitle:"2.2 粗体",charIndex:1844},{level:3,title:"2.3 粗斜体 (斜粗体)",slug:"_2-3-粗斜体-斜粗体",normalizedTitle:"2.3 粗斜体 (斜粗体)",charIndex:2046},{level:3,title:"2.4 斜体包含粗体",slug:"_2-4-斜体包含粗体",normalizedTitle:"2.4 斜体包含粗体",charIndex:2434},{level:3,title:"2.5 粗体包含斜体",slug:"_2-5-粗体包含斜体",normalizedTitle:"2.5 粗体包含斜体",charIndex:2973},{level:2,title:"3. 线",slug:"_3-线",normalizedTitle:"3. 线",charIndex:3505},{level:3,title:"3.1 水平分割线",slug:"_3-1-水平分割线",normalizedTitle:"3.1 水平分割线",charIndex:3515},{level:3,title:"3.2 文本删除线",slug:"_3-2-文本删除线",normalizedTitle:"3.2 文本删除线",charIndex:3676},{level:3,title:"3.3 文本下划线",slug:"_3-3-文本下划线",normalizedTitle:"3.3 文本下划线",charIndex:3779},{level:2,title:"4. 列表&引用",slug:"_4-列表-引用",normalizedTitle:"4. 列表&amp;引用",charIndex:null},{level:3,title:"4.1 有序列表",slug:"_4-1-有序列表",normalizedTitle:"4.1 有序列表",charIndex:3903},{level:3,title:"4.2 无序列表",slug:"_4-2-无序列表",normalizedTitle:"4.2 无序列表",charIndex:4966},{level:3,title:"4.3 引用",slug:"_4-3-引用",normalizedTitle:"4.3 引用",charIndex:5496},{level:3,title:"4.4 缩进&退格",slug:"_4-4-缩进-退格",normalizedTitle:"4.4 缩进&amp;退格",charIndex:null},{level:2,title:"5. 网页链接与图像",slug:"_5-网页链接与图像",normalizedTitle:"5. 网页链接与图像",charIndex:8422},{level:3,title:"5.1 网页链接",slug:"_5-1-网页链接",normalizedTitle:"5.1 网页链接",charIndex:8438},{level:3,title:"5.2 图像",slug:"_5-2-图像",normalizedTitle:"5.2 图像",charIndex:9013},{level:2,title:"6. 表格",slug:"_6-表格",normalizedTitle:"6. 表格",charIndex:9834},{level:3,title:"6.1 表格中文本内容的换行",slug:"_6-1-表格中文本内容的换行",normalizedTitle:"6.1 表格中文本内容的换行",charIndex:10206},{level:2,title:"7. 代码域",slug:"_7-代码域",normalizedTitle:"7. 代码域",charIndex:10549},{level:3,title:"7.1 行内代码",slug:"_7-1-行内代码",normalizedTitle:"7.1 行内代码",charIndex:10561},{level:3,title:"7.2 代码块",slug:"_7-2-代码块",normalizedTitle:"7.2 代码块",charIndex:10969},{level:3,title:"7.3 如何在行内代码里显示反引号",slug:"_7-3-如何在行内代码里显示反引号",normalizedTitle:"7.3 如何在行内代码里显示反引号",charIndex:14992},{level:2,title:"8. 任务列表（待办）",slug:"_8-任务列表-待办",normalizedTitle:"8. 任务列表（待办）",charIndex:15142},{level:3,title:"示范",slug:"示范-22",normalizedTitle:"示范",charIndex:1805},{level:3,title:"示范",slug:"示范-23",normalizedTitle:"示范",charIndex:1805},{level:2,title:"9. 注释",slug:"_9-注释",normalizedTitle:"9. 注释",charIndex:15992},{level:3,title:"示范",slug:"示范-只有切换至-编辑模式-才能看到喔",normalizedTitle:"示范",charIndex:1805},{level:2,title:"10. 变量",slug:"_10-变量",normalizedTitle:"10. 变量",charIndex:16421},{level:3,title:"10.1 网页链接变量",slug:"_10-1-网页链接变量",normalizedTitle:"10.1 网页链接变量",charIndex:16433},{level:3,title:"10.2 脚注",slug:"_10-2-脚注",normalizedTitle:"10.2 脚注",charIndex:16756},{level:2,title:"11. 拓展文本格式标记",slug:"_11-拓展文本格式标记",normalizedTitle:"11. 拓展文本格式标记",charIndex:17028},{level:3,title:"11.1 键盘文本",slug:"_11-1-键盘文本",normalizedTitle:"11.1 键盘文本",charIndex:17130},{level:3,title:"11.2 放大文本",slug:"_11-2-放大文本",normalizedTitle:"11.2 放大文本",charIndex:17465},{level:3,title:"11.3 缩小文本",slug:"_11-3-缩小文本",normalizedTitle:"11.3 缩小文本",charIndex:17707},{level:3,title:"11.4 多彩文本",slug:"_11-4-多彩文本",normalizedTitle:"11.4 多彩文本",charIndex:17949},{level:2,title:"12. 拓展文本显示效果",slug:"_12-拓展文本显示效果",normalizedTitle:"12. 拓展文本显示效果",charIndex:18979},{level:3,title:"12.1 文本高亮",slug:"_12-1-文本高亮",normalizedTitle:"12.1 文本高亮",charIndex:19120},{level:3,title:"12.2 上标",slug:"_12-2-上标",normalizedTitle:"12.2 上标",charIndex:19194},{level:3,title:"12.3 下标",slug:"_12-3-下标",normalizedTitle:"12.3 下标",charIndex:19368},{level:3,title:"12.4 Emoji 符号",slug:"_12-4-emoji-符号",normalizedTitle:"12.4 emoji 符号",charIndex:19547},{level:2,title:"13. 转义字符",slug:"_13-转义字符",normalizedTitle:"13. 转义字符",charIndex:19898},{level:3,title:"例1 以普通字符显示星号",slug:"例1-以普通字符显示星号",normalizedTitle:"例1 以普通字符显示星号",charIndex:20457},{level:3,title:"例2 表格内 单元格中的竖杠",slug:"例2-表格内-单元格中的竖杠",normalizedTitle:"例2 表格内 单元格中的竖杠",charIndex:20722},{level:3,title:"例3 不会变成代码的反引号",slug:"例3-不会变成代码的反引号",normalizedTitle:"例3 不会变成代码的反引号",charIndex:21280},{level:3,title:"例4 链接中的中括号",slug:"例4-链接中的中括号",normalizedTitle:"例4 链接中的中括号",charIndex:21554},{level:3,title:"例5 不是列表的连接符(横杠)",slug:"例5-不是列表的连接符-横杠",normalizedTitle:"例5 不是列表的连接符(横杠)",charIndex:21719},{level:3,title:"例6 不是标题的 \\#",slug:"例6-不是标题的",normalizedTitle:"例6 不是标题的 #",charIndex:22158},{level:3,title:"例7 不会注释的 \\%",slug:"例7-不会注释的",normalizedTitle:"例7 不会注释的 %",charIndex:22235},{level:3,title:"例8 木有链接的双链",slug:"例8-木有链接的双链",normalizedTitle:"例8 木有链接的双链",charIndex:22364},{level:3,title:"例9 页链接里 显示文本内的 中括号",slug:"例9-页链接里-显示文本内的-中括号",normalizedTitle:"例9 页链接里 显示文本内的 中括号",charIndex:22501},{level:3,title:"特殊情况 文本修饰的中括号",slug:"特殊情况-文本修饰的中括号",normalizedTitle:"特殊情况 文本修饰的中括号",charIndex:22675},{level:2,title:"14. 空格&换行&强制删除",slug:"_14-空格-换行-强制删除",normalizedTitle:"14. 空格&amp;换行&amp;强制删除",charIndex:null},{level:3,title:"14.1 空格",slug:"_14-1-空格",normalizedTitle:"14.1 空格",charIndex:22819},{level:3,title:"14.2 换行",slug:"_14-2-换行",normalizedTitle:"14.2 换行",charIndex:23070},{level:3,title:"14.3 强制删除",slug:"_14-3-强制删除",normalizedTitle:"14.3 强制删除",charIndex:23728},{level:2,title:"15. 嵌入",slug:"_15-嵌入",normalizedTitle:"15. 嵌入",charIndex:23900},{level:3,title:"15.1 嵌入音频",slug:"_15-1-嵌入音频",normalizedTitle:"15.1 嵌入音频",charIndex:24080},{level:3,title:"15.2 嵌入视频",slug:"_15-2-嵌入视频",normalizedTitle:"15.2 嵌入视频",charIndex:24341},{level:3,title:"15.3 嵌入页面",slug:"_15-3-嵌入页面",normalizedTitle:"15.3 嵌入页面",charIndex:24721},{level:2,title:"16. Latex 数学公式",slug:"_16-latex-数学公式",normalizedTitle:"16. latex 数学公式",charIndex:25471},{level:3,title:"16.1 行内公式",slug:"_16-1-行内公式",normalizedTitle:"16.1 行内公式",charIndex:25516},{level:3,title:"16.2 公式块",slug:"_16-2-公式块",normalizedTitle:"16.2 公式块",charIndex:25823},{level:2,title:"17. Mermaid",slug:"_17-mermaid",normalizedTitle:"17. mermaid",charIndex:27121},{level:3,title:"17.1 流程图",slug:"_17-1-流程图",normalizedTitle:"17.1 流程图",charIndex:27284},{level:3,title:"17.2 饼图",slug:"_17-2-饼图",normalizedTitle:"17.2 饼图",charIndex:28498},{level:3,title:"17.3 序列图 (时序图)",slug:"_17-3-序列图-时序图",normalizedTitle:"17.3 序列图 (时序图)",charIndex:28739},{level:3,title:"17.4 甘特图",slug:"_17-4-甘特图",normalizedTitle:"17.4 甘特图",charIndex:32416},{level:3,title:"17.5 类图",slug:"_17-5-类图",normalizedTitle:"17.5 类图",charIndex:32999},{level:2,title:"18. 标签 (Tag)",slug:"_18-标签-tag",normalizedTitle:"18. 标签 (tag)",charIndex:33887},{level:3,title:"关于空格",slug:"关于空格",normalizedTitle:"关于空格",charIndex:33986},{level:3,title:"关于数字",slug:"关于数字",normalizedTitle:"关于数字",charIndex:34197},{level:3,title:"标签的嵌套",slug:"标签的嵌套",normalizedTitle:"标签的嵌套",charIndex:34299},{level:3,title:"能被使用的符号",slug:"能被使用的符号",normalizedTitle:"能被使用的符号",charIndex:34521},{level:3,title:"如何让 \\# 不被识别",slug:"如何让-不被识别",normalizedTitle:"如何让 # 不被识别",charIndex:34585},{level:2,title:"19. 避免标识符的滥用",slug:"_19-避免标识符的滥用",normalizedTitle:"19. 避免标识符的滥用",charIndex:34678}],headersStr:"为什么要使用 Markdown? Markdown 相关软件推荐 1. 标题&目录 1.1 标题 1.2 目录 2. 斜体&粗体 2.1 斜体 2.2 粗体 2.3 粗斜体 (斜粗体) 2.4 斜体包含粗体 2.5 粗体包含斜体 3. 线 3.1 水平分割线 3.2 文本删除线 3.3 文本下划线 4. 列表&引用 4.1 有序列表 4.2 无序列表 4.3 引用 4.4 缩进&退格 5. 网页链接与图像 5.1 网页链接 5.2 图像 6. 表格 6.1 表格中文本内容的换行 7. 代码域 7.1 行内代码 7.2 代码块 7.3 如何在行内代码里显示反引号 8. 任务列表（待办） 示范 示范 9. 注释 示范 10. 变量 10.1 网页链接变量 10.2 脚注 11. 拓展文本格式标记 11.1 键盘文本 11.2 放大文本 11.3 缩小文本 11.4 多彩文本 12. 拓展文本显示效果 12.1 文本高亮 12.2 上标 12.3 下标 12.4 Emoji 符号 13. 转义字符 例1 以普通字符显示星号 例2 表格内 单元格中的竖杠 例3 不会变成代码的反引号 例4 链接中的中括号 例5 不是列表的连接符(横杠) 例6 不是标题的 \\# 例7 不会注释的 \\% 例8 木有链接的双链 例9 页链接里 显示文本内的 中括号 特殊情况 文本修饰的中括号 14. 空格&换行&强制删除 14.1 空格 14.2 换行 14.3 强制删除 15. 嵌入 15.1 嵌入音频 15.2 嵌入视频 15.3 嵌入页面 16. Latex 数学公式 16.1 行内公式 16.2 公式块 17. Mermaid 17.1 流程图 17.2 饼图 17.3 序列图 (时序图) 17.4 甘特图 17.5 类图 18. 标签 (Tag) 关于空格 关于数字 标签的嵌套 能被使用的符号 如何让 \\# 不被识别 19. 避免标识符的滥用",content:'这里是 two-test-1 的内容。\n\n\n\n以下Markdown内容转载自：Markdown超级教程 Obsidian版\n\n这里仅作为展示Vuepress解析Markdown效果的一个展示。\n\n\n# 什么是 Markdown?\n\n 1. Markdown 是一款轻量级标记语言，不同于HTML (Hypertext Markup Language)，Markdown 的语法非常简单，且容易上手\n 2. Markdown 以 纯文本格式 编写文档，依赖键盘而非鼠标，专注于写作本身，感受书写的魅力\n 3. Markdown 的通过添加一些简单的 标识符，让文本具有恰到好处的格式\n 4. Markdown 核心特征就是 删繁剪芜， 简扼 + 精炼\n 5. Markdown 是 笔记 与 网页文章 的最佳载体\n 6. Down 的核心：坐 下 来，就能把思维写 下 来\n    * 牛津高阶英汉双解词典第九版 中，关于 down 的释义：\n\n\n\n\n\n\n# 为什么要使用 Markdown?\n\n有朋友问我 ，Markdown 的效果 用Word 完全可以复现，甚至功能更多，那为何要用 Markdown 呢？\n\n答：\n\n * 功能多，不一定是好事\n   * 功能一多，选择就会变多，然后你会开始纠结……\n     * 这个字号是不是该大一点呢？\n     * 这个颜色好像有点不太搭呢？\n     * 这个粗体，是不是该再加点颜色呢？\n     * 这个图片的位置看起来有点不大对劲呢？\n   * 结果，写了半天，就憋出一点点东西\n     * 写出来的内容...好像...也不咋滴\n\nMD的优势：\n\n 1. Markdown 让我们免于 被繁杂臃肿的功能晃花了眼 的困扰\n 2. Markdown 让我们回归内容本身，拥抱笔记的内核，而非浮于表象的样式，写出高效精练的笔记！\n\n用 Markdown 写东西，记住一个原则\n\n> 能用10个字搞定的，绝不用11个字\n\n经常使用 Markdown 书写的朋友，也许会有一种奇妙的感触\n\n * 书写，会==倒逼==思维的跃进。像是有东西拽着你的思绪往前冲\n   * 倒逼：逆向逼迫，反向推动\n\n关于标识符的滥用\n\n这个其实是写在最后的，之所以放在这里，是因为它很重要！\n\n如果你有一定的MD语法基础，可以直接[[#19 避免标识符的滥用|点击跳转]]\n\n\n\n# Markdown 相关软件推荐\n\n * Markdown 书写软件 推荐：Typora 优秀的 MD网页文章 书写软件\n   * 点击跳转下载地址\n     * #提示 以前是免费的，现在收费了，不过是买断制\n * Markdown 笔记软件 推荐：Obsidian 银河系最强 MD+双向链 笔记软件\n   * 点击跳转下载地址\n\n\n\n\n\n\n# Markdown 语法\n\n * 提示1： 本教程推荐使用 Obsidian 打开阅读\n * 提示2： 下文提到的所有标识符都是 英文状态 的 ！\n\n\n# 1. 标题&目录\n\n\n\n# 1.1 标题\n\n * Markdown标题共有 六级，和 HTML 一样\n * 区分 一级标题 → 六级标题\n   * 标题 的格式：\n     * # × 标题级数 + 空格 + 文本内容\n\n这是一段普通的文本\n\n# 这是一级标题\n## 这是二级标题\n### 这是三级标题\n#### 这是四级标题\n##### 这是五级标题\n###### 这是六级标题\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n# 1.2 目录\n\n * 目录的 格式：\n   * 在文档的顶部 输入 [toc] ，会根据 标题 自动生成目录 ( Table of Content )\n * 不是所有 MD编辑器 都支持目录生成\n   * Obsidian 就不支持，不过 OB 是自带大纲的，就是目录的效果\n\n输入下方内容会生成一个目录：\n\n[toc]\n\n\n1\n2\n3\n\n\n\n\n\n\n\n# 2. 斜体&粗体\n\n\n\n# 2.1 斜体\n\n * 斜体 的格式：\n   1. * + 文本内容 + *\n   2. _ + 文本内容 + _ ( 下划线 )\n * 说明：\n   * 斜体文本，首尾只有 单个 标识符\n\n这是一段普通文本\n\n*这里是一段斜体文本*\n_这也是一段斜体文本_\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段斜体文本 这也是一段斜体文本\n\n\n\n# 2.2 粗体\n\n * 粗体 的格式：\n   \n   1. ** + 文本内容 + **\n   2. __ + 文本内容 + __ (这里是两个 _ )\n\n * 说明：\n   \n   * 粗体文本，首尾各有 两个 标识符\n\n这是一段普通文本\n\n**这里是一段加粗文本**\n__这也是一段加粗文本__\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段加粗文本 这也是一段加粗文本\n\n\n\n# 2.3 粗斜体 (斜粗体)\n\n * 粗斜体 的格式：\n   \n   1. *** + 文本内容 + ***\n   2. ___ + 文本内容 + ___ （ 这里是3个 _ )\n   3. **_ + 文本内容 + _**\n   4. __* + 文本内容 + *__\n   5. *__ + 文本内容 + __*\n   6. _** + 文本内容 + **_\n\n * 说明：\n   \n   * 粗斜体文本，首尾各有 三个 标识符\n\n这是一段普通文本\n\n***粗斜体文本1***\n___粗斜体文本2___\n**_粗斜体文本3_**\n__*粗斜体文本4*__\n*__粗斜体文本5__*\n_**粗斜体文本6**_\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 示范\n\n这是一段普通文本\n\n粗斜体文本1 粗斜体文本2 粗斜体文本3 粗斜体文本4 粗斜体文本5 粗斜体文本6\n\n\n\n# 2.4 斜体包含粗体\n\n * 斜体中包含粗体 的格式：\n   \n   1. * + 斜体文本 + ** + 粗体文本 + ** + 斜体文本 + *\n   2. _ + 斜体文本 + __ + 粗体文本 + __ + 斜体文本 + _ （ 这里是两个 _ )\n   3. * + 斜体文本 + __ + 粗体文本 + __ + 斜体文本 + *\n   4. _ + 斜体文本 + ** + 粗体文本 + ** + 斜体文本 + _\n\n * 说明：\n   \n   * 斜体 中包含 粗体，其实就是嵌套的关系，外层 是 斜体，内层 是 粗体\n   * 外层是斜体，标识符是单个；内层是粗体，标识符是两个\n   * 因为 粗体 是被包裹在 斜体 中的，所以显示效果为 斜粗体\n\n这是一段普通文本\n\n*这里是一段斜体中**包含粗体**的文字*\n_这也是一段斜体中**包含粗体**的文字_\n*这又是一段斜体中__包含粗体__的文字*\n_这还是一段斜体中**包含粗体**的文字_\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段斜体中包含粗体的文字 这也是一段斜体中包含粗体的文字 这又是一段斜体中__包含粗体__的文字 这还是一段斜体中包含粗体的文字\n\n\n\n# 2.5 粗体包含斜体\n\n * 粗体中包含斜体 的格式：\n   1. ** + 粗体文本 + * + 斜体文本 + * + 粗体文本 + **\n   2. __ + 粗体文本 + _ + 斜体文本 + _ + 粗体文本 + __ （ 这里是两个 _ )\n   3. ** + 粗体文本 + _ + 斜体文本 + _ + 粗体文本 + **\n   4. __ + 粗体文本 + * + 斜体文本 + * + 粗体文本 + __\n * 说明：\n   * 粗体 中包含 斜体，也就是嵌套的关系，外层 是 粗体，内层 是 斜体\n   * 外层是粗体，标识符是两个；内层是斜体，标识符是单个\n   * 因为 斜体 是被包裹在 粗体 中的，所以显示效果为 粗斜体\n\n这是一段普通文本\n\n**这里是一段粗体中*包含斜体*的文字**\n__这也是一段粗体中_包含斜体_的文字__\n**这又是一段粗体中_包含斜体_的文字**\n__这还是一段粗体中*包含斜体*的文字__\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段粗体中包含斜体的文字 这也是一段粗体中_包含斜体_的文字 这又是一段粗体中_包含斜体_的文字 这还是一段粗体中包含斜体的文字\n\n\n\n\n\n\n# 3. 线\n\n\n\n# 3.1 水平分割线\n\n * 水平分割线由至少 3 个 * 或 - 组成\n\n下面是一条水平分割线：\n---\n***\n\n\n1\n2\n3\n\n\n# 示范\n\n----------------------------------------\n\n----------------------------------------\n\n\n\n# 3.2 文本删除线\n\n * 删除线 的格式：\n   * ~~ + 文本内容 +~~ 首尾各加两个 ~ 波浪号\n\n~~这是一段加了删除线的文本~~\n\n\n1\n\n\n# 示范\n\n这是一段加了删除线的文本\n\n\n\n# 3.3 文本下划线\n\n * 下划线的格式，和 HTML 是一样的\n   * <u> + 文本内容 + </u>\n\n<u>这是一段加了下划线的文本</u>\n\n\n1\n\n\n# 示范\n\n这是一段加了下划线的文本\n\n\n\n\n\n\n# 4. 列表&引用\n\n\n\n# 4.1 有序列表\n\n * 有序列表 的格式：\n   \n   * 1. + 空格 + 文本内容\n\n * 说明：\n   \n   * 输入文本内容后，敲击 Enter 自动补全格式，并进入 下个 有序列表\n   * 若需要在同个列表内，增加 换行显示 的内容 (但不进入下个列表) 敲击 Shift + Enter ，即可另起一行输入文本\n   * 在有序列表的中间，插入一个新的列表，后面列表的 数字序号 会自动 递进 一层\n   * 即便在源代码模式中修改了数字序号，渲染界面依然是 依照顺序 显示的\n\n1. 这是第一个有序列表 \x3c!-- (Enter) --\x3e\n2. 这是第二个有序列表 \x3c!-- (Enter) --\x3e\n3. 这是第三个有序列表\n\n\n1. 这是第一个有序列表 \x3c!-- (Shift + Enter) --\x3e\n   这是同个列表下，另起一行的文本内容 \x3c!-- (Enter) --\x3e\n2. 这是第二个有序列表 \x3c!-- (Shift + Enter) --\x3e\n   这是同个列表下，另起一行的文本内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 示范\n\n 1. 这是第一个有序列表\n\n 2. 这是第二个有序列表\n\n 3. 这是第三个有序列表\n\n 4. 这是第一个有序列表 这是同个列表下，另起一行的文本内容\n\n 5. 这是第二个有序列表 这是同个列表下，另起一行的文本内容\n\n# 补充\n\n * 由于有序列表存在强制排序性，它的数字序号必然是逐一递进的 若你希望内容前的数字，不依照递进顺序排序，或者以 整百，整十数 排序\n * 可以配合无序列表，在无序列表中输入：\n   * 数字 + . + 内容 #注意 点号 与 内容 之间，没有空格 (其实有空格也行，就是会感觉有点奇怪)\n\n- 10.这是无序列表下，整十数排列的内容\n- 20.这是无序列表下，整十数排列的内容\n- 30.这是无序列表下，整十数排列的内容\n\n\n- 100.这是无序列表下，整百数排列的内容\n- 200.这是无序列表下，整百数排列的内容\n- 300.这是无序列表下，整百数排列的内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n效果：\n\n * 10.这是无序列表下，整十数排列的内容\n * 20.这是无序列表下，整十数排列的内容\n * 30.这是无序列表下，整十数排列的内容\n\n\n * 100.这是无序列表下，整百数排列的内容\n * 200.这是无序列表下，整百数排列的内容\n * 300.这是无序列表下，整百数排列的内容\n\n\n\n# 4.2 无序列表\n\n * 无序列表 的格式：\n * - + 空格 + 文本内容\n * 说明：\n   * 输入文本内容后，敲击 Enter 自动补全格式，并进入 下个 无序列表\n   * 若需要在同个列表内，增加换行显示的内容 (但不进入下个列表) 敲击 Shift + Enter ，即可另起一行输入文本\n * 补充：\n   * 在Obsidian中，按下 Ctrl + Enter\n   * 即可快速生成一个无序列表\n\n- 这是第1个无序列表 \x3c!-- (Enter) --\x3e\n- 这是第2个无序列表 \x3c!-- (Enter) --\x3e\n- 这是第3个无序列表\n\n- 这是第一个无序列表 \x3c!-- (Shift + Enter) --\x3e\n  这是同个列表下，另起一行的文本内容\n- 这是第二个无序列表 \x3c!-- (Shift + Enter) --\x3e\n  这是同个列表下，另起一行的文本内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 示范\n\n * 这是第1个无序列表\n * 这是第2个无序列表\n * 这是第3个无序列表\n\n\n * 这是第一个无序列表 这是同个列表下，另起一行的文本内容\n * 这是第二个无序列表 这是同个列表下，另起一行的文本内容\n\n\n\n# 4.3 引用\n\n * 引用 的格式：\n   * > + 文本内容 （不需要空格)\n * 说明：\n   * 同个引用段落内的换行直接敲击 Enter 即可\n   * 若需添加 第二个独立引用段落 ，连续敲击 两下 Enter 即可\n\n>这是第一段引用文本的第1行 \x3c!-- (Enter) --\x3e\n>这是第一段引用文本的第2行 \x3c!-- (Enter) --\x3e\n\x3c!-- (Enter) --\x3e\n>这是第二段引用文本的第1行 \x3c!-- (Enter) --\x3e\n>这是第二段引用文本内第2行\n\n\n1\n2\n3\n4\n5\n\n\n# 示范\n\n> 这是第一段引用文本的第1行 这是第一段引用文本的第2行\n\n> 这是第二段引用文本的第1行 这是第二段引用文本的第2行\n\n\n\n# 4.4 缩进&退格\n\n在列表和引用的书写过程中，我们需要利用 ==缩进== 与 ==退格== ，让文章肌理分明，更具层级\n\n * 缩进：\n   \n   1. Tab\n   2. Ctrl + [   (左中括号)\n\n * 退格：\n   \n   1. Shift + Tab\n   2. Ctrl + ] （右中括号）\n\n\n# 4.4.1 有序列表的缩&退\n\n1. 第一级有序列表1 \x3c!-- (Enter) --\x3e\n\t1. 第二级有序列表1    \x3c!-- 写文本之前，先( Tab 或 Ctrl + ] ) ；写完文本后，再(Enter) --\x3e\n\t2. 第二级有序列表2 \x3c!-- (Enter) --\x3e\n2. 第一级有序列表2    \x3c!-- 写文本前，先 ( Shift + Tab 或 Ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n * 补充说明：\n   * 有序列表的数字序号，即便你在源代码模式里 强行改掉 数字，它仍然会 依照顺序 显示\n\n# 示范\n\n 1. 第一级有序列表1\n    1. 第二级有序列表1\n    2. 第二级有序列表2\n 2. 第一级有序列表2\n\n\n# 4.4.2 无序列表的缩&退\n\n- 第一级无序列表1 \x3c!-- (Enter) --\x3e\n\t- 第二级无序列表1  \x3c!-- 写文本前，先( Tab 或 Ctrl + ] ) ；写完后，再(Enter) --\x3e\n\t- 第二级无序列表2 \x3c!-- (Enter) --\x3e\n- 第一级无序列表2  \x3c!-- 写文本前，先 ( Shift + Tab 或 Ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n * 第一级无序列表1\n   * 第二级无序列表1\n   * 第二级无序列表2\n * 第一级无序列表2\n\n\n# 4.4.3 引用的缩&退\n\n * 引用的 缩进 和列表 不同\n   * 引用需另起一行，并额外多打一个 > 来完成 缩进\n * 引用的 退格 与列表 相同\n   1. Shift + Tab\n   2. Ctrl + ] （右中括号）\n\n>第一级引用1 \x3c!-- (enter) --\x3e\n>>第二级引用1 \x3c!-- 先打1个 > (这里的第一个 > 是会自动补充的，只需额外增补1个即可) ，再(enter) --\x3e\n>>第二级引用2 \x3c!-- (enter) --\x3e\n>第一级引用2   \x3c!-- 写文本前，先 ( Shift + Tab 或 Ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n> 第一级引用1\n> \n> > 第二级引用1 第二级引用2\n> \n> 第一级引用2\n\n\n * 补充： 在 Obsidian 中，引用的退格是不太一样的\n * **Obsidian **中，如果想让已经缩进的引用 退回一层\n   * 得使用 Shift + Enter ，配合方向键，在多个 > 之间灵活断行 并在下一行 根据需要 选择性补充 >\n * 这个用文字比较难以描述，这里选择用2个带键位的 Gif图 来描述\n\nGif演示1：\n\n\n\n\n\n * 效果1：\n\n> 111\n> \n> > 222\n> > \n> > > 333\n> > \n> > 444\n> \n> 555\n\n\nGif演示2：\n\n\n\n\n\n * 效果2：\n\n> 111\n> \n> > 222\n> > \n> > > 333\n> \n> > 444\n> > \n> > > 555\n> \n> 666\n\n777\n\n\n# 4.4.4 有序&无序&引用 连续套娃\n\n * 有序列表、无序列表、引用 三者之间，可以相互嵌套\n * 核心键 ： Shift + Enter & Enter & Shift + Tab ( 或 Ctrl + [ )\n   * Shift + Enter 在切换格式的嵌套中，是 自带一层 缩进 效果的\n\n1. 第一级 有序列表1 \x3c!-- (Shift + Enter) --\x3e\n\t- 第二级 无序列表1 \x3c!-- (Shift + Enter) --\x3e\n\t\t>第三级 引用1  \x3c!-- (Enter) --\x3e\n\t\t\t- 第四级 无序列表2 \x3c!-- (Shift + Enter) --\x3e\n            \t1. 第五级 有序列表2 \x3c!-- (Enter) --\x3e\n            - 第四级 无序列表3   \x3c!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) ；写完后再 (Enter) --\x3e\n        >第三级 引用2  \x3c!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) ；写完后再 (Enter × 2) --\x3e\n    - 第二级 无序列表4  \x3c!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) --\x3e\n2. 第一级 有序列表3  \x3c!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 示范\n\n 1. 第一级 有序列表1\n    \n    * 第二级 无序列表1\n      \n      > 第三级 引用1\n      > \n      >  * 第四级 无序列表2\n      >    1. 第五级 有序列表2\n      >  * 第四级 无序列表3\n      > \n      > 第三级 引用2\n    \n    * 第二级 无序列表4\n\n 2. 第一级 有序列表3\n\n# 4.4.5 Obsidian 的一些缩退问题\n\n * Obsidian 在列表首行使用缩进的时候，后续的列表会出现一些问题\n   * Tab 和 Shift + tab 会无法 缩进 退格\n     * 可以使用 Ctrl + ] 与 Ctrl + [ 来解决问题\n\n- - 这是第一段就被缩进的列表\n\t- 这是第二段被再次缩进的列表  \x3c!-- 这里需按两次 Ctrl + ] ,Tab键是无效的 --\x3e\n  - 这是第三段列表  \x3c!-- Ctrl + [ --\x3e\n\n\n1\n2\n3\n\n * * 这是第一段就被缩进的列表 - 这是第二段被再次缩进的列表\n     * 这是第三段列表\n\n\n\n\n\n\n# 5. 网页链接与图像\n\n\n\n# 5.1 网页链接\n\n * 网页链接的 格式：\n   * [ + 显示文本内容 + ] + ( + 链接地址 + 空格 + " + 提示信息文本 + " + )\n * 说明：\n   * 显示文本内容，是在渲染界面实际 可见 的文本，用以 说明 链接\n   * 提示信息文本，需鼠标悬停于 显示文本内容 方可触发，用于增加额外提示信息\n     * #注意 "提示信息文本" 是可选项，一般不会填\n     * 一般来讲，需按住 Ctrl + 鼠标左键点击 才可跳转链接，不过也有 直接鼠标点击 就能跳转的\n\n[显示文本内容](链接地址 "提示信息文本")\n\n[百度一下，你就知道](http://www.baidu.com "按住Ctrl点击跳转百度")\n\n\n1\n2\n3\n\n\n示范：\n\n百度一下，你就知道\n\n\n# 5.1.1链接的加粗\n\n * 格式有两种：\n   \n   1. 把一对 ** 加在 ==显示文本内容==的首尾\n      \n      * 格式1：[**显示文本内容**](链接地址)\n      * 效果： 百度一下，你就知道\n   \n   2. 把一对 ** 加在 链接格式==整体== 的首尾\n      \n      * 格式2：**[显示文本内容](链接地址)**\n      * 效果： 百度一下，你就知道\n\n\n\n\n\n\n# 5.2 图像\n\n * 图像格式：\n   * 图像格式，就是在网页链接前面加个 ! (英文格式的)，! 代表 可见\n   * 图片的提示信息，和网页链接一样，写在 " " 内\n   * [ ] 方括号里的文字信息在 Markdown 没啥实质的作用，只是方便在源代码模式下，知道这个图片是什么，在渲染界面是不会显示的。有点类似于HTML img标签 里的 alt属性。\n\n![文字信息](图片链接 "提示文本信息")\n\n![湘湖1](https://z3.ax1x.com/2021/08/06/fuNkXq.jpg "湘湖一角")\n\n\n1\n2\n3\n\n\n * 补充：\n   \n   * 图像链接可以是本地的，也可以是在线的\n     * 本地图像直接 Ctrl + C 黏贴，Ctrl + V 复制 就可以\n     * 在线图像推荐使用 图床\n   * 调整图像的大小需要使用 HTML 和 CSS，在 Typora编辑器 中右键可以直接缩放图片 本质是转成了HTML的格式，最后会有一个 style="zoom: %;" ，这里数值可以自己修改\n   * 如果有使用 Obsidian 的朋友，在线图片链接是通用的。不过，因为 Obsidian 是双向链笔记 它的本地图片格式不太一样\n     * ![[图片名]]\n       * Obsidian 中的图片是以双链的格式引用在目标笔记中，用 ! 使它可见\n       * Obsidian的图片设置大小是用 | 分隔，后面写宽度数值，单位是px。 设定好宽度，高度会自动等比例调整\n         * ![[图片名|宽度数值]] - 若想自主调整图片宽高，则用： - ![[图片名|宽度数值x高度数值]] - #提示 这里的 x 是 英文字母x\n     * 如果是在线图床，需要调整图片大小：\n       * ![图床|宽度数值](链接地址)\n\n# 示范\n\n\n\n\n\n\n\n\n# 6. 表格\n\n * Markdown的表格，比HTML简单很多\n   * | 是构成表格的主要 框架\n   * - 区分 表头 和 表格主体\n   * : 控制 表格内 文本内容 的 对齐方式\n   * **Typora编辑器中 ** 输入 Ctrl + T 即可快速插入表格，自由定义样式\n\n|这里是表头1|这里是表头2|这里是表头3|\n|:-|:-:|-:|    \x3c!--区分表头和表格主体，:代表文本对齐方式，分别是左对齐，居中对齐，右对齐--\x3e\n|单元格数据1|单元格数据2|单元格数据3|\n|单元格数据4|单元格数据5|单元格数据6|\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这里是表头1   这里是表头2   这里是表头3\n单元格数据1   单元格数据2   单元格数据3\n单元格数据4   单元格数据5   单元格数据6\n\n\n\n# 6.1 表格中文本内容的换行\n\n * Mardown中表格，它的宽高是由 单元格数据内的文本内容 撑开 的\n * 当我们输入一段很长很长的文本，它所在的单元格会变得过宽\n\n如下图所示：\n\n表头1                                   表头2\n这是一段很长很长很长很长很长很长很长很长很长很长很长很长很长很长的文本   普通文本\n\n * 若想对一段长文本进行换行，可以在 中间 插入一个 <br> （ 换行标签 )\n\n| 表头1 |  表头2 |\n|:-:|:-:|\n|这是第一行文本<br>这是另起一行的文本|普通文本|\n\n\n1\n2\n3\n\n\n# 示范\n\n表头1         表头2\n这是第一行文本     普通文本\n这是另起一行的文本\n\n\n\n\n\n\n# 7. 代码域\n\n\n\n# 7.1 行内代码\n\n * 行内代码 的格式：\n   * 输入两个 ` 反引号 ，在中间写代码内容\n * 补充：\n   * 行内代码不一定非得写代码，也可以作为**着重标记**，突出显示内容\n   * 行内代码中，源代码界面和渲染界面是完全一致的，标识符会失效\n   * 所谓行内代码： 只要你的屏幕足够宽，它就不会换行\n\n`这是一段行内代码`\n\n`<table border="1" cellspacing="0" width="500" height="500">`\n\n`print("Hello, World!")`\n\n`这是一行突出显示的文本内容`\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 示范\n\n<table border="1" cellspacing="0" width="500" height="500">\n\n\nprint("Hello, World!")\n\n\n这是一行突出显示的文本内容\n\n\n\n# 7.2 代码块\n\n * 代码块 的格式：\n   1. 在首行和末行各加 三个 ` 反引号\n   * ``` + 语言种类 代码内容 ```\n   2. 在首行和末行各加 三个 ~ 波浪号\n      * ~~~ + 语言种类 代码内容 ~~~\n * 补充：\n   * 在代码块也不一定要写代码，可以写一段突出的文本内容，语言类型可以填写 txt 或者 干脆不写\n   * 代码块中，源代码界面和渲染界面是完全一致的，标识符会失效\n   * 在 Typora编辑器 ，用键盘按键脱离代码块区域，需输入： Ctrl + Enter\n\n```语言种类\n代码内容\n代码内容\n代码内容\n```\n\n下面是HTML代码块\n\n```html\n<table border="1">\n    <tr>\n        <td>row 1, cell 1</td>\n        <td>row 1, cell 2</td>\n    </tr>\n    <tr>\n        <td>row 2, cell 1</td>\n        <td>row 2, cell 2</td>\n    </tr>\n</table>\n```\n\n下面是CSS代码块\n\n```css\n.box {\n\twidth: 600px;\n\theight: 400px;\n\tmargin: 100px auto;\n\tbackground-image: linear-gradient(black 33.3%,red 33.3%, red 66.6%, yellow 66.6%, yellow);\n}\n```\n\n下面是JavaScript代码块\n\n```js\n    // 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程\n    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\n    let newarr = [];\n    for (let i = 0, count = 0, sum = 0, len = arr.length; i < len; i++) {\n        sum += arr.shift();\n        count++;\n        if (count % 5 === 0) {\n            newarr.push(sum / 5);\n            sum =  0;\n        }\n    }\n    console.log(newarr);\n\n    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\n    let newarr = [];\n    for (let i = 0, len = arr.length; i < len / 5; i++) {\n        let subarr = arr.splice(0, 5)\n        for (let j = 0, sum = 0; j < subarr.length; j++) {\n            sum += subarr[j];\n        }\n        newarr.push(sum / 5);\n    }\n    console.log(newarr);\n```\n\n\n下面是Python代码块\n\n```python\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\ni = 2\nwhile(i < 100):\n   j = 2\n   while(j <= (i/j)):\n      if not(i%j): break\n      j = j + 1\n   if (j > i/j) : print i, " 是素数"\n   i = i + 1\n\nprint "Good bye!"\n```\n\n下面是一块突出显示的文本\n\n```txt\n这是一段\n突出显示的\n文本内容\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n# 示范\n\n<table border="1">\n    <tr>\n        <td>row 1, cell 1</td>\n        <td>row 1, cell 2</td>\n    </tr>\n    <tr>\n        <td>row 2, cell 1</td>\n        <td>row 2, cell 2</td>\n    </tr>\n</table>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n.box {\n\twidth: 600px;\n\theight: 400px;\n\tmargin: 100px auto;\n\tbackground-image: linear-gradient(black 33.3%, red 33.3%, red 66.6%, yellow 66.6%, yellow);\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n// 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程\nlet arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\nlet newarr = [];\nfor (let i = 0, count = 0, sum = 0, len = arr.length; i < len; i++) {\n\tsum += arr.shift();\n\tcount++;\n\tif (count % 5 === 0) {\n\t\tnewarr.push(sum / 5);\n\t\tsum =  0;\n\t}\n}\nconsole.log(newarr);\n\nlet arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\nlet newarr = [];\nfor (let i = 0, len = arr.length; i < len / 5; i++) {\n\tlet subarr = arr.splice(0, 5)\n\tfor (let j = 0, sum = 0; j < subarr.length; j++) {\n\t\tsum += subarr[j];\n\t}\n\tnewarr.push(sum / 5);\n}\nconsole.log(newarr);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\ni = 2\nwhile(i < 100):\n   j = 2\n   while(j <= (i/j)):\n      if not(i%j): break\n      j = j + 1\n   if (j > i/j) : print i, " 是素数"\n   i = i + 1\n\nprint "Good bye!"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n这是一段\n突出显示的\n文本内容\n\n\n1\n2\n3\n\n\n\n# 7.2.1 代码块的嵌套\n\n\n格式：\n\n * 使用4个 ` 包裹 3个 `\n\n# 示范\n\n````txt\n```js\n// 3. 输出 100以内(不包括100) 所有偶数的和\n// 这类求和问题的核心 ： 利用循环  (总和 = 旧数的和 + 新数)\n\nlet sum = 0;\n\nfor (let i = 1, sum = 0; i < 100; i++) {\n if (i % 2 == 0) {\n // 筛选偶数\n sum += i; // sum = sum + i // 累加偶数并赋值给sum\n // sum为(旧的，已经进入循环的数)的和，i 为新进入循环的数。当加到(最后一个新数i)时，sum就是最后的 总和\n }\n}\n\nconsole.log(sum); // 打印总和\n```\n````\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n如果要再套一层，就在最外层 加 5个 ` ，以此类推……\n\n\n\n# 7.3 如何在行内代码里显示反引号\n\n首尾各用 两个反引号`+ 空格 包裹\n\n格式：\n\n``+空格+带`的内容+空格+``  \x3c!-- 不要忘记前后的两个空格 --\x3e\n\n`` 这是一段能显示`反引号`的行内代码 ``\n\n\n1\n2\n3\n\n\n效果：\n\n这是一段能显示`反引号`的行内代码\n\n\n\n\n\n\n# 8. 任务列表（待办）\n\n * 任务列表 的格式：\n   \n   * - + 空格 +[ ] +空格 + 任务列表内容 ( 中括号[ ] 里面必须有个空格)\n   * 给待办任务列表打 √ ，变成 已办\n     1. 在渲染界面，直接鼠标左键点击框框\n     2. 在源代码界面，在中括号内输入 英文字母x\n        * 部分编辑器，在 中括号内 输入任意字符都可以打 √ ( 例如 Obsidian )\n\n * 补充：\n   \n   * 大部分 MD编辑器 支持输入第一个任务列表后，按下 Enter 进入下一行会 自动补全待办格式\n   * 在Obsidian中，连续输入两次 Ctrl + Enter ，即可生成一个待办列表\n     * 再输入一次 Ctrl + Enter ，会在待办列表 打 √\n\n * 格式：\n\n- [ ] 待办任务列表1\n- [ ] 待办任务列表2\n- [x] 已办任务列表1    \x3c!-- 英文字母X --\x3e\n- [x] 已办任务列表2\n\n\n1\n2\n3\n4\n\n\n\n# 示范\n\n * [ ] 待办任务列表1\n * [ ] 待办任务列表2\n * [x] 已办任务列表1\n * [x] 已办任务列表2\n\n\n * 在 Obsidian 中，可以利用 Ctrl + Enter ，快速生成任务列表\n   1. - + 空格 + Ctrl + Enter +待办文本内容\n   2. 待办文本内容 + Ctrl + Enter ×2   ( 输入文本后，连续2次 Ctrl + enter )\n\n\n * 任务列表也是可以缩进+退格的，操作跟 无序、有序列表一样\n\n\n# 示范\n\n * [ ] 第一级待办列表1\n   * [ ] 第二级待办列表1 另起一行的第二级待办列表1\n     * [x] 第三级已办列表1\n     * [x] 第三级已办列表2\n   * [ ] 第二级待办列表2 另起一行的第二级待办列表2\n * [ ] 第一级待办列表2\n\n\n\n\n\n\n# 9. 注释\n\nMarkdown 的 注释 和 HMTL 一样，注释的内容在 渲染界面 不可见 （部分编辑器可见)\n\n * 注释 的格式：\n   * \x3c!-- 这里是注释的内容 --\x3e\n     * 注释可以是单行，也可以是多行\n   * 如果有在使用 Obsidian 的，它的注释格式是不一样的\n     * %%这是Obsidian的注释内容%%\n\n\x3c!-- 这里是一行注释 --\x3e\n\n\x3c!--\n这里是\n一段\n假装有\n很多行的\n注释\n--\x3e\n\n%%这是一行Obsidian里的注释%%\n\n%%\n这里是\n一段\n假装有\n很多行的\nObsidian里的\n注释\n%%\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 示范 (只有切换至 编辑模式 才能看到喔)\n\n%%这是一行Obsidian里的注释%%\n\n%% 这里是 一段 假装有 很多行的 Obsidian里的 注释 %%\n\n\n\n\n\n\n# 10. 变量\n\n\n\n# 10.1 网页链接变量\n\n * 网页链接变量 的格式：\n   1. 首先输入\n      * [显示文本内容] + [变量名]\n        * 变量名可以自己取，没啥限制，任意字符都可以\n   2. 在文档任意一个区域，输入：\n      * [变量名] + : + 空格 + 链接地址 （这个**空格** 不打也没事)\n\n[百度一下，你就知道][度娘]\n[知乎-有问题，就会有答案][知乎]\n\n\x3c!-- 这里是变量区域 --\x3e\n[度娘]: http://www.baidu.com\n[知乎]: https://www.zhihu.com\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n百度一下，你就知道\n\n知乎-有问题，就会有答案\n\n\n\n# 10.2 脚注\n\n * 脚注 的格式：\n   * 在需要脚注的地方，输入：\n     * [^脚注代号] ( 脚注代号会直接显示在渲染界面 )\n       * 脚注代号可以随便命名，不过推荐使用 数字序号\n   * 在其他区域，输入：\n     * [^脚注代号] + : + 空格 + 脚注内容 （这个 空格 不打也没事)\n\n鲁迅原名是什么[^1] ，浙江哪里人[^2]\n\n\x3c!-- 这里是变量区域 --\x3e\n[^1]: 周树人\n[^2]: 绍兴人\n\n\n1\n2\n3\n4\n5\n\n\n# 示范\n\n鲁迅原名是什么^1，浙江哪里人^2\n\n\n\n\n\n\n# 11. 拓展文本格式标记\n\n * Markdown 想实现更多的文本显示效果，只能依赖HTML标记实现\n * 个人不是很推荐在 MD 中使用 HTML，不过一些简单的标记还是可以 轻度使用 的\n\n\n\n# 11.1 键盘文本\n\n * 键盘文本的 格式：\n   \n   * <kbd>键盘文本</kbd>\n   * <kbd>Ctrl</kbd> + <kbd>X</kbd>\n\n * 效果：\n   \n   * 键盘文本\n   * Ctrl + X ( 剪切 )\n\n * 说明：\n   \n   * 键盘文本也不一定非得是键盘按键，也可以作为着重文本突出显示\n     * 效果： 这也算一种着重文本的方式\n\n# 11.1.1 加粗键盘文本\n\n * 加粗键盘文本的格式有两种：\n   \n   * <kbd>**键盘文本**</kbd>\n   * **<kbd>ctrl + x</kbd>**\n\n * 效果：\n   \n   1. 键盘文本\n   2. ctrl + x\n\n\n\n# 11.2 放大文本\n\n * 放大文本 的格式：\n   \n   * 这是一段普通文本 <big>这是一段放大文本</big>\n\n * 效果：\n   \n   * 这是一段普通文本 这是一段放大文本\n\n# 11.2.1 放大粗体文本\n\n * 放大加粗文本的格式有两种：\n   1. **<big>这是一段放大粗体文本</big>**\n   2. <big>**这是一段放大粗体文本**</big>\n * 效果：\n   1. 这是一段放大粗体文本\n   2. 这是一段放大粗体文本\n\n\n\n# 11.3 缩小文本\n\n * 缩小文本 的格式：\n   * 这是一段普通文本 <small>这是一段缩小文本</small>\n * 效果：\n   * 这是一段普通文本 这是一段缩小文本\n\n# 11.3.1 缩小斜体文本\n\n * 斜体缩小文本 的格式有两种：\n   1. <small>*这是一段缩小斜体文本*</small>\n   2. *<small>这是一段缩小斜体文本</small>*\n * 效果：\n   1. 这是一段缩小斜体文本\n   2. 这是一段缩小斜体文本\n\n\n\n# 11.4 多彩文本\n\n * 多彩文本 的格式：\n   * <font color=orange>这是一段橘色文本</font>\n * 效果：\n   * 这是一段橘色文本\n     * color 里的颜色支持 英文单词，16进制，rgb，rgba\n\n\n# 11.4.1 多彩粗体文本\n\n * 只需要在上面示例的基础上，加上 加粗标识符，有两种格式：\n   1. 格式1： **<font color=teal>这是一段加粗的水鸭色文本</font>**\n      * 效果： 这是一段加粗的水鸭色文本\n   2. 格式2： <font color=teal>**这是一段加粗的水鸭色文本**</font>\n      * 效果： 这是一段加粗的水鸭色文本\n * 若上述混搭方法的样式失效 ，可以使用 纯HTML标记\n   * 格式： <strong style="color:teal;">这是一段加粗的水鸭色文本</strong> (标记略复杂，不是很推荐)\n   * 效果： 这是一段加粗的水鸭色文本\n\n\n# 11.4.2 多彩斜体文本\n\n * 跟多彩加粗文本完全一样，只需把首尾的 ** 换成 * 即可\n\n 1. 格式1： *<font color=teal>This is an italic teal text</font>*\n    * 效果： This is an italic teal text\n 2. 格式2： <font color=teal>*This is an italic teal text*</font>\n    * 效果： This is an italic teal text\n\n\n# 11.4.2 多彩粗斜体文本\n\n * 首尾换成 ***\n\n 1. 格式1： ***<font color=teal>This is a bold italic teal text</font>***\n    * 效果： This is a bold italic teal text\n 2. 格式2： <font color=teal>***This is a bold italic teal text***</font>\n    * 效果： This is a bold italic teal text\n\n\n#注意 多彩文本尽量慎用，Markdown 的核心就是 简洁精炼，注重 实质内容，而非花哨的 颜色样式\n\n\n\n\n\n\n# 12. 拓展文本显示效果\n\n * 拓展显示效果既不是原生 Markdown语法 支持的，也非 HTML标记，而是部分编辑器 提供的 额外标识符，属于拓展语法，旨在为 Markdown使用者 提供更多样式选择\n * 不同编辑器，支持不一样，这里以 Typora编辑器 为例\n\n\n\n# 12.1 文本高亮\n\n * 文本高亮 的格式：\n   * ==这里是一段高亮文本==\n * 效果：\n   * ==这里是一段高亮文本==\n\n\n\n# 12.2 上标\n\n * 用一对 ^ 包裹 (Shift+ 6)\n   * 格式： x^2^\n   * 效果： x^2^\n * Obsidian 没效果的，可以用后面会讲的 Latex\n * 或者，也可以使用 HTML标记\n   * <sup>这里是上标内容</sup>\n   * X<sup>2</sup>\n * 效果：\n   * X2\n\n\n\n# 12.3 下标\n\n * 用一对 ~ 包裹 (Shift + `)\n   * 格式： H~2~O\n   * 效果： H~2~O\n * Obsidian 没效果的，可以用后面会讲的 Latex\n * 或者，也可以使用 HTML标记\n   * <sub>这里是下标内容</sub>\n   * H<sub>2</sub>O\n * 效果：\n   * H2O\n\n\n\n# 12.4 Emoji 符号\n\n用一对 : 包裹，里面是 Emoji 符号的 语义化文本 ( Typora编辑器 中，输入 : 就会带提示器 )\n\n * 示例：\n   * :smile: :sweat: :cat: :woman_cartwheeling:\n * 效果：\n   * 😄 😓 🐱 🤸‍♀\n\n\n * 补充：\n   * 不支持上述方式的 MD编辑器或笔记软件，直接用 输入法 输入也是可以的\n   * Windows系统 用户 win + . 就可以输入 Emoji 了\n   * Obsidian 用户可以安装第三方插件来支持 Emoji 的输入，推荐两个\n     1. ==Emoji Shortcodes==\n     2. ==Emoji Toolbar==\n\n\n\n\n\n\n# 13. 转义字符\n\n * 在 Markdown 中，我们 通过 标识符 改变 文本显示效果\n * 现在我们希望它不作为标识符，而是 作为字符本身呈现出来 （不具备改变文本显示效果的功能，只是一个普通字符)\n   * 首先我们可以用前面介绍的 代码域 ，因为代码模式的显示效果就是源代码完全一致的\n   * 还有一种方法，可以利用转义字符，在这些标识符 前面 加上 反斜线 \\ ( 反斜线要紧贴在标识符前面，不能 有 空格 )\n     * 原理：\n       * \\ 的作用是让标识符 转义 变为一个普通字符，完成这个效果后，反斜线会自动隐藏\n       * 隐藏后的反斜线仅在源代码界面可见，在渲染界面不可见\n       * 反斜线只争对标识符起作用，其他字符添加 \\，\\ 不会自动隐藏\n     * 补充：\n       * 如果想给已经被加在标识符前面，会自动隐藏的 \\ 显示出来，可以在反斜线前面再加一个 \\ ，用它自己来转义自己\n         * 示例： 这里紧跟在标识符前面的反斜线\\\\*会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体*\n         * **效果： ** 这里紧跟在标识符前面的 反斜线\\会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体\n\n\n\n# 例1 以普通字符显示星号\n\n * 如何让被一对或多对 * 号 包裹的文本内容，能够正常显示 * ，且文本不改变格式\n   * \\*这段文本被一对星号包裹，但不会倾斜\\*\n     * 效果： *这段文本被1对星号包裹，但不会倾斜*\n   * \\*\\*这段文本被2对星号包裹，但不会加粗\\*\\*\n     * 效果： **这段文本被2对星号包裹，但不会加粗**\n   * \\*\\*\\*这段文本被3对星号包裹，但它既不倾斜也不加粗\\*\\*\\*\n     * 效果： ***这段文本被3对星号包裹，但它既不倾斜也不加粗***\n\n\n\n# 例2 表格内 单元格中的竖杠\n\n * 在表格中，使用 | 作为单元格的内容，但不会被识别为表格的结构，不会增加额外的单元格\n\n|表头1|表头2|\n|-|-|\n|这里的文本被\\|分隔|这里的文本也被\\|分隔|\n\n\n1\n2\n3\n\n * 效果：\n\n表头1         表头2\n这里的文本被|分隔   这里的文本也被|分隔\n\n\n#补充 该技巧可用于 Obsidian 表格内 双链的文本修饰\n\n文本修饰：\n\n在 双链[[ ]]内 以 | 引导的内容\n\n * 格式： [[链接的内容|文本修饰]]\n * 说明： 文本修饰是渲染界面实际显示的文本，便于更好地融入语境\n\n表格内的格式：\n\n在 | 前面加上 \\\n\n * [[表格内的链接内容\\|文本修饰]]\n\n示例：\n\n|                  表头1                  |                        表头2                        |\n|:---------------------------------------:|:---------------------------------------------------:|\n| [[#例2 表格内 单元格中的竖杠\\|单元格中的竖杠]] | [[#例3 不会变成代码的反引号\\|不会变成代码的反引号]] |\n\n\n1\n2\n3\n\n\n效果：\n\n表头1                           表头2\n[[#例2 表格内 单元格中的竖杠|单元格中的竖杠]]   [[#例3 不会变成代码的反引号|不会变成代码的反引号]]\n\n\n\n# 例3 不会变成代码的反引号\n\n使用 转义符号\\ 让 反引号` 变成普通字符，不再具有[[#7 1 行内代码|行内代码]]的标识符功能\n\n格式：\n\n\\`这段被反引号包裹的内容不会变成行内代码\\`\n\n效果：\n\n`这段被反引号包裹的内容不会变成行内代码`\n\n\n\n# 例4 链接中的中括号\n\n在 网页链接 的 显示文本内容 中，使用 中括号 [ ]\n\n * 在显示文本内容中，在其中一个中括号前面，加上转义符号 反斜杠 \\\n   * 格式： [链接里的 \\[中括号\\] 能被正常显示](https://www.runoob.com)\n   * 效果： 链接里的 [中括号] 能被正常显示\n\n\n\n# 例5 不是列表的连接符(横杠)\n\n * 引用一段话，一般会在换行之后，加上 - 出处\n * 因为 - 是标识符，会变成一个无序列表\n\n如下所示：\n\n> The Web, the Tree, and the String. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。\n> \n>  * 史蒂芬·平克\n\n * 解决方法：\n   \n   * 在 - 前面加上 转义符号 \\\n   \n   >The Web, the Tree, and the String.\n   >写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。\n   >\\- 史蒂芬·平克   \x3c!-- 加上转义符号 \\ , 不会变成无序列表 --\x3e\n   \n   \n   1\n   2\n   3\n   \n\n * 效果：\n\n> The Web, the Tree, and the String. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。 - 史蒂芬·平克\n\n\n\n# 例6 不是标题的 #\n\n让 # 不被识别为标题标识符\n\n格式：\n\n\\# 这里的内容不会被识别为标题\n\n效果：\n\n# 这里的内容不会被识别为标题\n\n\n\n# 例7 不会注释的 %\n\n在 Obsidian 中 注释是前后各两个 % 号\n\n使用 转义符号\\，让 %% 作为普通字符显示出来，不具备注释的功能\n\n * 格式： \\%\\%这里的内容可以被显示喔\\%\\%\n * 效果： %%这里的内容可以被显示喔%%\n\n\n\n# 例8 木有链接的双链\n\nObsidian 的双向链格式是2个方括号 [[ ]] (双方)，使用 转义符号\\，让 [ ] 不再具有 双链功能\n\n格式：\n\n\\[\\[这段文本被双方包裹，但不是一个双向链\\]\\]\n\n效果：\n\n[[这段文本被双方包裹，但不是一个双向链]]\n\n\n\n# 例9 页链接里 显示文本内的 中括号\n\n使用转义符号\\，让中括号可以作为显示文本 在[[#5 1 网页链接|网页链接]]中显示出来\n\n格式：\n\n[\\[这是一个带中括号的网页链接显示文本，点击会跳转至百度\\]](https://www.baidu.com/)\n\n\n1\n\n\n效果：\n\n[这是一个带中括号的网页链接显示文本，点击会跳转至百度]\n\n\n\n# 特殊情况 文本修饰的中括号\n\n文本修饰的 中括号[ ] 不需要使用 转义符号\\\n\n示范：\n\n[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]\n\n效果：\n\n[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]\n\n\n\n\n\n\n# 14. 空格&换行&强制删除\n\n\n\n# 14.1 空格\n\n * 在一些编辑器或者支持MD的笔记软件里，无论你打多少个空格，它只会显示单个 空格 的距离\n   * 可以使用 HTML中 空格 的 字符实体 —— &nbsp;\n   * 若要添加 多个 空格，就输入多个 —— &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n * 格式：\n   * 这里有&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6个空格分隔\n * 效果：\n   * 这里有      6个空格分隔\n\n\n\n# 14.2 换行\n\n场景1：\n\n * 在一些编辑器或者支持MD的笔记软件里，无论你打多少个 回车，它只会显示单个 回车 的空行间距\n   * 可以使用之前表格里提到的 <br> 标签，在 单独一行 中使用，增加额外的空行间距\n   * 如果要增加 多个，就输入 多个 —— <br><br><br><br><br>\n   * #注意 当单独一行使用 <br> 标签的时候，如果前后有标题标识符或者列表标识符，确保 br元素 前后两行都是空白行\n\n格式：\n\n这里是第一段文本\n\n<br><br><br><br><br>     \x3c!-- 这里插入了5个空行间距 --\x3e\n\n这里是第二段文本\n\n\n1\n2\n3\n4\n5\n\n\n效果：\n\n这里是第一段文本\n\n\n\n\n\n\n\n\n这里是第二段文本\n\n\n\n\n\n场景2：\n\n * 在列表中也可以插入换行符\n\n- 这是一段无序列表\n  <br>     \x3c!-- 插入一个空行间距，需单独一行，上下不用预留空格 --\x3e\n  这是同一段无序列表中，空一行距离显示的内容\n- 这是第二段无序列表\n\n\n1\n2\n3\n4\n\n\n效果：\n\n * 这里是第一段无序列表\n   这里是同一段无序列表中，空一行距离显示的内容\n * 这里是第二段无序列表\n\n\n * 补充：\n   * 有一些MD编辑器或笔记软件，严格遵循MD的换行规则，你敲一个回车是没法换行的，必须在 行末 敲 2个空格，再按回车键\n     * 格式：\n       * 这里是一段想换行的文本空格 空格 Enter 这是换行后的文本\n\n\n\n# 14.3 强制删除\n\n * 很多编辑器都有英文标点自动补全功能，自动生成一对，光标落在中间 只想删除前面1个，却会把 一整对 都删掉\n * 在多个列表的嵌套中，也许会遇到一些 无法被删除 的 列表标识符\n * 解决方法： 使用 Shift + Backspace 即可强制删除\n   * Bcakspace   ( 退格键 )\n\n\n\n\n\n\n# 15. 嵌入\n\n * 嵌入都是依赖 HTML标签 实现的，嵌入的都是在线链接格式\n   * 如果是本地的，Obsidian 中音频是有自带的可录制的录音机插件的，其他的 音频、视频 直接复制黏贴就可以了，也可以直接拖拽到OB的笔记界面\n     * 其他的媒体文件在 Obsidian 也和图片一样，以双链的格式引用在目标笔记中，使用 ! 使它可见\n\n\n\n# 15.1 嵌入音频\n\n * 格式：\n   \n   * <audio controls="controls" preload="none" src="音频链接地址"></audio>\n\n * 示例：\n\n<audio controls="controls" preload="none" src="https://www.ldoceonline.com/media/english/exaProns/p008-001803372.mp3?version=1.2.37"></audio>\n\n\n1\n\n * 效果：\n\n\n\n\n\n# 15.2 嵌入视频\n\n * 格式：\n\n<video width="600" height="420" controls>\n  <source src="movie.mp4" type="video/mp4">\n  <source src="movie.ogg" type="video/ogg">\n  <source src="movie.webm" type="video/webm">\n</video>\n\n\n1\n2\n3\n4\n5\n\n * 说明：\n   * width ( 宽度 ) height ( 高度 ) ，可以自己设置，直接输入数字即可，单位默认是 px(像素) 也可以使用 百分比 width=100% 代表水平撑满整个窗口 height=50% 代表垂直撑满半个窗口\n   * Video标签 支持的视频格式 ：MP4 ogg webm\n\n\n\n# 15.3 嵌入页面\n\n * 格式： <iframe width=600 height=400 src="页面链接地址" scrolling="auto" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n<iframe width=600 height=400 src="https://www.runoob.com/html/html-tutorial.html" scrolling="auto" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n\n1\n\n * 效果：\n\n\n * iframe标签 除了嵌入页面，也可以嵌入在线视频，主流的视频网站都会提供嵌入代码\n   \n   * 具体可以看这个 iframe视频嵌入教程\n   * B站 的视频，得在 // 前面补充 http:\n   * 不是所有的 编辑器和笔记软件 都支持这个\n\n * 示例：\n\n<iframe width=600 height=400 src="http://player.bilibili.com/player.html?aid=20190823&bvid=BV1yW411s7og&cid=32964980&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n\n1\n\n * 宽高设置和前面的 video 一样\n\n\n * 效果：\n\n\n\n\n\n\n# 16. Latex 数学公式\n\n * 主要用于 数学公式 与 化学公式 的书写\n\n\n\n# 16.1 行内公式\n\n * 格式：\n   \n   * $ + 行内公式 + $\n\n\n * 示例：\n   * $x^2 + 2x + 5 + \\sqrt x = 0$\n   * $\\ce{CO2 + C -> 2 CO}$\n   * $\\ce{CO2 + C -> 2 CO}$\n   * $\\ce{2Mg + O2 ->[燃烧] 2 MgO}$\n\n\n * 效果：\n   * $x^2 + 2x + 5 + \\sqrt x = 0$\n   * $e^{i\\pi} + 1 = 0$\n   * $\\ce{CO2 + C -> 2 CO}$\n   * $\\ce{2Mg + O2 ->[燃烧] 2 MgO}$\n\n\n\n# 16.2 公式块\n\n * 格式：\n   * $$ 公式块 $$\n\n\n * 示例：\n\n% 化学公式\n$$\n\\ce{Zn^2+  <=>[+ 2OH-][+ 2H+]  $\\underset{\\text{amphoteres Hydroxid}}{\\ce{Zn(OH)2 v}}$  <=>[+ 2OH-][+ 2H+]  $\\underset{\\text{Hydroxozikat}}{\\ce{[Zn(OH)4]^2-}}$}\n$$\n\n\n1\n2\n3\n4\n\n\n% 麦克斯韦方程组\n$$\n\\begin{array}{lll}\n\\nabla\\times E &=& -\\;\\frac{\\partial{B}}{\\partial{t}}\n\\ \\nabla\\times H &=& \\frac{\\partial{D}}{\\partial{t}}+J\n\\ \\nabla\\cdot D &=& \\rho\n\\ \\nabla\\cdot B &=& 0\n\\ \\end{array}\n$$\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n% 薛定谔方程\n$$\ni\\hbar\\frac{\\partial \\psi}{\\partial t} = \\frac{-\\hbar^2}{2m} \\left(\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} \\right) \\psi + V \\psi\n$$\n\n\n1\n2\n3\n4\n\n\n * 效果：\n\n$$ % 化学公式 \\ce{Zn^2+ <=>[+ 2OH-][+ 2H+] $\\underset{\\text{amphoteres Hydroxid}}{\\ce{Zn(OH)2 v}}$ <=>[+ 2OH-][+ 2H+] $\\underset{\\text{Hydroxozikat}}{\\ce{[Zn(OH)4]^2-}}$} $$\n\n\n$$ % 麦克斯韦方程组 \\begin{array}{lll} \\nabla\\times E &=& -;\\frac{\\partial{B}}{\\partial{t}} \\ \\nabla\\times H &=& \\frac{\\partial{D}}{\\partial{t}}+J \\ \\nabla\\cdot D &=& \\rho \\ \\nabla\\cdot B &=& 0 \\ \\end{array} $$\n\n\n$$ i\\hbar\\frac{\\partial \\psi}{\\partial t} = \\frac{-\\hbar^2}{2m} \\left(\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} \\right) \\psi + V \\psi $$\n\n * 补充：\n   * 需要详细教程的，可戳下方链接\n   * Latex详细教程\n\n\n\n\n\n\n# 17. Mermaid\n\n * 一些 MD编辑器 和 笔记软件 支持通过 Mermaid 及其所提供的 编译器 来为用户提供图表的绘制功能\n\n * 这里只提供一些演示的图表，具体教程可戳下方\n   \n   * [[MOC Mermiad 教程 Obsidian版| Mermiad 超级教程 Obsidian版]]\n\n\n\n# 17.1 流程图\n\n\n源码1：\n\n```mermaid\ngraph TB\n\t%% s=start  e=end  f=fork  n=normal\n\n\ts([开始])--\x3ef1{{if条件}};\n\n\t%% 分支点2\n\tf1--true--\x3en1[if语句块]--\x3ee([结束]);\n\tf1--false--\x3ef2{{else if条件}};\n\n\t%% 分支点1\n\tf2--true--\x3en2[else if语句块]--\x3ee;\n\tf2--false--\x3en3[else语句块]--\x3ee;\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n渲染1：\n\ngraph TB\n\t%% s=start  e=end  f=fork  n=normal\n\n\ts([开始])--\x3ef1{{if条件}};\n\n\t%% 分支点1\n\tf1--true--\x3en1[if语句块]--\x3ee([结束]);\n\tf1--false--\x3ef2{{else if条件}};\n\n\t%% 分支点2\n\tf2--true--\x3en2[else if语句块]--\x3ee;\n\tf2--false--\x3en3[else语句块]--\x3ee;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n源码2：\n\n```mermaid\ngraph LR\n\t%% s=start  e=end  f= fork n=normal\n\n\t%% 虚线\n\ts[朱百六]-.->|子|n1[朱四九]-.->|子|n2[朱五四]-.->|子|f1_帝((朱八八))\n\n\t%% 分支点 朱八八\n\tf1_帝--\x3e|长子|f2[朱标]\n\tf1_帝--\x3e|次子|n3[朱樉]\n\tf1_帝--\x3e|三子|n4[朱棢]\n\tf1_帝--\x3e|四子|n5_帝((朱棣))\n\n\t%% 分支点 朱标\n\tf2--\x3e|长子|e1[朱雄英]\n\tf2--\x3e|次子|e2_帝((朱允炆))\n\n\tn5_帝--\x3e|长子|e3[朱高炽]\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n渲染2：\n\ngraph LR\n\t%% s=start  e=end  f= fork n=normal\n\n\t%% 虚线\n\ts[朱百六]-.->|子|n1[朱四九]-.->|子|n2[朱五四]-.->|子|f1_帝((朱八八))\n\n\t%% 分支点 朱八八\n\tf1_帝--\x3e|长子|f2[朱标]\n\tf1_帝--\x3e|次子|n3[朱樉]\n\tf1_帝--\x3e|三子|n4[朱棢]\n\tf1_帝--\x3e|四子|n5_帝((朱棣))\n\n\t%% 分支点 朱标\n\tf2--\x3e|长子|e1[朱雄英]\n\tf2--\x3e|次子|e2_帝((朱允炆))\n\n\tn5_帝--\x3e|长子|e3[朱高炽]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n# 17.2 饼图\n\n\n源码：\n\n```mermaid\npie\n    title 为什么总是宅在家里？\n    "喜欢宅" : 45\n    "天气太热" : 70\n    "穷" : 500\n\t"关你屁事" : 95\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n渲染：\n\npie\n    title 为什么总是宅在家里？\n    "喜欢宅" : 45\n    "天气太热" : 70\n    "穷" : 500\n\t"关你屁事" : 95\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n# 17.3 序列图 (时序图)\n\n\n源码：\n\n```mermaid\nsequenceDiagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant A as Aly\n        participant B as Bob\n        participant C as CofCai\n        %% 便签说明\n        Note left of A: 只复习了一部分\n        Note right of B: 没复习\n        Note over A,B: are contacting\n\n        A->>B: 明天是要考试吗？\n        B--\x3e>A: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par askA\n                C --\x3e> A:你复习好了吗？\n            and askB\n                C --\x3e> B:你复习好了吗？\n            and self\n                C ->>C:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/Every min\n            %% <br/>可以换行\n            C ->> C:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                A ->> C:复习了一点\n            else is common\n                B ->> C:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt Extra response\n                C ->> C:你们怎么不回答我\n            end\n        endsequenceDiagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant A as Aly\n        participant B as Bob\n        participant C as CofCai\n        %% 便签说明\n        Note left of A: 只复习了一部分\n        Note right of B: 没复习\n        Note over A,B: are contacting\n\n        A->>B: 明天是要考试吗？\n        B--\x3e>A: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par askA\n                C --\x3e> A:你复习好了吗？\n            and askB\n                C --\x3e> B:你复习好了吗？\n            and self\n                C ->>C:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/Every min\n            %% <br/>可以换行\n            C ->> C:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                A ->> C:复习了一点\n            else is common\n                B ->> C:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt Extra response\n                C ->> C:你们怎么不回答我\n            end\n        end\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n\n\n渲染：\n\nsequenceDiagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant A as Aly\n        participant B as Bob\n        participant C as CofCai\n        %% 便签说明\n        Note left of A: 只复习了一部分\n        Note right of B: 没复习\n        Note over A,B: are contacting\n\n        A->>B: 明天是要考试吗？\n        B--\x3e>A: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par askA\n                C --\x3e> A:你复习好了吗？\n            and askB\n                C --\x3e> B:你复习好了吗？\n            and self\n                C ->>C:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/Every min\n            %% <br/>可以换行\n            C ->> C:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                A ->> C:复习了一点\n            else is common\n                B ->> C:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt Extra response\n                C ->> C:你们怎么不回答我\n            end\n        end\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n\n# 17.4 甘特图\n\n\n源码：\n\n```mermaid\ngantt\n    title A Gantt Diagram\n    dateFormat  YYYY-MM-DD\n    section Section\n    A task           :a1, 2014-01-01, 30d\n    Another task     :after a1  , 20d\n    section Another\n    Task in sec      :2014-01-12  , 12d\n    another task      : 24d\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n渲染：\n\ngantt\n    title A Gantt Diagram\n    dateFormat  YYYY-MM-DD\n    section Section\n    A task           :a1, 2014-01-01, 30d\n    Another task     :after a1  , 20d\n    section Another\n    Task in sec      :2014-01-12  , 12d\n    another task      : 24d\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 17.5 类图\n\n\n源码：\n\n```mermaid\nclassDiagram\n    Animal <|-- Duck\n    Animal <|-- Fish\n    Animal <|-- Zebra\n    Animal : +int age\n    Animal : +String gender\n    Animal: +isMammal()\n    Animal: +mate()\n    class Duck{\n      +String beakColor\n      +swim()\n      +quack()\n    }\n    class Fish{\n      -int sizeInFeet\n      -canEat()\n    }\n    class Zebra{\n      +bool is_wild\n      +run()\n    }\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n渲染：\n\nclassDiagram\n    Animal <|-- Duck\n    Animal <|-- Fish\n    Animal <|-- Zebra\n    Animal : +int age\n    Animal : +String gender\n    Animal: +isMammal()\n    Animal: +mate()\n    class Duck{\n      +String beakColor\n      +swim()\n      +quack()\n    }\n    class Fish{\n      -int sizeInFeet\n      -canEat()\n    }\n    class Zebra{\n      +bool is_wild\n      +run()\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\n\n\n\n# 18. 标签 (Tag)\n\n * 标签是 Obsidian 特有的一个功能，标签可以通过点击唤起快速搜索 (搜索包含该标签的所有笔记)\n\n格式：\n\n * # + 标签名\n   * #标签名\n\n\n# 关于空格\n\n * 在一段正文文本的后面添加 Tag， # 的前面 需要有个空格\n   * 空格 + # + 标签名\n\n\n * # 与 标签名 之间，不能有空格，否则就变成 一级标题 了\n\n\n * 标签名的内部，不允许使用空格，若想区分标签中的词语，可使用以下三种方法：\n   1. 驼峰式大小写： #BlueTopaz\n   2. 下划线： #blue_topaz\n   3. 连字符： #blue-topaz\n\n\n\n# 关于数字\n\n * 标签内允许使用数字，但不能完全由数字组成\n   * #1984 ❌\n   * #1984Date ⭕\n   * #da_1984_te ⭕\n   * #date-1984 ⭕\n\n\n\n# 标签的嵌套\n\n在标签名内，使用 / 斜杠 可以实现标签的嵌套\n\n格式：\n\n * #主标签/子标签1\n * #主标签/子标签2\n * #主标签/子标签3\n\n嵌套标签可以像普通标签一样通过点击来唤起搜索，嵌套标签允许你选择搜索的层次。例如：\n\n * 搜索 #主标签 ，即可找到包含任意一个子标签的所有笔记\n   * 返回的结果会是上述的三个例子\n * 当你在一个主分类下设置了多个子分类，想找到这个主分类包含的所有内容时，该功能会很实用\n\n\n\n# 能被使用的符号\n\n综上所述，标签内能被使用的符号共有三种\n\n 1. _ 下划线\n 2. - 连字符\n 3. / 斜杠\n\n\n\n# 如何让 # 不被识别\n\n可以使用前面提到的转义符号 \\ 反斜杠，与上述的 转义标题 类似\n\n格式：\n\n\\#这里的内容不会被识别为标签\n\n效果：\n\n#这里的内容不会被识别为标签\n\n\n\n# 19. 避免标识符的滥用\n\n即使在 Markdown 中，也要尽量避免标识符的滥用\n\n比如我的这篇教程，就存在一定程度的滥用\n\n * 其实是因为我这篇是教学性质的，不太一样，有些不能避免\n   * (好吧，我就是在甩锅)\n\n标识符的本质是突出显示，代表重点\n\n * 一篇笔记里的某段文本，使用各式各样的的标识符，会造成重点不清晰\n\n有三种标识，慎用！\n\n 1. 词中对单个汉字的标识\n    1. 卧==虎==藏==龙==\n 2. 短语中对单个英语单词的标识\n    1. get a ==bang== out of\n 3. 标识符的多层嵌套\n    1. 我感觉快要==原地起飞==了\n\n原因：\n\n * 词义的割裂\n * 视觉的混乱\n * 不利于搜索\n   * 卧==虎==藏==龙==\n     * 搜 卧虎 -- 搜不到\n     * 搜 藏龙 -- 搜不到',normalizedContent:'这里是 two-test-1 的内容。\n\n\n\n以下markdown内容转载自：markdown超级教程 obsidian版\n\n这里仅作为展示vuepress解析markdown效果的一个展示。\n\n\n# 什么是 markdown?\n\n 1. markdown 是一款轻量级标记语言，不同于html (hypertext markup language)，markdown 的语法非常简单，且容易上手\n 2. markdown 以 纯文本格式 编写文档，依赖键盘而非鼠标，专注于写作本身，感受书写的魅力\n 3. markdown 的通过添加一些简单的 标识符，让文本具有恰到好处的格式\n 4. markdown 核心特征就是 删繁剪芜， 简扼 + 精炼\n 5. markdown 是 笔记 与 网页文章 的最佳载体\n 6. down 的核心：坐 下 来，就能把思维写 下 来\n    * 牛津高阶英汉双解词典第九版 中，关于 down 的释义：\n\n\n\n\n\n\n# 为什么要使用 markdown?\n\n有朋友问我 ，markdown 的效果 用word 完全可以复现，甚至功能更多，那为何要用 markdown 呢？\n\n答：\n\n * 功能多，不一定是好事\n   * 功能一多，选择就会变多，然后你会开始纠结……\n     * 这个字号是不是该大一点呢？\n     * 这个颜色好像有点不太搭呢？\n     * 这个粗体，是不是该再加点颜色呢？\n     * 这个图片的位置看起来有点不大对劲呢？\n   * 结果，写了半天，就憋出一点点东西\n     * 写出来的内容...好像...也不咋滴\n\nmd的优势：\n\n 1. markdown 让我们免于 被繁杂臃肿的功能晃花了眼 的困扰\n 2. markdown 让我们回归内容本身，拥抱笔记的内核，而非浮于表象的样式，写出高效精练的笔记！\n\n用 markdown 写东西，记住一个原则\n\n> 能用10个字搞定的，绝不用11个字\n\n经常使用 markdown 书写的朋友，也许会有一种奇妙的感触\n\n * 书写，会==倒逼==思维的跃进。像是有东西拽着你的思绪往前冲\n   * 倒逼：逆向逼迫，反向推动\n\n关于标识符的滥用\n\n这个其实是写在最后的，之所以放在这里，是因为它很重要！\n\n如果你有一定的md语法基础，可以直接[[#19 避免标识符的滥用|点击跳转]]\n\n\n\n# markdown 相关软件推荐\n\n * markdown 书写软件 推荐：typora 优秀的 md网页文章 书写软件\n   * 点击跳转下载地址\n     * #提示 以前是免费的，现在收费了，不过是买断制\n * markdown 笔记软件 推荐：obsidian 银河系最强 md+双向链 笔记软件\n   * 点击跳转下载地址\n\n\n\n\n\n\n# markdown 语法\n\n * 提示1： 本教程推荐使用 obsidian 打开阅读\n * 提示2： 下文提到的所有标识符都是 英文状态 的 ！\n\n\n# 1. 标题&目录\n\n\n\n# 1.1 标题\n\n * markdown标题共有 六级，和 html 一样\n * 区分 一级标题 → 六级标题\n   * 标题 的格式：\n     * # × 标题级数 + 空格 + 文本内容\n\n这是一段普通的文本\n\n# 这是一级标题\n## 这是二级标题\n### 这是三级标题\n#### 这是四级标题\n##### 这是五级标题\n###### 这是六级标题\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n# 1.2 目录\n\n * 目录的 格式：\n   * 在文档的顶部 输入 [toc] ，会根据 标题 自动生成目录 ( table of content )\n * 不是所有 md编辑器 都支持目录生成\n   * obsidian 就不支持，不过 ob 是自带大纲的，就是目录的效果\n\n输入下方内容会生成一个目录：\n\n[toc]\n\n\n1\n2\n3\n\n\n\n\n\n\n\n# 2. 斜体&粗体\n\n\n\n# 2.1 斜体\n\n * 斜体 的格式：\n   1. * + 文本内容 + *\n   2. _ + 文本内容 + _ ( 下划线 )\n * 说明：\n   * 斜体文本，首尾只有 单个 标识符\n\n这是一段普通文本\n\n*这里是一段斜体文本*\n_这也是一段斜体文本_\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段斜体文本 这也是一段斜体文本\n\n\n\n# 2.2 粗体\n\n * 粗体 的格式：\n   \n   1. ** + 文本内容 + **\n   2. __ + 文本内容 + __ (这里是两个 _ )\n\n * 说明：\n   \n   * 粗体文本，首尾各有 两个 标识符\n\n这是一段普通文本\n\n**这里是一段加粗文本**\n__这也是一段加粗文本__\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段加粗文本 这也是一段加粗文本\n\n\n\n# 2.3 粗斜体 (斜粗体)\n\n * 粗斜体 的格式：\n   \n   1. *** + 文本内容 + ***\n   2. ___ + 文本内容 + ___ （ 这里是3个 _ )\n   3. **_ + 文本内容 + _**\n   4. __* + 文本内容 + *__\n   5. *__ + 文本内容 + __*\n   6. _** + 文本内容 + **_\n\n * 说明：\n   \n   * 粗斜体文本，首尾各有 三个 标识符\n\n这是一段普通文本\n\n***粗斜体文本1***\n___粗斜体文本2___\n**_粗斜体文本3_**\n__*粗斜体文本4*__\n*__粗斜体文本5__*\n_**粗斜体文本6**_\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 示范\n\n这是一段普通文本\n\n粗斜体文本1 粗斜体文本2 粗斜体文本3 粗斜体文本4 粗斜体文本5 粗斜体文本6\n\n\n\n# 2.4 斜体包含粗体\n\n * 斜体中包含粗体 的格式：\n   \n   1. * + 斜体文本 + ** + 粗体文本 + ** + 斜体文本 + *\n   2. _ + 斜体文本 + __ + 粗体文本 + __ + 斜体文本 + _ （ 这里是两个 _ )\n   3. * + 斜体文本 + __ + 粗体文本 + __ + 斜体文本 + *\n   4. _ + 斜体文本 + ** + 粗体文本 + ** + 斜体文本 + _\n\n * 说明：\n   \n   * 斜体 中包含 粗体，其实就是嵌套的关系，外层 是 斜体，内层 是 粗体\n   * 外层是斜体，标识符是单个；内层是粗体，标识符是两个\n   * 因为 粗体 是被包裹在 斜体 中的，所以显示效果为 斜粗体\n\n这是一段普通文本\n\n*这里是一段斜体中**包含粗体**的文字*\n_这也是一段斜体中**包含粗体**的文字_\n*这又是一段斜体中__包含粗体__的文字*\n_这还是一段斜体中**包含粗体**的文字_\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段斜体中包含粗体的文字 这也是一段斜体中包含粗体的文字 这又是一段斜体中__包含粗体__的文字 这还是一段斜体中包含粗体的文字\n\n\n\n# 2.5 粗体包含斜体\n\n * 粗体中包含斜体 的格式：\n   1. ** + 粗体文本 + * + 斜体文本 + * + 粗体文本 + **\n   2. __ + 粗体文本 + _ + 斜体文本 + _ + 粗体文本 + __ （ 这里是两个 _ )\n   3. ** + 粗体文本 + _ + 斜体文本 + _ + 粗体文本 + **\n   4. __ + 粗体文本 + * + 斜体文本 + * + 粗体文本 + __\n * 说明：\n   * 粗体 中包含 斜体，也就是嵌套的关系，外层 是 粗体，内层 是 斜体\n   * 外层是粗体，标识符是两个；内层是斜体，标识符是单个\n   * 因为 斜体 是被包裹在 粗体 中的，所以显示效果为 粗斜体\n\n这是一段普通文本\n\n**这里是一段粗体中*包含斜体*的文字**\n__这也是一段粗体中_包含斜体_的文字__\n**这又是一段粗体中_包含斜体_的文字**\n__这还是一段粗体中*包含斜体*的文字__\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段粗体中包含斜体的文字 这也是一段粗体中_包含斜体_的文字 这又是一段粗体中_包含斜体_的文字 这还是一段粗体中包含斜体的文字\n\n\n\n\n\n\n# 3. 线\n\n\n\n# 3.1 水平分割线\n\n * 水平分割线由至少 3 个 * 或 - 组成\n\n下面是一条水平分割线：\n---\n***\n\n\n1\n2\n3\n\n\n# 示范\n\n----------------------------------------\n\n----------------------------------------\n\n\n\n# 3.2 文本删除线\n\n * 删除线 的格式：\n   * ~~ + 文本内容 +~~ 首尾各加两个 ~ 波浪号\n\n~~这是一段加了删除线的文本~~\n\n\n1\n\n\n# 示范\n\n这是一段加了删除线的文本\n\n\n\n# 3.3 文本下划线\n\n * 下划线的格式，和 html 是一样的\n   * <u> + 文本内容 + </u>\n\n<u>这是一段加了下划线的文本</u>\n\n\n1\n\n\n# 示范\n\n这是一段加了下划线的文本\n\n\n\n\n\n\n# 4. 列表&引用\n\n\n\n# 4.1 有序列表\n\n * 有序列表 的格式：\n   \n   * 1. + 空格 + 文本内容\n\n * 说明：\n   \n   * 输入文本内容后，敲击 enter 自动补全格式，并进入 下个 有序列表\n   * 若需要在同个列表内，增加 换行显示 的内容 (但不进入下个列表) 敲击 shift + enter ，即可另起一行输入文本\n   * 在有序列表的中间，插入一个新的列表，后面列表的 数字序号 会自动 递进 一层\n   * 即便在源代码模式中修改了数字序号，渲染界面依然是 依照顺序 显示的\n\n1. 这是第一个有序列表 \x3c!-- (enter) --\x3e\n2. 这是第二个有序列表 \x3c!-- (enter) --\x3e\n3. 这是第三个有序列表\n\n\n1. 这是第一个有序列表 \x3c!-- (shift + enter) --\x3e\n   这是同个列表下，另起一行的文本内容 \x3c!-- (enter) --\x3e\n2. 这是第二个有序列表 \x3c!-- (shift + enter) --\x3e\n   这是同个列表下，另起一行的文本内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 示范\n\n 1. 这是第一个有序列表\n\n 2. 这是第二个有序列表\n\n 3. 这是第三个有序列表\n\n 4. 这是第一个有序列表 这是同个列表下，另起一行的文本内容\n\n 5. 这是第二个有序列表 这是同个列表下，另起一行的文本内容\n\n# 补充\n\n * 由于有序列表存在强制排序性，它的数字序号必然是逐一递进的 若你希望内容前的数字，不依照递进顺序排序，或者以 整百，整十数 排序\n * 可以配合无序列表，在无序列表中输入：\n   * 数字 + . + 内容 #注意 点号 与 内容 之间，没有空格 (其实有空格也行，就是会感觉有点奇怪)\n\n- 10.这是无序列表下，整十数排列的内容\n- 20.这是无序列表下，整十数排列的内容\n- 30.这是无序列表下，整十数排列的内容\n\n\n- 100.这是无序列表下，整百数排列的内容\n- 200.这是无序列表下，整百数排列的内容\n- 300.这是无序列表下，整百数排列的内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n效果：\n\n * 10.这是无序列表下，整十数排列的内容\n * 20.这是无序列表下，整十数排列的内容\n * 30.这是无序列表下，整十数排列的内容\n\n\n * 100.这是无序列表下，整百数排列的内容\n * 200.这是无序列表下，整百数排列的内容\n * 300.这是无序列表下，整百数排列的内容\n\n\n\n# 4.2 无序列表\n\n * 无序列表 的格式：\n * - + 空格 + 文本内容\n * 说明：\n   * 输入文本内容后，敲击 enter 自动补全格式，并进入 下个 无序列表\n   * 若需要在同个列表内，增加换行显示的内容 (但不进入下个列表) 敲击 shift + enter ，即可另起一行输入文本\n * 补充：\n   * 在obsidian中，按下 ctrl + enter\n   * 即可快速生成一个无序列表\n\n- 这是第1个无序列表 \x3c!-- (enter) --\x3e\n- 这是第2个无序列表 \x3c!-- (enter) --\x3e\n- 这是第3个无序列表\n\n- 这是第一个无序列表 \x3c!-- (shift + enter) --\x3e\n  这是同个列表下，另起一行的文本内容\n- 这是第二个无序列表 \x3c!-- (shift + enter) --\x3e\n  这是同个列表下，另起一行的文本内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 示范\n\n * 这是第1个无序列表\n * 这是第2个无序列表\n * 这是第3个无序列表\n\n\n * 这是第一个无序列表 这是同个列表下，另起一行的文本内容\n * 这是第二个无序列表 这是同个列表下，另起一行的文本内容\n\n\n\n# 4.3 引用\n\n * 引用 的格式：\n   * > + 文本内容 （不需要空格)\n * 说明：\n   * 同个引用段落内的换行直接敲击 enter 即可\n   * 若需添加 第二个独立引用段落 ，连续敲击 两下 enter 即可\n\n>这是第一段引用文本的第1行 \x3c!-- (enter) --\x3e\n>这是第一段引用文本的第2行 \x3c!-- (enter) --\x3e\n\x3c!-- (enter) --\x3e\n>这是第二段引用文本的第1行 \x3c!-- (enter) --\x3e\n>这是第二段引用文本内第2行\n\n\n1\n2\n3\n4\n5\n\n\n# 示范\n\n> 这是第一段引用文本的第1行 这是第一段引用文本的第2行\n\n> 这是第二段引用文本的第1行 这是第二段引用文本的第2行\n\n\n\n# 4.4 缩进&退格\n\n在列表和引用的书写过程中，我们需要利用 ==缩进== 与 ==退格== ，让文章肌理分明，更具层级\n\n * 缩进：\n   \n   1. tab\n   2. ctrl + [   (左中括号)\n\n * 退格：\n   \n   1. shift + tab\n   2. ctrl + ] （右中括号）\n\n\n# 4.4.1 有序列表的缩&退\n\n1. 第一级有序列表1 \x3c!-- (enter) --\x3e\n\t1. 第二级有序列表1    \x3c!-- 写文本之前，先( tab 或 ctrl + ] ) ；写完文本后，再(enter) --\x3e\n\t2. 第二级有序列表2 \x3c!-- (enter) --\x3e\n2. 第一级有序列表2    \x3c!-- 写文本前，先 ( shift + tab 或 ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n * 补充说明：\n   * 有序列表的数字序号，即便你在源代码模式里 强行改掉 数字，它仍然会 依照顺序 显示\n\n# 示范\n\n 1. 第一级有序列表1\n    1. 第二级有序列表1\n    2. 第二级有序列表2\n 2. 第一级有序列表2\n\n\n# 4.4.2 无序列表的缩&退\n\n- 第一级无序列表1 \x3c!-- (enter) --\x3e\n\t- 第二级无序列表1  \x3c!-- 写文本前，先( tab 或 ctrl + ] ) ；写完后，再(enter) --\x3e\n\t- 第二级无序列表2 \x3c!-- (enter) --\x3e\n- 第一级无序列表2  \x3c!-- 写文本前，先 ( shift + tab 或 ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n * 第一级无序列表1\n   * 第二级无序列表1\n   * 第二级无序列表2\n * 第一级无序列表2\n\n\n# 4.4.3 引用的缩&退\n\n * 引用的 缩进 和列表 不同\n   * 引用需另起一行，并额外多打一个 > 来完成 缩进\n * 引用的 退格 与列表 相同\n   1. shift + tab\n   2. ctrl + ] （右中括号）\n\n>第一级引用1 \x3c!-- (enter) --\x3e\n>>第二级引用1 \x3c!-- 先打1个 > (这里的第一个 > 是会自动补充的，只需额外增补1个即可) ，再(enter) --\x3e\n>>第二级引用2 \x3c!-- (enter) --\x3e\n>第一级引用2   \x3c!-- 写文本前，先 ( shift + tab 或 ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n> 第一级引用1\n> \n> > 第二级引用1 第二级引用2\n> \n> 第一级引用2\n\n\n * 补充： 在 obsidian 中，引用的退格是不太一样的\n * **obsidian **中，如果想让已经缩进的引用 退回一层\n   * 得使用 shift + enter ，配合方向键，在多个 > 之间灵活断行 并在下一行 根据需要 选择性补充 >\n * 这个用文字比较难以描述，这里选择用2个带键位的 gif图 来描述\n\ngif演示1：\n\n\n\n\n\n * 效果1：\n\n> 111\n> \n> > 222\n> > \n> > > 333\n> > \n> > 444\n> \n> 555\n\n\ngif演示2：\n\n\n\n\n\n * 效果2：\n\n> 111\n> \n> > 222\n> > \n> > > 333\n> \n> > 444\n> > \n> > > 555\n> \n> 666\n\n777\n\n\n# 4.4.4 有序&无序&引用 连续套娃\n\n * 有序列表、无序列表、引用 三者之间，可以相互嵌套\n * 核心键 ： shift + enter & enter & shift + tab ( 或 ctrl + [ )\n   * shift + enter 在切换格式的嵌套中，是 自带一层 缩进 效果的\n\n1. 第一级 有序列表1 \x3c!-- (shift + enter) --\x3e\n\t- 第二级 无序列表1 \x3c!-- (shift + enter) --\x3e\n\t\t>第三级 引用1  \x3c!-- (enter) --\x3e\n\t\t\t- 第四级 无序列表2 \x3c!-- (shift + enter) --\x3e\n            \t1. 第五级 有序列表2 \x3c!-- (enter) --\x3e\n            - 第四级 无序列表3   \x3c!-- 写文本前，先( shift + tab 或 ctrl + [ ) ；写完后再 (enter) --\x3e\n        >第三级 引用2  \x3c!-- 写文本前，先( shift + tab 或 ctrl + [ ) ；写完后再 (enter × 2) --\x3e\n    - 第二级 无序列表4  \x3c!-- 写文本前，先( shift + tab 或 ctrl + [ ) --\x3e\n2. 第一级 有序列表3  \x3c!-- 写文本前，先( shift + tab 或 ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 示范\n\n 1. 第一级 有序列表1\n    \n    * 第二级 无序列表1\n      \n      > 第三级 引用1\n      > \n      >  * 第四级 无序列表2\n      >    1. 第五级 有序列表2\n      >  * 第四级 无序列表3\n      > \n      > 第三级 引用2\n    \n    * 第二级 无序列表4\n\n 2. 第一级 有序列表3\n\n# 4.4.5 obsidian 的一些缩退问题\n\n * obsidian 在列表首行使用缩进的时候，后续的列表会出现一些问题\n   * tab 和 shift + tab 会无法 缩进 退格\n     * 可以使用 ctrl + ] 与 ctrl + [ 来解决问题\n\n- - 这是第一段就被缩进的列表\n\t- 这是第二段被再次缩进的列表  \x3c!-- 这里需按两次 ctrl + ] ,tab键是无效的 --\x3e\n  - 这是第三段列表  \x3c!-- ctrl + [ --\x3e\n\n\n1\n2\n3\n\n * * 这是第一段就被缩进的列表 - 这是第二段被再次缩进的列表\n     * 这是第三段列表\n\n\n\n\n\n\n# 5. 网页链接与图像\n\n\n\n# 5.1 网页链接\n\n * 网页链接的 格式：\n   * [ + 显示文本内容 + ] + ( + 链接地址 + 空格 + " + 提示信息文本 + " + )\n * 说明：\n   * 显示文本内容，是在渲染界面实际 可见 的文本，用以 说明 链接\n   * 提示信息文本，需鼠标悬停于 显示文本内容 方可触发，用于增加额外提示信息\n     * #注意 "提示信息文本" 是可选项，一般不会填\n     * 一般来讲，需按住 ctrl + 鼠标左键点击 才可跳转链接，不过也有 直接鼠标点击 就能跳转的\n\n[显示文本内容](链接地址 "提示信息文本")\n\n[百度一下，你就知道](http://www.baidu.com "按住ctrl点击跳转百度")\n\n\n1\n2\n3\n\n\n示范：\n\n百度一下，你就知道\n\n\n# 5.1.1链接的加粗\n\n * 格式有两种：\n   \n   1. 把一对 ** 加在 ==显示文本内容==的首尾\n      \n      * 格式1：[**显示文本内容**](链接地址)\n      * 效果： 百度一下，你就知道\n   \n   2. 把一对 ** 加在 链接格式==整体== 的首尾\n      \n      * 格式2：**[显示文本内容](链接地址)**\n      * 效果： 百度一下，你就知道\n\n\n\n\n\n\n# 5.2 图像\n\n * 图像格式：\n   * 图像格式，就是在网页链接前面加个 ! (英文格式的)，! 代表 可见\n   * 图片的提示信息，和网页链接一样，写在 " " 内\n   * [ ] 方括号里的文字信息在 markdown 没啥实质的作用，只是方便在源代码模式下，知道这个图片是什么，在渲染界面是不会显示的。有点类似于html img标签 里的 alt属性。\n\n![文字信息](图片链接 "提示文本信息")\n\n![湘湖1](https://z3.ax1x.com/2021/08/06/funkxq.jpg "湘湖一角")\n\n\n1\n2\n3\n\n\n * 补充：\n   \n   * 图像链接可以是本地的，也可以是在线的\n     * 本地图像直接 ctrl + c 黏贴，ctrl + v 复制 就可以\n     * 在线图像推荐使用 图床\n   * 调整图像的大小需要使用 html 和 css，在 typora编辑器 中右键可以直接缩放图片 本质是转成了html的格式，最后会有一个 style="zoom: %;" ，这里数值可以自己修改\n   * 如果有使用 obsidian 的朋友，在线图片链接是通用的。不过，因为 obsidian 是双向链笔记 它的本地图片格式不太一样\n     * ![[图片名]]\n       * obsidian 中的图片是以双链的格式引用在目标笔记中，用 ! 使它可见\n       * obsidian的图片设置大小是用 | 分隔，后面写宽度数值，单位是px。 设定好宽度，高度会自动等比例调整\n         * ![[图片名|宽度数值]] - 若想自主调整图片宽高，则用： - ![[图片名|宽度数值x高度数值]] - #提示 这里的 x 是 英文字母x\n     * 如果是在线图床，需要调整图片大小：\n       * ![图床|宽度数值](链接地址)\n\n# 示范\n\n\n\n\n\n\n\n\n# 6. 表格\n\n * markdown的表格，比html简单很多\n   * | 是构成表格的主要 框架\n   * - 区分 表头 和 表格主体\n   * : 控制 表格内 文本内容 的 对齐方式\n   * **typora编辑器中 ** 输入 ctrl + t 即可快速插入表格，自由定义样式\n\n|这里是表头1|这里是表头2|这里是表头3|\n|:-|:-:|-:|    \x3c!--区分表头和表格主体，:代表文本对齐方式，分别是左对齐，居中对齐，右对齐--\x3e\n|单元格数据1|单元格数据2|单元格数据3|\n|单元格数据4|单元格数据5|单元格数据6|\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这里是表头1   这里是表头2   这里是表头3\n单元格数据1   单元格数据2   单元格数据3\n单元格数据4   单元格数据5   单元格数据6\n\n\n\n# 6.1 表格中文本内容的换行\n\n * mardown中表格，它的宽高是由 单元格数据内的文本内容 撑开 的\n * 当我们输入一段很长很长的文本，它所在的单元格会变得过宽\n\n如下图所示：\n\n表头1                                   表头2\n这是一段很长很长很长很长很长很长很长很长很长很长很长很长很长很长的文本   普通文本\n\n * 若想对一段长文本进行换行，可以在 中间 插入一个 <br> （ 换行标签 )\n\n| 表头1 |  表头2 |\n|:-:|:-:|\n|这是第一行文本<br>这是另起一行的文本|普通文本|\n\n\n1\n2\n3\n\n\n# 示范\n\n表头1         表头2\n这是第一行文本     普通文本\n这是另起一行的文本\n\n\n\n\n\n\n# 7. 代码域\n\n\n\n# 7.1 行内代码\n\n * 行内代码 的格式：\n   * 输入两个 ` 反引号 ，在中间写代码内容\n * 补充：\n   * 行内代码不一定非得写代码，也可以作为**着重标记**，突出显示内容\n   * 行内代码中，源代码界面和渲染界面是完全一致的，标识符会失效\n   * 所谓行内代码： 只要你的屏幕足够宽，它就不会换行\n\n`这是一段行内代码`\n\n`<table border="1" cellspacing="0" width="500" height="500">`\n\n`print("hello, world!")`\n\n`这是一行突出显示的文本内容`\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 示范\n\n<table border="1" cellspacing="0" width="500" height="500">\n\n\nprint("hello, world!")\n\n\n这是一行突出显示的文本内容\n\n\n\n# 7.2 代码块\n\n * 代码块 的格式：\n   1. 在首行和末行各加 三个 ` 反引号\n   * ``` + 语言种类 代码内容 ```\n   2. 在首行和末行各加 三个 ~ 波浪号\n      * ~~~ + 语言种类 代码内容 ~~~\n * 补充：\n   * 在代码块也不一定要写代码，可以写一段突出的文本内容，语言类型可以填写 txt 或者 干脆不写\n   * 代码块中，源代码界面和渲染界面是完全一致的，标识符会失效\n   * 在 typora编辑器 ，用键盘按键脱离代码块区域，需输入： ctrl + enter\n\n```语言种类\n代码内容\n代码内容\n代码内容\n```\n\n下面是html代码块\n\n```html\n<table border="1">\n    <tr>\n        <td>row 1, cell 1</td>\n        <td>row 1, cell 2</td>\n    </tr>\n    <tr>\n        <td>row 2, cell 1</td>\n        <td>row 2, cell 2</td>\n    </tr>\n</table>\n```\n\n下面是css代码块\n\n```css\n.box {\n\twidth: 600px;\n\theight: 400px;\n\tmargin: 100px auto;\n\tbackground-image: linear-gradient(black 33.3%,red 33.3%, red 66.6%, yellow 66.6%, yellow);\n}\n```\n\n下面是javascript代码块\n\n```js\n    // 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程\n    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\n    let newarr = [];\n    for (let i = 0, count = 0, sum = 0, len = arr.length; i < len; i++) {\n        sum += arr.shift();\n        count++;\n        if (count % 5 === 0) {\n            newarr.push(sum / 5);\n            sum =  0;\n        }\n    }\n    console.log(newarr);\n\n    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\n    let newarr = [];\n    for (let i = 0, len = arr.length; i < len / 5; i++) {\n        let subarr = arr.splice(0, 5)\n        for (let j = 0, sum = 0; j < subarr.length; j++) {\n            sum += subarr[j];\n        }\n        newarr.push(sum / 5);\n    }\n    console.log(newarr);\n```\n\n\n下面是python代码块\n\n```python\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\ni = 2\nwhile(i < 100):\n   j = 2\n   while(j <= (i/j)):\n      if not(i%j): break\n      j = j + 1\n   if (j > i/j) : print i, " 是素数"\n   i = i + 1\n\nprint "good bye!"\n```\n\n下面是一块突出显示的文本\n\n```txt\n这是一段\n突出显示的\n文本内容\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n# 示范\n\n<table border="1">\n    <tr>\n        <td>row 1, cell 1</td>\n        <td>row 1, cell 2</td>\n    </tr>\n    <tr>\n        <td>row 2, cell 1</td>\n        <td>row 2, cell 2</td>\n    </tr>\n</table>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n.box {\n\twidth: 600px;\n\theight: 400px;\n\tmargin: 100px auto;\n\tbackground-image: linear-gradient(black 33.3%, red 33.3%, red 66.6%, yellow 66.6%, yellow);\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n// 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程\nlet arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\nlet newarr = [];\nfor (let i = 0, count = 0, sum = 0, len = arr.length; i < len; i++) {\n\tsum += arr.shift();\n\tcount++;\n\tif (count % 5 === 0) {\n\t\tnewarr.push(sum / 5);\n\t\tsum =  0;\n\t}\n}\nconsole.log(newarr);\n\nlet arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\nlet newarr = [];\nfor (let i = 0, len = arr.length; i < len / 5; i++) {\n\tlet subarr = arr.splice(0, 5)\n\tfor (let j = 0, sum = 0; j < subarr.length; j++) {\n\t\tsum += subarr[j];\n\t}\n\tnewarr.push(sum / 5);\n}\nconsole.log(newarr);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\ni = 2\nwhile(i < 100):\n   j = 2\n   while(j <= (i/j)):\n      if not(i%j): break\n      j = j + 1\n   if (j > i/j) : print i, " 是素数"\n   i = i + 1\n\nprint "good bye!"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n这是一段\n突出显示的\n文本内容\n\n\n1\n2\n3\n\n\n\n# 7.2.1 代码块的嵌套\n\n\n格式：\n\n * 使用4个 ` 包裹 3个 `\n\n# 示范\n\n````txt\n```js\n// 3. 输出 100以内(不包括100) 所有偶数的和\n// 这类求和问题的核心 ： 利用循环  (总和 = 旧数的和 + 新数)\n\nlet sum = 0;\n\nfor (let i = 1, sum = 0; i < 100; i++) {\n if (i % 2 == 0) {\n // 筛选偶数\n sum += i; // sum = sum + i // 累加偶数并赋值给sum\n // sum为(旧的，已经进入循环的数)的和，i 为新进入循环的数。当加到(最后一个新数i)时，sum就是最后的 总和\n }\n}\n\nconsole.log(sum); // 打印总和\n```\n````\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n如果要再套一层，就在最外层 加 5个 ` ，以此类推……\n\n\n\n# 7.3 如何在行内代码里显示反引号\n\n首尾各用 两个反引号`+ 空格 包裹\n\n格式：\n\n``+空格+带`的内容+空格+``  \x3c!-- 不要忘记前后的两个空格 --\x3e\n\n`` 这是一段能显示`反引号`的行内代码 ``\n\n\n1\n2\n3\n\n\n效果：\n\n这是一段能显示`反引号`的行内代码\n\n\n\n\n\n\n# 8. 任务列表（待办）\n\n * 任务列表 的格式：\n   \n   * - + 空格 +[ ] +空格 + 任务列表内容 ( 中括号[ ] 里面必须有个空格)\n   * 给待办任务列表打 √ ，变成 已办\n     1. 在渲染界面，直接鼠标左键点击框框\n     2. 在源代码界面，在中括号内输入 英文字母x\n        * 部分编辑器，在 中括号内 输入任意字符都可以打 √ ( 例如 obsidian )\n\n * 补充：\n   \n   * 大部分 md编辑器 支持输入第一个任务列表后，按下 enter 进入下一行会 自动补全待办格式\n   * 在obsidian中，连续输入两次 ctrl + enter ，即可生成一个待办列表\n     * 再输入一次 ctrl + enter ，会在待办列表 打 √\n\n * 格式：\n\n- [ ] 待办任务列表1\n- [ ] 待办任务列表2\n- [x] 已办任务列表1    \x3c!-- 英文字母x --\x3e\n- [x] 已办任务列表2\n\n\n1\n2\n3\n4\n\n\n\n# 示范\n\n * [ ] 待办任务列表1\n * [ ] 待办任务列表2\n * [x] 已办任务列表1\n * [x] 已办任务列表2\n\n\n * 在 obsidian 中，可以利用 ctrl + enter ，快速生成任务列表\n   1. - + 空格 + ctrl + enter +待办文本内容\n   2. 待办文本内容 + ctrl + enter ×2   ( 输入文本后，连续2次 ctrl + enter )\n\n\n * 任务列表也是可以缩进+退格的，操作跟 无序、有序列表一样\n\n\n# 示范\n\n * [ ] 第一级待办列表1\n   * [ ] 第二级待办列表1 另起一行的第二级待办列表1\n     * [x] 第三级已办列表1\n     * [x] 第三级已办列表2\n   * [ ] 第二级待办列表2 另起一行的第二级待办列表2\n * [ ] 第一级待办列表2\n\n\n\n\n\n\n# 9. 注释\n\nmarkdown 的 注释 和 hmtl 一样，注释的内容在 渲染界面 不可见 （部分编辑器可见)\n\n * 注释 的格式：\n   * \x3c!-- 这里是注释的内容 --\x3e\n     * 注释可以是单行，也可以是多行\n   * 如果有在使用 obsidian 的，它的注释格式是不一样的\n     * %%这是obsidian的注释内容%%\n\n\x3c!-- 这里是一行注释 --\x3e\n\n\x3c!--\n这里是\n一段\n假装有\n很多行的\n注释\n--\x3e\n\n%%这是一行obsidian里的注释%%\n\n%%\n这里是\n一段\n假装有\n很多行的\nobsidian里的\n注释\n%%\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 示范 (只有切换至 编辑模式 才能看到喔)\n\n%%这是一行obsidian里的注释%%\n\n%% 这里是 一段 假装有 很多行的 obsidian里的 注释 %%\n\n\n\n\n\n\n# 10. 变量\n\n\n\n# 10.1 网页链接变量\n\n * 网页链接变量 的格式：\n   1. 首先输入\n      * [显示文本内容] + [变量名]\n        * 变量名可以自己取，没啥限制，任意字符都可以\n   2. 在文档任意一个区域，输入：\n      * [变量名] + : + 空格 + 链接地址 （这个**空格** 不打也没事)\n\n[百度一下，你就知道][度娘]\n[知乎-有问题，就会有答案][知乎]\n\n\x3c!-- 这里是变量区域 --\x3e\n[度娘]: http://www.baidu.com\n[知乎]: https://www.zhihu.com\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n百度一下，你就知道\n\n知乎-有问题，就会有答案\n\n\n\n# 10.2 脚注\n\n * 脚注 的格式：\n   * 在需要脚注的地方，输入：\n     * [^脚注代号] ( 脚注代号会直接显示在渲染界面 )\n       * 脚注代号可以随便命名，不过推荐使用 数字序号\n   * 在其他区域，输入：\n     * [^脚注代号] + : + 空格 + 脚注内容 （这个 空格 不打也没事)\n\n鲁迅原名是什么[^1] ，浙江哪里人[^2]\n\n\x3c!-- 这里是变量区域 --\x3e\n[^1]: 周树人\n[^2]: 绍兴人\n\n\n1\n2\n3\n4\n5\n\n\n# 示范\n\n鲁迅原名是什么^1，浙江哪里人^2\n\n\n\n\n\n\n# 11. 拓展文本格式标记\n\n * markdown 想实现更多的文本显示效果，只能依赖html标记实现\n * 个人不是很推荐在 md 中使用 html，不过一些简单的标记还是可以 轻度使用 的\n\n\n\n# 11.1 键盘文本\n\n * 键盘文本的 格式：\n   \n   * <kbd>键盘文本</kbd>\n   * <kbd>ctrl</kbd> + <kbd>x</kbd>\n\n * 效果：\n   \n   * 键盘文本\n   * ctrl + x ( 剪切 )\n\n * 说明：\n   \n   * 键盘文本也不一定非得是键盘按键，也可以作为着重文本突出显示\n     * 效果： 这也算一种着重文本的方式\n\n# 11.1.1 加粗键盘文本\n\n * 加粗键盘文本的格式有两种：\n   \n   * <kbd>**键盘文本**</kbd>\n   * **<kbd>ctrl + x</kbd>**\n\n * 效果：\n   \n   1. 键盘文本\n   2. ctrl + x\n\n\n\n# 11.2 放大文本\n\n * 放大文本 的格式：\n   \n   * 这是一段普通文本 <big>这是一段放大文本</big>\n\n * 效果：\n   \n   * 这是一段普通文本 这是一段放大文本\n\n# 11.2.1 放大粗体文本\n\n * 放大加粗文本的格式有两种：\n   1. **<big>这是一段放大粗体文本</big>**\n   2. <big>**这是一段放大粗体文本**</big>\n * 效果：\n   1. 这是一段放大粗体文本\n   2. 这是一段放大粗体文本\n\n\n\n# 11.3 缩小文本\n\n * 缩小文本 的格式：\n   * 这是一段普通文本 <small>这是一段缩小文本</small>\n * 效果：\n   * 这是一段普通文本 这是一段缩小文本\n\n# 11.3.1 缩小斜体文本\n\n * 斜体缩小文本 的格式有两种：\n   1. <small>*这是一段缩小斜体文本*</small>\n   2. *<small>这是一段缩小斜体文本</small>*\n * 效果：\n   1. 这是一段缩小斜体文本\n   2. 这是一段缩小斜体文本\n\n\n\n# 11.4 多彩文本\n\n * 多彩文本 的格式：\n   * <font color=orange>这是一段橘色文本</font>\n * 效果：\n   * 这是一段橘色文本\n     * color 里的颜色支持 英文单词，16进制，rgb，rgba\n\n\n# 11.4.1 多彩粗体文本\n\n * 只需要在上面示例的基础上，加上 加粗标识符，有两种格式：\n   1. 格式1： **<font color=teal>这是一段加粗的水鸭色文本</font>**\n      * 效果： 这是一段加粗的水鸭色文本\n   2. 格式2： <font color=teal>**这是一段加粗的水鸭色文本**</font>\n      * 效果： 这是一段加粗的水鸭色文本\n * 若上述混搭方法的样式失效 ，可以使用 纯html标记\n   * 格式： <strong style="color:teal;">这是一段加粗的水鸭色文本</strong> (标记略复杂，不是很推荐)\n   * 效果： 这是一段加粗的水鸭色文本\n\n\n# 11.4.2 多彩斜体文本\n\n * 跟多彩加粗文本完全一样，只需把首尾的 ** 换成 * 即可\n\n 1. 格式1： *<font color=teal>this is an italic teal text</font>*\n    * 效果： this is an italic teal text\n 2. 格式2： <font color=teal>*this is an italic teal text*</font>\n    * 效果： this is an italic teal text\n\n\n# 11.4.2 多彩粗斜体文本\n\n * 首尾换成 ***\n\n 1. 格式1： ***<font color=teal>this is a bold italic teal text</font>***\n    * 效果： this is a bold italic teal text\n 2. 格式2： <font color=teal>***this is a bold italic teal text***</font>\n    * 效果： this is a bold italic teal text\n\n\n#注意 多彩文本尽量慎用，markdown 的核心就是 简洁精炼，注重 实质内容，而非花哨的 颜色样式\n\n\n\n\n\n\n# 12. 拓展文本显示效果\n\n * 拓展显示效果既不是原生 markdown语法 支持的，也非 html标记，而是部分编辑器 提供的 额外标识符，属于拓展语法，旨在为 markdown使用者 提供更多样式选择\n * 不同编辑器，支持不一样，这里以 typora编辑器 为例\n\n\n\n# 12.1 文本高亮\n\n * 文本高亮 的格式：\n   * ==这里是一段高亮文本==\n * 效果：\n   * ==这里是一段高亮文本==\n\n\n\n# 12.2 上标\n\n * 用一对 ^ 包裹 (shift+ 6)\n   * 格式： x^2^\n   * 效果： x^2^\n * obsidian 没效果的，可以用后面会讲的 latex\n * 或者，也可以使用 html标记\n   * <sup>这里是上标内容</sup>\n   * x<sup>2</sup>\n * 效果：\n   * x2\n\n\n\n# 12.3 下标\n\n * 用一对 ~ 包裹 (shift + `)\n   * 格式： h~2~o\n   * 效果： h~2~o\n * obsidian 没效果的，可以用后面会讲的 latex\n * 或者，也可以使用 html标记\n   * <sub>这里是下标内容</sub>\n   * h<sub>2</sub>o\n * 效果：\n   * h2o\n\n\n\n# 12.4 emoji 符号\n\n用一对 : 包裹，里面是 emoji 符号的 语义化文本 ( typora编辑器 中，输入 : 就会带提示器 )\n\n * 示例：\n   * :smile: :sweat: :cat: :woman_cartwheeling:\n * 效果：\n   * 😄 😓 🐱 🤸‍♀\n\n\n * 补充：\n   * 不支持上述方式的 md编辑器或笔记软件，直接用 输入法 输入也是可以的\n   * windows系统 用户 win + . 就可以输入 emoji 了\n   * obsidian 用户可以安装第三方插件来支持 emoji 的输入，推荐两个\n     1. ==emoji shortcodes==\n     2. ==emoji toolbar==\n\n\n\n\n\n\n# 13. 转义字符\n\n * 在 markdown 中，我们 通过 标识符 改变 文本显示效果\n * 现在我们希望它不作为标识符，而是 作为字符本身呈现出来 （不具备改变文本显示效果的功能，只是一个普通字符)\n   * 首先我们可以用前面介绍的 代码域 ，因为代码模式的显示效果就是源代码完全一致的\n   * 还有一种方法，可以利用转义字符，在这些标识符 前面 加上 反斜线 \\ ( 反斜线要紧贴在标识符前面，不能 有 空格 )\n     * 原理：\n       * \\ 的作用是让标识符 转义 变为一个普通字符，完成这个效果后，反斜线会自动隐藏\n       * 隐藏后的反斜线仅在源代码界面可见，在渲染界面不可见\n       * 反斜线只争对标识符起作用，其他字符添加 \\，\\ 不会自动隐藏\n     * 补充：\n       * 如果想给已经被加在标识符前面，会自动隐藏的 \\ 显示出来，可以在反斜线前面再加一个 \\ ，用它自己来转义自己\n         * 示例： 这里紧跟在标识符前面的反斜线\\\\*会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体*\n         * **效果： ** 这里紧跟在标识符前面的 反斜线\\会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体\n\n\n\n# 例1 以普通字符显示星号\n\n * 如何让被一对或多对 * 号 包裹的文本内容，能够正常显示 * ，且文本不改变格式\n   * \\*这段文本被一对星号包裹，但不会倾斜\\*\n     * 效果： *这段文本被1对星号包裹，但不会倾斜*\n   * \\*\\*这段文本被2对星号包裹，但不会加粗\\*\\*\n     * 效果： **这段文本被2对星号包裹，但不会加粗**\n   * \\*\\*\\*这段文本被3对星号包裹，但它既不倾斜也不加粗\\*\\*\\*\n     * 效果： ***这段文本被3对星号包裹，但它既不倾斜也不加粗***\n\n\n\n# 例2 表格内 单元格中的竖杠\n\n * 在表格中，使用 | 作为单元格的内容，但不会被识别为表格的结构，不会增加额外的单元格\n\n|表头1|表头2|\n|-|-|\n|这里的文本被\\|分隔|这里的文本也被\\|分隔|\n\n\n1\n2\n3\n\n * 效果：\n\n表头1         表头2\n这里的文本被|分隔   这里的文本也被|分隔\n\n\n#补充 该技巧可用于 obsidian 表格内 双链的文本修饰\n\n文本修饰：\n\n在 双链[[ ]]内 以 | 引导的内容\n\n * 格式： [[链接的内容|文本修饰]]\n * 说明： 文本修饰是渲染界面实际显示的文本，便于更好地融入语境\n\n表格内的格式：\n\n在 | 前面加上 \\\n\n * [[表格内的链接内容\\|文本修饰]]\n\n示例：\n\n|                  表头1                  |                        表头2                        |\n|:---------------------------------------:|:---------------------------------------------------:|\n| [[#例2 表格内 单元格中的竖杠\\|单元格中的竖杠]] | [[#例3 不会变成代码的反引号\\|不会变成代码的反引号]] |\n\n\n1\n2\n3\n\n\n效果：\n\n表头1                           表头2\n[[#例2 表格内 单元格中的竖杠|单元格中的竖杠]]   [[#例3 不会变成代码的反引号|不会变成代码的反引号]]\n\n\n\n# 例3 不会变成代码的反引号\n\n使用 转义符号\\ 让 反引号` 变成普通字符，不再具有[[#7 1 行内代码|行内代码]]的标识符功能\n\n格式：\n\n\\`这段被反引号包裹的内容不会变成行内代码\\`\n\n效果：\n\n`这段被反引号包裹的内容不会变成行内代码`\n\n\n\n# 例4 链接中的中括号\n\n在 网页链接 的 显示文本内容 中，使用 中括号 [ ]\n\n * 在显示文本内容中，在其中一个中括号前面，加上转义符号 反斜杠 \\\n   * 格式： [链接里的 \\[中括号\\] 能被正常显示](https://www.runoob.com)\n   * 效果： 链接里的 [中括号] 能被正常显示\n\n\n\n# 例5 不是列表的连接符(横杠)\n\n * 引用一段话，一般会在换行之后，加上 - 出处\n * 因为 - 是标识符，会变成一个无序列表\n\n如下所示：\n\n> the web, the tree, and the string. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。\n> \n>  * 史蒂芬·平克\n\n * 解决方法：\n   \n   * 在 - 前面加上 转义符号 \\\n   \n   >the web, the tree, and the string.\n   >写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。\n   >\\- 史蒂芬·平克   \x3c!-- 加上转义符号 \\ , 不会变成无序列表 --\x3e\n   \n   \n   1\n   2\n   3\n   \n\n * 效果：\n\n> the web, the tree, and the string. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。 - 史蒂芬·平克\n\n\n\n# 例6 不是标题的 #\n\n让 # 不被识别为标题标识符\n\n格式：\n\n\\# 这里的内容不会被识别为标题\n\n效果：\n\n# 这里的内容不会被识别为标题\n\n\n\n# 例7 不会注释的 %\n\n在 obsidian 中 注释是前后各两个 % 号\n\n使用 转义符号\\，让 %% 作为普通字符显示出来，不具备注释的功能\n\n * 格式： \\%\\%这里的内容可以被显示喔\\%\\%\n * 效果： %%这里的内容可以被显示喔%%\n\n\n\n# 例8 木有链接的双链\n\nobsidian 的双向链格式是2个方括号 [[ ]] (双方)，使用 转义符号\\，让 [ ] 不再具有 双链功能\n\n格式：\n\n\\[\\[这段文本被双方包裹，但不是一个双向链\\]\\]\n\n效果：\n\n[[这段文本被双方包裹，但不是一个双向链]]\n\n\n\n# 例9 页链接里 显示文本内的 中括号\n\n使用转义符号\\，让中括号可以作为显示文本 在[[#5 1 网页链接|网页链接]]中显示出来\n\n格式：\n\n[\\[这是一个带中括号的网页链接显示文本，点击会跳转至百度\\]](https://www.baidu.com/)\n\n\n1\n\n\n效果：\n\n[这是一个带中括号的网页链接显示文本，点击会跳转至百度]\n\n\n\n# 特殊情况 文本修饰的中括号\n\n文本修饰的 中括号[ ] 不需要使用 转义符号\\\n\n示范：\n\n[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]\n\n效果：\n\n[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]\n\n\n\n\n\n\n# 14. 空格&换行&强制删除\n\n\n\n# 14.1 空格\n\n * 在一些编辑器或者支持md的笔记软件里，无论你打多少个空格，它只会显示单个 空格 的距离\n   * 可以使用 html中 空格 的 字符实体 —— &nbsp;\n   * 若要添加 多个 空格，就输入多个 —— &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n * 格式：\n   * 这里有&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6个空格分隔\n * 效果：\n   * 这里有      6个空格分隔\n\n\n\n# 14.2 换行\n\n场景1：\n\n * 在一些编辑器或者支持md的笔记软件里，无论你打多少个 回车，它只会显示单个 回车 的空行间距\n   * 可以使用之前表格里提到的 <br> 标签，在 单独一行 中使用，增加额外的空行间距\n   * 如果要增加 多个，就输入 多个 —— <br><br><br><br><br>\n   * #注意 当单独一行使用 <br> 标签的时候，如果前后有标题标识符或者列表标识符，确保 br元素 前后两行都是空白行\n\n格式：\n\n这里是第一段文本\n\n<br><br><br><br><br>     \x3c!-- 这里插入了5个空行间距 --\x3e\n\n这里是第二段文本\n\n\n1\n2\n3\n4\n5\n\n\n效果：\n\n这里是第一段文本\n\n\n\n\n\n\n\n\n这里是第二段文本\n\n\n\n\n\n场景2：\n\n * 在列表中也可以插入换行符\n\n- 这是一段无序列表\n  <br>     \x3c!-- 插入一个空行间距，需单独一行，上下不用预留空格 --\x3e\n  这是同一段无序列表中，空一行距离显示的内容\n- 这是第二段无序列表\n\n\n1\n2\n3\n4\n\n\n效果：\n\n * 这里是第一段无序列表\n   这里是同一段无序列表中，空一行距离显示的内容\n * 这里是第二段无序列表\n\n\n * 补充：\n   * 有一些md编辑器或笔记软件，严格遵循md的换行规则，你敲一个回车是没法换行的，必须在 行末 敲 2个空格，再按回车键\n     * 格式：\n       * 这里是一段想换行的文本空格 空格 enter 这是换行后的文本\n\n\n\n# 14.3 强制删除\n\n * 很多编辑器都有英文标点自动补全功能，自动生成一对，光标落在中间 只想删除前面1个，却会把 一整对 都删掉\n * 在多个列表的嵌套中，也许会遇到一些 无法被删除 的 列表标识符\n * 解决方法： 使用 shift + backspace 即可强制删除\n   * bcakspace   ( 退格键 )\n\n\n\n\n\n\n# 15. 嵌入\n\n * 嵌入都是依赖 html标签 实现的，嵌入的都是在线链接格式\n   * 如果是本地的，obsidian 中音频是有自带的可录制的录音机插件的，其他的 音频、视频 直接复制黏贴就可以了，也可以直接拖拽到ob的笔记界面\n     * 其他的媒体文件在 obsidian 也和图片一样，以双链的格式引用在目标笔记中，使用 ! 使它可见\n\n\n\n# 15.1 嵌入音频\n\n * 格式：\n   \n   * <audio controls="controls" preload="none" src="音频链接地址"></audio>\n\n * 示例：\n\n<audio controls="controls" preload="none" src="https://www.ldoceonline.com/media/english/exaprons/p008-001803372.mp3?version=1.2.37"></audio>\n\n\n1\n\n * 效果：\n\n\n\n\n\n# 15.2 嵌入视频\n\n * 格式：\n\n<video width="600" height="420" controls>\n  <source src="movie.mp4" type="video/mp4">\n  <source src="movie.ogg" type="video/ogg">\n  <source src="movie.webm" type="video/webm">\n</video>\n\n\n1\n2\n3\n4\n5\n\n * 说明：\n   * width ( 宽度 ) height ( 高度 ) ，可以自己设置，直接输入数字即可，单位默认是 px(像素) 也可以使用 百分比 width=100% 代表水平撑满整个窗口 height=50% 代表垂直撑满半个窗口\n   * video标签 支持的视频格式 ：mp4 ogg webm\n\n\n\n# 15.3 嵌入页面\n\n * 格式： <iframe width=600 height=400 src="页面链接地址" scrolling="auto" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n<iframe width=600 height=400 src="https://www.runoob.com/html/html-tutorial.html" scrolling="auto" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n\n1\n\n * 效果：\n\n\n * iframe标签 除了嵌入页面，也可以嵌入在线视频，主流的视频网站都会提供嵌入代码\n   \n   * 具体可以看这个 iframe视频嵌入教程\n   * b站 的视频，得在 // 前面补充 http:\n   * 不是所有的 编辑器和笔记软件 都支持这个\n\n * 示例：\n\n<iframe width=600 height=400 src="http://player.bilibili.com/player.html?aid=20190823&bvid=bv1yw411s7og&cid=32964980&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n\n1\n\n * 宽高设置和前面的 video 一样\n\n\n * 效果：\n\n\n\n\n\n\n# 16. latex 数学公式\n\n * 主要用于 数学公式 与 化学公式 的书写\n\n\n\n# 16.1 行内公式\n\n * 格式：\n   \n   * $ + 行内公式 + $\n\n\n * 示例：\n   * $x^2 + 2x + 5 + \\sqrt x = 0$\n   * $\\ce{co2 + c -> 2 co}$\n   * $\\ce{co2 + c -> 2 co}$\n   * $\\ce{2mg + o2 ->[燃烧] 2 mgo}$\n\n\n * 效果：\n   * $x^2 + 2x + 5 + \\sqrt x = 0$\n   * $e^{i\\pi} + 1 = 0$\n   * $\\ce{co2 + c -> 2 co}$\n   * $\\ce{2mg + o2 ->[燃烧] 2 mgo}$\n\n\n\n# 16.2 公式块\n\n * 格式：\n   * $$ 公式块 $$\n\n\n * 示例：\n\n% 化学公式\n$$\n\\ce{zn^2+  <=>[+ 2oh-][+ 2h+]  $\\underset{\\text{amphoteres hydroxid}}{\\ce{zn(oh)2 v}}$  <=>[+ 2oh-][+ 2h+]  $\\underset{\\text{hydroxozikat}}{\\ce{[zn(oh)4]^2-}}$}\n$$\n\n\n1\n2\n3\n4\n\n\n% 麦克斯韦方程组\n$$\n\\begin{array}{lll}\n\\nabla\\times e &=& -\\;\\frac{\\partial{b}}{\\partial{t}}\n\\ \\nabla\\times h &=& \\frac{\\partial{d}}{\\partial{t}}+j\n\\ \\nabla\\cdot d &=& \\rho\n\\ \\nabla\\cdot b &=& 0\n\\ \\end{array}\n$$\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n% 薛定谔方程\n$$\ni\\hbar\\frac{\\partial \\psi}{\\partial t} = \\frac{-\\hbar^2}{2m} \\left(\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} \\right) \\psi + v \\psi\n$$\n\n\n1\n2\n3\n4\n\n\n * 效果：\n\n$$ % 化学公式 \\ce{zn^2+ <=>[+ 2oh-][+ 2h+] $\\underset{\\text{amphoteres hydroxid}}{\\ce{zn(oh)2 v}}$ <=>[+ 2oh-][+ 2h+] $\\underset{\\text{hydroxozikat}}{\\ce{[zn(oh)4]^2-}}$} $$\n\n\n$$ % 麦克斯韦方程组 \\begin{array}{lll} \\nabla\\times e &=& -;\\frac{\\partial{b}}{\\partial{t}} \\ \\nabla\\times h &=& \\frac{\\partial{d}}{\\partial{t}}+j \\ \\nabla\\cdot d &=& \\rho \\ \\nabla\\cdot b &=& 0 \\ \\end{array} $$\n\n\n$$ i\\hbar\\frac{\\partial \\psi}{\\partial t} = \\frac{-\\hbar^2}{2m} \\left(\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} \\right) \\psi + v \\psi $$\n\n * 补充：\n   * 需要详细教程的，可戳下方链接\n   * latex详细教程\n\n\n\n\n\n\n# 17. mermaid\n\n * 一些 md编辑器 和 笔记软件 支持通过 mermaid 及其所提供的 编译器 来为用户提供图表的绘制功能\n\n * 这里只提供一些演示的图表，具体教程可戳下方\n   \n   * [[moc mermiad 教程 obsidian版| mermiad 超级教程 obsidian版]]\n\n\n\n# 17.1 流程图\n\n\n源码1：\n\n```mermaid\ngraph tb\n\t%% s=start  e=end  f=fork  n=normal\n\n\ts([开始])--\x3ef1{{if条件}};\n\n\t%% 分支点2\n\tf1--true--\x3en1[if语句块]--\x3ee([结束]);\n\tf1--false--\x3ef2{{else if条件}};\n\n\t%% 分支点1\n\tf2--true--\x3en2[else if语句块]--\x3ee;\n\tf2--false--\x3en3[else语句块]--\x3ee;\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n渲染1：\n\ngraph tb\n\t%% s=start  e=end  f=fork  n=normal\n\n\ts([开始])--\x3ef1{{if条件}};\n\n\t%% 分支点1\n\tf1--true--\x3en1[if语句块]--\x3ee([结束]);\n\tf1--false--\x3ef2{{else if条件}};\n\n\t%% 分支点2\n\tf2--true--\x3en2[else if语句块]--\x3ee;\n\tf2--false--\x3en3[else语句块]--\x3ee;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n源码2：\n\n```mermaid\ngraph lr\n\t%% s=start  e=end  f= fork n=normal\n\n\t%% 虚线\n\ts[朱百六]-.->|子|n1[朱四九]-.->|子|n2[朱五四]-.->|子|f1_帝((朱八八))\n\n\t%% 分支点 朱八八\n\tf1_帝--\x3e|长子|f2[朱标]\n\tf1_帝--\x3e|次子|n3[朱樉]\n\tf1_帝--\x3e|三子|n4[朱棢]\n\tf1_帝--\x3e|四子|n5_帝((朱棣))\n\n\t%% 分支点 朱标\n\tf2--\x3e|长子|e1[朱雄英]\n\tf2--\x3e|次子|e2_帝((朱允炆))\n\n\tn5_帝--\x3e|长子|e3[朱高炽]\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n渲染2：\n\ngraph lr\n\t%% s=start  e=end  f= fork n=normal\n\n\t%% 虚线\n\ts[朱百六]-.->|子|n1[朱四九]-.->|子|n2[朱五四]-.->|子|f1_帝((朱八八))\n\n\t%% 分支点 朱八八\n\tf1_帝--\x3e|长子|f2[朱标]\n\tf1_帝--\x3e|次子|n3[朱樉]\n\tf1_帝--\x3e|三子|n4[朱棢]\n\tf1_帝--\x3e|四子|n5_帝((朱棣))\n\n\t%% 分支点 朱标\n\tf2--\x3e|长子|e1[朱雄英]\n\tf2--\x3e|次子|e2_帝((朱允炆))\n\n\tn5_帝--\x3e|长子|e3[朱高炽]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n# 17.2 饼图\n\n\n源码：\n\n```mermaid\npie\n    title 为什么总是宅在家里？\n    "喜欢宅" : 45\n    "天气太热" : 70\n    "穷" : 500\n\t"关你屁事" : 95\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n渲染：\n\npie\n    title 为什么总是宅在家里？\n    "喜欢宅" : 45\n    "天气太热" : 70\n    "穷" : 500\n\t"关你屁事" : 95\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n# 17.3 序列图 (时序图)\n\n\n源码：\n\n```mermaid\nsequencediagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant a as aly\n        participant b as bob\n        participant c as cofcai\n        %% 便签说明\n        note left of a: 只复习了一部分\n        note right of b: 没复习\n        note over a,b: are contacting\n\n        a->>b: 明天是要考试吗？\n        b--\x3e>a: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par aska\n                c --\x3e> a:你复习好了吗？\n            and askb\n                c --\x3e> b:你复习好了吗？\n            and self\n                c ->>c:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/every min\n            %% <br/>可以换行\n            c ->> c:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                a ->> c:复习了一点\n            else is common\n                b ->> c:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt extra response\n                c ->> c:你们怎么不回答我\n            end\n        endsequencediagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant a as aly\n        participant b as bob\n        participant c as cofcai\n        %% 便签说明\n        note left of a: 只复习了一部分\n        note right of b: 没复习\n        note over a,b: are contacting\n\n        a->>b: 明天是要考试吗？\n        b--\x3e>a: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par aska\n                c --\x3e> a:你复习好了吗？\n            and askb\n                c --\x3e> b:你复习好了吗？\n            and self\n                c ->>c:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/every min\n            %% <br/>可以换行\n            c ->> c:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                a ->> c:复习了一点\n            else is common\n                b ->> c:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt extra response\n                c ->> c:你们怎么不回答我\n            end\n        end\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n\n\n渲染：\n\nsequencediagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant a as aly\n        participant b as bob\n        participant c as cofcai\n        %% 便签说明\n        note left of a: 只复习了一部分\n        note right of b: 没复习\n        note over a,b: are contacting\n\n        a->>b: 明天是要考试吗？\n        b--\x3e>a: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par aska\n                c --\x3e> a:你复习好了吗？\n            and askb\n                c --\x3e> b:你复习好了吗？\n            and self\n                c ->>c:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/every min\n            %% <br/>可以换行\n            c ->> c:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                a ->> c:复习了一点\n            else is common\n                b ->> c:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt extra response\n                c ->> c:你们怎么不回答我\n            end\n        end\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n\n# 17.4 甘特图\n\n\n源码：\n\n```mermaid\ngantt\n    title a gantt diagram\n    dateformat  yyyy-mm-dd\n    section section\n    a task           :a1, 2014-01-01, 30d\n    another task     :after a1  , 20d\n    section another\n    task in sec      :2014-01-12  , 12d\n    another task      : 24d\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n渲染：\n\ngantt\n    title a gantt diagram\n    dateformat  yyyy-mm-dd\n    section section\n    a task           :a1, 2014-01-01, 30d\n    another task     :after a1  , 20d\n    section another\n    task in sec      :2014-01-12  , 12d\n    another task      : 24d\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 17.5 类图\n\n\n源码：\n\n```mermaid\nclassdiagram\n    animal <|-- duck\n    animal <|-- fish\n    animal <|-- zebra\n    animal : +int age\n    animal : +string gender\n    animal: +ismammal()\n    animal: +mate()\n    class duck{\n      +string beakcolor\n      +swim()\n      +quack()\n    }\n    class fish{\n      -int sizeinfeet\n      -caneat()\n    }\n    class zebra{\n      +bool is_wild\n      +run()\n    }\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n渲染：\n\nclassdiagram\n    animal <|-- duck\n    animal <|-- fish\n    animal <|-- zebra\n    animal : +int age\n    animal : +string gender\n    animal: +ismammal()\n    animal: +mate()\n    class duck{\n      +string beakcolor\n      +swim()\n      +quack()\n    }\n    class fish{\n      -int sizeinfeet\n      -caneat()\n    }\n    class zebra{\n      +bool is_wild\n      +run()\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\n\n\n\n# 18. 标签 (tag)\n\n * 标签是 obsidian 特有的一个功能，标签可以通过点击唤起快速搜索 (搜索包含该标签的所有笔记)\n\n格式：\n\n * # + 标签名\n   * #标签名\n\n\n# 关于空格\n\n * 在一段正文文本的后面添加 tag， # 的前面 需要有个空格\n   * 空格 + # + 标签名\n\n\n * # 与 标签名 之间，不能有空格，否则就变成 一级标题 了\n\n\n * 标签名的内部，不允许使用空格，若想区分标签中的词语，可使用以下三种方法：\n   1. 驼峰式大小写： #bluetopaz\n   2. 下划线： #blue_topaz\n   3. 连字符： #blue-topaz\n\n\n\n# 关于数字\n\n * 标签内允许使用数字，但不能完全由数字组成\n   * #1984 ❌\n   * #1984date ⭕\n   * #da_1984_te ⭕\n   * #date-1984 ⭕\n\n\n\n# 标签的嵌套\n\n在标签名内，使用 / 斜杠 可以实现标签的嵌套\n\n格式：\n\n * #主标签/子标签1\n * #主标签/子标签2\n * #主标签/子标签3\n\n嵌套标签可以像普通标签一样通过点击来唤起搜索，嵌套标签允许你选择搜索的层次。例如：\n\n * 搜索 #主标签 ，即可找到包含任意一个子标签的所有笔记\n   * 返回的结果会是上述的三个例子\n * 当你在一个主分类下设置了多个子分类，想找到这个主分类包含的所有内容时，该功能会很实用\n\n\n\n# 能被使用的符号\n\n综上所述，标签内能被使用的符号共有三种\n\n 1. _ 下划线\n 2. - 连字符\n 3. / 斜杠\n\n\n\n# 如何让 # 不被识别\n\n可以使用前面提到的转义符号 \\ 反斜杠，与上述的 转义标题 类似\n\n格式：\n\n\\#这里的内容不会被识别为标签\n\n效果：\n\n#这里的内容不会被识别为标签\n\n\n\n# 19. 避免标识符的滥用\n\n即使在 markdown 中，也要尽量避免标识符的滥用\n\n比如我的这篇教程，就存在一定程度的滥用\n\n * 其实是因为我这篇是教学性质的，不太一样，有些不能避免\n   * (好吧，我就是在甩锅)\n\n标识符的本质是突出显示，代表重点\n\n * 一篇笔记里的某段文本，使用各式各样的的标识符，会造成重点不清晰\n\n有三种标识，慎用！\n\n 1. 词中对单个汉字的标识\n    1. 卧==虎==藏==龙==\n 2. 短语中对单个英语单词的标识\n    1. get a ==bang== out of\n 3. 标识符的多层嵌套\n    1. 我感觉快要==原地起飞==了\n\n原因：\n\n * 词义的割裂\n * 视觉的混乱\n * 不利于搜索\n   * 卧==虎==藏==龙==\n     * 搜 卧虎 -- 搜不到\n     * 搜 藏龙 -- 搜不到',charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"how LLM works",frontmatter:{title:"how LLM works",date:"2024-01-02T15:32:49.000Z",permalink:"/pages/dc7035/",tags:[null]},regularPath:"/05.llm/01.How_LLM_Works.html",relativePath:"05.llm/01.How_LLM_Works.md",key:"v-26a9c736",path:"/pages/dc7035/",headers:[{level:3,title:"1. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2012]",slug:"_1-llm-as-os-agents-as-apps-envisioning-aios-agents-and-the-aios-agent-ecosystem-2012",normalizedTitle:"1. llm as os, agents as apps: envisioning aios, agents and the aios-agent ecosystem [2012]",charIndex:174},{level:3,title:"2. NVIDIA Mastering LLM Techniques",slug:"_2-nvidia-mastering-llm-techniques",normalizedTitle:"2. nvidia mastering llm techniques",charIndex:93},{level:3,title:"3. Finetuning",slug:"_3-finetuning",normalizedTitle:"3. finetuning",charIndex:2746},{level:3,title:"4. Function Calling",slug:"_4-function-calling",normalizedTitle:"4. function calling",charIndex:3035}],headersStr:"1. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2012] 2. NVIDIA Mastering LLM Techniques 3. Finetuning 4. Function Calling",content:" 1. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2023]\n 2. NVIDIA Mastering LLM Techniques\n\n----------------------------------------\n\n\n# 1. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2012]\n\n👍 Analogy of LLM and OS.\n\nBlog: https://huggingface.co/blog/shivance/illustrated-llm-os\n\nYoutube: Andrej Karpathy https://www.youtube.com/watch?v=kCc8FmEb1nY\n\nParallel decoding(Multi threading): This is a technique that allows multiple decoding processes to occur simultaneously, which can speed up the decoding process. For example, instead of generating one token at a time, parallel decoding can generate several tokens in parallel, using different models or different parts of the same model. This can reduce the latency and increase the throughput of the decoding process. A recent paper by Apple researchers proposed a method called Parallel Speculative Sampling (PaSS) that introduces parallel decoding for LLMs, maintaining model quality while achieving remarkable speed. Related Paper: Accelerating LLM Inference with Staged Speculative Decoding\n\nEnsemble decoding(Multi processing): This is a technique that involves using multiple models to decode a single input sequence, which can improve the accuracy of the decoding process. For example, instead of relying on one model to generate the output, ensemble decoding can combine the outputs of several models, using methods such as voting, averaging, or reranking. This can increase the diversity and robustness of the decoding process. A common approach for ensemble decoding is to use models that have been trained with different architectures, hyperparameters, or data sources.\n\nSpeculative execution: This is a technique that involves predicting the outcome of a computation before it is actually executed, which can speed up the decoding process. For example, instead of waiting for the final hidden state of the model to generate the next token, speculative execution can use the early hidden states to predict the next token and execute the model in parallel on the predicted token. This can reduce the dependency between tokens and increase the parallelism of the decoding process. A recent paper by Berkeley researchers proposed a method called SPEED, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states.\n\nRelated Paper: SPEED: Speculative Pipelined Execution for Efficient Decoding\n\n\n# 2. NVIDIA Mastering LLM Techniques\n\nLink: https://developer.nvidia.com/blog/search-posts/?q=Mastering+LLM+Techniques\n\n 1. Customization\n 2. LLMOps\n 3. Training\n 4. Inference Optimization\n 5. \n\n\n# 3. Finetuning\n\n 1. How RLHF Preference Model Tuning Works (And How Things May Go Wrong) Blog: https://www.assemblyai.com/blog/how-rlhf-preference-model-tuning-works-and-how-things-may-go-wrong/\n\nPaper: RRHF: Rank Responses to Align Language Models with Human Feedback without tears 2.\n\n\n# 4. Function Calling\n\n 1. Blog: https://crunchingthedata.com/when-to-use-function-calling-for-llms/\n 2. Paper: An LLM Compiler for Parallel Function Calling",normalizedContent:" 1. llm as os, agents as apps: envisioning aios, agents and the aios-agent ecosystem [2023]\n 2. nvidia mastering llm techniques\n\n----------------------------------------\n\n\n# 1. llm as os, agents as apps: envisioning aios, agents and the aios-agent ecosystem [2012]\n\n👍 analogy of llm and os.\n\nblog: https://huggingface.co/blog/shivance/illustrated-llm-os\n\nyoutube: andrej karpathy https://www.youtube.com/watch?v=kcc8fmeb1ny\n\nparallel decoding(multi threading): this is a technique that allows multiple decoding processes to occur simultaneously, which can speed up the decoding process. for example, instead of generating one token at a time, parallel decoding can generate several tokens in parallel, using different models or different parts of the same model. this can reduce the latency and increase the throughput of the decoding process. a recent paper by apple researchers proposed a method called parallel speculative sampling (pass) that introduces parallel decoding for llms, maintaining model quality while achieving remarkable speed. related paper: accelerating llm inference with staged speculative decoding\n\nensemble decoding(multi processing): this is a technique that involves using multiple models to decode a single input sequence, which can improve the accuracy of the decoding process. for example, instead of relying on one model to generate the output, ensemble decoding can combine the outputs of several models, using methods such as voting, averaging, or reranking. this can increase the diversity and robustness of the decoding process. a common approach for ensemble decoding is to use models that have been trained with different architectures, hyperparameters, or data sources.\n\nspeculative execution: this is a technique that involves predicting the outcome of a computation before it is actually executed, which can speed up the decoding process. for example, instead of waiting for the final hidden state of the model to generate the next token, speculative execution can use the early hidden states to predict the next token and execute the model in parallel on the predicted token. this can reduce the dependency between tokens and increase the parallelism of the decoding process. a recent paper by berkeley researchers proposed a method called speed, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states.\n\nrelated paper: speed: speculative pipelined execution for efficient decoding\n\n\n# 2. nvidia mastering llm techniques\n\nlink: https://developer.nvidia.com/blog/search-posts/?q=mastering+llm+techniques\n\n 1. customization\n 2. llmops\n 3. training\n 4. inference optimization\n 5. \n\n\n# 3. finetuning\n\n 1. how rlhf preference model tuning works (and how things may go wrong) blog: https://www.assemblyai.com/blog/how-rlhf-preference-model-tuning-works-and-how-things-may-go-wrong/\n\npaper: rrhf: rank responses to align language models with human feedback without tears 2.\n\n\n# 4. function calling\n\n 1. blog: https://crunchingthedata.com/when-to-use-function-calling-for-llms/\n 2. paper: an llm compiler for parallel function calling",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"RISC-V Design",frontmatter:{title:"RISC-V Design",date:"2024-10-30T15:32:49.000Z",permalink:"/pages/cc7041/",tags:[null]},regularPath:"/04.cpu/08.%20riscv.html",relativePath:"04.cpu/08. riscv.md",key:"v-bdcb41a8",path:"/pages/cc7041/",headers:[{level:3,title:"1. [24] RISCV2 A Scalable RISC-V Vector Processor",slug:"_1-24-riscv2-a-scalable-risc-v-vector-processor",normalizedTitle:"1. [24] riscv2 a scalable risc-v vector processor",charIndex:385},{level:3,title:"[7 HPCA 2022] Adaptable Register File Organization for Vector Processors",slug:"_7-hpca-2022-adaptable-register-file-organization-for-vector-processors",normalizedTitle:"[7 hpca 2022] adaptable register file organization for vector processors",charIndex:439},{level:3,title:"Ara: A 1 GHz+ Scalable and Energy-Efficient RISC-V Vector Processor with Multi-Precision Floating Point Support in 22 nm FD-SOI",slug:"ara-a-1-ghz-scalable-and-energy-efficient-risc-v-vector-processor-with-multi-precision-floating-point-support-in-22-nm-fd-soi",normalizedTitle:"ara: a 1 ghz+ scalable and energy-efficient risc-v vector processor with multi-precision floating point support in 22 nm fd-soi",charIndex:522},{level:3,title:'A "New Ara" for Vector Computing An Open Source Efficient RISCV Vector Processor Design',slug:"a-new-ara-for-vector-computing-an-open-source-efficient-riscv-vector-processor-design",normalizedTitle:"a &quot;new ara&quot; for vector computing an open source efficient riscv vector processor design",charIndex:null},{level:3,title:"Yun: An Open-Source 64-Bit RISCV-Based Vector Processor With Multi-Precision Integer and Floating-Point Support in 65nm CMOS",slug:"yun-an-open-source-64-bit-riscv-based-vector-processor-with-multi-precision-integer-and-floating-point-support-in-65nm-cmos",normalizedTitle:"yun: an open-source 64-bit riscv-based vector processor with multi-precision integer and floating-point support in 65nm cmos",charIndex:761},{level:3,title:"Introduction to the SX-Aurora Vector Engine",slug:"introduction-to-the-sx-aurora-vector-engine",normalizedTitle:"introduction to the sx-aurora vector engine",charIndex:940}],headersStr:'1. [24] RISCV2 A Scalable RISC-V Vector Processor [7 HPCA 2022] Adaptable Register File Organization for Vector Processors Ara: A 1 GHz+ Scalable and Energy-Efficient RISC-V Vector Processor with Multi-Precision Floating Point Support in 22 nm FD-SOI A "New Ara" for Vector Computing An Open Source Efficient RISCV Vector Processor Design Yun: An Open-Source 64-Bit RISCV-Based Vector Processor With Multi-Precision Integer and Floating-Point Support in 65nm CMOS Introduction to the SX-Aurora Vector Engine',content:'----------------------------------------\n\nBerkely RISCV Design\n\n 1. [200] The Berkeley Out-of-Order Machine (BOOM): An Industry-Competitive, Synthesizable, Parameterized RISC-V Processor\n 2. BOOM-Explorer: RISC-V BOOM Microarchitecture Design Space Exploration\n 3. [115] BOOM v2: an open-source out-of-order RISC-V core\n\n----------------------------------------\n\nRISCV Vector Design\n\n 1. [24] RISCV2 A Scalable RISC-V Vector Processor\n 2. [7 HPCA 2022] Adaptable Register File Organization for Vector Processors\n 3. [119] Ara: A 1 GHz+ Scalable and Energy-Efficient RISC-V Vector Processor with Multi-Precision Floating Point Support in 22 nm FD-SOI\n 4. [40 ASAP 2022] A "New Ara" for Vector Computing An Open Source Efficient RISCV Vector Processor Design\n 5. Yun: An Open-Source 64-Bit RISCV-Based Vector Processor With Multi-Precision Integer and Floating-Point Support in 65nm CMOS\n 6. [117 Year 1996] Decoupled Vector Architecture\n 7. Introduction to the SX-Aurora Vector Engine\n\n----------------------------------------\n\nNot Read\n\n 1.  [46] Vicuna: A Timing-Predictable RISC-V Vector Coprocessor for Scalable Parallel Computation\n 2.  [1 Master Thesis] Design of an edge-oriented vector accelerator based on the RISC-V “V” extension\n 3.  [36] A RISC-V Simulator and Benchmark Suite for Designing and Evaluating Vector Architectures\n 4.  Challenges and Opportunities in the Co-design of Convolutions and RISC-V Vector Processors\n 5.  [23 Year 2021] Arrow: A RISC-V Vector Accelerator for Machine Learning Inference\n 6.  [36 DATE] Design and Evaluation of SmallFloat SIMD extensions to the RISC-V ISA\n 7.  [11 CANDARW] Proposal of Scalable Vector Extension for Embedded RISC-V Soft-Core Processor\n 8.  [14] Performance Left on the Table: An Evaluation of Compiler Autovectorization for RISC-V\n 9.  [15] Vectorizing posit operations on RISC-V for faster deep neural networks: experiments and comparison with ARM SVE\n 10. [3] Interrupting the Status Quo: A First Glance at the RISC-V Advanced Interrupt Architecture (AIA)\n 11. [22 Year 2016] Vector Processors for Energy-Efficient Embedded Systems\n 12. [50 Year 2006] Vector Lane Threading\n\n----------------------------------------\n\n\n# 1. [24] RISCV2 A Scalable RISC-V Vector Processor\n\nCoupled with dynamically allocated register, at run time, the new register remapping mechanism enables:\n\n * dynamic hardware-based loop unrolling\n * optimized instruction scheduling\n\nDecoupled execution scheme employs resource acquire-and-release semantics to disambiguate between parallel computation and memory-access instruction streams\n\n\n\n# Register Remapping and dynamic register file allocation\n\nVector instruction operate on mutiple elements, the vIS stages transfroms vector instructions into multuple micro-operations(uops), each uop operating on a different register groups.\n\n\n\nEach logic register is mapped to a group of reigster, instread of one-on-one mapping.\n\nThe new reigster remapping mechanism facilitates dynamic loop unrolling in hardware.\n\nThe unrolling mitigates the stall incurred by data dependencies, since direct consumer of a result is now seperated from its producer by multiple uops.\n\nConsequently, resource utilization increase substantially.\n\n# Decoupled execution: computation and memory access\n\n\n\nAs to memory instructions that does not access execution lanes, they are routed after vRRM pipeline stage directly to the memory unit.\n\nThe memory unit features two parallel engines that allows the simultaneous processing and disambiguaing of one load and one store instructions.\n\nTraditionally, synchronization is decoupled processor schemes is achieved by employing so called synchronization queues and specical move operation.\n\nThese are not amenable to vector processors. Here I dont understand.\n\nThey keep a ghost copy of instructin dispatched to vIS stage which updates scoreboard, maintain the wakeup function.\n\n----------------------------------------\n\n\n# [7 HPCA 2022] Adaptable Register File Organization for Vector Processors\n\nBasic Idea: two level registers: physical register and registers in l2 cache\n\nSwap register in L2 Cache with physical register if physical register is in shortage.\n\n\n\nSince they have two level registers, two level register mapping is also invented.\n\n * map logical to physical vectore register file(P-VRF)\n * map P-VRF to Memory Vector Register File(M-VRF)\n\nAVA, an Adaptable Vector Architecture designed for short vectors (MVL = 16 elements2),\ncapable of reconfiguring the MVL when executing applications with abundant DLP.\n\nMVL: Maximum Vector Length.\n\n----------------------------------------\n\n\n# Ara: A 1 GHz+ Scalable and Energy-Efficient RISC-V Vector Processor with Multi-Precision Floating Point Support in 22 nm FD-SOI\n\n\n\n----------------------------------------\n\n\n# A "New Ara" for Vector Computing An Open Source Efficient RISCV Vector Processor Design\n\n\n\n----------------------------------------\n\n\n# Yun: An Open-Source 64-Bit RISCV-Based Vector Processor With Multi-Precision Integer and Floating-Point Support in 65nm CMOS\n\n\n\n----------------------------------------\n\n\n# Introduction to the SX-Aurora Vector Engine\n\n',normalizedContent:'----------------------------------------\n\nberkely riscv design\n\n 1. [200] the berkeley out-of-order machine (boom): an industry-competitive, synthesizable, parameterized risc-v processor\n 2. boom-explorer: risc-v boom microarchitecture design space exploration\n 3. [115] boom v2: an open-source out-of-order risc-v core\n\n----------------------------------------\n\nriscv vector design\n\n 1. [24] riscv2 a scalable risc-v vector processor\n 2. [7 hpca 2022] adaptable register file organization for vector processors\n 3. [119] ara: a 1 ghz+ scalable and energy-efficient risc-v vector processor with multi-precision floating point support in 22 nm fd-soi\n 4. [40 asap 2022] a "new ara" for vector computing an open source efficient riscv vector processor design\n 5. yun: an open-source 64-bit riscv-based vector processor with multi-precision integer and floating-point support in 65nm cmos\n 6. [117 year 1996] decoupled vector architecture\n 7. introduction to the sx-aurora vector engine\n\n----------------------------------------\n\nnot read\n\n 1.  [46] vicuna: a timing-predictable risc-v vector coprocessor for scalable parallel computation\n 2.  [1 master thesis] design of an edge-oriented vector accelerator based on the risc-v “v” extension\n 3.  [36] a risc-v simulator and benchmark suite for designing and evaluating vector architectures\n 4.  challenges and opportunities in the co-design of convolutions and risc-v vector processors\n 5.  [23 year 2021] arrow: a risc-v vector accelerator for machine learning inference\n 6.  [36 date] design and evaluation of smallfloat simd extensions to the risc-v isa\n 7.  [11 candarw] proposal of scalable vector extension for embedded risc-v soft-core processor\n 8.  [14] performance left on the table: an evaluation of compiler autovectorization for risc-v\n 9.  [15] vectorizing posit operations on risc-v for faster deep neural networks: experiments and comparison with arm sve\n 10. [3] interrupting the status quo: a first glance at the risc-v advanced interrupt architecture (aia)\n 11. [22 year 2016] vector processors for energy-efficient embedded systems\n 12. [50 year 2006] vector lane threading\n\n----------------------------------------\n\n\n# 1. [24] riscv2 a scalable risc-v vector processor\n\ncoupled with dynamically allocated register, at run time, the new register remapping mechanism enables:\n\n * dynamic hardware-based loop unrolling\n * optimized instruction scheduling\n\ndecoupled execution scheme employs resource acquire-and-release semantics to disambiguate between parallel computation and memory-access instruction streams\n\n\n\n# register remapping and dynamic register file allocation\n\nvector instruction operate on mutiple elements, the vis stages transfroms vector instructions into multuple micro-operations(uops), each uop operating on a different register groups.\n\n\n\neach logic register is mapped to a group of reigster, instread of one-on-one mapping.\n\nthe new reigster remapping mechanism facilitates dynamic loop unrolling in hardware.\n\nthe unrolling mitigates the stall incurred by data dependencies, since direct consumer of a result is now seperated from its producer by multiple uops.\n\nconsequently, resource utilization increase substantially.\n\n# decoupled execution: computation and memory access\n\n\n\nas to memory instructions that does not access execution lanes, they are routed after vrrm pipeline stage directly to the memory unit.\n\nthe memory unit features two parallel engines that allows the simultaneous processing and disambiguaing of one load and one store instructions.\n\ntraditionally, synchronization is decoupled processor schemes is achieved by employing so called synchronization queues and specical move operation.\n\nthese are not amenable to vector processors. here i dont understand.\n\nthey keep a ghost copy of instructin dispatched to vis stage which updates scoreboard, maintain the wakeup function.\n\n----------------------------------------\n\n\n# [7 hpca 2022] adaptable register file organization for vector processors\n\nbasic idea: two level registers: physical register and registers in l2 cache\n\nswap register in l2 cache with physical register if physical register is in shortage.\n\n\n\nsince they have two level registers, two level register mapping is also invented.\n\n * map logical to physical vectore register file(p-vrf)\n * map p-vrf to memory vector register file(m-vrf)\n\nava, an adaptable vector architecture designed for short vectors (mvl = 16 elements2),\ncapable of reconfiguring the mvl when executing applications with abundant dlp.\n\nmvl: maximum vector length.\n\n----------------------------------------\n\n\n# ara: a 1 ghz+ scalable and energy-efficient risc-v vector processor with multi-precision floating point support in 22 nm fd-soi\n\n\n\n----------------------------------------\n\n\n# a "new ara" for vector computing an open source efficient riscv vector processor design\n\n\n\n----------------------------------------\n\n\n# yun: an open-source 64-bit riscv-based vector processor with multi-precision integer and floating-point support in 65nm cmos\n\n\n\n----------------------------------------\n\n\n# introduction to the sx-aurora vector engine\n\n',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM Hardware Optimization",frontmatter:{title:"LLM Hardware Optimization",date:"2024-01-02T23:32:49.000Z",permalink:"/pages/dc7036/",tags:[null]},regularPath:"/05.llm/02.LLM_HW_Opt.html",relativePath:"05.llm/02.LLM_HW_Opt.md",key:"v-b267e276",path:"/pages/dc7036/",headers:[{level:3,title:"1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]",slug:"_1-hat-hardware-aware-transformers-for-efficient-natural-language-processing-mit-247",normalizedTitle:"1. hat: hardware-aware transformers for efficient natural language processing [mit 247]",charIndex:1},{level:3,title:"4. Making Transformer inference faster on GPUs[Blog]",slug:"_4-making-transformer-inference-faster-on-gpus-blog",normalizedTitle:"4. making transformer inference faster on gpus[blog]",charIndex:909},{level:3,title:"9. [C17 Y2024 ASPLOS] AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference",slug:"_9-c17-y2024-asplos-attacc-unleashing-the-power-of-pim-for-batched-transformer-based-generative-model-inference",normalizedTitle:"9. [c17 y2024 asplos] attacc! unleashing the power of pim for batched transformer-based generative model inference",charIndex:643}],headersStr:"1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247] 4. Making Transformer inference faster on GPUs[Blog] 9. [C17 Y2024 ASPLOS] AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference",content:" 1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]\n 2. TurboTransformers: An Efficient GPU Serving System For Transformer Models [82]\n 3. Improving the Efficiency of Transformers for Resource-Constrained Devices [8]\n 4. Bag of Tricks for Optimizing Transformer Efficiency [5]\n 5. Making Transformer inference faster on GPUs[Blog]\n 6. Energy-efficient Inference Service of Transformer-based Deep Learning Models on GPUs [4]\n 7. Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs [TACO 2023 Ref 2]\n 8. hugging face https://huggingface.co/docs/transformers/performance\n 9. [C17 Y2024 ASPLOS] AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference\n\n----------------------------------------\n\n\n# 1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]\n\n👍 👍 👍 👍\n\n\n# 4. Making Transformer inference faster on GPUs[Blog]\n\nhttps://dev-discuss.pytorch.org/t/making-transformer-inference-faster-on-gpus/190\n\n\n# 9. [C17 Y2024 ASPLOS] AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference\n\nI dont understand how PIM works, but the discussion about compute-efficiency and batching in LLM inference is solid.\n\nBatching to process multiple input sequences together can be a potential solution to inference throughput, but it is effective only for the FC layer, not the attention layer.\n\nSpecifically, batching can improve the utilization of the system’s compute and memory resources for the FC layer by reusing the weight matrices and increasing arithmetic intensity, quantified by the Operation per Byte (Op/B).\n\nHowever, the Key and Value (KV) matrices of the attention layer are unique per (inference) request. That is, batching can neither reuse the KV matrices nor improve the throughput of processing the attention layer.\n\nMoreover, the attention layer even limits the maximum batch size and impacts the FC layer throughput due to two critical constraints: memory capacity and service level objective (SLO).\n\n(1) The memory capacity required to store KV matrices can be prohibitively high. (2) Even if the memory capacity constraint is resolved, the SLO requirement becomes another limiting factor. As batching does not improve the throughput of the attention layer (§3.2), a larger batch leads to a longer processing time and thus violation of a given SLO constraint.\n\n\n\nThe Gen (Prefilling) stages typically overwhelm the Sum (Decoding) stage in execution time due to their sequential nature of reading the entire pre-trained weights per generated (output) token.\n\n\n\n\n\nAs batching does not improve the throughput of the attention layer, the execution time for processing a batch increases with the batch size.\n\nThat is, when the SLO is fixed, the maximum batch size is limited due to the attention layer.\n\nFurther, the attention layer has a low arithmetic intensity regardless of batch size (see Figure 3).\n\nThe primary operation of the attention layer in the Gen stage is the GEMV of the score and context operations, exhibiting a low Op/B (∼1).\n\nUnlike the FC layer, the attention layer still has memoryintensive GEMV operations even after batching because each request uses unique KV matrices,",normalizedContent:" 1. hat: hardware-aware transformers for efficient natural language processing [mit 247]\n 2. turbotransformers: an efficient gpu serving system for transformer models [82]\n 3. improving the efficiency of transformers for resource-constrained devices [8]\n 4. bag of tricks for optimizing transformer efficiency [5]\n 5. making transformer inference faster on gpus[blog]\n 6. energy-efficient inference service of transformer-based deep learning models on gpus [4]\n 7. improving computation and memory efficiency for real-world transformer inference on gpus [taco 2023 ref 2]\n 8. hugging face https://huggingface.co/docs/transformers/performance\n 9. [c17 y2024 asplos] attacc! unleashing the power of pim for batched transformer-based generative model inference\n\n----------------------------------------\n\n\n# 1. hat: hardware-aware transformers for efficient natural language processing [mit 247]\n\n👍 👍 👍 👍\n\n\n# 4. making transformer inference faster on gpus[blog]\n\nhttps://dev-discuss.pytorch.org/t/making-transformer-inference-faster-on-gpus/190\n\n\n# 9. [c17 y2024 asplos] attacc! unleashing the power of pim for batched transformer-based generative model inference\n\ni dont understand how pim works, but the discussion about compute-efficiency and batching in llm inference is solid.\n\nbatching to process multiple input sequences together can be a potential solution to inference throughput, but it is effective only for the fc layer, not the attention layer.\n\nspecifically, batching can improve the utilization of the system’s compute and memory resources for the fc layer by reusing the weight matrices and increasing arithmetic intensity, quantified by the operation per byte (op/b).\n\nhowever, the key and value (kv) matrices of the attention layer are unique per (inference) request. that is, batching can neither reuse the kv matrices nor improve the throughput of processing the attention layer.\n\nmoreover, the attention layer even limits the maximum batch size and impacts the fc layer throughput due to two critical constraints: memory capacity and service level objective (slo).\n\n(1) the memory capacity required to store kv matrices can be prohibitively high. (2) even if the memory capacity constraint is resolved, the slo requirement becomes another limiting factor. as batching does not improve the throughput of the attention layer (§3.2), a larger batch leads to a longer processing time and thus violation of a given slo constraint.\n\n\n\nthe gen (prefilling) stages typically overwhelm the sum (decoding) stage in execution time due to their sequential nature of reading the entire pre-trained weights per generated (output) token.\n\n\n\n\n\nas batching does not improve the throughput of the attention layer, the execution time for processing a batch increases with the batch size.\n\nthat is, when the slo is fixed, the maximum batch size is limited due to the attention layer.\n\nfurther, the attention layer has a low arithmetic intensity regardless of batch size (see figure 3).\n\nthe primary operation of the attention layer in the gen stage is the gemv of the score and context operations, exhibiting a low op/b (∼1).\n\nunlike the fc layer, the attention layer still has memoryintensive gemv operations even after batching because each request uses unique kv matrices,",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Memory Usage in Training LLM",frontmatter:{title:"Memory Usage in Training LLM",date:"2024-05-29T23:32:49.000Z",permalink:"/pages/dc7038/",tags:[null]},regularPath:"/05.llm/04.mem_usage_llm.html",relativePath:"05.llm/04.mem_usage_llm.md",key:"v-0ee7ecc5",path:"/pages/dc7038/",headersStr:null,content:"1) Nvidia Paper on Traning LLM\nReducing Activation Recomputation in Large Transformer Models\n\n2) Blog Understanding and Estimating GPU Memory\nUnderstanding and Estimating GPU Memory Demands for Training LLMs in practice\n\n3) Blog Memory-Efficient Training\nMemory-Efficient Training of Large Language Models: Overcoming Constraints on Consumer GPUs for Large Neural Networks\n\n4）Stanford Paper Low-Memory Neural Network Training:A Technical Report\nLow-Memory Neural Network Training:A Technical Report\n\n5) Blog Gradient / Activation checkpointing\nhttps://iq.opengenus.org/gradient-checkpointing/\n\n6) Tianqi Chen Gradient Checkpointing Paper\nTraining Deep Nets with Sublinear Memory Cost\n\n7）UCSD Efficient Finetuning of LLMs\nhttps://cseweb.ucsd.edu/classes/wi24/cse234-a/slides/CSE234-GuestLecture-SumanthHegde.pdf",normalizedContent:"1) nvidia paper on traning llm\nreducing activation recomputation in large transformer models\n\n2) blog understanding and estimating gpu memory\nunderstanding and estimating gpu memory demands for training llms in practice\n\n3) blog memory-efficient training\nmemory-efficient training of large language models: overcoming constraints on consumer gpus for large neural networks\n\n4）stanford paper low-memory neural network training:a technical report\nlow-memory neural network training:a technical report\n\n5) blog gradient / activation checkpointing\nhttps://iq.opengenus.org/gradient-checkpointing/\n\n6) tianqi chen gradient checkpointing paper\ntraining deep nets with sublinear memory cost\n\n7）ucsd efficient finetuning of llms\nhttps://cseweb.ucsd.edu/classes/wi24/cse234-a/slides/cse234-guestlecture-sumanthhegde.pdf",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"How to run llama.cpp with gem5",frontmatter:{title:"How to run llama.cpp with gem5",date:"2024-01-21T23:32:49.000Z",permalink:"/pages/dc7037/",tags:[null]},regularPath:"/05.llm/03.gem5_LLAMA.html",relativePath:"05.llm/03.gem5_LLAMA.md",key:"v-259f44f9",path:"/pages/dc7037/",headersStr:null,content:'1) LLama.cpp\n\nllama support compilation of x86, arm and gpu.\n\na) github download llama.cpp\n\nhttps://github.com/ggerganov/llama.cpp.git\n\nb) gem5 support ARM architecture better, thus we compile llama.cpp with arm.\n\n\n\nThen we start to compile: make UNAME_M=aarch64\n\nthe compile tool chain is based on aarch64-linux-gnu-gc-10. It will generate "main" binary if compress successfully.\n\nuse file main to check the file:\n\n\n\nc) download a model to llama.cpp/models directory.\n\nHere I downloaded llama-2-7b-chat.Q2_K.gguf. It utilize 2bit quantization and only needs 3GB memory.\n\nGGML_TYPE_Q2_K - "type-1" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\n\n\n\nd) then we can run the main binary and model, my prompt is "How are you".\n\n./main -m ./models/llama-2-7b-chat.Q2_K.gguf -p "How are you" -n 16\n\nThe last line in the following figure is the output.\n\n\n\n2) gem5\n\nafter we have build gem5 successfully, we can run the model with gem5.\n\nHere I plan to run with 8 core.\n\n> build/ARM/gem5.fast --outdir=./m5out/llm_9 ./configs/example/se.py -c $LLAMA_path/llama.cpp/main-arm \'--options=-m $LLAMA_path/llama-2-7b-chat.Q2_K.gguf -p Hi -n 16\' --cpu-type=ArmAtomicSimpleCPU --mem-size=8GB -n 8\n\n> The output is like the following:\n\n\n\nThe left several columns are output of LLAMA model. The followings are cpu ID with instruction executed.\n\nThe output of the model is "Hi，I\'m a 30-year-old male, and..."\n\nHowever, only 4 core has been used, since LLAMA.cpp default threads configuration is 4.\n\nThen we can configure the model with 8 thread.\n\n> build/ARM/gem5.fast --outdir=./m5out/llm_9 ./configs/example/se.py -c $LLAMA_path/llama.cpp/main-arm \'--options=-m $LLAMA_path/llama-2-7b-chat.Q2_K.gguf -p Hi -n 16 -t 8\' --cpu-type=ArmAtomicSimpleCPU --mem-size=8GB -n 8\n\n\n\nNow, you can see that by default, only CPU0 execute 2.9 billion instruction with the output of "Hi" in 8 core simulation.\n\nHowever, with default 4 core, it has to run 5.4 billion instruction to the same result. This complies to the number of cores runing parallely.',normalizedContent:'1) llama.cpp\n\nllama support compilation of x86, arm and gpu.\n\na) github download llama.cpp\n\nhttps://github.com/ggerganov/llama.cpp.git\n\nb) gem5 support arm architecture better, thus we compile llama.cpp with arm.\n\n\n\nthen we start to compile: make uname_m=aarch64\n\nthe compile tool chain is based on aarch64-linux-gnu-gc-10. it will generate "main" binary if compress successfully.\n\nuse file main to check the file:\n\n\n\nc) download a model to llama.cpp/models directory.\n\nhere i downloaded llama-2-7b-chat.q2_k.gguf. it utilize 2bit quantization and only needs 3gb memory.\n\nggml_type_q2_k - "type-1" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. block scales and mins are quantized with 4 bits. this ends up effectively using 2.5625 bits per weight (bpw)\n\n\n\nd) then we can run the main binary and model, my prompt is "how are you".\n\n./main -m ./models/llama-2-7b-chat.q2_k.gguf -p "how are you" -n 16\n\nthe last line in the following figure is the output.\n\n\n\n2) gem5\n\nafter we have build gem5 successfully, we can run the model with gem5.\n\nhere i plan to run with 8 core.\n\n> build/arm/gem5.fast --outdir=./m5out/llm_9 ./configs/example/se.py -c $llama_path/llama.cpp/main-arm \'--options=-m $llama_path/llama-2-7b-chat.q2_k.gguf -p hi -n 16\' --cpu-type=armatomicsimplecpu --mem-size=8gb -n 8\n\n> the output is like the following:\n\n\n\nthe left several columns are output of llama model. the followings are cpu id with instruction executed.\n\nthe output of the model is "hi，i\'m a 30-year-old male, and..."\n\nhowever, only 4 core has been used, since llama.cpp default threads configuration is 4.\n\nthen we can configure the model with 8 thread.\n\n> build/arm/gem5.fast --outdir=./m5out/llm_9 ./configs/example/se.py -c $llama_path/llama.cpp/main-arm \'--options=-m $llama_path/llama-2-7b-chat.q2_k.gguf -p hi -n 16 -t 8\' --cpu-type=armatomicsimplecpu --mem-size=8gb -n 8\n\n\n\nnow, you can see that by default, only cpu0 execute 2.9 billion instruction with the output of "hi" in 8 core simulation.\n\nhowever, with default 4 core, it has to run 5.4 billion instruction to the same result. this complies to the number of cores runing parallely.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM optimizations",frontmatter:{title:"LLM optimizations",date:"2024-11-03T23:32:49.000Z",permalink:"/pages/dc7039/",tags:[null]},regularPath:"/05.llm/05.llm_optimizations.html",relativePath:"05.llm/05.llm_optimizations.md",key:"v-c6222e76",path:"/pages/dc7039/",headers:[{level:3,title:"[1900] Mixed Precision Training",slug:"_1900-mixed-precision-training",normalizedTitle:"[1900] mixed precision training",charIndex:4},{level:3,title:"[1519] Training Compute-Optimal Large Language Models",slug:"_1519-training-compute-optimal-large-language-models",normalizedTitle:"[1519] training compute-optimal large language models",charIndex:40},{level:3,title:"[440] Measuring the Effects of Data Parallelism on Neural Network Training [Google]",slug:"_440-measuring-the-effects-of-data-parallelism-on-neural-network-training-google",normalizedTitle:"[440] measuring the effects of data parallelism on neural network training [google]",charIndex:3048},{level:3,title:"[142] Performance, Design, and Autotuning of Batched GEMM for GPUs",slug:"_142-performance-design-and-autotuning-of-batched-gemm-for-gpus",normalizedTitle:"[142] performance, design, and autotuning of batched gemm for gpus",charIndex:343},{level:3,title:"[341] ZeRO-Offload: Democratizing Billion-Scale Model Training",slug:"_341-zero-offload-democratizing-billion-scale-model-training",normalizedTitle:"[341] zero-offload: democratizing billion-scale model training",charIndex:177},{level:3,title:"[255] FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU",slug:"_255-flexgen-high-throughput-generative-inference-of-large-language-models-with-a-single-gpu",normalizedTitle:"[255] flexgen: high-throughput generative inference of large language models with a single gpu",charIndex:244},{level:3,title:"[13] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",slug:"_13-inference-scaling-laws-an-empirical-analysis-of-compute-optimal-inference-for-llm-problem-solving",normalizedTitle:"[13] inference scaling laws: an empirical analysis of compute-optimal inference for llm problem-solving",charIndex:481},{level:3,title:"[0] ProTrain Efficient LLM Training via Adaptive Memory Management [AMD]",slug:"_0-protrain-efficient-llm-training-via-adaptive-memory-management-amd",normalizedTitle:"[0] protrain efficient llm training via adaptive memory management [amd]",charIndex:13923},{level:3,title:"[31] LLM Inference Unveiled Survey and Roofline Model Insights",slug:"_31-llm-inference-unveiled-survey-and-roofline-model-insights",normalizedTitle:"[31] llm inference unveiled survey and roofline model insights",charIndex:414},{level:3,title:"[1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization",slug:"_1-flattenquant-breaking-through-the-inference-compute-bound-for-large-language-models-with-per-tensor-quantization",normalizedTitle:"[1] flattenquant: breaking through the inference compute-bound for large language models with per-tensor quantization",charIndex:589}],headersStr:"[1900] Mixed Precision Training [1519] Training Compute-Optimal Large Language Models [440] Measuring the Effects of Data Parallelism on Neural Network Training [Google] [142] Performance, Design, and Autotuning of Batched GEMM for GPUs [341] ZeRO-Offload: Democratizing Billion-Scale Model Training [255] FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU [13] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving [0] ProTrain Efficient LLM Training via Adaptive Memory Management [AMD] [31] LLM Inference Unveiled Survey and Roofline Model Insights [1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization",content:" 1. [1900] Mixed Precision Training\n 2. [1519] Training Compute-Optimal Large Language Models\n 3. [440] Measuring the Effects of Data Parallelism on Neural Network Training\n 4. [341] ZeRO-Offload: Democratizing Billion-Scale Model Training\n 5. [255] FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU\n 6. [142] Performance, Design, and Autotuning of Batched GEMM for GPUs\n 7. [31] LLM Inference Unveiled Survey and Roofline Model Insights\n 8. [13] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving\n 9. [1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization\n\n----------------------------------------\n\n\n# [1900] Mixed Precision Training\n\n\n\n\n\nLoss Scaling\n\nNote that much of the FP16 representable range was left unused, while many values were below the minimum representable range and became zeros.\nScaling up the gradients will shift them to occupy more of the representable range and preserve values that are otherwise lost to zeros.\nThis particular network diverges when gradients are not scaled, but scaling them by a factor of 8 (increasing the exponents by 3) is sufficient to match the accuracy achieved with FP32 training.\n\nactivation gradient values below 2^−27 in magnitude were irrelevant to the training of this model, but values in the [2^−27, 2^−24) range were important to preserve.\n\nOne efficient way to shift the gradient values into FP16-representable range is to scale the loss value computed in the forward pass, prior to starting back-propagation.\n\nBy chain rule back-propagation ensures that all the gradient values are scaled by the same amount.\n\nThe gradients need to be unscaled before the final weight update.\n\n----------------------------------------\n\n\n# [1519] Training Compute-Optimal Large Language Models\n\nFix model sizes and vary number of training tokens\n\nOn the left we show all of our different runs. We launched a range of model sizes going from 70M to 10B, each for four different cosine cycle lengths.\nFrom these curves, we extracted the envelope of minimal loss per FLOP, and we used these points to estimate the optimal model size (center) for a given compute budget and the optimal number of training tokens (right).\nIn green, we show projections of optimal model size and training token count based on the number of FLOPs used to train Gopher (5.76 × 1023).\n\nIsoFLOP profiles\n\nFor various model sizes, we choose the number of training tokens such that the final FLOPs is a constant.\nThe cosine cycle length is set to match the target FLOP count.\nWe find a clear valley in loss, meaning that for a given FLOP budget there is an optimal model to train (left).\nUsing the location of these valleys, we project optimal model size and number of tokens for larger models (center and right).\nIn green, we show the estimated number of parameters and tokens for an optimal model trained with the compute budget of Gopher.\n\n----------------------------------------\n\n\n# [440] Measuring the Effects of Data Parallelism on Neural Network Training [Google]\n\n# Steps to Result Depends on Batch Size in a Similar Way Across Problems.\n\nIn all cases, as the batch size grows, there is an initial period of perfect scaling (indicated with a dashed line) where the steps needed to achieve the error goal halves for each doubling of the batch size.\nThen there is a region of diminishing returns that eventually leads to a region of maximal data parallelism where additional parallelism provides no benefit whatsoever.\n\n\n\n\n\nIf the curves in Figure 1 were very sensitive to the goal validation error, then measuring the steps needed to reach our particular choice of the goal would not be a meaningful proxy for training speed.\\\n\nFor small changes in the goal validation error, we do not care about vertical shifts as long as the transition points between the three scaling regions remain relatively unchanged.\\\n\nFigure 2 shows that varying the error goal only vertically shifts the stepsto-result curve, at least for modest variations centered around a good absolute validation error.\n\nFurthermore, although we ultimately care about out-of-sample error, if our plots looked very different when measuring the steps needed to reach a particular training error, then we would need to include both curves when presenting our results.\n\n# Validating Our Measurement Protocol\n\n# Some Models Can Exploit Much Larger Batch Sizes Than Others\n\n\n\nThis might be the begining of the scale law.\n\n# Momentum Extends Perfect Scaling to Larger Batch Sizes, but Matches Plain SGD at Small Batch Sizes\n\n# The Data Set Matters, at Least Somewaht\n\n# Regularization Can Be More Helpful at Some Batch Sizes than Others\n\n# The Best Learning Rate and Momentrum Vary with Batch Size\n\n# Solution Quality Depends on Compute Budget More Than Batch Size\n\nTaken together, these observations suggest that in practice the relevant question is not which batch size leads to the best performance, but rather how compute budget varies as a function of batch size.\n\n\n\n----------------------------------------\n\n\n# [142] Performance, Design, and Autotuning of Batched GEMM for GPUs\n\n\n\n\n\n----------------------------------------\n\n\n# [341] ZeRO-Offload: Democratizing Billion-Scale Model Training\n\nKey Sentence: Offload calucation of updating fp32 parameter in CPU to save memory.\n\nMixed precision training often keeps two copies of the parameters, one in float-16 (fp16) and the other in float-32 (fp32).\nThe gradients are stored in fp16.\nIn addition to the parameters and gradients, the Adam optimizer keeps track of the momentum and variance of the gradients. These optimizer states are stored in fp32.\n\ntraining a model in mixed precision with the Adam optimizer requires at least:\n\n * 2 bytes of memory for each fp16 parameter and gradient\n * 4 byte of memory for each fp32 parameter\n * moementum and variance of each gradient.\n\n\n\n * 2M parameter FP16\n * 2M gradient FP16\n * 12M for parameter, momentum, variance FP32\n\nIn total, a model with M parameters requires 16×M bytes of memory.\n\n\n\n# Offload update of parameter to CPU\n\nFWD-BWD Super node in GPU\nUpdate Super node in CPU\n\n\n\n# CPU optimizer\n\n 1. SIMD vector instruction [15] for fully exploiting the hardware parallelism supported on CPU architectures.\n 2. Loop unrolling [31], an effective technique for increasing instruction level parallelism that is crucial for better memory bandwidth utilization.\n 3. OMP multithreading for effective utilization of multiple cores and threads on the CPU in parallel.\n\n# One-Step Delayed Parameter Update\n\nDespite using a highly optimized CPU optimizer, the CPU computation overhead can become a bottleneck during training with very small batch sizes, when the GPU computation time is not much larger than CPU compute.\nFor such limited cases, we develop one-step delayed parameter update (DPU) that overlaps CPU and GPU compute to hide the CPU computation overhead by delaying the parameter update by a single step.\n\n\n\n----------------------------------------\n\n\n# [255] FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU\n\n * We formally define a search space of possible offloading strategies by considering computation schedule, tensor placement, and computation delegation.\n   We prove that our search space captures a computation order with I/O complexity within 2× of optimality.\n   We then develop a linear programming-based search algorithm to optimize the throughput within the search space.\n\n * We show that it is possible to compress both the weights and KV cache for LLMs like OPT-175B to 4 bits without retraining or calibration, all with negligible accuracy loss.\n   This is achieved through fine-grained groupwise quantization (Shen et al., 2020), which is suitable for reducing I/O costs and memory usage during offloading.\n\n\n\n\n\nAll existing systems (Aminabadi et al., 2022; HuggingFace, 2022) traverse the graph row-by-row, as shown in Fig. 3(a).\nThis is reasonable because it is the fastest way to finish the generation for one batch and the KV cache can be freed immediately after a row.\nHowever, because every two contiguous squares do not share weights, this schedule has to repeatedly load the weights and incurs huge I/O costs.\n\n----------------------------------------\n\n\n# [13] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving\n\ncost-performance trade-offs for inference strategies:\n\n * greedy search\n * majority voting\n * best-of-n\n * weighted voting\n * two different tree search algorithms, using different model sizes and compute budgets.\n\nSmaller models (e.g., Llemma-7B) can outperform larger models given the same computation budgets\nSmaller models paired with advanced inference algorithms yield Pareto-optimal cost-performance tradeoffs. For instance, the Llemma-7B model, equipped with our novel tree search algorithm, consistently outperforms Llemma-34B with standard majority voting on the MATH benchmark across all FLOPs budgets.\n\n\n\nthe accuracy of the language model will ultimately saturate to a fixed limit which is determined by the output probabilities assigned by the model, exhibiting exponential convergence speed through sampling and voting.\nthis highlights the necessity for more sophisticated inference algorithms.\n\nthe commonly-used MCTS method does not perform well with weighted voting, as it often yields many unfinished solutions, hence having less effective votes.\n\nTo address this issue, we propose a novel tree search algorithm, REward BAlanced SEarch (REBASE), which pairs well with weighted voting and achieves a Pareto-optimal trade-off between accuracy and inference compute.\nThe key idea of REBASE is to use a node-quality reward to control node expansion, which eliminates the need for explicit rollouts while ensuring enough candidate solutions for voting.\n\n\n\n# Inference Strategy\n\nGreedy search. This strategy generates tokens one at a time by selecting the highest probability token at each step. It is computationally efficient but often suboptimal in terms of diversity.\n\n * Best-of-n. This strategy, also known as rejection sampling, generates a set of candidates and chooses the one with the highest score given by the reward model.\n * Majority voting. In this strategy, a set of candidates are generated, and the final answer to the problem is determined by the most frequently occurring answer in all the outputs.\n * Weighted majority voting. This strategy is a variant of majority voting in which the candidates are weighted based on the scores given by the reward model.\n\nSampling-based if it uses a standard autoregressive sampling algorithm (e.g., temperature sampling) to generate the candidate set (greedy search is separate, in that it only has a single deterministic candidate).\n\nA tree-search variant uses a tree search to generate the candidate set.\n\nInformally, as long as the reward model is “better than random”, i.e., assigning higher rewards to correct solutions on average, the accuracy limit of weighted majority voting is higher than that of majority voting.\n\n * Monte Carlo Tree Search(MCTS)\n\nMCTS consumes substantially more resources, often requiring dozens of times more generated tokens than simpler methods.\n\na significant portion of the paths in the search tree are used to estimate and select nodes, and these paths do not necessarily become a part of the final candidate solution, although MCTS ensures that the sampled solutions comprise high-quality intermediate steps.\n\nsampling methods generate multiple solutions in parallel and independently, and all the generated sequences are included in the candidate solutions.\nHowever, the intermediate steps in these sequences are not guaranteed to be of high quality, as there is no mechanism for pruning poor steps or exploiting promising ones.\n\n * Reward Balanced Search\n\nThe REBASE tree search method, illustrated in Fig. 3, inherits the exploitation and pruning properties of tree search, while using a reward model alone to estimate quality of intermediate nodes.\nThis saves on computation compared to methods such as MCTS, since it does not involve estimate node quality with explicit rollouts.\nIn short, the underlying idea is to use a process reward model to determine how much each node should be expanded at each depth.\n\nNamely, REBASE expands nodes at a given depth according to their softmax-normalized reward scores, subject to a total expansion budget. We describe this procedure in more detail below.\n\n\n\n\n\n# Compute Model Size\n\nScaling law of compute-optimal inference for model size.\n\nInitially, sampling many times from smaller models is compute-optimal.\nAt larger compute budgets the larger models are preferable, since the performance of small models has saturated.\nAs highlighted in the right panel of Fig. 1, the optimal model size varies based on the inference budget.\n\nLlemma-7B achieves competitive accuracy to Llemma-34B with less compute.\n\nLlemma-7B requires around 2× less total FLOPs than Llemma-34B to achieve comparable accuracy.\nThis held across inference strategies (sampling strategies, MCTS, REBASE) and tasks (MATH, GSM8K).\nThis result suggests that, with the same training dataset and model family, generating more tokens with a suitable inference strategy using a smaller model can have more favorable cost-performance tradeoffs than using a larger model.\n\n# Compute-Optimal Inference Strategy\n\nWeaker models gain more from tree search.\n\nweaker models, as indicated by their lower greedy search accuracy, benefit more from tree search methods like REBASE\n\nREBASE saturates later than sampling with higher accuracy.\n\ndrawing samples from REBASE corresponds to sampling from a policy that assigns high probability to true answers compared to sampling from the underlying language model.\n\n----------------------------------------\n\n\n# [0] ProTrain Efficient LLM Training via Adaptive Memory Management [AMD]\n\n# Prior Art\n\n 1. They only support coarse-grained control, such as the fixed parameter replication mode (ZeRO-2 or ZeRO-3), and binary options for offloading and gradient checkpointing. For instance, FSDP requires all model states to be either entirely offloaded to the CPU or kept on the GPU, and all transformer blocks either use gradient checkpointing or not at all.\n 2. They require significant manual effort to specify various configurations. In DeepSpeed, users must select the ZeRO optimization stage, configure offloading options (CPU or NVMe) for both parameters and optimizer states, and set multiple thresholds for parameter fetching and collective communications.\n\n# ProTrain\n\n 1. To reduce memory consumption, ProTrain adaptively decides whether to use offloading or gradient checkpointing, determines the amount of model states and activations to offload and the number of transformer blocks to apply gradient checkpointing, all without user inputs.\n 2. For computation, ProTrain keeps forward/backward computation on the GPU for efficiency, while dynamically determining the portion of parameter updates to be performed on the CPU and GPU.\n\nProTrain proposes a Chunk-Based Model State Management system that organizes model states into uniformly sized chunks, and further introduces persistent chunks and chunk buffers to minimize unnecessary data copying and reduce dynamic memory allocations.\n\nProTrain also proposes Block-Wise Activation Management to handle activations at the transformer block level, performing swapping or gradient checkpointing as needed for each block.\n\n# Background\n\nFor the training of large models, it is a common practice to adopt mixed-precision training , which uses reduced precision data types for FWD （Forward Propagation）and BWD (Backward Propagation), while maintaining higher precision for OPTIM (parameter updating) to ensure accuracy.\n\nMemory consumption during training primarily comes from two sources:\n\n * model states\n * residual states\n\nModel states include\n\n * parameters\n * gradients\n * optimizer states residual states consist of\n * activations\n * temporary buffers\n\nThe computational complexity of the FWD and BWD stages scales with model size and batch size, necessitating their execution on GPUs due to the intensive computational demands.\n\nIn contrast, the OPTIM stage involves simpler operations and can be efficiently offloaded to the CPU (40), which brings significant GPU memory savings by allocating memory-intensive optimizer states on the CPU.\n\n# Chunk Based Model State Management\n\n\n\n# Block-Wise Activation Mangement Layout and Memory Usage Trend\n\nThe balance between:\n\n * Activation Swapping Swapping indicates that the block will be swapped out the block.\n\n * Gradient Checkpointing checkpointing means that the entire block will be recomputed by saving the input tensor of\n\n\n\nThe concept of checkpointing comes from TianQi Chen's Paper.\n\nTraining Deep Nets with Sublinear Memory Cost\n\nhttps://github.com/cybertronai/gradient-checkpointing\n\n\n\n----------------------------------------\n\n\n# [31] LLM Inference Unveiled Survey and Roofline Model Insights\n\n\n\nThe inference process of Large Language Models (LLMs) is divided into two stages:\n\n * the Prefill Stage\n * the Decode Stage\n\nThe Prefill Stage serves as the initial step in LLM inference.\nIn this stage, the model takes a prompt sequence as input and engages in the generation of a key-value cache (KV cache) for each Transformer layer within the LLM.\nThe KV cache plays a crucial role in storing and organizing information that the model deems relevant for subsequent token generation.\nEach Transformer layer is equipped with its own unique KV cache, and this prefilling process establishes the foundation for the subsequent decoding stage.\n\nThe Decode Stage represents the core of the LLM inference process.\nIn the Decode Stage, the model uses the KV caches prepared earlier and might add new information to them.\nThe goal here is to generate tokens, which are essentially words or parts of words.\nThis happens step by step.\nThe creation of each new token is influenced by the tokens that were generated before it, like building a sentence word by word.\n\n# Memory Bound & Compute Bound\n\nIf the layer is memory-bound, consider optimization techniques such as quantization, kernel fusion and increasing batch size to alleviate the memory footprint.\n\n\n\nIt implies that the layer is constrained by computation (compute-bound), with some memory units potentially remaining idle.\nIn this case, we should investigate strategies such as enabling low-bit computation to enhance computational efficiency.\n\n# Memory Access\n\nQuantizing tensors in LLM can significantly reduce memory access, resulting in fewer data bytes to be moved for the same amount of computation.\n\nThis increase in arithmetic intensity contributes to the Roofline model, leading to three scenarios:\n\n 1. After quantization, the arithmetic intensity remains within the memory-bound range.\n    With the improvement in arithmetic intensity, the average data access per computation is reduced, alleviating the pressure on data memory access.\n    Consequently, the the-oretical performance is enhanced. This can greatly boost the performance during the memory-bound decode stage.\\\n 2. The arithmetic intensity transitions from being memory-bound to compute-bound.\n    This shift also reduces the pressure on data memory access, resulting in improved theoretical performance.\n 3. Both before and after quantization, the arithmetic intensity remains within the compute-bound range.\n    In this case, there is no performance improvement. For example, this scenario can occur during the compute-bound prefill stage or when the batch size is large in the decode stage.\n\nLarge Batch Size: It can be seen that on the right of Figure8, there is no difference from W1-W8. Compute-bound prefill stage: It can be seen that on the right of Figure9, there is no difference from W1-W8.\n\n\n\nThe trend is from Memory Bound to Compute Bound.\n\n# Algorithms for Fast Decoding\n\n * Early Exiting\n * Contextual Sparsity The paper reveals that contextual sparsity can go up as high as 80%, meaning that the majority of the weights can be left out while still preserving the original model performance.\n   However, the chosen weights are dynamic and different for different input tokens.\n\nTo save compute, the paper proposes to train a small MLP network as the Sparse Predictor in front of the Multi-Head Attention (MHA) and the Feed-Forward Networks (FFN) of the LLM\n\n * Mixture of Expert\n\nThe mixture of expert (MoE) technique is a well-studied topic (Yuksel et al. [2012]) that effectively decouples the parameter count of the model and the computation FLOPs required by the model training and inference.\n\nAn expert network is inserted into the transformer architecture to replace the FFN layers. Also, a gating function is introduced between the Multi-Head Attention and the expert network which aims to select the best-fit expert or experts for the given input token.\n\n\n\nAlthough both rely on the input token to determine sparse structure, We deliberately separate MoE and the contextual sparsity techniques because the latter operates on pre-trained dense language models and exploits the sparsity from the dense neural networks, while the prior trains a sparse model from the beginning.\n\n# Speculative Decoding\n\n * LLM Distribution Preserving\n * Building a Three of Draft Tokens\n * Knoweledge Distillation and Self-Speculative Decoding\n\n\n\n# Parallel Decoding\n\n * Simultaneously Predicting Multiple Future Tokens\n * Retrieval of Frequent N-grams\n * Hierarchical Structure In Language\n * Jacobi and Gaussian-Seidel Iterative Algorithms\n\n\n\n# Memory Mangement and Workload Offloading\n\nThe length of the user’s input prompt may vary, affecting the length of the sequence in the prefill phase.\nAdditionally, the sequence length increases incrementally during the decode phase as tokens are generated.\n\nPage Attention efficiently handles the KV cache by dividing it into blocks.\n\nThis mapping is similar to how virtual memory works in a CPU’s memory management system.\n\nDeepSpeed-inference introduces ZeRO-Inference, which offloads the weights of large models to CPU memory.\n\nThis mechanism performs well with large batch sizes because the increased batch size increase the computation requirement and make the computation latency overlap the latency of fetching model weights,\n\nFlexGen provides a way to explore different ways of offloading computations considering constraints imposed by available hardware resources from the GPU, CPU, and disk.\n\n----------------------------------------\n\n\n# [1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization\n\nFlattenQuant can directly use 4 bits to achieve 48.29% of the linear layer calculation in LLMs, with the remaining layers using 8 bits.\n\nThe 4-bit matrix multiplication introduced in the FlattenQuant method can effectively address the compute-bound caused by large matrix calculation.\nOur work achieves up to 2× speedup and 2.3× memory reduction for LLMs with negligible loss in accuracy.\n\n\n\nFramework\n\n",normalizedContent:" 1. [1900] mixed precision training\n 2. [1519] training compute-optimal large language models\n 3. [440] measuring the effects of data parallelism on neural network training\n 4. [341] zero-offload: democratizing billion-scale model training\n 5. [255] flexgen: high-throughput generative inference of large language models with a single gpu\n 6. [142] performance, design, and autotuning of batched gemm for gpus\n 7. [31] llm inference unveiled survey and roofline model insights\n 8. [13] inference scaling laws: an empirical analysis of compute-optimal inference for llm problem-solving\n 9. [1] flattenquant: breaking through the inference compute-bound for large language models with per-tensor quantization\n\n----------------------------------------\n\n\n# [1900] mixed precision training\n\n\n\n\n\nloss scaling\n\nnote that much of the fp16 representable range was left unused, while many values were below the minimum representable range and became zeros.\nscaling up the gradients will shift them to occupy more of the representable range and preserve values that are otherwise lost to zeros.\nthis particular network diverges when gradients are not scaled, but scaling them by a factor of 8 (increasing the exponents by 3) is sufficient to match the accuracy achieved with fp32 training.\n\nactivation gradient values below 2^−27 in magnitude were irrelevant to the training of this model, but values in the [2^−27, 2^−24) range were important to preserve.\n\none efficient way to shift the gradient values into fp16-representable range is to scale the loss value computed in the forward pass, prior to starting back-propagation.\n\nby chain rule back-propagation ensures that all the gradient values are scaled by the same amount.\n\nthe gradients need to be unscaled before the final weight update.\n\n----------------------------------------\n\n\n# [1519] training compute-optimal large language models\n\nfix model sizes and vary number of training tokens\n\non the left we show all of our different runs. we launched a range of model sizes going from 70m to 10b, each for four different cosine cycle lengths.\nfrom these curves, we extracted the envelope of minimal loss per flop, and we used these points to estimate the optimal model size (center) for a given compute budget and the optimal number of training tokens (right).\nin green, we show projections of optimal model size and training token count based on the number of flops used to train gopher (5.76 × 1023).\n\nisoflop profiles\n\nfor various model sizes, we choose the number of training tokens such that the final flops is a constant.\nthe cosine cycle length is set to match the target flop count.\nwe find a clear valley in loss, meaning that for a given flop budget there is an optimal model to train (left).\nusing the location of these valleys, we project optimal model size and number of tokens for larger models (center and right).\nin green, we show the estimated number of parameters and tokens for an optimal model trained with the compute budget of gopher.\n\n----------------------------------------\n\n\n# [440] measuring the effects of data parallelism on neural network training [google]\n\n# steps to result depends on batch size in a similar way across problems.\n\nin all cases, as the batch size grows, there is an initial period of perfect scaling (indicated with a dashed line) where the steps needed to achieve the error goal halves for each doubling of the batch size.\nthen there is a region of diminishing returns that eventually leads to a region of maximal data parallelism where additional parallelism provides no benefit whatsoever.\n\n\n\n\n\nif the curves in figure 1 were very sensitive to the goal validation error, then measuring the steps needed to reach our particular choice of the goal would not be a meaningful proxy for training speed.\\\n\nfor small changes in the goal validation error, we do not care about vertical shifts as long as the transition points between the three scaling regions remain relatively unchanged.\\\n\nfigure 2 shows that varying the error goal only vertically shifts the stepsto-result curve, at least for modest variations centered around a good absolute validation error.\n\nfurthermore, although we ultimately care about out-of-sample error, if our plots looked very different when measuring the steps needed to reach a particular training error, then we would need to include both curves when presenting our results.\n\n# validating our measurement protocol\n\n# some models can exploit much larger batch sizes than others\n\n\n\nthis might be the begining of the scale law.\n\n# momentum extends perfect scaling to larger batch sizes, but matches plain sgd at small batch sizes\n\n# the data set matters, at least somewaht\n\n# regularization can be more helpful at some batch sizes than others\n\n# the best learning rate and momentrum vary with batch size\n\n# solution quality depends on compute budget more than batch size\n\ntaken together, these observations suggest that in practice the relevant question is not which batch size leads to the best performance, but rather how compute budget varies as a function of batch size.\n\n\n\n----------------------------------------\n\n\n# [142] performance, design, and autotuning of batched gemm for gpus\n\n\n\n\n\n----------------------------------------\n\n\n# [341] zero-offload: democratizing billion-scale model training\n\nkey sentence: offload calucation of updating fp32 parameter in cpu to save memory.\n\nmixed precision training often keeps two copies of the parameters, one in float-16 (fp16) and the other in float-32 (fp32).\nthe gradients are stored in fp16.\nin addition to the parameters and gradients, the adam optimizer keeps track of the momentum and variance of the gradients. these optimizer states are stored in fp32.\n\ntraining a model in mixed precision with the adam optimizer requires at least:\n\n * 2 bytes of memory for each fp16 parameter and gradient\n * 4 byte of memory for each fp32 parameter\n * moementum and variance of each gradient.\n\n\n\n * 2m parameter fp16\n * 2m gradient fp16\n * 12m for parameter, momentum, variance fp32\n\nin total, a model with m parameters requires 16×m bytes of memory.\n\n\n\n# offload update of parameter to cpu\n\nfwd-bwd super node in gpu\nupdate super node in cpu\n\n\n\n# cpu optimizer\n\n 1. simd vector instruction [15] for fully exploiting the hardware parallelism supported on cpu architectures.\n 2. loop unrolling [31], an effective technique for increasing instruction level parallelism that is crucial for better memory bandwidth utilization.\n 3. omp multithreading for effective utilization of multiple cores and threads on the cpu in parallel.\n\n# one-step delayed parameter update\n\ndespite using a highly optimized cpu optimizer, the cpu computation overhead can become a bottleneck during training with very small batch sizes, when the gpu computation time is not much larger than cpu compute.\nfor such limited cases, we develop one-step delayed parameter update (dpu) that overlaps cpu and gpu compute to hide the cpu computation overhead by delaying the parameter update by a single step.\n\n\n\n----------------------------------------\n\n\n# [255] flexgen: high-throughput generative inference of large language models with a single gpu\n\n * we formally define a search space of possible offloading strategies by considering computation schedule, tensor placement, and computation delegation.\n   we prove that our search space captures a computation order with i/o complexity within 2× of optimality.\n   we then develop a linear programming-based search algorithm to optimize the throughput within the search space.\n\n * we show that it is possible to compress both the weights and kv cache for llms like opt-175b to 4 bits without retraining or calibration, all with negligible accuracy loss.\n   this is achieved through fine-grained groupwise quantization (shen et al., 2020), which is suitable for reducing i/o costs and memory usage during offloading.\n\n\n\n\n\nall existing systems (aminabadi et al., 2022; huggingface, 2022) traverse the graph row-by-row, as shown in fig. 3(a).\nthis is reasonable because it is the fastest way to finish the generation for one batch and the kv cache can be freed immediately after a row.\nhowever, because every two contiguous squares do not share weights, this schedule has to repeatedly load the weights and incurs huge i/o costs.\n\n----------------------------------------\n\n\n# [13] inference scaling laws: an empirical analysis of compute-optimal inference for llm problem-solving\n\ncost-performance trade-offs for inference strategies:\n\n * greedy search\n * majority voting\n * best-of-n\n * weighted voting\n * two different tree search algorithms, using different model sizes and compute budgets.\n\nsmaller models (e.g., llemma-7b) can outperform larger models given the same computation budgets\nsmaller models paired with advanced inference algorithms yield pareto-optimal cost-performance tradeoffs. for instance, the llemma-7b model, equipped with our novel tree search algorithm, consistently outperforms llemma-34b with standard majority voting on the math benchmark across all flops budgets.\n\n\n\nthe accuracy of the language model will ultimately saturate to a fixed limit which is determined by the output probabilities assigned by the model, exhibiting exponential convergence speed through sampling and voting.\nthis highlights the necessity for more sophisticated inference algorithms.\n\nthe commonly-used mcts method does not perform well with weighted voting, as it often yields many unfinished solutions, hence having less effective votes.\n\nto address this issue, we propose a novel tree search algorithm, reward balanced search (rebase), which pairs well with weighted voting and achieves a pareto-optimal trade-off between accuracy and inference compute.\nthe key idea of rebase is to use a node-quality reward to control node expansion, which eliminates the need for explicit rollouts while ensuring enough candidate solutions for voting.\n\n\n\n# inference strategy\n\ngreedy search. this strategy generates tokens one at a time by selecting the highest probability token at each step. it is computationally efficient but often suboptimal in terms of diversity.\n\n * best-of-n. this strategy, also known as rejection sampling, generates a set of candidates and chooses the one with the highest score given by the reward model.\n * majority voting. in this strategy, a set of candidates are generated, and the final answer to the problem is determined by the most frequently occurring answer in all the outputs.\n * weighted majority voting. this strategy is a variant of majority voting in which the candidates are weighted based on the scores given by the reward model.\n\nsampling-based if it uses a standard autoregressive sampling algorithm (e.g., temperature sampling) to generate the candidate set (greedy search is separate, in that it only has a single deterministic candidate).\n\na tree-search variant uses a tree search to generate the candidate set.\n\ninformally, as long as the reward model is “better than random”, i.e., assigning higher rewards to correct solutions on average, the accuracy limit of weighted majority voting is higher than that of majority voting.\n\n * monte carlo tree search(mcts)\n\nmcts consumes substantially more resources, often requiring dozens of times more generated tokens than simpler methods.\n\na significant portion of the paths in the search tree are used to estimate and select nodes, and these paths do not necessarily become a part of the final candidate solution, although mcts ensures that the sampled solutions comprise high-quality intermediate steps.\n\nsampling methods generate multiple solutions in parallel and independently, and all the generated sequences are included in the candidate solutions.\nhowever, the intermediate steps in these sequences are not guaranteed to be of high quality, as there is no mechanism for pruning poor steps or exploiting promising ones.\n\n * reward balanced search\n\nthe rebase tree search method, illustrated in fig. 3, inherits the exploitation and pruning properties of tree search, while using a reward model alone to estimate quality of intermediate nodes.\nthis saves on computation compared to methods such as mcts, since it does not involve estimate node quality with explicit rollouts.\nin short, the underlying idea is to use a process reward model to determine how much each node should be expanded at each depth.\n\nnamely, rebase expands nodes at a given depth according to their softmax-normalized reward scores, subject to a total expansion budget. we describe this procedure in more detail below.\n\n\n\n\n\n# compute model size\n\nscaling law of compute-optimal inference for model size.\n\ninitially, sampling many times from smaller models is compute-optimal.\nat larger compute budgets the larger models are preferable, since the performance of small models has saturated.\nas highlighted in the right panel of fig. 1, the optimal model size varies based on the inference budget.\n\nllemma-7b achieves competitive accuracy to llemma-34b with less compute.\n\nllemma-7b requires around 2× less total flops than llemma-34b to achieve comparable accuracy.\nthis held across inference strategies (sampling strategies, mcts, rebase) and tasks (math, gsm8k).\nthis result suggests that, with the same training dataset and model family, generating more tokens with a suitable inference strategy using a smaller model can have more favorable cost-performance tradeoffs than using a larger model.\n\n# compute-optimal inference strategy\n\nweaker models gain more from tree search.\n\nweaker models, as indicated by their lower greedy search accuracy, benefit more from tree search methods like rebase\n\nrebase saturates later than sampling with higher accuracy.\n\ndrawing samples from rebase corresponds to sampling from a policy that assigns high probability to true answers compared to sampling from the underlying language model.\n\n----------------------------------------\n\n\n# [0] protrain efficient llm training via adaptive memory management [amd]\n\n# prior art\n\n 1. they only support coarse-grained control, such as the fixed parameter replication mode (zero-2 or zero-3), and binary options for offloading and gradient checkpointing. for instance, fsdp requires all model states to be either entirely offloaded to the cpu or kept on the gpu, and all transformer blocks either use gradient checkpointing or not at all.\n 2. they require significant manual effort to specify various configurations. in deepspeed, users must select the zero optimization stage, configure offloading options (cpu or nvme) for both parameters and optimizer states, and set multiple thresholds for parameter fetching and collective communications.\n\n# protrain\n\n 1. to reduce memory consumption, protrain adaptively decides whether to use offloading or gradient checkpointing, determines the amount of model states and activations to offload and the number of transformer blocks to apply gradient checkpointing, all without user inputs.\n 2. for computation, protrain keeps forward/backward computation on the gpu for efficiency, while dynamically determining the portion of parameter updates to be performed on the cpu and gpu.\n\nprotrain proposes a chunk-based model state management system that organizes model states into uniformly sized chunks, and further introduces persistent chunks and chunk buffers to minimize unnecessary data copying and reduce dynamic memory allocations.\n\nprotrain also proposes block-wise activation management to handle activations at the transformer block level, performing swapping or gradient checkpointing as needed for each block.\n\n# background\n\nfor the training of large models, it is a common practice to adopt mixed-precision training , which uses reduced precision data types for fwd （forward propagation）and bwd (backward propagation), while maintaining higher precision for optim (parameter updating) to ensure accuracy.\n\nmemory consumption during training primarily comes from two sources:\n\n * model states\n * residual states\n\nmodel states include\n\n * parameters\n * gradients\n * optimizer states residual states consist of\n * activations\n * temporary buffers\n\nthe computational complexity of the fwd and bwd stages scales with model size and batch size, necessitating their execution on gpus due to the intensive computational demands.\n\nin contrast, the optim stage involves simpler operations and can be efficiently offloaded to the cpu (40), which brings significant gpu memory savings by allocating memory-intensive optimizer states on the cpu.\n\n# chunk based model state management\n\n\n\n# block-wise activation mangement layout and memory usage trend\n\nthe balance between:\n\n * activation swapping swapping indicates that the block will be swapped out the block.\n\n * gradient checkpointing checkpointing means that the entire block will be recomputed by saving the input tensor of\n\n\n\nthe concept of checkpointing comes from tianqi chen's paper.\n\ntraining deep nets with sublinear memory cost\n\nhttps://github.com/cybertronai/gradient-checkpointing\n\n\n\n----------------------------------------\n\n\n# [31] llm inference unveiled survey and roofline model insights\n\n\n\nthe inference process of large language models (llms) is divided into two stages:\n\n * the prefill stage\n * the decode stage\n\nthe prefill stage serves as the initial step in llm inference.\nin this stage, the model takes a prompt sequence as input and engages in the generation of a key-value cache (kv cache) for each transformer layer within the llm.\nthe kv cache plays a crucial role in storing and organizing information that the model deems relevant for subsequent token generation.\neach transformer layer is equipped with its own unique kv cache, and this prefilling process establishes the foundation for the subsequent decoding stage.\n\nthe decode stage represents the core of the llm inference process.\nin the decode stage, the model uses the kv caches prepared earlier and might add new information to them.\nthe goal here is to generate tokens, which are essentially words or parts of words.\nthis happens step by step.\nthe creation of each new token is influenced by the tokens that were generated before it, like building a sentence word by word.\n\n# memory bound & compute bound\n\nif the layer is memory-bound, consider optimization techniques such as quantization, kernel fusion and increasing batch size to alleviate the memory footprint.\n\n\n\nit implies that the layer is constrained by computation (compute-bound), with some memory units potentially remaining idle.\nin this case, we should investigate strategies such as enabling low-bit computation to enhance computational efficiency.\n\n# memory access\n\nquantizing tensors in llm can significantly reduce memory access, resulting in fewer data bytes to be moved for the same amount of computation.\n\nthis increase in arithmetic intensity contributes to the roofline model, leading to three scenarios:\n\n 1. after quantization, the arithmetic intensity remains within the memory-bound range.\n    with the improvement in arithmetic intensity, the average data access per computation is reduced, alleviating the pressure on data memory access.\n    consequently, the the-oretical performance is enhanced. this can greatly boost the performance during the memory-bound decode stage.\\\n 2. the arithmetic intensity transitions from being memory-bound to compute-bound.\n    this shift also reduces the pressure on data memory access, resulting in improved theoretical performance.\n 3. both before and after quantization, the arithmetic intensity remains within the compute-bound range.\n    in this case, there is no performance improvement. for example, this scenario can occur during the compute-bound prefill stage or when the batch size is large in the decode stage.\n\nlarge batch size: it can be seen that on the right of figure8, there is no difference from w1-w8. compute-bound prefill stage: it can be seen that on the right of figure9, there is no difference from w1-w8.\n\n\n\nthe trend is from memory bound to compute bound.\n\n# algorithms for fast decoding\n\n * early exiting\n * contextual sparsity the paper reveals that contextual sparsity can go up as high as 80%, meaning that the majority of the weights can be left out while still preserving the original model performance.\n   however, the chosen weights are dynamic and different for different input tokens.\n\nto save compute, the paper proposes to train a small mlp network as the sparse predictor in front of the multi-head attention (mha) and the feed-forward networks (ffn) of the llm\n\n * mixture of expert\n\nthe mixture of expert (moe) technique is a well-studied topic (yuksel et al. [2012]) that effectively decouples the parameter count of the model and the computation flops required by the model training and inference.\n\nan expert network is inserted into the transformer architecture to replace the ffn layers. also, a gating function is introduced between the multi-head attention and the expert network which aims to select the best-fit expert or experts for the given input token.\n\n\n\nalthough both rely on the input token to determine sparse structure, we deliberately separate moe and the contextual sparsity techniques because the latter operates on pre-trained dense language models and exploits the sparsity from the dense neural networks, while the prior trains a sparse model from the beginning.\n\n# speculative decoding\n\n * llm distribution preserving\n * building a three of draft tokens\n * knoweledge distillation and self-speculative decoding\n\n\n\n# parallel decoding\n\n * simultaneously predicting multiple future tokens\n * retrieval of frequent n-grams\n * hierarchical structure in language\n * jacobi and gaussian-seidel iterative algorithms\n\n\n\n# memory mangement and workload offloading\n\nthe length of the user’s input prompt may vary, affecting the length of the sequence in the prefill phase.\nadditionally, the sequence length increases incrementally during the decode phase as tokens are generated.\n\npage attention efficiently handles the kv cache by dividing it into blocks.\n\nthis mapping is similar to how virtual memory works in a cpu’s memory management system.\n\ndeepspeed-inference introduces zero-inference, which offloads the weights of large models to cpu memory.\n\nthis mechanism performs well with large batch sizes because the increased batch size increase the computation requirement and make the computation latency overlap the latency of fetching model weights,\n\nflexgen provides a way to explore different ways of offloading computations considering constraints imposed by available hardware resources from the gpu, cpu, and disk.\n\n----------------------------------------\n\n\n# [1] flattenquant: breaking through the inference compute-bound for large language models with per-tensor quantization\n\nflattenquant can directly use 4 bits to achieve 48.29% of the linear layer calculation in llms, with the remaining layers using 8 bits.\n\nthe 4-bit matrix multiplication introduced in the flattenquant method can effectively address the compute-bound caused by large matrix calculation.\nour work achieves up to 2× speedup and 2.3× memory reduction for llms with negligible loss in accuracy.\n\n\n\nframework\n\n",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM flash algorthms",frontmatter:{title:"LLM flash algorthms",date:"2024-11-05T23:32:49.000Z",permalink:"/pages/dc7040/",tags:[null]},regularPath:"/05.llm/06.llm_flash.html",relativePath:"05.llm/06.llm_flash.md",key:"v-4edbad25",path:"/pages/dc7040/",headers:[{level:3,title:"1. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective",slug:"_1-large-language-model-inference-acceleration-a-comprehensive-hardware-perspective",normalizedTitle:"1. large language model inference acceleration: a comprehensive hardware perspective",charIndex:1},{level:3,title:"2. Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward",slug:"_2-faster-and-lighter-llms-a-survey-on-current-challenges-and-way-forward",normalizedTitle:"2. faster and lighter llms: a survey on current challenges and way forward",charIndex:87}],headersStr:"1. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective 2. Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward",content:" 1. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective\n 2. Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward\n 3. [51] A Systematic Survey of Resource-Efficient Large Language Models 👍 👍 👍 👍 👍\n\n----------------------------------------\n\n\n# 1. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective\n\n# Optimizations on Hardware Platforms\n\n# Quantization\n\nData Format\n\n * Uniform Quantization\n * Non-uniform Quantization\n\nGranularity\n\n * group-wise: Group-wise granularity is a coarser approach where multiple channels or layers are quantized with the same parameters.\n * channel-wise: Channel-wise granularity involves quantizing each channel individually within the model.\n * tensor-wise: Tensor-wise granularity is the most detailed approach, where each tensor (such as weight tensors or activation tensors) is quantized separately.\n\nWeight-Only Quantization\n\nUniform & Norn Uniform\n\nMatrix decomposition quantization is a specialized method where a large matrix is approximated by the product of several smaller matrices.\n\nWeight-Activation Quantization\n\nQuantization include the activations generated during model inference.\n\nIn this method, both the weights and the activations at each layer are quantized to lower precision formats.\n\nThis reduces memory bandwidth requirements and enhances inference speed.\n\nThe challenge with weight-activation quantization is to manage the trade-off between quantization errors and model accuracy.\n\nTechniques such as dynamic range quantization or specific quantization schemes are used to balance precision and computational efficiency.\n\n\n\n# Weight-Only Quantization\n\nShen et al. [97] leverage Intel Neural Compressor to automate the INT4 quantization process with negligible accuracy loss, supporting various quantization recipes such as GPTQ, AWQ and TEQ.\n\nDue to the overheads of weight dequantization from integer to floating, T-MAC leverages lookup tables (LUTs) for efficient low-bit LLM inference on edge CPUs, circumventing the need for dequantization and mixed precision matrix multiplication.\n\nGPTQ GPTQ is an one-shot weight quantization method based on approximate second-order information and error compensation, that is both highly-accurate and highly-efficient.\nIt can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3-bit or 4-bit per weight, with negligible accuracy degradation relative to the uncompressed baseline.\n\nAWQ AWQ is based on the observation that protecting 1% of salient weights whose activations are extremely large can greatly reduce quantization error.\nIt first searches for the optimal per-channel scaling and then multiplies the salient weights with the per-channel scalings.\nIt also reduces the bitwidth down to 3 or 4 bits per weight.\n\nSpQR To further reduce the accuracy loss for smaller models in the 1-10B parameter range, SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision like half data type (16-bit), while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs.\n\nSqueezeLLM SqueezeLLM proposes a sensitivity-based non-uniform quantization method, which searches for the optimal bit precision assignment based on second-order information.\nIt also applies dense and sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format.\n\nLLM-MQ LLM-MQ proposes sensitivity-based precision allocation to assign the proper bitwidth for each layer within the given budget for weight memory based on their first-order information and quantization error. It also develops an efficient CUDA core kernels to accelerate LLMs by fusing the dequantization and general matrix-vector multiplication (GEMV).\n\nAPTQ APTQ proposes an attention-aware 2/4-bit mixed-precision quantization for LLMs, which considers not only the second-order information of each layer’s weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model.\n\nLUT-GEMM LUT-GEMM proposes an efficient LUT-based GPU kernel for quantized matrix multiplication, which not only eliminates the resource-intensive dequantization process but also reduces computational costs compared to previous kernels for weight-only quantization.\n\nFLUTE FLUTE is a flexible lookup table engine for LUT-quantized LLMs, which uses offline restructuring of the quantized weight matrix to minimize bit manipulations associated with unpacking, and vectorization and duplication of the lookup table to mitigate shared memory bandwidth constraints.\n\n# Weight-Activation Quantization\n\nIn addition to hardware units that support FP16 computations, NVIDIA GPUs also provide hardware units that support INT4, INT8, and FP8 computations.\nThe number of these computation units can be 2× and 4× greater than FP16 on each chip.\nCompared to weight-only quantization, weight-activation quantization can utilize INT4, INT8, and FP8 computations, thereby maximizing the peak computational performance of the GPU.\\\n\nSince the prefill phase in LLM inference is compute-bound, weight-activation quantization can significantly enhance performance during this stage.\n\nLLM.int8 LLM.int8 uses vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features.\n\nSmmothQuant SmoothQuant enables 8-bit weight and 8-bit activation (W8A8) quantization for LLMs.\n\nQUIK QUIK is for the first time, that the majority of inference computations for LLMs can be performed with both weights and activations being cast to 4 bits.\n\nPrevalent quantization schemes (e.g., W8A8) cannot fully leverage the capabilities of modern GPUs, such as 4-bit integer operators, resulting in sub-optimal performance.\n\nAtom significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization.\n\n# Quantitative Comparison\n\nFor CPUs, power consumption ranges from 3W to 385W, with inference speeds between 3 tokens/s and 50 tokens/s, located in the bottom part of the figure.\n\nFor GPUs, power consumption ranges from 40W to 450W, with inference speeds between 18 tokens/s and 194 tokens/s, situated in the upper right part of the figure.\n\n\n\n# Sparsity\n\nSparsity patterns can be categorized into random and structured sparsity as shown in Figure 5.\n\n\n\nRandom pattern involves a random distribution of zero elements within the matrix, achieving higher accuracy but potentially lower speed for computation.\nStructured pattern applies a specific pattern to the sparsity, improving computational efficiency by aligning with hardware optimizations.\nWithin structured sparsity, common patterns include block-wise sparsity, N:M sparsity, channel-wise sparsity and some combinations of structured pattern sparsity.\n\n# GPU\n\n# Weight Sparsity\n\n# Attention Sparsity\n\nStatic Sparisty\n\n * Sparse Transformer\n * StreamingLLM\n * Bigbird\n * Longformer\n\nAbove schemes use the naumal combination of global and local patterns to replace the full attention patterns.\n\n*Dynamic Sparsity\n\n * Adaptive Sparse Attention\n * Reformer\n * Sparse Flash Attention\n * Sparse Sinkhorn Attention\n * H2O Heavy Hitters\n\n# Speculative Decoding\n\nSpeculative decoding is proposed to overcome the inherently sequential process in the autoregressive decoding of LLM.\nThe essential decoding mechanism is to make predictions (i.e., draft tokens) parallelly for multiple time steps and then select the longest prefix verified by a scoring model as the final output.\n\n * Lookahead decoding\n * Medusa\n * Eagle\n * Ouroboros\n * Sequoia\n * Draft&Verify\n * Kangaroo\n * LayerSkip\n * LLAMA\n\n# Skip Layer\n\n * AdaInfer AdaInfer statistically analyzes the activated layers across tasks and proposes a simple algorithm to determine the inference termination moment based on the input instance adaptively.\n * RAEE RAEE proposes to build the retrieval database to store the token information offline and leverages the information of the retrieved similar token by searching the pre-built retrieval database to guide the backbone model to exit at the layer.\n * MOD MOD decides whether to skip the current layer or not by pretraining the model to add a router in each layer like Mixture-of-Experts.\n\n----------------------------------------\n\n\n# 2. Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward\n\n# Compression of LLMs\n\n * Architecture Pruning\n * Quantization\n * Knowledge distillation\n * Low-rank decomposition\n\n# System Level Approaches\n\n * Paged Attention inspired by the classical virtual memory and paging techniques in operating systems, it allows storage of continuous keys and values cached in non-contiguous memory.\n * Tensor Parallelisim entails dividing a tensor into shards distributed across various GPUs, processing each shard independently and in parallel, and subsequently synchronizing the results at the end of the step.\n * Pipeline Parallelism allows a model to be vertically split across multiple GPUs at the layer level, where each GPU handles one or several layers, enabling parallel processing of distinct stages in the pipeline.\n * GPU/CPU Offloadding [Song et al., 2023]- involves transferring specific weight layers to GPU devices for matrix multiplication, subsequently transmitting the computed results back to the secondary device (RAM), thus optimizing parallel processing capabilities while allowing the secondary device to handle the remaining memory intensive computations\n * Flash Attention optimizes attention computation by employing incremental softmax reduction through input block tiling, avoiding the need for whole-input access, and expedites the backward pass by storing the softmax normalization factor from the forward pass, eliminating the requirement to read the large attention matrix from high bandwidth memory (HBM).\n * Fused Operations involves consolidating multiple computational tasks, such as combining existing kernels or creating new ones, to minimize the overhead associated with multiple kernel API invocations.\n * Speculative Decoding efficiently generates multiple future tokens from a chosen smaller model and verifies them in parallel using the larger model, enabling the simultaneous decoding of multiple tokens per step.",normalizedContent:" 1. large language model inference acceleration: a comprehensive hardware perspective\n 2. faster and lighter llms: a survey on current challenges and way forward\n 3. [51] a systematic survey of resource-efficient large language models 👍 👍 👍 👍 👍\n\n----------------------------------------\n\n\n# 1. large language model inference acceleration: a comprehensive hardware perspective\n\n# optimizations on hardware platforms\n\n# quantization\n\ndata format\n\n * uniform quantization\n * non-uniform quantization\n\ngranularity\n\n * group-wise: group-wise granularity is a coarser approach where multiple channels or layers are quantized with the same parameters.\n * channel-wise: channel-wise granularity involves quantizing each channel individually within the model.\n * tensor-wise: tensor-wise granularity is the most detailed approach, where each tensor (such as weight tensors or activation tensors) is quantized separately.\n\nweight-only quantization\n\nuniform & norn uniform\n\nmatrix decomposition quantization is a specialized method where a large matrix is approximated by the product of several smaller matrices.\n\nweight-activation quantization\n\nquantization include the activations generated during model inference.\n\nin this method, both the weights and the activations at each layer are quantized to lower precision formats.\n\nthis reduces memory bandwidth requirements and enhances inference speed.\n\nthe challenge with weight-activation quantization is to manage the trade-off between quantization errors and model accuracy.\n\ntechniques such as dynamic range quantization or specific quantization schemes are used to balance precision and computational efficiency.\n\n\n\n# weight-only quantization\n\nshen et al. [97] leverage intel neural compressor to automate the int4 quantization process with negligible accuracy loss, supporting various quantization recipes such as gptq, awq and teq.\n\ndue to the overheads of weight dequantization from integer to floating, t-mac leverages lookup tables (luts) for efficient low-bit llm inference on edge cpus, circumventing the need for dequantization and mixed precision matrix multiplication.\n\ngptq gptq is an one-shot weight quantization method based on approximate second-order information and error compensation, that is both highly-accurate and highly-efficient.\nit can quantize gpt models with 175 billion parameters in approximately four gpu hours, reducing the bitwidth down to 3-bit or 4-bit per weight, with negligible accuracy degradation relative to the uncompressed baseline.\n\nawq awq is based on the observation that protecting 1% of salient weights whose activations are extremely large can greatly reduce quantization error.\nit first searches for the optimal per-channel scaling and then multiplies the salient weights with the per-channel scalings.\nit also reduces the bitwidth down to 3 or 4 bits per weight.\n\nspqr to further reduce the accuracy loss for smaller models in the 1-10b parameter range, spqr works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision like half data type (16-bit), while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate llama and falcon llms.\n\nsqueezellm squeezellm proposes a sensitivity-based non-uniform quantization method, which searches for the optimal bit precision assignment based on second-order information.\nit also applies dense and sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format.\n\nllm-mq llm-mq proposes sensitivity-based precision allocation to assign the proper bitwidth for each layer within the given budget for weight memory based on their first-order information and quantization error. it also develops an efficient cuda core kernels to accelerate llms by fusing the dequantization and general matrix-vector multiplication (gemv).\n\naptq aptq proposes an attention-aware 2/4-bit mixed-precision quantization for llms, which considers not only the second-order information of each layer’s weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model.\n\nlut-gemm lut-gemm proposes an efficient lut-based gpu kernel for quantized matrix multiplication, which not only eliminates the resource-intensive dequantization process but also reduces computational costs compared to previous kernels for weight-only quantization.\n\nflute flute is a flexible lookup table engine for lut-quantized llms, which uses offline restructuring of the quantized weight matrix to minimize bit manipulations associated with unpacking, and vectorization and duplication of the lookup table to mitigate shared memory bandwidth constraints.\n\n# weight-activation quantization\n\nin addition to hardware units that support fp16 computations, nvidia gpus also provide hardware units that support int4, int8, and fp8 computations.\nthe number of these computation units can be 2× and 4× greater than fp16 on each chip.\ncompared to weight-only quantization, weight-activation quantization can utilize int4, int8, and fp8 computations, thereby maximizing the peak computational performance of the gpu.\\\n\nsince the prefill phase in llm inference is compute-bound, weight-activation quantization can significantly enhance performance during this stage.\n\nllm.int8 llm.int8 uses vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features.\n\nsmmothquant smoothquant enables 8-bit weight and 8-bit activation (w8a8) quantization for llms.\n\nquik quik is for the first time, that the majority of inference computations for llms can be performed with both weights and activations being cast to 4 bits.\n\nprevalent quantization schemes (e.g., w8a8) cannot fully leverage the capabilities of modern gpus, such as 4-bit integer operators, resulting in sub-optimal performance.\n\natom significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization.\n\n# quantitative comparison\n\nfor cpus, power consumption ranges from 3w to 385w, with inference speeds between 3 tokens/s and 50 tokens/s, located in the bottom part of the figure.\n\nfor gpus, power consumption ranges from 40w to 450w, with inference speeds between 18 tokens/s and 194 tokens/s, situated in the upper right part of the figure.\n\n\n\n# sparsity\n\nsparsity patterns can be categorized into random and structured sparsity as shown in figure 5.\n\n\n\nrandom pattern involves a random distribution of zero elements within the matrix, achieving higher accuracy but potentially lower speed for computation.\nstructured pattern applies a specific pattern to the sparsity, improving computational efficiency by aligning with hardware optimizations.\nwithin structured sparsity, common patterns include block-wise sparsity, n:m sparsity, channel-wise sparsity and some combinations of structured pattern sparsity.\n\n# gpu\n\n# weight sparsity\n\n# attention sparsity\n\nstatic sparisty\n\n * sparse transformer\n * streamingllm\n * bigbird\n * longformer\n\nabove schemes use the naumal combination of global and local patterns to replace the full attention patterns.\n\n*dynamic sparsity\n\n * adaptive sparse attention\n * reformer\n * sparse flash attention\n * sparse sinkhorn attention\n * h2o heavy hitters\n\n# speculative decoding\n\nspeculative decoding is proposed to overcome the inherently sequential process in the autoregressive decoding of llm.\nthe essential decoding mechanism is to make predictions (i.e., draft tokens) parallelly for multiple time steps and then select the longest prefix verified by a scoring model as the final output.\n\n * lookahead decoding\n * medusa\n * eagle\n * ouroboros\n * sequoia\n * draft&verify\n * kangaroo\n * layerskip\n * llama\n\n# skip layer\n\n * adainfer adainfer statistically analyzes the activated layers across tasks and proposes a simple algorithm to determine the inference termination moment based on the input instance adaptively.\n * raee raee proposes to build the retrieval database to store the token information offline and leverages the information of the retrieved similar token by searching the pre-built retrieval database to guide the backbone model to exit at the layer.\n * mod mod decides whether to skip the current layer or not by pretraining the model to add a router in each layer like mixture-of-experts.\n\n----------------------------------------\n\n\n# 2. faster and lighter llms: a survey on current challenges and way forward\n\n# compression of llms\n\n * architecture pruning\n * quantization\n * knowledge distillation\n * low-rank decomposition\n\n# system level approaches\n\n * paged attention inspired by the classical virtual memory and paging techniques in operating systems, it allows storage of continuous keys and values cached in non-contiguous memory.\n * tensor parallelisim entails dividing a tensor into shards distributed across various gpus, processing each shard independently and in parallel, and subsequently synchronizing the results at the end of the step.\n * pipeline parallelism allows a model to be vertically split across multiple gpus at the layer level, where each gpu handles one or several layers, enabling parallel processing of distinct stages in the pipeline.\n * gpu/cpu offloadding [song et al., 2023]- involves transferring specific weight layers to gpu devices for matrix multiplication, subsequently transmitting the computed results back to the secondary device (ram), thus optimizing parallel processing capabilities while allowing the secondary device to handle the remaining memory intensive computations\n * flash attention optimizes attention computation by employing incremental softmax reduction through input block tiling, avoiding the need for whole-input access, and expedites the backward pass by storing the softmax normalization factor from the forward pass, eliminating the requirement to read the large attention matrix from high bandwidth memory (hbm).\n * fused operations involves consolidating multiple computational tasks, such as combining existing kernels or creating new ones, to minimize the overhead associated with multiple kernel api invocations.\n * speculative decoding efficiently generates multiple future tokens from a chosen smaller model and verifies them in parallel using the larger model, enabling the simultaneous decoding of multiple tokens per step.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM compute & memory bound",frontmatter:{title:"LLM compute & memory bound",date:"2024-12-05T23:32:49.000Z",permalink:"/pages/dc7041/",tags:[null]},regularPath:"/05.llm/07.llm_bound.html",relativePath:"05.llm/07.llm_bound.md",key:"v-e3c4ebf6",path:"/pages/dc7041/",headers:[{level:3,title:"1. Data Movement is All You Need: A Case Study on Optimizing Transformers",slug:"_1-data-movement-is-all-you-need-a-case-study-on-optimizing-transformers",normalizedTitle:"1. data movement is all you need: a case study on optimizing transformers",charIndex:205},{level:3,title:"2. Splitwise: Efficient Generative LLM Inference Using Phase Splitting",slug:"_2-splitwise-efficient-generative-llm-inference-using-phase-splitting",normalizedTitle:"2. splitwise: efficient generative llm inference using phase splitting",charIndex:1240}],headersStr:"1. Data Movement is All You Need: A Case Study on Optimizing Transformers 2. Splitwise: Efficient Generative LLM Inference Using Phase Splitting",content:"1.[148] Data Movement is All You Need: A Case Study on Optimizing Transformers 2.[70 2024] Splitwise: Efficient Generative LLM Inference Using Phase Splitting\n\n----------------------------------------\n\n\n# 1. Data Movement is All You Need: A Case Study on Optimizing Transformers\n\nContributions:\n\n * We find transformer training to be memory-bound and significantly underperforming on GPUs.\n * We develop a generic recipe for optimizing training using dataflow analyses.\n\n\n\nTensor Constraction: matrix-matrix multiplication\n\nWe consider only MMMs and batched MMMs for simplicity, as these are efficiently supported by cuBLAS.\nIn transformers, these are linear layers and components of MHA.\nThese operations are the most compute-intensive part of training a transformer.\nFor good performance, data layout and algorithm selection (e.g., tiling strategy) are critical.\n\nStatistical Normalization: softmax and layer normalization\nLess compute-intensive than tensors\nThis compute pattern means that data layout and vectorization is important for operator performance.\n\nElement-wise Operators: biases, dropout, activations, and residual connections\nThese are the least compute-intensive operations.\n\n\n\n----------------------------------------\n\n\n# 2. Splitwise: Efficient Generative LLM Inference Using Phase Splitting\n\nFirst, the prompt computation phase, in which all the input prompt tokens run through the forward pass of the model in parallel to generate the first output token.\nThis phase tends to be computationally intensive and requires the high FLOPs (floating point operations per second) of the latest GPUs today.\nSecond, the token generation phase, in which subsequent output tokens are generated sequentially based on the forward pass of the last token and all the cached context from previous tokens in the sequence.\nGiven the lack of compute parallelism, this phase tends to be more memory bandwidth and capacity bound, despite state-of-the-art batching.\n\nThe context generated from the attention layers during the prompt computation is saved in the key-value (KV) cache, since it is needed for all the future token generation iterations.\nAfter the first token is generated, the following tokens only use the last generated token and the KV-cache as inputs to the forward pass of the model.\nThis makes the subsequent token generation more memory bandwidth and capacity intensive than the computationally heavy prompt phase.\n\n",normalizedContent:"1.[148] data movement is all you need: a case study on optimizing transformers 2.[70 2024] splitwise: efficient generative llm inference using phase splitting\n\n----------------------------------------\n\n\n# 1. data movement is all you need: a case study on optimizing transformers\n\ncontributions:\n\n * we find transformer training to be memory-bound and significantly underperforming on gpus.\n * we develop a generic recipe for optimizing training using dataflow analyses.\n\n\n\ntensor constraction: matrix-matrix multiplication\n\nwe consider only mmms and batched mmms for simplicity, as these are efficiently supported by cublas.\nin transformers, these are linear layers and components of mha.\nthese operations are the most compute-intensive part of training a transformer.\nfor good performance, data layout and algorithm selection (e.g., tiling strategy) are critical.\n\nstatistical normalization: softmax and layer normalization\nless compute-intensive than tensors\nthis compute pattern means that data layout and vectorization is important for operator performance.\n\nelement-wise operators: biases, dropout, activations, and residual connections\nthese are the least compute-intensive operations.\n\n\n\n----------------------------------------\n\n\n# 2. splitwise: efficient generative llm inference using phase splitting\n\nfirst, the prompt computation phase, in which all the input prompt tokens run through the forward pass of the model in parallel to generate the first output token.\nthis phase tends to be computationally intensive and requires the high flops (floating point operations per second) of the latest gpus today.\nsecond, the token generation phase, in which subsequent output tokens are generated sequentially based on the forward pass of the last token and all the cached context from previous tokens in the sequence.\ngiven the lack of compute parallelism, this phase tends to be more memory bandwidth and capacity bound, despite state-of-the-art batching.\n\nthe context generated from the attention layers during the prompt computation is saved in the key-value (kv) cache, since it is needed for all the future token generation iterations.\nafter the first token is generated, the following tokens only use the last generated token and the kv-cache as inputs to the forward pass of the model.\nthis makes the subsequent token generation more memory bandwidth and capacity intensive than the computationally heavy prompt phase.\n\n",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM Paper List",frontmatter:{title:"LLM Paper List",date:"2024-12-15T23:32:49.000Z",permalink:"/pages/dc7042/",tags:[null]},regularPath:"/05.llm/08.llm_internals.html",relativePath:"05.llm/08.llm_internals.md",key:"v-936502b6",path:"/pages/dc7042/",headers:[{level:2,title:"Attention",slug:"attention",normalizedTitle:"attention",charIndex:1869},{level:3,title:"1. Transformers are RNNs:Fast Autoregressive Transformers with Linear Attention :+1: :+1:",slug:"_1-transformers-are-rnns-fast-autoregressive-transformers-with-linear-attention",normalizedTitle:"1. transformers are rnns:fast autoregressive transformers with linear attention 👍 👍",charIndex:11370},{level:3,title:"2. Efficient Attention: Attention With Linear Complexities",slug:"_2-efficient-attention-attention-with-linear-complexities",normalizedTitle:"2. efficient attention: attention with linear complexities",charIndex:11460},{level:2,title:"Is All You Need",slug:"is-all-you-need",normalizedTitle:"is all you need",charIndex:2565},{level:3,title:"Tensor Product Attention Is All You Need",slug:"tensor-product-attention-is-all-you-need",normalizedTitle:"tensor product attention is all you need",charIndex:2643},{level:2,title:"Interesting",slug:"interesting",normalizedTitle:"interesting",charIndex:4077},{level:3,title:"21. [C21 2022] MoEfication: Transformer Feed-forward Layers are Mixtures of Experts",slug:"_21-c21-2022-moefication-transformer-feed-forward-layers-are-mixtures-of-experts",normalizedTitle:"21. [c21 2022] moefication: transformer feed-forward layers are mixtures of experts",charIndex:12829},{level:3,title:"28. [C10395 0221] GPT Understands,Too",slug:"_28-c10395-0221-gpt-understands-too",normalizedTitle:"28. [c10395 0221] gpt understands,too",charIndex:12941},{level:2,title:"Old Gold Time of Transformer",slug:"old-gold-time-of-transformer",normalizedTitle:"old gold time of transformer",charIndex:10765},{level:3,title:"1. tansformer-xl",slug:"_1-tansformer-xl",normalizedTitle:"1. tansformer-xl",charIndex:13572}],headersStr:"Attention 1. Transformers are RNNs:Fast Autoregressive Transformers with Linear Attention :+1: :+1: 2. Efficient Attention: Attention With Linear Complexities Is All You Need Tensor Product Attention Is All You Need Interesting 21. [C21 2022] MoEfication: Transformer Feed-forward Layers are Mixtures of Experts 28. [C10395 0221] GPT Understands,Too Old Gold Time of Transformer 1. tansformer-xl",content:"Too many paper on llms...\n\nSurvey\n\n 1.  [C60 2024] Understanding LLMs: A Comprehensive Overview from Training to Inference\n 2.  [C20 2024] Mobile Edge Intelligence for Large Language Models: A Contemporary Survey\n 3.  [P 58] A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms\n 4.  [P 30] A Comprehensive Evaluation of Quantization Strategies for Large Language Models\n 5.  [P 8] A Comprehensive Study on Quantization Techniques for Large Language Models\n 6.  [C24] Model Compression and Efficient Inference for Large Language Models: A Survey\n 7.  [C26 2024] Towards Better Chain-of-Thought Prompting Strategies: A Survey\n 8.  [C24 2024] A Survey of Reasoning with Foundation Models\n 9.  [C75 2025] Resource-efficient Algorithms and Systems of Foundation Models: A Survey 👍\n 10. [C4 2024] LLM for Mobile: An Initial Roadmap\n 11. [C1 2024] Achieving Peak Performance for Large Language Models: A Systematic Review\n 12. [C1 2024] A Survey: Collaborative Hardware and Software Design in the Era of Large Language Models\n 13. [C577 2014] Mixture of experts: a literature survey\n 14. [C187 2024] Large Language Model based Multi-Agents: A Survey of Progress and Challenges\n 15. [C80 2024] Understanding the planning of LLM agents: A survey\n 16. [C895 2023] A survey on large language model based autonomous agents\\\n 17. [C60 2024] Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects\n\nKV Cache\n\n 1. [C60 2024 USENIX] InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management 👍\n 2. [C1 2024]LayerKV: Optimizing Large Language Model Serving with Layer-wise KV Cache Management\n 3. [C1 2024 wei] Unifying KV Cache Compression for Large Language Models with LeanKV\n\nQuantization\n\n 1. [C8 Y2024] An Empirical Study of LLaMA3 Quantization: From LLMs to MLLMs\n\nCross-layer Attention\n\n 1. [C25 Y2024] Reducing Transformer Key-Value Cache Size with Cross-Layer Attention\n 2. [C4 2024] Cross-layer Attention Sharing for Large Language Models\n\nAttention\n\n 1. [C1709 2020] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\n 2. [C573 2018] Efficient Attention: Attention With Linear Complexities\n 3. [C93 2024] Gated Linear Attention Transformers with Hardware-Efficient Training\n 4. [C24 2024] Tensor Attention Training: Provably Efficient Learning of Higher-order Transformers\n 5. [2024] When Attention Sink Emerges in Language Models: An Empirical View 👍\n 6. [C40 2024] Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon\n\nIs All You Need\n\n 1. [2025] Element-wise Attention Is All You Need\n 2. [2025] Tensor Product Attention Is All You Need\n\nFeedforward Layers\n\n 1. [2024] Building on Efficient Foundations: Effectively Training LLMs with Structured Feedforward Layers\n 2. [C3 2024] FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference\n\nAttatch Memory\n\n 1. [C2 2024] Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU\n 2. [C63 2023] LLM in a flash: Efficient Large Language Model Inference with Limited Memory\n\nNovel LLM\n\n 1. [C2 2024] Larimar: Large Language Models with Episodic Memory Control\n\nBatch\n\n 1. [C59 2023] Batch Prompting: Efficient Inference with Large Language Model APIs\n\nPruning\n\n 1. [C25 2024] ZipLM: Inference-Aware Structured Pruning of Language Models\n\nSpeculative decoding\n\n 1. [C25 2024] Efficient Inference for Large Language Model-based Generative Recommendation\n 2. [C39 2024] Enhancing Inference Efficiency and Accuracy in Large Language Models through Next-Phrase Prediction 👍\n 3. [C5 2023] Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy\n 4. [C64 2024] Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding 👍\n 5. [C4 2024] Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference\n 6. [C46 2023] SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference 👍\n\nInteresting\n\n 1.  [C60 2024] SnapKV: LLM Knows What You Are Looking for before Generation 👍\n 2.  [C6 2024] Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training\n 3.  [C165 2019] Efficient Training of BERT by Progressively Stacking\n 4.  [C9 ISCA 2024] LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference\n 5.  [C6 2024] Efficient Large Foundation Models Design: A Perspective From Model and System Co-Design\n 6.  [C83 2023] Compressing Context to Enhance Inference Efficiency of Large Language Models 👍\n 7.  [C13 2024 OSDI] ServerlessLLM: Low-Latency Serverless Inference for Large Language Models\n 8.  [C12 2024] From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models\n 9.  [C418 2022] Transformers Learn In-Context by Gradient Descent 👍\n 10. [C51 2024] Massive Activations in Large Language Models 👍\n 11. [C2007 2019] Generating Long Sequences with Sparse Transformers 👍\n 12. [C1890 2019] What does BERT look at? An Analysis of BERT’s Attention\n 13. [C1798 2019] BERT Rediscovers the Classical NLP Pipeline\n 14. [C402 2019] Analyzing the Structure of Attention in a Transformer Language Model\n 15. [C347 2018] Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures\n 16. [C112 2021] MoEfication: Transformer Feed-forward Layers are Mixtures of Experts\n 17. [C1325 2019] Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Prune\n 18. [C101] An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation\n 19. [C38517 2020] Language Models are Few-Shot Learners 👍\n 20. [C24 2024] What can a Single Attention Layer Learn? A Study Through the Random Features Lens\n 21. [C9 2023] Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps\n 22. [C606 2021] Transformer Feed-Forward Layers Are Key-Value Memories 👍\n 23. [Blog] The Feedforward Demystified: A Core Operation of Transformers\n 24. [C211 2024] ConvBERT: Improving BERT with Span-based Dynamic Convolution 👍\n 25. [C15 2020] Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models\n 26. [C147 2023] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective\n 27. [C24 2024] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\n 28. [C0 2024] Densing Law of LLMs\n 29. [C9 2024] Demystifying the Compression of Mixture-of-Experts Through a Unified Framework\n 30. [C7 2024] Configurable Foundation Models: Building LLMs from a Modular Perspective\n 31. [C2 2024] Task Scheduling for Efficient Inference of Large Language Models on Single Moderate GPU Systems\n 32. [C7 2024] Towards Sustainable Large Language Model Serving 👍\n 33. [C42 2024] LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding\n 34. [C10395 0221] GPT Understands,Too\n 35. [C36 2023] Simplifying Transformer Blocks\n 36. [C2 2024] When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1\n 37. [C188 2023] LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion\n\nWhy Infer?\n\n 1.  [C78 2024] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU 👍\n 2.  [C25 2024] Powerinfer-2: fast large language model inference on a smartphone\n 3.  [C6 2024] InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference\n 4.  [C0 2024] CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation\n 5.  [C3 2024] NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference\n 6.  [C2 2024] MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs\n 7.  [C2 2025] SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference\n 8.  [C1 2024] BlendServe: Optimizing Offline Inference for Auto-regressive Large Models with Resource-aware Batching\n 9.  [C5 2024] Efficient LLM Inference with Activation Checkpointing and Hybrid Caching\n 10. [C12 2024]Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache\n 11. [C2-24] HiRE: High Recall Approximate Top-k Estimation for Efficient LLM Inference*\n 12. [C1 2024] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference\n\nChain of Thoughts\n\n 1. [C1954 2024] Tree of Thoughts: Deliberate Problem Solving with Large Language Models 👍\n 2. [C795 2022] Automatic Chain of Thought Prompting in Large Language Models\n 3. [C109 2022] Iteratively Prompt Pre-trained Language Models for Chain of Thought\n 4. [C2 2024] Reducing Costs - The Path of Optimization for ChainofThought Reasoning via Sparse Attention Mechanism\n 5. [C17 2024] Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future\n 6. [C63 2024] Chain of Thought Empowers Transformers to Solve Inherently Serial Problems\n\nGeneral Efficient\n\n 1. [C44 2023] Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity\n 2. [C4 2024] Efficient Training and Inference: Techniques for Large Language Models Using Llama\n 3. [C226 2023] H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models\n 4. [C17 2024] EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism\n\nBig tech\n\n 1. [NVIDIA 1868] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\n 2. [2579] Scaling Laws for Neural Language Models\n 3. [C510 2021] Baidu ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation\n 4. [C6 2024] Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent\n 5. [C1671] Qwen Technical Report\n 6. [C1212] Constitutional AI: Harmlessness from AI Feedback\n 7. [C1082] Scaling Language Models: Methods, Analysis & Insights from Training Gopher\n 8. [C11289] Llama 2: Open Foundation and Fine-Tuned Chat Models\n\nHyperparameter\n\n 1. [C2070 2020] Designing Network Design Spaces\n 2. [C25389 2019] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\n 3. [C1422 2018] A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay\n\nNew Design\n\n 1. [1719 2022] Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n 2. [29 2024] Demystify Mamba in Vision: A Linear Attention Perspective\n    This paper explains that Mamba and linear attention Transformer can be formulated within a unified framework.\n\nOld Gold Time of Transformer\n\n 1. [C4622 2019] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\n 2. [C10646 XLNet 2020] XLNet: Generalized Autoregressive Pretraining for Language Understanding\n\nAI Agent\n\n 1. [C147 2023] MemoryBank: Enhancing Large Language Models with Long-Term Memory\n 2. [C22 2023] Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges\n 3. [C2051 2022] ReAct: Synergizing Reasoning and Acting in Language Models\n 4. [C220 2023] OpenAGI: When LLM Meets Domain Experts\n\n----------------------------------------\n\n\n# Attention\n\n\n# 1. Transformers are RNNs:Fast Autoregressive Transformers with Linear Attention 👍 👍\n\n\n# 2. Efficient Attention: Attention With Linear Complexities\n\nAlso another paper: Gaussian‑Linearized Transformer with Tranquilized Time‑Series Decomposition Methods for Fault Diagnosis and Forecasting of Methane Gas Sensor Arrays.\n\nThese two paper are almost the same. Basic idea: classic calculation of attention:\n\nInstead of calculating QK, it calculates KV.\n\nFigure from: Gaussian‑Linearized Transformer\n\nClass calculation complexity:\n\nProvided the model dimension is << sequence length,\n\ncompute complexity in linear attention:\n\nFigure Source\n\nSource:Demystify Mamba in Vision: A Linear Attention Perspective\n\nFigure Source\n\nProvided the model dimension is << sequence length, linear complexity is much lower compute complexity compared with classic attention.\n\nPlease notice that in the KV result. The intermediate result is dmodel * dmodel, instead of N * N. N is context length and dmodel is model dimension.\n\n----------------------------------------\n\n\n# Is All You Need\n\n\n# Tensor Product Attention Is All You Need\n\nIdea: factorizes Q, K, and V activations using contextual tensor-decompositions\n\nInstead of caching Q,K,V, it could only cache A,B of Q and A,B of K. This matrix factorization might be similar to LORA, using multiplication of two matrices A and B to represent Q.\n\nBy caching A,B, it reduces large memory required by KV cache.\n\n\n# Interesting\n\n\n# 21. [C21 2022] MoEfication: Transformer Feed-forward Layers are Mixtures of Experts\n\n\nSource from the paper\n\n\n# 28. [C10395 0221] GPT Understands,Too\n\nStraightforward Idea.\nChanging a single word in the prompt might result in substantial performance drop.\nP-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts.\nEmpirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE.\nP-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.\n\n\n\n\n# Old Gold Time of Transformer\n\n\n# 1. tansformer-xl\n\n# Segment-Level Recurrence Mechanism\n\n * Extends context length by reusing hidden states (memory) across segments.\n * Instead of processing fixed-length context windows independently, Transformer-XL introduces a memory module:\n   \n   * Memory at segment t is represented as:\n   \n   * This allows the model to attend to representations from previous segments.\n\n * Enables modeling of long-term dependencies without recomputation of overlapping segments.\n\n# Relative Positional Encoding (RPE):\n\n * Introduces position encodings relative to each query-key pair, improving generalization across sequence lengths.\n * The positional bias is calculated as:\n\n\n\n# Segment-Level Attention:\n\n * The attention mechanism incorporates both the current segment and memory from previous segments:",normalizedContent:"too many paper on llms...\n\nsurvey\n\n 1.  [c60 2024] understanding llms: a comprehensive overview from training to inference\n 2.  [c20 2024] mobile edge intelligence for large language models: a contemporary survey\n 3.  [p 58] a survey of low-bit large language models: basics, systems, and algorithms\n 4.  [p 30] a comprehensive evaluation of quantization strategies for large language models\n 5.  [p 8] a comprehensive study on quantization techniques for large language models\n 6.  [c24] model compression and efficient inference for large language models: a survey\n 7.  [c26 2024] towards better chain-of-thought prompting strategies: a survey\n 8.  [c24 2024] a survey of reasoning with foundation models\n 9.  [c75 2025] resource-efficient algorithms and systems of foundation models: a survey 👍\n 10. [c4 2024] llm for mobile: an initial roadmap\n 11. [c1 2024] achieving peak performance for large language models: a systematic review\n 12. [c1 2024] a survey: collaborative hardware and software design in the era of large language models\n 13. [c577 2014] mixture of experts: a literature survey\n 14. [c187 2024] large language model based multi-agents: a survey of progress and challenges\n 15. [c80 2024] understanding the planning of llm agents: a survey\n 16. [c895 2023] a survey on large language model based autonomous agents\\\n 17. [c60 2024] exploring large language model based intelligent agents: definitions, methods, and prospects\n\nkv cache\n\n 1. [c60 2024 usenix] infinigen: efficient generative inference of large language models with dynamic kv cache management 👍\n 2. [c1 2024]layerkv: optimizing large language model serving with layer-wise kv cache management\n 3. [c1 2024 wei] unifying kv cache compression for large language models with leankv\n\nquantization\n\n 1. [c8 y2024] an empirical study of llama3 quantization: from llms to mllms\n\ncross-layer attention\n\n 1. [c25 y2024] reducing transformer key-value cache size with cross-layer attention\n 2. [c4 2024] cross-layer attention sharing for large language models\n\nattention\n\n 1. [c1709 2020] transformers are rnns: fast autoregressive transformers with linear attention\n 2. [c573 2018] efficient attention: attention with linear complexities\n 3. [c93 2024] gated linear attention transformers with hardware-efficient training\n 4. [c24 2024] tensor attention training: provably efficient learning of higher-order transformers\n 5. [2024] when attention sink emerges in language models: an empirical view 👍\n 6. [c40 2024] soaring from 4k to 400k: extending llm's context with activation beacon\n\nis all you need\n\n 1. [2025] element-wise attention is all you need\n 2. [2025] tensor product attention is all you need\n\nfeedforward layers\n\n 1. [2024] building on efficient foundations: effectively training llms with structured feedforward layers\n 2. [c3 2024] ffsplit: split feed-forward network for optimizing accuracy-efficiency trade-off in language model inference\n\nattatch memory\n\n 1. [c2 2024] adding nvme ssds to enable and accelerate 100b model fine-tuning on a single gpu\n 2. [c63 2023] llm in a flash: efficient large language model inference with limited memory\n\nnovel llm\n\n 1. [c2 2024] larimar: large language models with episodic memory control\n\nbatch\n\n 1. [c59 2023] batch prompting: efficient inference with large language model apis\n\npruning\n\n 1. [c25 2024] ziplm: inference-aware structured pruning of language models\n\nspeculative decoding\n\n 1. [c25 2024] efficient inference for large language model-based generative recommendation\n 2. [c39 2024] enhancing inference efficiency and accuracy in large language models through next-phrase prediction 👍\n 3. [c5 2023] lookahead: an inference acceleration framework for large language model with lossless generation accuracy\n 4. [c64 2024] unlocking efficiency in large language model inference: a comprehensive survey of speculative decoding 👍\n 5. [c4 2024] dovetail: a cpu/gpu heterogeneous speculative decoding for llm inference\n 6. [c46 2023] skipdecode: autoregressive skip decoding with batching and caching for efficient llm inference 👍\n\ninteresting\n\n 1.  [c60 2024] snapkv: llm knows what you are looking for before generation 👍\n 2.  [c6 2024] stacking your transformers: a closer look at model growth for efficient llm pre-training\n 3.  [c165 2019] efficient training of bert by progressively stacking\n 4.  [c9 isca 2024] llmcompass: enabling efficient hardware design for large language model inference\n 5.  [c6 2024] efficient large foundation models design: a perspective from model and system co-design\n 6.  [c83 2023] compressing context to enhance inference efficiency of large language models 👍\n 7.  [c13 2024 osdi] serverlessllm: low-latency serverless inference for large language models\n 8.  [c12 2024] from decoding to meta-generation: inference-time algorithms for large language models\n 9.  [c418 2022] transformers learn in-context by gradient descent 👍\n 10. [c51 2024] massive activations in large language models 👍\n 11. [c2007 2019] generating long sequences with sparse transformers 👍\n 12. [c1890 2019] what does bert look at? an analysis of bert’s attention\n 13. [c1798 2019] bert rediscovers the classical nlp pipeline\n 14. [c402 2019] analyzing the structure of attention in a transformer language model\n 15. [c347 2018] why self-attention? a targeted evaluation of neural machine translation architectures\n 16. [c112 2021] moefication: transformer feed-forward layers are mixtures of experts\n 17. [c1325 2019] analyzing multi-head self-attention: specialized heads do the heavy lifting, the rest can be prune\n 18. [c101] an analysis of attention mechanisms: the case of word sense disambiguation in neural machine translation\n 19. [c38517 2020] language models are few-shot learners 👍\n 20. [c24 2024] what can a single attention layer learn? a study through the random features lens\n 21. [c9 2023] analyzing feed-forward blocks in transformers through the lens of attention maps\n 22. [c606 2021] transformer feed-forward layers are key-value memories 👍\n 23. [blog] the feedforward demystified: a core operation of transformers\n 24. [c211 2024] convbert: improving bert with span-based dynamic convolution 👍\n 25. [c15 2020] convolutions and self-attention: re-interpreting relative positions in pre-trained language models\n 26. [c147 2023] towards revealing the mystery behind chain of thought: a theoretical perspective\n 27. [c24 2024] minference 1.0: accelerating pre-filling for long-context llms via dynamic sparse attention\n 28. [c0 2024] densing law of llms\n 29. [c9 2024] demystifying the compression of mixture-of-experts through a unified framework\n 30. [c7 2024] configurable foundation models: building llms from a modular perspective\n 31. [c2 2024] task scheduling for efficient inference of large language models on single moderate gpu systems\n 32. [c7 2024] towards sustainable large language model serving 👍\n 33. [c42 2024] layerskip: enabling early exit inference and self-speculative decoding\n 34. [c10395 0221] gpt understands,too\n 35. [c36 2023] simplifying transformer blocks\n 36. [c2 2024] when a language model is optimized for reasoning, does it still show embers of autoregression? an analysis of openai o1\n 37. [c188 2023] llm-blender: ensembling large language models with pairwise ranking and generative fusion\n\nwhy infer?\n\n 1.  [c78 2024] powerinfer: fast large language model serving with a consumer-grade gpu 👍\n 2.  [c25 2024] powerinfer-2: fast large language model inference on a smartphone\n 3.  [c6 2024] instinfer: in-storage attention offloading for cost-effective long-context llm inference\n 4.  [c0 2024] coreinfer: accelerating large language model inference with semantics-inspired adaptive sparse activation\n 5.  [c3 2024] neo: saving gpu memory crisis with cpu offloading for online llm inference\n 6.  [c2 2024] moe-lightning: high-throughput moe inference on memory-constrained gpus\n 7.  [c2 2025] sparseinfer: training-free prediction of activation sparsity for fast llm inference\n 8.  [c1 2024] blendserve: optimizing offline inference for auto-regressive large models with resource-aware batching\n 9.  [c5 2024] efficient llm inference with activation checkpointing and hybrid caching\n 10. [c12 2024]q-hitter: a better token oracle for efficient llm inference via sparse-quantized kv cache\n 11. [c2-24] hire: high recall approximate top-k estimation for efficient llm inference*\n 12. [c1 2024] shadowkv: kv cache in shadows for high-throughput long-context llm inference\n\nchain of thoughts\n\n 1. [c1954 2024] tree of thoughts: deliberate problem solving with large language models 👍\n 2. [c795 2022] automatic chain of thought prompting in large language models\n 3. [c109 2022] iteratively prompt pre-trained language models for chain of thought\n 4. [c2 2024] reducing costs - the path of optimization for chainofthought reasoning via sparse attention mechanism\n 5. [c17 2024] navigate through enigmatic labyrinth a survey of chain of thought reasoning: advances, frontiers and future\n 6. [c63 2024] chain of thought empowers transformers to solve inherently serial problems\n\ngeneral efficient\n\n 1. [c44 2023] flash-llm: enabling cost-effective and highly-efficient large generative model inference with unstructured sparsity\n 2. [c4 2024] efficient training and inference: techniques for large language models using llama\n 3. [c226 2023] h2o: heavy-hitter oracle for efficient generative inference of large language models\n 4. [c17 2024] ee-llm: large-scale training and inference of early-exit large language models with 3d parallelism\n\nbig tech\n\n 1. [nvidia 1868] megatron-lm: training multi-billion parameter language models using model parallelism\n 2. [2579] scaling laws for neural language models\n 3. [c510 2021] baidu ernie 3.0: large-scale knowledge enhanced pre-training for language understanding and generation\n 4. [c6 2024] hunyuan-large: an open-source moe model with 52 billion activated parameters by tencent\n 5. [c1671] qwen technical report\n 6. [c1212] constitutional ai: harmlessness from ai feedback\n 7. [c1082] scaling language models: methods, analysis & insights from training gopher\n 8. [c11289] llama 2: open foundation and fine-tuned chat models\n\nhyperparameter\n\n 1. [c2070 2020] designing network design spaces\n 2. [c25389 2019] efficientnet: rethinking model scaling for convolutional neural networks\n 3. [c1422 2018] a disciplined approach to neural network hyper-parameters: part 1 -- learning rate, batch size, momentum, and weight decay\n\nnew design\n\n 1. [1719 2022] mamba: linear-time sequence modeling with selective state spaces\n 2. [29 2024] demystify mamba in vision: a linear attention perspective\n    this paper explains that mamba and linear attention transformer can be formulated within a unified framework.\n\nold gold time of transformer\n\n 1. [c4622 2019] transformer-xl: attentive language models beyond a fixed-length context\n 2. [c10646 xlnet 2020] xlnet: generalized autoregressive pretraining for language understanding\n\nai agent\n\n 1. [c147 2023] memorybank: enhancing large language models with long-term memory\n 2. [c22 2023] utilizing bert for information retrieval: survey, applications, resources, and challenges\n 3. [c2051 2022] react: synergizing reasoning and acting in language models\n 4. [c220 2023] openagi: when llm meets domain experts\n\n----------------------------------------\n\n\n# attention\n\n\n# 1. transformers are rnns:fast autoregressive transformers with linear attention 👍 👍\n\n\n# 2. efficient attention: attention with linear complexities\n\nalso another paper: gaussian‑linearized transformer with tranquilized time‑series decomposition methods for fault diagnosis and forecasting of methane gas sensor arrays.\n\nthese two paper are almost the same. basic idea: classic calculation of attention:\n\ninstead of calculating qk, it calculates kv.\n\nfigure from: gaussian‑linearized transformer\n\nclass calculation complexity:\n\nprovided the model dimension is << sequence length,\n\ncompute complexity in linear attention:\n\nfigure source\n\nsource:demystify mamba in vision: a linear attention perspective\n\nfigure source\n\nprovided the model dimension is << sequence length, linear complexity is much lower compute complexity compared with classic attention.\n\nplease notice that in the kv result. the intermediate result is dmodel * dmodel, instead of n * n. n is context length and dmodel is model dimension.\n\n----------------------------------------\n\n\n# is all you need\n\n\n# tensor product attention is all you need\n\nidea: factorizes q, k, and v activations using contextual tensor-decompositions\n\ninstead of caching q,k,v, it could only cache a,b of q and a,b of k. this matrix factorization might be similar to lora, using multiplication of two matrices a and b to represent q.\n\nby caching a,b, it reduces large memory required by kv cache.\n\n\n# interesting\n\n\n# 21. [c21 2022] moefication: transformer feed-forward layers are mixtures of experts\n\n\nsource from the paper\n\n\n# 28. [c10395 0221] gpt understands,too\n\nstraightforward idea.\nchanging a single word in the prompt might result in substantial performance drop.\np-tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts.\nempirically, p-tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of nlu tasks including lama and superglue.\np-tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.\n\n\n\n\n# old gold time of transformer\n\n\n# 1. tansformer-xl\n\n# segment-level recurrence mechanism\n\n * extends context length by reusing hidden states (memory) across segments.\n * instead of processing fixed-length context windows independently, transformer-xl introduces a memory module:\n   \n   * memory at segment t is represented as:\n   \n   * this allows the model to attend to representations from previous segments.\n\n * enables modeling of long-term dependencies without recomputation of overlapping segments.\n\n# relative positional encoding (rpe):\n\n * introduces position encodings relative to each query-key pair, improving generalization across sequence lengths.\n * the positional bias is calculated as:\n\n\n\n# segment-level attention:\n\n * the attention mechanism incorporates both the current segment and memory from previous segments:",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Efficient LLM",frontmatter:{title:"Efficient LLM",date:"2025-01-20T23:32:49.000Z",permalink:"/pages/dc7043/",tags:[null]},regularPath:"/05.llm/09.eff_llm.html",relativePath:"05.llm/09.eff_llm.md",key:"v-2925b9e5",path:"/pages/dc7043/",headers:[{level:2,title:"1. Resource-Efficient Architectures",slug:"_1-resource-efficient-architectures",normalizedTitle:"1. resource-efficient architectures",charIndex:555},{level:3,title:"1.1 Efficient Attention",slug:"_1-1-efficient-attention",normalizedTitle:"1.1 efficient attention",charIndex:595},{level:3,title:"1.2 Dynamic Neural Network",slug:"_1-2-dynamic-neural-network",normalizedTitle:"1.2 dynamic neural network",charIndex:1724},{level:3,title:"1.3 Diffusion-Specific Optimization",slug:"_1-3-diffusion-specific-optimization",normalizedTitle:"1.3 diffusion-specific optimization",charIndex:2975},{level:3,title:"1.4 ViT-Specific Optimizations",slug:"_1-4-vit-specific-optimizations",normalizedTitle:"1.4 vit-specific optimizations",charIndex:3102},{level:2,title:"2. Resource-Efficient Algorithms",slug:"_2-resource-efficient-algorithms",normalizedTitle:"2. resource-efficient algorithms",charIndex:3202},{level:3,title:"2.1 Pre-Training Algorithms",slug:"_2-1-pre-training-algorithms",normalizedTitle:"2.1 pre-training algorithms",charIndex:3239},{level:3,title:"2.2 Fine-Tuning Algorithms",slug:"_2-2-fine-tuning-algorithms",normalizedTitle:"2.2 fine-tuning algorithms",charIndex:4275},{level:3,title:"2.3 Inference Algorithms",slug:"_2-3-inference-algorithms",normalizedTitle:"2.3 inference algorithms",charIndex:5788},{level:3,title:"2.4 Model Compression",slug:"_2-4-model-compression",normalizedTitle:"2.4 model compression",charIndex:8133},{level:2,title:"3. Resource-Efficient Systems",slug:"_3-resource-efficient-systems",normalizedTitle:"3. resource-efficient systems",charIndex:13682},{level:3,title:"3.1 Distributed Training",slug:"_3-1-distributed-training",normalizedTitle:"3.1 distributed training",charIndex:13716},{level:3,title:"3.2 Hardware-Aware Optimizations",slug:"_3-2-hardware-aware-optimizations",normalizedTitle:"3.2 hardware-aware optimizations",charIndex:14522},{level:3,title:"3.3 Serving on Cloud",slug:"_3-3-serving-on-cloud",normalizedTitle:"3.3 serving on cloud",charIndex:14722},{level:3,title:"3.4 Serving on Edge",slug:"_3-4-serving-on-edge",normalizedTitle:"3.4 serving on edge",charIndex:16647}],headersStr:"1. Resource-Efficient Architectures 1.1 Efficient Attention 1.2 Dynamic Neural Network 1.3 Diffusion-Specific Optimization 1.4 ViT-Specific Optimizations 2. Resource-Efficient Algorithms 2.1 Pre-Training Algorithms 2.2 Fine-Tuning Algorithms 2.3 Inference Algorithms 2.4 Model Compression 3. Resource-Efficient Systems 3.1 Distributed Training 3.2 Hardware-Aware Optimizations 3.3 Serving on Cloud 3.4 Serving on Edge",content:" * [1] Resource-efficient Algorithms and Systems of Foundation Models: A Survey 😄\n * [2] A Survey on Efficient Inference for Large Language Models 🙋\n * [3] Efficient Large Language Models: A Survey 🙋\n * [4] Efficient Transformers: A Survey 🙋\n\n----------------------------------------\n\n\n\nSource: Resource-efficient\n\nComputation complexity of attention is O(T 2D), whereas that of the FFN is O(TD2), where T represents the sequence length and D the hidden state dimension of the model.\nThe FFN layer is the most computationally intensive component.\n\n\n# 1. Resource-Efficient Architectures\n\n\n# 1.1 Efficient Attention\n\n\n\n * Sparse Attention: Reduces complexity (e.g., Longformer[C4522 2020], BigBird).\n   Motivated by graph sparsification, sparse attention aims to build a sparse attention matrix.\n   This approach aims to retain the empirical advantages of a fully quadratic self-attention scheme while employing a reduced number of inner products.\n\n[C2016 2019] Generating Long Sequences with Sparse Transformers\n[C602 2020] Efficient Content-Based Sparse Attention with Routing Transformers [C136 2021] Scatterbrain: Unifying Sparse and Low-rank Attention Approximation [C55 2021] Is Sparse Attention more Interpretable?\n\n * Approximate Attention: Low-rank approximations (e.g., Linformer, Reformer). Approximate attention mainly includes low-rank approximations of the self-attention matrix and innovative reformulations of the self-attention.\n\n * Attention-Free Approaches: Alternatives like Hyena, Mamba.\n   Despite the dominance of attention-based transformer architectures in large FMs, several works have put forth innovative architectures that hold the potential to replace the traditional transformer model.\n\n\n# 1.2 Dynamic Neural Network\n\n\n\n * Mixture of Experts (MoE): (e.g., Switch Transformer, GLaM, MoEfication, FFF).\n   * C1950 2021 Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n   * Janus[C20 2023] Janus: A Unified Distributed Training Framework for Sparse Mixture-of-Experts Models\n   * C124 2018 Deep Mixture of Experts via Shallow Embedding\n   * C364 2013 Learning Factored Representations in a Deep Mixture of Experts\n   * C264 2022 Mixture-of-Experts with Expert Choice Routing\n   * C594 2022 GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\n   * C113 2021 MoEfication: Transformer Feed-forward Layers are Mixtures of Experts\n * Early Exiting: Premature termination based on confidence (e.g., FREE, SkipDecode,DeeBERT, PABEE). early-exiting optimization is a strategy that allows a model to terminate its computational process prematurely when it attains high confidence in the prediction or encounters resource constraints. [C342 2020] BERT Loses Patience: Fast and Robust Inference with Early Exit [C383 2020] DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference [C46 2023] SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference\n\n\n# 1.3 Diffusion-Specific Optimization\n\n * Efficient Sampling\n * Diffusion in Latent Space\n * Diffusion Architecture Variants\n\n\n# 1.4 ViT-Specific Optimizations\n\n * Efficient ViT Variants: MobileViT, EfficientFormer, EdgeViT.\n\n\n# 2. Resource-Efficient Algorithms\n\n\n# 2.1 Pre-Training Algorithms\n\n * Training Data Quality Control: DataComp, DFN.\n   A portion of work focus on controlling the quality of training data.\n * Training Data Reduction: Deduplication, image patch removal.\n   Pre-training for large FMs needs a dataset at the trillion scale, exemplified by 0.3 trillion tokens for GPT-3-175B [25] and 2 trillion tokens for LLaMa-2-70B [238].\n   prior literature resorts to reduce vast training data through two aspects: deduplicating text datasets and image patch removal.\n * Progressive Learning: StackingBERT, CompoundGrow. Progressive learning is a training strategy that begins by training a small model and then gradually increases the model size, throughout the training process.\n * Mixed Precision Training: Mesa, GACT. Mixed precision training often utilizes half-precision floating-point data representation instead of single precision. This approach significantly reduces memory requirements, approximately halving the storage space needed for weights, activations, and gradients.\n\n\n# 2.2 Fine-Tuning Algorithms\n\n\n\n * Additive Tuning:\n   \n   * Adapter tuning aims to reduce training costs by introducing adapter modules to specific layers (or all layers) of pre-trained large FMs. During tuning, the backbone of the pre-trained model remains frozen, and adapter modules are utilized to acquire task-specific knowledge.\n   * prompt tuning involves designing a task-specific prompt for each task, with the aim of replacing the traditional fine-tuning of pre-trained large FMs parameters.\n     By tuning the input prompts instead, this method significantly reduces the resources and time required for the fine-tuning.\n     [C3778] The Power of Scale for Parameter-Efficient Prompt Tuning\n   * prefix tuning introduces a trainable, task-specific prefix part to each layer of large FMs. This technique aims to reduce the tuning cost by limiting the updates to the parameters in this prefix.\n\n * Selective Tuning: Freezing most parameters, selective updates. Selective tuning aims to maintain high performance on new tasks with low training costs by freezing the majority of parameters in large FMs and selectively updating only a small portion of the parameters.\n\n * Re-parameter Tuning: Low-rank adaptation (e.g., LoRA, Delta-LoRA).\n   \n   Re-parameter tuning adapts large FMs by targeting a significantly smaller subspace than the original, expansive training space.\n   This approach involves fine-tuning low-rank matrix parameters, a technique that effectively reduces the overall training cost.\n\n\n# 2.3 Inference Algorithms\n\n * Opportunistic Decoding:\n   \n   * Speculative decoding (SpecInfer, LLMCad) generating sequences autoregressively with a cost-efficient small model, followed by parallel token verification using a larger model.\n   * Look-ahead decoding accelerates inference in large FMs without relying on a draft model or data store, reducing decoding steps in proportion to log(FLOPs).\n\n * Input Filtering and Compression:\n   \n   * Prompt compression(LLMLingua,LLMZip,ICAE,COT-Max) LLMZip [241] employs LLaMA-7B for compressing natural language. Experimental results demonstrate that LLMZip outperforms cutting-edge text compression methods, including BSC, ZPAQ, and paq8h.\n   * Token pruning Pruning of input sequences for transformers, often involving the incremental removal of less important tokens during inference.\n\n * KV Cache Optimization: Memory-efficient sparse attention. most sparse attention designs, which primarily target the reduction of computational complexity [24, 294], do not necessarily lead to a reduction in KV cache memory consumption.\n   This is because achieving a reduced memory footprint for the KV cache necessitates a more stringent sparsity pattern.\n   \n   * H2O KV cache eviction stragegy: employs attention scores to identify and select the least important KV cache tokens in the current state for eviction\n   * Dynamic Context Pruning learns a memory-efficient KV cache eviction strategy during the pre-training phase.\n   * Scissorhands: innovative compact KV cache\n   * Landmark Attention enables the storage of most KV caches in a slower but larger capacity memory\n   * [C1 2025] Efficient LLM Inference with Activation Checkpointing and Hybrid Caching\n   * [C1 2024] Unifying KV Cache Compression for Large Language Models with LeanKV\n   * [C316 2023] Efficient Streaming Language Models with Attention Sinks\n\n * Long Context Handling: LM-Infinite, StreamingLLM. Due to the quadratic computational cost associated with attention mechanisms, various resource-efficient optimizations have been proposed to handle long inputs.\n   \n   * LM-Infinite introduces a Λ-shaped attention mechanism to handle long contexts efficiently.\n   * StreamingLLM facilitates large FMs trained with a finite-length attention window to generalize to infinite stream decoding without the need for any fine-tuning.\n\n\n# 2.4 Model Compression\n\n\n\n * Pruning\n   \n   * Structured pruning eliminates entire structural components, such as groups of consecutive parameters or hierarchical structures\n     LLM Pruner[C372 2023] selectively removes non-essential model structures based on gradient information and incorporates LoRA to recover the model’s accuracy after pruning.\n     Structured pruning is also employed in training.\n     Sheared LLaMA adopts an end-to-end to remove channels, encompassing layers, attention heads, intermediate layers, and hidden layers.\n     AdaPrune accelerates neural network training using transposable masks.\n     GUM considers neuron specificity and introduces pruning through network component-based global mobility and local uniqueness scores.\n     PLATON tackles the uncertainty in importance scores during pruning by employing the upper confidence bound of importance estimation.\n   * Unstructred pruning It removes neurons with weights below a threshold, thereby compressing the model.、 SparseGPT[C497 2023] sparse regression solver\n     Wanda[C346 2023] prunes weights with smallest magnitude multiplied by corresponding input activations.\n     SIGE converts computation reduction into latency reduction.\n   * Contextual pruning selects the sparse state of each layer.、 Deja Vu 🙋 dynamically predicts the sparsity of the next layer using the activations of the previous layer. It determines which neurons of MLP blocks and the heads of attention blocks need to be retained. To mitigate the overhead of this predictor, Deja Vu asynchronously predicts the next layer.\n     PowerInfer utilizes the sparsity of activation to dynamically predict the hotactivated neurons of the next layer and computes them on the GPU, whereas other cold-activated neurons are computed on the CPU.\n\n * Knowledge Distillation: Black-box and white-box KD.\n\n * Quantization: Quantization-aware training (QAT), post-training quantization (PTQ).\n   \n   * Quantization-aware training\n     LLM-QAT obtains training data for LLMs by leveraging pre-trained models to generate samples through data-free distillation. it quantizes weights, activations, and KV cache, thereby improving training throughput.\n     QuantGPT incorporats contrastive distillation from a full-precision teacher model and distilling logit information to a quantized student model during autoregressive pre-training.\n     BitNet pioneers QAT for 1-bit language models, training the language model with 1-bit weights and activations. [C31 2024] KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization\n   * Post-training quantization\n     \n     * Weights-only Quantization\n       \n       identification of outliers and important weights in weights that significantly contribute to accuracy and treating these outliers specially.\n       \n       SpQR identifies outlier weights and maintains them with high precision while quantizing the rest of the weights.\n       \n       LLM.int8() employs vectorized quantization and mixed-precision decomposition to handle outlier values for efficient inference.\n       \n       AWQ[C523 2023] reduces quantization error by protecting the top 1% important weights in the model, utilizing per-channel scaling to determine the optimal scaling factor.\n       \n       OWQ[C37 2023] analysis suggests that abnormal activations amplify quantization errors, and it employs a mixed-precision scheme, applying higher-precision quantization to weights with a significant impact from activated outlier values.\n       \n       SqueezeLLM[C160 2023] observes that sensitive weights determine the final model’s quantization performance and proposes a non-uniform quantization approach to minimize quantization errors in these sensitive weights.\n     \n     * Weights-Activation Coquantization\n       \n       SmoothQuant[C770 2022] takes advantage of the similarity in the channel-wise activations of different tokens and performs quantization on both weight and activation using per-channel scaling transforms.\n       \n       RPTQ recognizes the substantial range differences across different channels, reordering the channels for quantization and integrating them into layer normalization and linear layer weights.\n       \n       OliVe adopts outlier-victim pair quantization and locally processes outliers.\n       \n       Outlier Suppression+ builds upon Outlier Suppression, discovering that harmful outliers exhibit an asymmetric distribution mainly concentrated in specific channels.\n       Considering the asymmetry of outliers and quantization errors from the weights of the next layer, this approach performs channel-level translation and scaling operations.\n       \n       QLLM addresses the issue of activation outliers through an adaptive channel reassembly method and mitigates the information loss caused by quantization using calibration data.\n       \n       LLM-FP4 quantizes weights into 4-bit float points, proposes per-channel activation quantization, and reparameters additional scaling factors as exponential biases of weights.\n       \n       ZeroQuant combines layer-wise KD and optimized quantization support to achieve 8-bit quantization.\n       \n       FlexRound updates the quantization scale of weights and activations by minimizing the error between the quantized values and the full-precision values.\n       \n       ATOM significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization.\n\n * Low-Rank Decomposition (LoRD): TensorGPT, LoSparse.\n\n\n# 3. Resource-Efficient Systems\n\n\n# 3.1 Distributed Training\n\n * Resilience: Checkpointing, redundant computation.\n\n * Parallelism: Data, model, and sequence parallelism.\n\n * Communication: Compression, overlapping with computation.\n\n * Storage: Offloading, heterogeneous GPUs.\n   ZeRO-Offload offloads data and computations to CPU to train large models on a single GPU.\n   FlashNeuron[C52 2021] offloads selective data to the SSD for higher throughput.\n\n * MoE Optimization: optimizes the dynamism-related mechanisms, parallelism, and communication in MoE training. MegaBlocks[C74 2022] leverages sparse primitives to handle dynamic routing and load-imbalanced computation.\n   \n   FlexMoE focuses on the dynamic expert management and device placement problem.\n   \n   Tutel designs dynamic adaptive parallelism and pipelining strategies.\n\n\n# 3.2 Hardware-Aware Optimizations\n\n * EdgeBERT: Latency-aware energy optimization.\n * FlightLLM: FPGA-based LLM inference.\n * SpAtten: Sparse attention with cascade token pruning.\n * A3[C227 2020]\n\n\n# 3.3 Serving on Cloud\n\n * Inference Accelerating:\n   \n   * FlashAttention, FlashAttention-2\n   * Flash-Decoding, FlashDecoding++\n   * DeepSpeed-Inference\n   * ByteTransformer\n   * Google PaLM\n   \n   Batching and scheduling\n   \n   Orca proposes selective batching and iteration-level scheduling to batch requests of different lengths at the granularity of iterations to increase the maximum batch size.\n   \n   FlexGen proposes a request scheduling algorithm to mitigate the impact of offloading on the performance of latencyinsensitive FM serving in a single GPU.\n   \n   FastServe proposes an iteration-level preemptive scheduling and proactive KV cache swapping to mitigate the impact of head-of-line blocking on the performance of distributed FM serving.\n   \n   SARATHI and DeepSpeed-FastGen split the computation of the prefill phase into small chunks and schedule these chunks with the decoding phase to mitigate the impact of the prefill phase on the performance of large FMs serving.\n   \n   Splitwise splits the prefill phase and the decoding phase onto different machines according to their different computation and memory requirements.\n   \n   Sarathi-Serve introduces a chunked-prefills scheduler which splits a prefill request into near equal sized chunks and creates stall-free schedules that adds new requests in a batch without pausing ongoing decodes.\n   \n   dLoRA dynamically merges/unmerges adapters with the base model and migrating requests/adapters between worker replicas, significantly improving the serving throughput.\n\n * Memory Saving\n   \n   * DeepSpeed-Inference and FlexGen offload activations or model parameters to the DRAM or NVMe memories\n   * KV cache managing\n     * vLLM adopts block-level on-demand memory allocation mechanism.\n     * S-LoRA extends this idea to Unified Paging to manage multiple LoRA adapters at the same time.\n\n * Emerging Platforms: Spot instances, heterogeneous GPUs.\n\n\n# 3.4 Serving on Edge\n\n * Edge-Cloud Collaboration\n   \n   EdgeFM queries and adapts the large FMs to the specific edge models with customized knowledge and architectures so that the dynamic edge model can ensure both low latency and close accuracy to the original large FMs.\n\n * On-Device MoE\n   \n   EdgeMoe identifies the problem that experts have to be dynamically loaded into memory during inference. To tackle this issue, this approach proposes expert-wise bit-width adaptation to reduce the size of expert parameters with acceptable accuracy loss, saving parameters’ loading time.\n   \n   PC-MoE is based on a crucial observation that expert activations are subject to temporal locality. Based on this observation, PC-MoE proposes Parameter Committee, which intelligently maintains a subset of crucial experts in use to reduce resource consumption.\n   \n   [C21 2024] Mobile foundation model as firmware\n\n * Memory Optimization: LLMCad, PowerInfer.\n   \n   LLMCad utilizes speculative decoding, which can offload most workloads to a smaller memory-resident draft model.\n   \n   PowerInfer relies on large FMs runtime sparsity (i.e., only hot neurons are consistently activated across inputs). PowerInfer pre-loads hot-activated neurons onto the GPU for fast access, whereas cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPUGPU data transfers.\n\n * I/O Optimization: STI, LLM in a flash. STI [C16] identifies that loading parameters time is highly longer than computation time. To address this problem, STI proposes dynamically adapting weights bit-width during the loading procedure according to parameters importance, minimizing loading overhead under maximum inference accuracy.\n\n * Kernel Optimization: Integer-based edge kernels.",normalizedContent:" * [1] resource-efficient algorithms and systems of foundation models: a survey 😄\n * [2] a survey on efficient inference for large language models 🙋\n * [3] efficient large language models: a survey 🙋\n * [4] efficient transformers: a survey 🙋\n\n----------------------------------------\n\n\n\nsource: resource-efficient\n\ncomputation complexity of attention is o(t 2d), whereas that of the ffn is o(td2), where t represents the sequence length and d the hidden state dimension of the model.\nthe ffn layer is the most computationally intensive component.\n\n\n# 1. resource-efficient architectures\n\n\n# 1.1 efficient attention\n\n\n\n * sparse attention: reduces complexity (e.g., longformer[c4522 2020], bigbird).\n   motivated by graph sparsification, sparse attention aims to build a sparse attention matrix.\n   this approach aims to retain the empirical advantages of a fully quadratic self-attention scheme while employing a reduced number of inner products.\n\n[c2016 2019] generating long sequences with sparse transformers\n[c602 2020] efficient content-based sparse attention with routing transformers [c136 2021] scatterbrain: unifying sparse and low-rank attention approximation [c55 2021] is sparse attention more interpretable?\n\n * approximate attention: low-rank approximations (e.g., linformer, reformer). approximate attention mainly includes low-rank approximations of the self-attention matrix and innovative reformulations of the self-attention.\n\n * attention-free approaches: alternatives like hyena, mamba.\n   despite the dominance of attention-based transformer architectures in large fms, several works have put forth innovative architectures that hold the potential to replace the traditional transformer model.\n\n\n# 1.2 dynamic neural network\n\n\n\n * mixture of experts (moe): (e.g., switch transformer, glam, moefication, fff).\n   * c1950 2021 switch transformers: scaling to trillion parameter models with simple and efficient sparsity\n   * janus[c20 2023] janus: a unified distributed training framework for sparse mixture-of-experts models\n   * c124 2018 deep mixture of experts via shallow embedding\n   * c364 2013 learning factored representations in a deep mixture of experts\n   * c264 2022 mixture-of-experts with expert choice routing\n   * c594 2022 glam: efficient scaling of language models with mixture-of-experts\n   * c113 2021 moefication: transformer feed-forward layers are mixtures of experts\n * early exiting: premature termination based on confidence (e.g., free, skipdecode,deebert, pabee). early-exiting optimization is a strategy that allows a model to terminate its computational process prematurely when it attains high confidence in the prediction or encounters resource constraints. [c342 2020] bert loses patience: fast and robust inference with early exit [c383 2020] deebert: dynamic early exiting for accelerating bert inference [c46 2023] skipdecode: autoregressive skip decoding with batching and caching for efficient llm inference\n\n\n# 1.3 diffusion-specific optimization\n\n * efficient sampling\n * diffusion in latent space\n * diffusion architecture variants\n\n\n# 1.4 vit-specific optimizations\n\n * efficient vit variants: mobilevit, efficientformer, edgevit.\n\n\n# 2. resource-efficient algorithms\n\n\n# 2.1 pre-training algorithms\n\n * training data quality control: datacomp, dfn.\n   a portion of work focus on controlling the quality of training data.\n * training data reduction: deduplication, image patch removal.\n   pre-training for large fms needs a dataset at the trillion scale, exemplified by 0.3 trillion tokens for gpt-3-175b [25] and 2 trillion tokens for llama-2-70b [238].\n   prior literature resorts to reduce vast training data through two aspects: deduplicating text datasets and image patch removal.\n * progressive learning: stackingbert, compoundgrow. progressive learning is a training strategy that begins by training a small model and then gradually increases the model size, throughout the training process.\n * mixed precision training: mesa, gact. mixed precision training often utilizes half-precision floating-point data representation instead of single precision. this approach significantly reduces memory requirements, approximately halving the storage space needed for weights, activations, and gradients.\n\n\n# 2.2 fine-tuning algorithms\n\n\n\n * additive tuning:\n   \n   * adapter tuning aims to reduce training costs by introducing adapter modules to specific layers (or all layers) of pre-trained large fms. during tuning, the backbone of the pre-trained model remains frozen, and adapter modules are utilized to acquire task-specific knowledge.\n   * prompt tuning involves designing a task-specific prompt for each task, with the aim of replacing the traditional fine-tuning of pre-trained large fms parameters.\n     by tuning the input prompts instead, this method significantly reduces the resources and time required for the fine-tuning.\n     [c3778] the power of scale for parameter-efficient prompt tuning\n   * prefix tuning introduces a trainable, task-specific prefix part to each layer of large fms. this technique aims to reduce the tuning cost by limiting the updates to the parameters in this prefix.\n\n * selective tuning: freezing most parameters, selective updates. selective tuning aims to maintain high performance on new tasks with low training costs by freezing the majority of parameters in large fms and selectively updating only a small portion of the parameters.\n\n * re-parameter tuning: low-rank adaptation (e.g., lora, delta-lora).\n   \n   re-parameter tuning adapts large fms by targeting a significantly smaller subspace than the original, expansive training space.\n   this approach involves fine-tuning low-rank matrix parameters, a technique that effectively reduces the overall training cost.\n\n\n# 2.3 inference algorithms\n\n * opportunistic decoding:\n   \n   * speculative decoding (specinfer, llmcad) generating sequences autoregressively with a cost-efficient small model, followed by parallel token verification using a larger model.\n   * look-ahead decoding accelerates inference in large fms without relying on a draft model or data store, reducing decoding steps in proportion to log(flops).\n\n * input filtering and compression:\n   \n   * prompt compression(llmlingua,llmzip,icae,cot-max) llmzip [241] employs llama-7b for compressing natural language. experimental results demonstrate that llmzip outperforms cutting-edge text compression methods, including bsc, zpaq, and paq8h.\n   * token pruning pruning of input sequences for transformers, often involving the incremental removal of less important tokens during inference.\n\n * kv cache optimization: memory-efficient sparse attention. most sparse attention designs, which primarily target the reduction of computational complexity [24, 294], do not necessarily lead to a reduction in kv cache memory consumption.\n   this is because achieving a reduced memory footprint for the kv cache necessitates a more stringent sparsity pattern.\n   \n   * h2o kv cache eviction stragegy: employs attention scores to identify and select the least important kv cache tokens in the current state for eviction\n   * dynamic context pruning learns a memory-efficient kv cache eviction strategy during the pre-training phase.\n   * scissorhands: innovative compact kv cache\n   * landmark attention enables the storage of most kv caches in a slower but larger capacity memory\n   * [c1 2025] efficient llm inference with activation checkpointing and hybrid caching\n   * [c1 2024] unifying kv cache compression for large language models with leankv\n   * [c316 2023] efficient streaming language models with attention sinks\n\n * long context handling: lm-infinite, streamingllm. due to the quadratic computational cost associated with attention mechanisms, various resource-efficient optimizations have been proposed to handle long inputs.\n   \n   * lm-infinite introduces a λ-shaped attention mechanism to handle long contexts efficiently.\n   * streamingllm facilitates large fms trained with a finite-length attention window to generalize to infinite stream decoding without the need for any fine-tuning.\n\n\n# 2.4 model compression\n\n\n\n * pruning\n   \n   * structured pruning eliminates entire structural components, such as groups of consecutive parameters or hierarchical structures\n     llm pruner[c372 2023] selectively removes non-essential model structures based on gradient information and incorporates lora to recover the model’s accuracy after pruning.\n     structured pruning is also employed in training.\n     sheared llama adopts an end-to-end to remove channels, encompassing layers, attention heads, intermediate layers, and hidden layers.\n     adaprune accelerates neural network training using transposable masks.\n     gum considers neuron specificity and introduces pruning through network component-based global mobility and local uniqueness scores.\n     platon tackles the uncertainty in importance scores during pruning by employing the upper confidence bound of importance estimation.\n   * unstructred pruning it removes neurons with weights below a threshold, thereby compressing the model.、 sparsegpt[c497 2023] sparse regression solver\n     wanda[c346 2023] prunes weights with smallest magnitude multiplied by corresponding input activations.\n     sige converts computation reduction into latency reduction.\n   * contextual pruning selects the sparse state of each layer.、 deja vu 🙋 dynamically predicts the sparsity of the next layer using the activations of the previous layer. it determines which neurons of mlp blocks and the heads of attention blocks need to be retained. to mitigate the overhead of this predictor, deja vu asynchronously predicts the next layer.\n     powerinfer utilizes the sparsity of activation to dynamically predict the hotactivated neurons of the next layer and computes them on the gpu, whereas other cold-activated neurons are computed on the cpu.\n\n * knowledge distillation: black-box and white-box kd.\n\n * quantization: quantization-aware training (qat), post-training quantization (ptq).\n   \n   * quantization-aware training\n     llm-qat obtains training data for llms by leveraging pre-trained models to generate samples through data-free distillation. it quantizes weights, activations, and kv cache, thereby improving training throughput.\n     quantgpt incorporats contrastive distillation from a full-precision teacher model and distilling logit information to a quantized student model during autoregressive pre-training.\n     bitnet pioneers qat for 1-bit language models, training the language model with 1-bit weights and activations. [c31 2024] kvquant: towards 10 million context length llm inference with kv cache quantization\n   * post-training quantization\n     \n     * weights-only quantization\n       \n       identification of outliers and important weights in weights that significantly contribute to accuracy and treating these outliers specially.\n       \n       spqr identifies outlier weights and maintains them with high precision while quantizing the rest of the weights.\n       \n       llm.int8() employs vectorized quantization and mixed-precision decomposition to handle outlier values for efficient inference.\n       \n       awq[c523 2023] reduces quantization error by protecting the top 1% important weights in the model, utilizing per-channel scaling to determine the optimal scaling factor.\n       \n       owq[c37 2023] analysis suggests that abnormal activations amplify quantization errors, and it employs a mixed-precision scheme, applying higher-precision quantization to weights with a significant impact from activated outlier values.\n       \n       squeezellm[c160 2023] observes that sensitive weights determine the final model’s quantization performance and proposes a non-uniform quantization approach to minimize quantization errors in these sensitive weights.\n     \n     * weights-activation coquantization\n       \n       smoothquant[c770 2022] takes advantage of the similarity in the channel-wise activations of different tokens and performs quantization on both weight and activation using per-channel scaling transforms.\n       \n       rptq recognizes the substantial range differences across different channels, reordering the channels for quantization and integrating them into layer normalization and linear layer weights.\n       \n       olive adopts outlier-victim pair quantization and locally processes outliers.\n       \n       outlier suppression+ builds upon outlier suppression, discovering that harmful outliers exhibit an asymmetric distribution mainly concentrated in specific channels.\n       considering the asymmetry of outliers and quantization errors from the weights of the next layer, this approach performs channel-level translation and scaling operations.\n       \n       qllm addresses the issue of activation outliers through an adaptive channel reassembly method and mitigates the information loss caused by quantization using calibration data.\n       \n       llm-fp4 quantizes weights into 4-bit float points, proposes per-channel activation quantization, and reparameters additional scaling factors as exponential biases of weights.\n       \n       zeroquant combines layer-wise kd and optimized quantization support to achieve 8-bit quantization.\n       \n       flexround updates the quantization scale of weights and activations by minimizing the error between the quantized values and the full-precision values.\n       \n       atom significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization.\n\n * low-rank decomposition (lord): tensorgpt, losparse.\n\n\n# 3. resource-efficient systems\n\n\n# 3.1 distributed training\n\n * resilience: checkpointing, redundant computation.\n\n * parallelism: data, model, and sequence parallelism.\n\n * communication: compression, overlapping with computation.\n\n * storage: offloading, heterogeneous gpus.\n   zero-offload offloads data and computations to cpu to train large models on a single gpu.\n   flashneuron[c52 2021] offloads selective data to the ssd for higher throughput.\n\n * moe optimization: optimizes the dynamism-related mechanisms, parallelism, and communication in moe training. megablocks[c74 2022] leverages sparse primitives to handle dynamic routing and load-imbalanced computation.\n   \n   flexmoe focuses on the dynamic expert management and device placement problem.\n   \n   tutel designs dynamic adaptive parallelism and pipelining strategies.\n\n\n# 3.2 hardware-aware optimizations\n\n * edgebert: latency-aware energy optimization.\n * flightllm: fpga-based llm inference.\n * spatten: sparse attention with cascade token pruning.\n * a3[c227 2020]\n\n\n# 3.3 serving on cloud\n\n * inference accelerating:\n   \n   * flashattention, flashattention-2\n   * flash-decoding, flashdecoding++\n   * deepspeed-inference\n   * bytetransformer\n   * google palm\n   \n   batching and scheduling\n   \n   orca proposes selective batching and iteration-level scheduling to batch requests of different lengths at the granularity of iterations to increase the maximum batch size.\n   \n   flexgen proposes a request scheduling algorithm to mitigate the impact of offloading on the performance of latencyinsensitive fm serving in a single gpu.\n   \n   fastserve proposes an iteration-level preemptive scheduling and proactive kv cache swapping to mitigate the impact of head-of-line blocking on the performance of distributed fm serving.\n   \n   sarathi and deepspeed-fastgen split the computation of the prefill phase into small chunks and schedule these chunks with the decoding phase to mitigate the impact of the prefill phase on the performance of large fms serving.\n   \n   splitwise splits the prefill phase and the decoding phase onto different machines according to their different computation and memory requirements.\n   \n   sarathi-serve introduces a chunked-prefills scheduler which splits a prefill request into near equal sized chunks and creates stall-free schedules that adds new requests in a batch without pausing ongoing decodes.\n   \n   dlora dynamically merges/unmerges adapters with the base model and migrating requests/adapters between worker replicas, significantly improving the serving throughput.\n\n * memory saving\n   \n   * deepspeed-inference and flexgen offload activations or model parameters to the dram or nvme memories\n   * kv cache managing\n     * vllm adopts block-level on-demand memory allocation mechanism.\n     * s-lora extends this idea to unified paging to manage multiple lora adapters at the same time.\n\n * emerging platforms: spot instances, heterogeneous gpus.\n\n\n# 3.4 serving on edge\n\n * edge-cloud collaboration\n   \n   edgefm queries and adapts the large fms to the specific edge models with customized knowledge and architectures so that the dynamic edge model can ensure both low latency and close accuracy to the original large fms.\n\n * on-device moe\n   \n   edgemoe identifies the problem that experts have to be dynamically loaded into memory during inference. to tackle this issue, this approach proposes expert-wise bit-width adaptation to reduce the size of expert parameters with acceptable accuracy loss, saving parameters’ loading time.\n   \n   pc-moe is based on a crucial observation that expert activations are subject to temporal locality. based on this observation, pc-moe proposes parameter committee, which intelligently maintains a subset of crucial experts in use to reduce resource consumption.\n   \n   [c21 2024] mobile foundation model as firmware\n\n * memory optimization: llmcad, powerinfer.\n   \n   llmcad utilizes speculative decoding, which can offload most workloads to a smaller memory-resident draft model.\n   \n   powerinfer relies on large fms runtime sparsity (i.e., only hot neurons are consistently activated across inputs). powerinfer pre-loads hot-activated neurons onto the gpu for fast access, whereas cold-activated neurons are computed on the cpu, thus significantly reducing gpu memory demands and cpugpu data transfers.\n\n * i/o optimization: sti, llm in a flash. sti [c16] identifies that loading parameters time is highly longer than computation time. to address this problem, sti proposes dynamically adapting weights bit-width during the loading procedure according to parameters importance, minimizing loading overhead under maximum inference accuracy.\n\n * kernel optimization: integer-based edge kernels.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Summery of Inner Workings of LLM",frontmatter:{title:"Summery of Inner Workings of LLM",date:"2025-01-26T23:32:49.000Z",permalink:"/pages/dc7046/",tags:[null]},regularPath:"/05.llm/11.inner_working.html",relativePath:"05.llm/11.inner_working.md",key:"v-a65b4fb6",path:"/pages/dc7046/",headers:[{level:2,title:"1. Attention Block",slug:"_1-attention-block",normalizedTitle:"1. attention block",charIndex:462},{level:3,title:"1.1 Attention Heads with Interpretable Attention Weights Patterns",slug:"_1-1-attention-heads-with-interpretable-attention-weights-patterns",normalizedTitle:"1.1 attention heads with interpretable attention weights patterns",charIndex:1173},{level:3,title:"1.2 Attention Heads with Interpretable QK and OV Circuits",slug:"_1-2-attention-heads-with-interpretable-qk-and-ov-circuits",normalizedTitle:"1.2 attention heads with interpretable qk and ov circuits",charIndex:2013},{level:3,title:"1.3 Other Noteworthy Attention Properties",slug:"_1-3-other-noteworthy-attention-properties",normalizedTitle:"1.3 other noteworthy attention properties",charIndex:3003},{level:2,title:"2. Feedforward Network Block",slug:"_2-feedforward-network-block",normalizedTitle:"2. feedforward network block",charIndex:482},{level:3,title:"2.1 Neuron's Input Behavior",slug:"_2-1-neuron-s-input-behavior",normalizedTitle:"2.1 neuron's input behavior",charIndex:3744},{level:3,title:"2.2 Neuron's Output Behavior",slug:"_2-2-neuron-s-output-behavior",normalizedTitle:"2.2 neuron's output behavior",charIndex:4150},{level:3,title:"2.3 Polysemantic Neurons",slug:"_2-3-polysemantic-neurons",normalizedTitle:"2.3 polysemantic neurons",charIndex:4686},{level:3,title:"2.4 Universality of Neurons",slug:"_2-4-universality-of-neurons",normalizedTitle:"2.4 universality of neurons",charIndex:5007},{level:2,title:"3. Residual Stream",slug:"_3-residual-stream",normalizedTitle:"3. residual stream",charIndex:512},{level:3,title:"3.1 Information Flow in the Residual Stream",slug:"_3-1-information-flow-in-the-residual-stream",normalizedTitle:"3.1 information flow in the residual stream",charIndex:5542},{level:3,title:"3.2 Outlier Dimensions",slug:"_3-2-outlier-dimensions",normalizedTitle:"3.2 outlier dimensions",charIndex:5976},{level:3,title:"3.3 Features in the Residual Stream",slug:"_3-3-features-in-the-residual-stream",normalizedTitle:"3.3 features in the residual stream",charIndex:6378},{level:2,title:"4. Emergent Multi-component Behavior",slug:"_4-emergent-multi-component-behavior",normalizedTitle:"4. emergent multi-component behavior",charIndex:532},{level:3,title:"4.1 Evidence of Multi-component Behavior",slug:"_4-1-evidence-of-multi-component-behavior",normalizedTitle:"4.1 evidence of multi-component behavior",charIndex:7006},{level:3,title:"4.2 Circuits Analysis",slug:"_4-2-circuits-analysis",normalizedTitle:"4.2 circuits analysis",charIndex:7529},{level:3,title:"4.3 Generality of Circuits",slug:"_4-3-generality-of-circuits",normalizedTitle:"4.3 generality of circuits",charIndex:8126},{level:2,title:"5. Factuality and Hallucinations in Model Predictions",slug:"_5-factuality-and-hallucinations-in-model-predictions",normalizedTitle:"5. factuality and hallucinations in model predictions",charIndex:570},{level:3,title:"5.1 Intrinsic Views on Hallucinatory Behavior",slug:"_5-1-intrinsic-views-on-hallucinatory-behavior",normalizedTitle:"5.1 intrinsic views on hallucinatory behavior",charIndex:8862},{level:3,title:"5.2 Recall of Factual Associations",slug:"_5-2-recall-of-factual-associations",normalizedTitle:"5.2 recall of factual associations",charIndex:9186},{level:3,title:"5.3 Factuality Issues and Model Editing",slug:"_5-3-factuality-issues-and-model-editing",normalizedTitle:"5.3 factuality issues and model editing",charIndex:9757},{level:2,title:"Conclusion",slug:"conclusion",normalizedTitle:"conclusion",charIndex:10092},{level:2,title:"[7 2024] Interpreting Attention Layer Outputs with Sparse Autoencoders",slug:"_7-2024-interpreting-attention-layer-outputs-with-sparse-autoencoders",normalizedTitle:"[7 2024] interpreting attention layer outputs with sparse autoencoders",charIndex:11024}],headersStr:"1. Attention Block 1.1 Attention Heads with Interpretable Attention Weights Patterns 1.2 Attention Heads with Interpretable QK and OV Circuits 1.3 Other Noteworthy Attention Properties 2. Feedforward Network Block 2.1 Neuron's Input Behavior 2.2 Neuron's Output Behavior 2.3 Polysemantic Neurons 2.4 Universality of Neurons 3. Residual Stream 3.1 Information Flow in the Residual Stream 3.2 Outlier Dimensions 3.3 Features in the Residual Stream 4. Emergent Multi-component Behavior 4.1 Evidence of Multi-component Behavior 4.2 Circuits Analysis 4.3 Generality of Circuits 5. Factuality and Hallucinations in Model Predictions 5.1 Intrinsic Views on Hallucinatory Behavior 5.2 Recall of Factual Associations 5.3 Factuality Issues and Model Editing Conclusion [7 2024] Interpreting Attention Layer Outputs with Sparse Autoencoders",content:'# Discovered Inner Behaviors of Transformer-based Language Models\n\nIn this blog, we will explore the discovered inner behaviors of Transformer-based language models (LMs) as outlined in the paper 👍 👍 "A Primer on the Inner Workings of Transformer-based Language Models". The paper provides a comprehensive overview of the internal mechanisms that enable these models to perform complex language tasks. We will break down the findings into five key sections:\n\n 1. Attention Block\n 2. Feedforward Network Block\n 3. Residual Stream\n 4. Emergent Multi-component Behavior\n 5. Factuality and Hallucinations in Model Predictions\n\nLet’s dive into each of these sections to understand how these components contribute to the overall functioning of Transformer-based LMs.\n\n----------------------------------------\n\n\n# 1. Attention Block\n\nThe attention mechanism is a cornerstone of Transformer models, allowing them to contextualize token representations at each layer. The attention block consists of multiple attention heads, each responsible for attending to different parts of the input sequence. The paper categorizes the behaviors of attention heads into two main groups:\n\n\n# 1.1 Attention Heads with Interpretable Attention Weights Patterns\n\n * Positional Heads: These heads attend to specific positions relative to the current token, such as the previous or next token. For example, previous token heads are crucial for copying information from the previous token to the current position, which is essential for tasks like name concatenation.\n * Subword Joiner Heads: These heads focus on subwords that belong to the same word, helping the model understand word-level structures.\n * Syntactic Heads: These heads attend to tokens with specific syntactic roles, such as subjects or objects, and are crucial for understanding grammatical relationships.\n * Duplicate Token Heads: These heads attend to previous occurrences of the same token, which is useful for tasks like identifying repeated names in a context.\n\n\n# 1.2 Attention Heads with Interpretable QK and OV Circuits\n\n * Copying Heads: These heads have OV (output-value) circuits that exhibit copying behavior, meaning they can replicate information from one part of the sequence to another.\n * Induction Heads: These heads are responsible for completing patterns. For example, given a sequence like "A B ... A", the model predicts "B". This mechanism involves two heads: a previous token head that writes information into the residual stream, and an induction head that reads this information to complete the pattern.\n * Copy Suppression Heads: These heads reduce the logit score of a token if it appears in the context and is being confidently predicted. This mechanism improves model calibration by preventing naive copying.\n * Successor Heads: These heads predict the next element in an ordinal sequence (e.g., "Monday" → "Tuesday"). They rely on the output of the first feedforward network (FFN) block, which encodes a numerical structure.\n\n\n# 1.3 Other Noteworthy Attention Properties\n\n * Domain Specialization: Some heads are specialized for specific domains, such as non-English languages or coding sequences.\n * Attention Sinks: Certain heads attend to special tokens (e.g., BOS or punctuation) when their specialized function is not applicable. This behavior is crucial for streaming generation and model performance.\n\n----------------------------------------\n\n\n# 2. Feedforward Network Block\n\nThe feedforward network (FFN) block is another critical component of Transformer models. It consists of two learnable weight matrices and an element-wise non-linear activation function. The FFN block has been studied extensively, with a focus on the behavior of individual neurons.\n\n\n# 2.1 Neuron\'s Input Behavior\n\n * Position Ranges: Some neurons fire exclusively on specific position ranges within the input sequence.\n * Skill Neurons: These neurons activate based on the task of the input prompt, such as detecting whether the input is Python code or French text.\n * Concept-Specific Neurons: These neurons respond to specific concepts, such as grammatical features or semantic roles.\n\n\n# 2.2 Neuron\'s Output Behavior\n\n * Knowledge Neurons: These neurons are responsible for predicting factual information, such as the capital of a country.\n * Linguistically Acceptable Predictions: Some neurons ensure that the model\'s predictions are grammatically correct, such as predicting the correct verb number based on the subject.\n * Token Frequency Neurons: These neurons adjust the logits of tokens based on their frequency in the training data, shifting the output distribution towards or away from the unigram distribution.\n\n\n# 2.3 Polysemantic Neurons\n\n * N-gram Detectors: Many neurons in early layers specialize in detecting n-grams, but they often fire on a large number of unrelated n-grams, indicating polysemanticity.\n * Dead Neurons: Some neurons in models like OPT remain inactive (zero activation) due to the ReLU activation function.\n\n\n# 2.4 Universality of Neurons\n\n * Universal Neurons: Across different models, a small subset of neurons (1-5%) activate on the same inputs. These include alphabet neurons, previous token neurons, and entropy neurons, which modulate the model\'s uncertainty over the next token prediction.\n\n----------------------------------------\n\n\n# 3. Residual Stream\n\nThe residual stream is the main communication channel in a Transformer model. It carries information from one layer to the next, with each layer adding its updates to the stream.\n\n\n# 3.1 Information Flow in the Residual Stream\n\n * Direct Path: The direct path from the input embedding to the unembedding matrix mainly models bigram statistics.\n * Memory Management: Some components, such as attention heads and FFN neurons, remove information from the residual stream to manage memory. For example, certain neurons write vectors in the opposite direction of what they read, effectively canceling out information.\n\n\n# 3.2 Outlier Dimensions\n\n * Rogue Dimensions: These are dimensions in the residual stream with unusually large magnitudes. They are associated with the generation of anisotropic representations, where the residual stream states of random tokens tend to point in the same direction. Ablating these dimensions significantly decreases model performance, suggesting they encode task-specific knowledge.\n\n\n# 3.3 Features in the Residual Stream\n\n * Sparse Autoencoders (SAEs): SAEs have been used to identify interpretable features in the residual stream. These features include local context features, partition features (which promote or suppress specific sets of tokens), and suppression features (which reduce the likelihood of certain tokens).\n\n----------------------------------------\n\n\n# 4. Emergent Multi-component Behavior\n\nTransformer models achieve their remarkable performance through the interaction of multiple components. The paper highlights several examples of emergent behaviors that arise from these interactions.\n\n\n# 4.1 Evidence of Multi-component Behavior\n\n * Induction Mechanism: This mechanism involves two attention heads working together to complete patterns. A previous token head writes information into the residual stream, and an induction head reads this information to predict the next token.\n * Function Vectors: Multiple attention heads create function vectors that describe a task when given in-context examples. Intervening in the residual stream with these vectors can produce outputs that align with the encoded task.\n\n\n# 4.2 Circuits Analysis\n\n * IOI Circuit: In the Indirect Object Identification (IOI) task, the model identifies the correct name (e.g., "Mary") in a sentence like "When Mary and John went to the store, John gave a drink to _____". The circuit involves duplicate token heads, name mover heads, and negative mover heads (which suppress incorrect predictions).\n * Greater-than Circuit: In the greater-than task, the model predicts a year greater than a given year (e.g., "1814" → "1815"). The circuit involves attention heads that attend to the initial date and FFNs that compute the correct year.\n\n\n# 4.3 Generality of Circuits\n\n * Fine-tuning: The functionality of circuit components remains consistent after fine-tuning, suggesting that fine-tuning improves the encoding of task-relevant information rather than rearranging the circuit.\n * Grokking: The sudden emergence of generalization capabilities in models (known as grokking) is linked to the formation of sparse circuits that replace dense, memorizing sub-networks.\n\n----------------------------------------\n\n\n# 5. Factuality and Hallucinations in Model Predictions\n\nOne of the challenges with large language models is their tendency to generate factually incorrect or nonsensical outputs (hallucinations). The paper explores the internal mechanisms behind these behaviors.\n\n\n# 5.1 Intrinsic Views on Hallucinatory Behavior\n\n * Probing for Truthfulness: Probes trained on middle and last layers\' representations can predict the truthfulness of model outputs.\n * Truthfulness Directions: Linear interventions in the direction of "truthfulness" can enhance the factual accuracy of model predictions.\n\n\n# 5.2 Recall of Factual Associations\n\n * Factual Recall Circuit: The model recalls factual information through a multi-step process. Early-middle FFNs add information about the subject to the residual stream, while later attention heads extract the correct attribute (e.g., the capital of a country).\n * Additive Mechanism: Attention heads\' OV circuits decode attributes by combining information from the subject and relation. Some heads are subject heads (independent of the relation), relation heads (independent of the subject), and mixed heads (dependent on both).\n\n\n# 5.3 Factuality Issues and Model Editing\n\n * Model Editing: Techniques like causal interventions and knowledge neuron localization have been used to edit factual associations in models. However, these approaches face challenges like catastrophic forgetting and downstream performance loss.\n\n----------------------------------------\n\n\n# Conclusion\n\nThe inner workings of Transformer-based language models are complex and multifaceted. By understanding the behaviors of individual components like attention heads, FFN neurons, and the residual stream, we can gain insights into how these models process and generate language. Moreover, the emergent behaviors that arise from the interaction of these components highlight the importance of studying models as a whole, rather than focusing on individual parts.\n\nAs research in this area continues, we can expect to uncover even more fascinating details about how these models work, paving the way for more interpretable, reliable, and safe AI systems.\n\n----------------------------------------\n\nThis blog is based on the paper "A Primer on the Inner Workings of Transformer-based Language Models" by Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R. Costa-jussà.\n\n----------------------------------------\n\n\n# [7 2024] Interpreting Attention Layer Outputs with Sparse Autoencoders\n\nThe paper provides a layer-wise understanding of how attention outputs evolve in terms of feature utilization:\n\n * Early Layers (0-3):\n   * Features:\n     * Focus on simple syntactic patterns (e.g., token pairs, local relationships).\n     * Begin building short-range context features.\n   * Utilization:\n     * Early heads primarily focus on short-range interactions and token-to-token dependencies.\n * Middle Layers (4-9):\n   * Features:\n     * Capture more abstract, semantic patterns (e.g., grammatical and reasoning constructs, topic tracking).\n     * Generate long-range context features and induction features.\n   * Utilization:\n     * Middle heads integrate global information and start building context-aware features (e.g., reasoning or induction patterns).\n * Late Layers (10-11):\n   * Features:\n     * Refine and finalize features for specific tasks like grammatical adjustments, long-range predictions, and sequence completions.\n   * Utilization:\n     * Late heads mostly adjust or finalize token choices using refined long-range context features.\n\n\n\nThe study also provides insights into how these features are distributed and utilized across layers and attention heads:\n\n1. Long-Range Context Features\n\n * Function : Capture information that spans long distances in the text (e.g., maintaining the topic or theme of a paragraph).\n\n * Source : Typically generated in middle to late layers .\n   \n   * Middle layers begin integrating broader semantic and contextual information.\n   \n   * Late layers refine the contextual understanding for tasks like logical reasoning or high-level decision-making.\n\n * Head Specialization : Some heads specialize in aggregating context from far-away tokens, while others focus on more localized interactions.\n\n2. Short-Range Context Features\n\n * Function : Focus on localized relationships, such as syntactic dependencies (e.g., a word\'s immediate neighbors).\n\n * Source : Dominantly found in early and middle layers .\n   \n   * Early layers handle low-level syntactic features like token pairs and adjacent word dependencies.\n   \n   * Some middle-layer heads expand on short-range features by integrating grammatical constructs.\n\n * Head Specialization : Specific attention heads in early layers are responsible for capturing these short-range patterns.\n\n3. Induction Features\n\n * Function : Capture patterns for in-context learning , such as recognizing repeated prefixes in sequences (e.g., completing "...ABC...AB" with "C").\n\n * Source : Strongly tied to specific induction heads in attention layers.\n   \n   * These heads are often polysemantic but include roles specialized for induction tasks.\n   \n   * Example: In GPT-2 Small, the study identifies two layer 5 heads (5.1 and 5.5) that specialize in long-prefix and short-prefix induction , respectively.\n\n * Head Specialization :\n   \n   * Induction features are directly associated with attention heads that attend to patterns of repeated tokens.',normalizedContent:'# discovered inner behaviors of transformer-based language models\n\nin this blog, we will explore the discovered inner behaviors of transformer-based language models (lms) as outlined in the paper 👍 👍 "a primer on the inner workings of transformer-based language models". the paper provides a comprehensive overview of the internal mechanisms that enable these models to perform complex language tasks. we will break down the findings into five key sections:\n\n 1. attention block\n 2. feedforward network block\n 3. residual stream\n 4. emergent multi-component behavior\n 5. factuality and hallucinations in model predictions\n\nlet’s dive into each of these sections to understand how these components contribute to the overall functioning of transformer-based lms.\n\n----------------------------------------\n\n\n# 1. attention block\n\nthe attention mechanism is a cornerstone of transformer models, allowing them to contextualize token representations at each layer. the attention block consists of multiple attention heads, each responsible for attending to different parts of the input sequence. the paper categorizes the behaviors of attention heads into two main groups:\n\n\n# 1.1 attention heads with interpretable attention weights patterns\n\n * positional heads: these heads attend to specific positions relative to the current token, such as the previous or next token. for example, previous token heads are crucial for copying information from the previous token to the current position, which is essential for tasks like name concatenation.\n * subword joiner heads: these heads focus on subwords that belong to the same word, helping the model understand word-level structures.\n * syntactic heads: these heads attend to tokens with specific syntactic roles, such as subjects or objects, and are crucial for understanding grammatical relationships.\n * duplicate token heads: these heads attend to previous occurrences of the same token, which is useful for tasks like identifying repeated names in a context.\n\n\n# 1.2 attention heads with interpretable qk and ov circuits\n\n * copying heads: these heads have ov (output-value) circuits that exhibit copying behavior, meaning they can replicate information from one part of the sequence to another.\n * induction heads: these heads are responsible for completing patterns. for example, given a sequence like "a b ... a", the model predicts "b". this mechanism involves two heads: a previous token head that writes information into the residual stream, and an induction head that reads this information to complete the pattern.\n * copy suppression heads: these heads reduce the logit score of a token if it appears in the context and is being confidently predicted. this mechanism improves model calibration by preventing naive copying.\n * successor heads: these heads predict the next element in an ordinal sequence (e.g., "monday" → "tuesday"). they rely on the output of the first feedforward network (ffn) block, which encodes a numerical structure.\n\n\n# 1.3 other noteworthy attention properties\n\n * domain specialization: some heads are specialized for specific domains, such as non-english languages or coding sequences.\n * attention sinks: certain heads attend to special tokens (e.g., bos or punctuation) when their specialized function is not applicable. this behavior is crucial for streaming generation and model performance.\n\n----------------------------------------\n\n\n# 2. feedforward network block\n\nthe feedforward network (ffn) block is another critical component of transformer models. it consists of two learnable weight matrices and an element-wise non-linear activation function. the ffn block has been studied extensively, with a focus on the behavior of individual neurons.\n\n\n# 2.1 neuron\'s input behavior\n\n * position ranges: some neurons fire exclusively on specific position ranges within the input sequence.\n * skill neurons: these neurons activate based on the task of the input prompt, such as detecting whether the input is python code or french text.\n * concept-specific neurons: these neurons respond to specific concepts, such as grammatical features or semantic roles.\n\n\n# 2.2 neuron\'s output behavior\n\n * knowledge neurons: these neurons are responsible for predicting factual information, such as the capital of a country.\n * linguistically acceptable predictions: some neurons ensure that the model\'s predictions are grammatically correct, such as predicting the correct verb number based on the subject.\n * token frequency neurons: these neurons adjust the logits of tokens based on their frequency in the training data, shifting the output distribution towards or away from the unigram distribution.\n\n\n# 2.3 polysemantic neurons\n\n * n-gram detectors: many neurons in early layers specialize in detecting n-grams, but they often fire on a large number of unrelated n-grams, indicating polysemanticity.\n * dead neurons: some neurons in models like opt remain inactive (zero activation) due to the relu activation function.\n\n\n# 2.4 universality of neurons\n\n * universal neurons: across different models, a small subset of neurons (1-5%) activate on the same inputs. these include alphabet neurons, previous token neurons, and entropy neurons, which modulate the model\'s uncertainty over the next token prediction.\n\n----------------------------------------\n\n\n# 3. residual stream\n\nthe residual stream is the main communication channel in a transformer model. it carries information from one layer to the next, with each layer adding its updates to the stream.\n\n\n# 3.1 information flow in the residual stream\n\n * direct path: the direct path from the input embedding to the unembedding matrix mainly models bigram statistics.\n * memory management: some components, such as attention heads and ffn neurons, remove information from the residual stream to manage memory. for example, certain neurons write vectors in the opposite direction of what they read, effectively canceling out information.\n\n\n# 3.2 outlier dimensions\n\n * rogue dimensions: these are dimensions in the residual stream with unusually large magnitudes. they are associated with the generation of anisotropic representations, where the residual stream states of random tokens tend to point in the same direction. ablating these dimensions significantly decreases model performance, suggesting they encode task-specific knowledge.\n\n\n# 3.3 features in the residual stream\n\n * sparse autoencoders (saes): saes have been used to identify interpretable features in the residual stream. these features include local context features, partition features (which promote or suppress specific sets of tokens), and suppression features (which reduce the likelihood of certain tokens).\n\n----------------------------------------\n\n\n# 4. emergent multi-component behavior\n\ntransformer models achieve their remarkable performance through the interaction of multiple components. the paper highlights several examples of emergent behaviors that arise from these interactions.\n\n\n# 4.1 evidence of multi-component behavior\n\n * induction mechanism: this mechanism involves two attention heads working together to complete patterns. a previous token head writes information into the residual stream, and an induction head reads this information to predict the next token.\n * function vectors: multiple attention heads create function vectors that describe a task when given in-context examples. intervening in the residual stream with these vectors can produce outputs that align with the encoded task.\n\n\n# 4.2 circuits analysis\n\n * ioi circuit: in the indirect object identification (ioi) task, the model identifies the correct name (e.g., "mary") in a sentence like "when mary and john went to the store, john gave a drink to _____". the circuit involves duplicate token heads, name mover heads, and negative mover heads (which suppress incorrect predictions).\n * greater-than circuit: in the greater-than task, the model predicts a year greater than a given year (e.g., "1814" → "1815"). the circuit involves attention heads that attend to the initial date and ffns that compute the correct year.\n\n\n# 4.3 generality of circuits\n\n * fine-tuning: the functionality of circuit components remains consistent after fine-tuning, suggesting that fine-tuning improves the encoding of task-relevant information rather than rearranging the circuit.\n * grokking: the sudden emergence of generalization capabilities in models (known as grokking) is linked to the formation of sparse circuits that replace dense, memorizing sub-networks.\n\n----------------------------------------\n\n\n# 5. factuality and hallucinations in model predictions\n\none of the challenges with large language models is their tendency to generate factually incorrect or nonsensical outputs (hallucinations). the paper explores the internal mechanisms behind these behaviors.\n\n\n# 5.1 intrinsic views on hallucinatory behavior\n\n * probing for truthfulness: probes trained on middle and last layers\' representations can predict the truthfulness of model outputs.\n * truthfulness directions: linear interventions in the direction of "truthfulness" can enhance the factual accuracy of model predictions.\n\n\n# 5.2 recall of factual associations\n\n * factual recall circuit: the model recalls factual information through a multi-step process. early-middle ffns add information about the subject to the residual stream, while later attention heads extract the correct attribute (e.g., the capital of a country).\n * additive mechanism: attention heads\' ov circuits decode attributes by combining information from the subject and relation. some heads are subject heads (independent of the relation), relation heads (independent of the subject), and mixed heads (dependent on both).\n\n\n# 5.3 factuality issues and model editing\n\n * model editing: techniques like causal interventions and knowledge neuron localization have been used to edit factual associations in models. however, these approaches face challenges like catastrophic forgetting and downstream performance loss.\n\n----------------------------------------\n\n\n# conclusion\n\nthe inner workings of transformer-based language models are complex and multifaceted. by understanding the behaviors of individual components like attention heads, ffn neurons, and the residual stream, we can gain insights into how these models process and generate language. moreover, the emergent behaviors that arise from the interaction of these components highlight the importance of studying models as a whole, rather than focusing on individual parts.\n\nas research in this area continues, we can expect to uncover even more fascinating details about how these models work, paving the way for more interpretable, reliable, and safe ai systems.\n\n----------------------------------------\n\nthis blog is based on the paper "a primer on the inner workings of transformer-based language models" by javier ferrando, gabriele sarti, arianna bisazza, and marta r. costa-jussa.\n\n----------------------------------------\n\n\n# [7 2024] interpreting attention layer outputs with sparse autoencoders\n\nthe paper provides a layer-wise understanding of how attention outputs evolve in terms of feature utilization:\n\n * early layers (0-3):\n   * features:\n     * focus on simple syntactic patterns (e.g., token pairs, local relationships).\n     * begin building short-range context features.\n   * utilization:\n     * early heads primarily focus on short-range interactions and token-to-token dependencies.\n * middle layers (4-9):\n   * features:\n     * capture more abstract, semantic patterns (e.g., grammatical and reasoning constructs, topic tracking).\n     * generate long-range context features and induction features.\n   * utilization:\n     * middle heads integrate global information and start building context-aware features (e.g., reasoning or induction patterns).\n * late layers (10-11):\n   * features:\n     * refine and finalize features for specific tasks like grammatical adjustments, long-range predictions, and sequence completions.\n   * utilization:\n     * late heads mostly adjust or finalize token choices using refined long-range context features.\n\n\n\nthe study also provides insights into how these features are distributed and utilized across layers and attention heads:\n\n1. long-range context features\n\n * function : capture information that spans long distances in the text (e.g., maintaining the topic or theme of a paragraph).\n\n * source : typically generated in middle to late layers .\n   \n   * middle layers begin integrating broader semantic and contextual information.\n   \n   * late layers refine the contextual understanding for tasks like logical reasoning or high-level decision-making.\n\n * head specialization : some heads specialize in aggregating context from far-away tokens, while others focus on more localized interactions.\n\n2. short-range context features\n\n * function : focus on localized relationships, such as syntactic dependencies (e.g., a word\'s immediate neighbors).\n\n * source : dominantly found in early and middle layers .\n   \n   * early layers handle low-level syntactic features like token pairs and adjacent word dependencies.\n   \n   * some middle-layer heads expand on short-range features by integrating grammatical constructs.\n\n * head specialization : specific attention heads in early layers are responsible for capturing these short-range patterns.\n\n3. induction features\n\n * function : capture patterns for in-context learning , such as recognizing repeated prefixes in sequences (e.g., completing "...abc...ab" with "c").\n\n * source : strongly tied to specific induction heads in attention layers.\n   \n   * these heads are often polysemantic but include roles specialized for induction tasks.\n   \n   * example: in gpt-2 small, the study identifies two layer 5 heads (5.1 and 5.5) that specialize in long-prefix and short-prefix induction , respectively.\n\n * head specialization :\n   \n   * induction features are directly associated with attention heads that attend to patterns of repeated tokens.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Estimation of LLM",frontmatter:{title:"Estimation of LLM",date:"2025-01-21T23:32:49.000Z",permalink:"/pages/dc7045/",tags:[null]},regularPath:"/05.llm/10.llm_production.html",relativePath:"05.llm/10.llm_production.md",key:"v-5ef480da",path:"/pages/dc7045/",headers:[{level:2,title:"LLM Training time",slug:"llm-training-time",normalizedTitle:"llm training time",charIndex:2},{level:2,title:"why it is 6 fold?",slug:"why-it-is-6-fold",normalizedTitle:"why it is 6 fold?",charIndex:439}],headersStr:"LLM Training time why it is 6 fold?",content:"# LLM Training time\n\nLlama3 405B\n\nC = 6 * N * D\n\n * C compute\n * N parameter number\n * D token number\n\nC = 6 * 405 * 10^9 * 15*10^12 = 3.6 * 10^25\n\nC/(16KGPU*TFlops)\n\nC/(16 * 1000 * 400 * 10^12) = 97 days\n\nSource\n\nsame as:\n\n\n\nPage125\n\nAlso in\n\n\n\nSource From paper PaLM 2 Technical Report\n\nThis is far different from:\n\n\n\nSource:Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM\n\nStill needs time to check.\n\n\n# why it is 6 fold?\n\nreference\n\n\n\nIn short, for backward pass, the back propagation loss needs to be multiplied by weight to pass get the input-loss for privous layer. It also needs to multiplied by the input of this layer to get the derivative of the weights for updating the weight.\n\nThose two above is multiplication operation, it also needs another two add operation to sum up input-loss for privous layer and update the weigth, one for each.\n\nPaper waiting to be read:\n\n * Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM\n * ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\n * How Does Critical Batch Size Scale in Pre-training?\n * Parallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory Management 👍\n * Comparative Study of Large Language Model Architectures on Frontier\n * Optimizing Distributed Training on Frontier for Large Language Models\n * Deep Optimizer States: Towards Scalable Training of Transformer Models Using Interleaved Offloading\n * The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities\n * Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies\n * Towards Scalable Automated Alignment of LLMs: A Survey\n * MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length\n * Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models\n * [C1731] Training Compute-Optimal Large Language Models 👍\n * [C138] Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers\n * [C1082] Scaling Language Models: Methods, Analysis & Insights from Training Gopher",normalizedContent:"# llm training time\n\nllama3 405b\n\nc = 6 * n * d\n\n * c compute\n * n parameter number\n * d token number\n\nc = 6 * 405 * 10^9 * 15*10^12 = 3.6 * 10^25\n\nc/(16kgpu*tflops)\n\nc/(16 * 1000 * 400 * 10^12) = 97 days\n\nsource\n\nsame as:\n\n\n\npage125\n\nalso in\n\n\n\nsource from paper palm 2 technical report\n\nthis is far different from:\n\n\n\nsource:efficient large-scale language model training on gpu clusters using megatron-lm\n\nstill needs time to check.\n\n\n# why it is 6 fold?\n\nreference\n\n\n\nin short, for backward pass, the back propagation loss needs to be multiplied by weight to pass get the input-loss for privous layer. it also needs to multiplied by the input of this layer to get the derivative of the weights for updating the weight.\n\nthose two above is multiplication operation, it also needs another two add operation to sum up input-loss for privous layer and update the weigth, one for each.\n\npaper waiting to be read:\n\n * efficient large-scale language model training on gpu clusters using megatron-lm\n * zero: memory optimizations toward training trillion parameter models\n * how does critical batch size scale in pre-training?\n * parallel training of pre-trained models via chunk-based dynamic memory management 👍\n * comparative study of large language model architectures on frontier\n * optimizing distributed training on frontier for large language models\n * deep optimizer states: towards scalable training of transformer models using interleaved offloading\n * the ultimate guide to fine-tuning llms from basics to breakthroughs: an exhaustive review of technologies, research, best practices, applied research challenges and opportunities\n * parameter-efficient fine-tuning in large models: a survey of methodologies\n * towards scalable automated alignment of llms: a survey\n * megalodon: efficient llm pretraining and inference with unlimited context length\n * memorization without overfitting: analyzing the training dynamics of large language models\n * [c1731] training compute-optimal large language models 👍\n * [c138] scale efficiently: insights from pre-training and fine-tuning transformers\n * [c1082] scaling language models: methods, analysis & insights from training gopher",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"List of LLM Optimization Techniques",frontmatter:{title:"List of LLM Optimization Techniques",date:"2025-01-26T23:32:49.000Z",permalink:"/pages/dc7047/",tags:[null]},regularPath:"/05.llm/12.llm_opt_list.html",relativePath:"05.llm/12.llm_opt_list.md",key:"v-03195875",path:"/pages/dc7047/",headers:[{level:2,title:"1. Compute Optimizations",slug:"_1-compute-optimizations",normalizedTitle:"1. compute optimizations",charIndex:98},{level:2,title:"2. Memory Optimizations",slug:"_2-memory-optimizations",normalizedTitle:"2. memory optimizations",charIndex:591},{level:2,title:"3. Layer-Specific Optimizations",slug:"_3-layer-specific-optimizations",normalizedTitle:"3. layer-specific optimizations",charIndex:981},{level:2,title:"4. Training-Specific Optimizations",slug:"_4-training-specific-optimizations",normalizedTitle:"4. training-specific optimizations",charIndex:1380},{level:2,title:"5. Algorithmic Optimizations",slug:"_5-algorithmic-optimizations",normalizedTitle:"5. algorithmic optimizations",charIndex:1885},{level:2,title:"6. Data Efficiency Optimizations",slug:"_6-data-efficiency-optimizations",normalizedTitle:"6. data efficiency optimizations",charIndex:2425},{level:2,title:"7. Inference Optimizations",slug:"_7-inference-optimizations",normalizedTitle:"7. inference optimizations",charIndex:2782},{level:2,title:"8. Hardware-Centric Optimizations",slug:"_8-hardware-centric-optimizations",normalizedTitle:"8. hardware-centric optimizations",charIndex:3057},{level:2,title:"9. Cross-Stack & Tools",slug:"_9-cross-stack-tools",normalizedTitle:"9. cross-stack &amp; tools",charIndex:null},{level:2,title:"10. Emerging Frontiers",slug:"_10-emerging-frontiers",normalizedTitle:"10. emerging frontiers",charIndex:3597}],headersStr:"1. Compute Optimizations 2. Memory Optimizations 3. Layer-Specific Optimizations 4. Training-Specific Optimizations 5. Algorithmic Optimizations 6. Data Efficiency Optimizations 7. Inference Optimizations 8. Hardware-Centric Optimizations 9. Cross-Stack & Tools 10. Emerging Frontiers",content:'# Comprehensive List of LLM Optimization Techniques\n\n----------------------------------------\n\n\n# 1. Compute Optimizations\n\n * Mixed-Precision Training\n   FP16/BF16 compute with FP32 master weights.\n * Operator Fusion\n   Combine ops (e.g., layer norm + activation) into single kernels.\n * Distributed Training\n   * 3D Parallelism: Combine data, pipeline, and tensor parallelism (e.g., DeepSpeed).\n   * Hierarchical Parallelism: Optimize for multi-node/multi-pod clusters.\n * FlashAttention\n   IO-aware attention for reduced memory reads/writes.\n\n----------------------------------------\n\n\n# 2. Memory Optimizations\n\n * Activation Checkpointing\n   Recomputation during backward pass.\n * Quantization-Aware Training (QAT)\n   Train with INT8/FP8 precision.\n * Dynamic Memory Allocation\n   Buffer reuse to avoid fragmentation.\n * Low-Rank Gradient Projection (GaLore)\n   NEW Compress gradients via low-rank approximations during training.\n\n----------------------------------------\n\n\n# 3. Layer-Specific Optimizations\n\n * Attention Layers\n   * Low-Rank Attention: Factorize attention matrices (e.g., Linformer).\n   * Dynamic Sparse Attention: Skip "unimportant" token pairs.\n * Feed-Forward Layers\n   * Low-Rank Activations: NEW Decompose activation matrices (e.g., ReLU + SVD).\n * Adaptive Computation\n   Early exiting for "easy" inputs.\n\n----------------------------------------\n\n\n# 4. Training-Specific Optimizations\n\n * Optimizer Selection\n   * SGD for Fine-Tuning: NEW Better generalization than Adam in some cases.\n   * LAMB/Novograd: Adaptive optimizers for large batches.\n   * 8-bit Adam: Reduced memory via quantized states.\n * Gradient Management\n   * Gradient Clipping: NEW Stabilize training with norm thresholds.\n   * Gradient Accumulation: Micro-batch aggregation.\n * Curriculum Learning\n   Progressively harder training samples.\n\n----------------------------------------\n\n\n# 5. Algorithmic Optimizations\n\n * Low-Rank Approximations\n   * Weight Matrices: LoRA for parameter-efficient tuning.\n   * Activations/Gradients: NEW Factorized representations (e.g., GaLore).\n * Sparsity Techniques\n   * Dynamic Masking: NEW On-the-fly pruning during inference.\n   * Activation Sparsity: Skip computations near-zero values.\n * Architecture Innovations\n   * State Space Models: Mamba for linear-time sequence processing.\n   * Retention Networks: RetNet\'s parallelizable decoding.\n\n----------------------------------------\n\n\n# 6. Data Efficiency Optimizations\n\nNEW SECTION\n\n * Efficient Tokenization\n   * Byte-level BPE (e.g., Llama) vs. WordPiece.\n   * Vocabulary pruning.\n * Data Pipeline Optimization\n   * Overlap preprocessing with compute.\n   * On-the-fly augmentation.\n * Dataset Distillation\n   Train on synthetic "core" datasets.\n\n----------------------------------------\n\n\n# 7. Inference Optimizations\n\n * Speculative Decoding\n   Draft-then-verify with small models.\n * KV Cache Quantization\n   NEW INT4 caching of attention key/value pairs.\n * SliceGPT\n   NEW Post-training sparsity via matrix slicing.\n\n----------------------------------------\n\n\n# 8. Hardware-Centric Optimizations\n\n * Energy Efficiency\n   NEW Metrics like FLOPs/watt.\n * Edge Device Adaptation\n   NEW Kernel optimizations for ARM/NPU.\n\n----------------------------------------\n\n\n# 9. Cross-Stack & Tools\n\n * Compiler Optimizations\n   * MLIR-Based Frameworks: IREE/CUDA Graphs.\n   * Kernel Auto-Tuning: Auto-schedule ops (e.g., Triton).\n * Parameter-Efficient Fine-Tuning\n   * QLoRA: NEW Quantized LoRA for memory reduction.\n   * DoRA: Weight-Decomposed Low-Rank Adaptation.\n\n----------------------------------------\n\n\n# 10. Emerging Frontiers\n\nEXPANDED\n\n * Hybrid Symbolic-Neural\n   Combine LLMs with rule-based systems.\n * Differentiable Data Storage\n   Learn compressed dataset representations.\n * Neuromorphic Computing\n   Explore spiking neural networks for LLMs.',normalizedContent:'# comprehensive list of llm optimization techniques\n\n----------------------------------------\n\n\n# 1. compute optimizations\n\n * mixed-precision training\n   fp16/bf16 compute with fp32 master weights.\n * operator fusion\n   combine ops (e.g., layer norm + activation) into single kernels.\n * distributed training\n   * 3d parallelism: combine data, pipeline, and tensor parallelism (e.g., deepspeed).\n   * hierarchical parallelism: optimize for multi-node/multi-pod clusters.\n * flashattention\n   io-aware attention for reduced memory reads/writes.\n\n----------------------------------------\n\n\n# 2. memory optimizations\n\n * activation checkpointing\n   recomputation during backward pass.\n * quantization-aware training (qat)\n   train with int8/fp8 precision.\n * dynamic memory allocation\n   buffer reuse to avoid fragmentation.\n * low-rank gradient projection (galore)\n   new compress gradients via low-rank approximations during training.\n\n----------------------------------------\n\n\n# 3. layer-specific optimizations\n\n * attention layers\n   * low-rank attention: factorize attention matrices (e.g., linformer).\n   * dynamic sparse attention: skip "unimportant" token pairs.\n * feed-forward layers\n   * low-rank activations: new decompose activation matrices (e.g., relu + svd).\n * adaptive computation\n   early exiting for "easy" inputs.\n\n----------------------------------------\n\n\n# 4. training-specific optimizations\n\n * optimizer selection\n   * sgd for fine-tuning: new better generalization than adam in some cases.\n   * lamb/novograd: adaptive optimizers for large batches.\n   * 8-bit adam: reduced memory via quantized states.\n * gradient management\n   * gradient clipping: new stabilize training with norm thresholds.\n   * gradient accumulation: micro-batch aggregation.\n * curriculum learning\n   progressively harder training samples.\n\n----------------------------------------\n\n\n# 5. algorithmic optimizations\n\n * low-rank approximations\n   * weight matrices: lora for parameter-efficient tuning.\n   * activations/gradients: new factorized representations (e.g., galore).\n * sparsity techniques\n   * dynamic masking: new on-the-fly pruning during inference.\n   * activation sparsity: skip computations near-zero values.\n * architecture innovations\n   * state space models: mamba for linear-time sequence processing.\n   * retention networks: retnet\'s parallelizable decoding.\n\n----------------------------------------\n\n\n# 6. data efficiency optimizations\n\nnew section\n\n * efficient tokenization\n   * byte-level bpe (e.g., llama) vs. wordpiece.\n   * vocabulary pruning.\n * data pipeline optimization\n   * overlap preprocessing with compute.\n   * on-the-fly augmentation.\n * dataset distillation\n   train on synthetic "core" datasets.\n\n----------------------------------------\n\n\n# 7. inference optimizations\n\n * speculative decoding\n   draft-then-verify with small models.\n * kv cache quantization\n   new int4 caching of attention key/value pairs.\n * slicegpt\n   new post-training sparsity via matrix slicing.\n\n----------------------------------------\n\n\n# 8. hardware-centric optimizations\n\n * energy efficiency\n   new metrics like flops/watt.\n * edge device adaptation\n   new kernel optimizations for arm/npu.\n\n----------------------------------------\n\n\n# 9. cross-stack & tools\n\n * compiler optimizations\n   * mlir-based frameworks: iree/cuda graphs.\n   * kernel auto-tuning: auto-schedule ops (e.g., triton).\n * parameter-efficient fine-tuning\n   * qlora: new quantized lora for memory reduction.\n   * dora: weight-decomposed low-rank adaptation.\n\n----------------------------------------\n\n\n# 10. emerging frontiers\n\nexpanded\n\n * hybrid symbolic-neural\n   combine llms with rule-based systems.\n * differentiable data storage\n   learn compressed dataset representations.\n * neuromorphic computing\n   explore spiking neural networks for llms.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Memory Optimizations in LLM",frontmatter:{title:"Memory Optimizations in LLM",date:"2025-01-26T23:32:49.000Z",permalink:"/pages/dc7048/",tags:[null]},regularPath:"/05.llm/13.llm_mem_opt.html",relativePath:"05.llm/13.llm_mem_opt.md",key:"v-43caabc5",path:"/pages/dc7048/",headers:[{level:2,title:"Memory Optimizations",slug:"memory-optimizations",normalizedTitle:"memory optimizations",charIndex:2},{level:3,title:"[C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources",slug:"c99-y2024-full-parameter-fine-tuning-for-large-language-models-with-limited-resources",normalizedTitle:"[c99 y2024] full parameter fine-tuning for large language models with limited resources",charIndex:389},{level:3,title:"[C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors",slug:"c25-y2024-flora-low-rank-adapters-are-secretly-gradient-compressors",normalizedTitle:"[c25 y2024] flora: low-rank adapters are secretly gradient compressors",charIndex:741},{level:3,title:"[C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",slug:"c42-y2024-galore-memory-efficient-llm-training-by-gradient-low-rank-projection",normalizedTitle:"[c42 y2024] galore: memory-efficient llm training by gradient low-rank projection",charIndex:2296},{level:3,title:"[C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training",slug:"c0-2024-compact-compressed-activations-for-memory-efficient-llm-training",normalizedTitle:"[c0 2024] compact: compressed activations for memory-efficient llm training",charIndex:2971}],headersStr:"Memory Optimizations [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources [C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors [C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection [C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training",content:"# Memory Optimizations\n\n * Activation Checkpointing\n   Recomputation during backward pass.\n * Quantization-Aware Training (QAT)\n   Train with INT8/FP8 precision.\n * Dynamic Memory Allocation\n   Buffer reuse to avoid fragmentation.\n * Low-Rank Gradient Projection (GaLore)\n   NEW Compress gradients via low-rank approximations during training.\n\n----------------------------------------\n\n\n# [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources\n\n * Use SGD instead of Adam for fine-tuning weights.\n * Update layer by layer in backward pass. Traditional Adam will backward probgation all layers and then update weigths.\n * SGD also avoid state memory of ADAM.\n\n\n\n\n\n----------------------------------------\n\n\n# [C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors\n\nThis paper discovers that LORA can be approximated by a random projection.\n\nLORA restricts overall weights update matrices to be low-rank.\n\nFLORA use random projection matrix, which allows high-rank update gradients.\n\n> Our intuition arises from investigating LoRA and observing that a LoRA update is dominated by a random projection, which compresses the gradient into a lower-dimensional space. Our FLORA resamples the random projection and is able to mitigate the low-rank limitation of LoRA. Further, our approach only stores the compressed gradient accumulation and momentum, thus saving the memory usage of optimization states to the sublinear level.\n\nGradident Accumulation:\n\n * Gradient accumulation stores the sum of gradients over multiple batches to simulate a larger effective batch size (helpful when memory limits prevent using large batches).\n * Normally, this requires a memory buffer equal to the model size to store the full gradient matrix.\n\nMomentum\n\n * Momentum smooths gradient updates by keeping an exponentially weighted moving average (EMA) of past gradients.\n * Maintaining momentum for large models requires significant memory since an additional buffer equal to the model size is needed.\n\nFLORA Compression:\n\n * compress gradients accumulation: Applying a random projection matrix A to reduce the dimensionality of the gradients.\n * compress momentum: Using random projection to compress the momentum term M.\n\n----------------------------------------\n\n\n# [C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection\n\n\n\n> Galore: gradient Low-Rank Projection (GaLore), a training strategy that allows fullparameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Key idea is to leverage the slowchanging low-rank structure of the gradient G(m×n) of the weight matrix W, rather than trying to approximate the weight matrix itself as low rank. while the weight matrices are not necessarily low-rank, the gradient indeed becomes low-rank during the training for certain gradient forms and associated network architectures.\n\n----------------------------------------\n\n\n# [C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training\n\n\n\n> By storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters. CompAct saves low-rank compressed activations during the forward pass, instead of the full activation tensors. The resulting gradients are low-rank as well, also reducing the size of optimizer states. As CompAct decompresses the gradients back to full size only for the update step, it compresses a large part of the compute graph, which in turn translates to major memory savings.\n\nCompAct is a logical next step from previous work, moving from low-rank parameters, through compressed low-rank gradients , to compressed activations.\n\n> compared to GaLore, our approach may be viewed as a simple change in the order of operations, applying the compression one step before GaLore does, to the activations rather than to the gradients.\n\n",normalizedContent:"# memory optimizations\n\n * activation checkpointing\n   recomputation during backward pass.\n * quantization-aware training (qat)\n   train with int8/fp8 precision.\n * dynamic memory allocation\n   buffer reuse to avoid fragmentation.\n * low-rank gradient projection (galore)\n   new compress gradients via low-rank approximations during training.\n\n----------------------------------------\n\n\n# [c99 y2024] full parameter fine-tuning for large language models with limited resources\n\n * use sgd instead of adam for fine-tuning weights.\n * update layer by layer in backward pass. traditional adam will backward probgation all layers and then update weigths.\n * sgd also avoid state memory of adam.\n\n\n\n\n\n----------------------------------------\n\n\n# [c25 y2024] flora: low-rank adapters are secretly gradient compressors\n\nthis paper discovers that lora can be approximated by a random projection.\n\nlora restricts overall weights update matrices to be low-rank.\n\nflora use random projection matrix, which allows high-rank update gradients.\n\n> our intuition arises from investigating lora and observing that a lora update is dominated by a random projection, which compresses the gradient into a lower-dimensional space. our flora resamples the random projection and is able to mitigate the low-rank limitation of lora. further, our approach only stores the compressed gradient accumulation and momentum, thus saving the memory usage of optimization states to the sublinear level.\n\ngradident accumulation:\n\n * gradient accumulation stores the sum of gradients over multiple batches to simulate a larger effective batch size (helpful when memory limits prevent using large batches).\n * normally, this requires a memory buffer equal to the model size to store the full gradient matrix.\n\nmomentum\n\n * momentum smooths gradient updates by keeping an exponentially weighted moving average (ema) of past gradients.\n * maintaining momentum for large models requires significant memory since an additional buffer equal to the model size is needed.\n\nflora compression:\n\n * compress gradients accumulation: applying a random projection matrix a to reduce the dimensionality of the gradients.\n * compress momentum: using random projection to compress the momentum term m.\n\n----------------------------------------\n\n\n# [c42 y2024] galore: memory-efficient llm training by gradient low-rank projection\n\n\n\n> galore: gradient low-rank projection (galore), a training strategy that allows fullparameter learning but is more memory-efficient than common low-rank adaptation methods such as lora. key idea is to leverage the slowchanging low-rank structure of the gradient g(m×n) of the weight matrix w, rather than trying to approximate the weight matrix itself as low rank. while the weight matrices are not necessarily low-rank, the gradient indeed becomes low-rank during the training for certain gradient forms and associated network architectures.\n\n----------------------------------------\n\n\n# [c0 2024] compact: compressed activations for memory-efficient llm training\n\n\n\n> by storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters. compact saves low-rank compressed activations during the forward pass, instead of the full activation tensors. the resulting gradients are low-rank as well, also reducing the size of optimizer states. as compact decompresses the gradients back to full size only for the update step, it compresses a large part of the compute graph, which in turn translates to major memory savings.\n\ncompact is a logical next step from previous work, moving from low-rank parameters, through compressed low-rank gradients , to compressed activations.\n\n> compared to galore, our approach may be viewed as a simple change in the order of operations, applying the compression one step before galore does, to the activations rather than to the gradients.\n\n",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Reasoning in LLM",frontmatter:{title:"Reasoning in LLM",date:"2025-01-27T23:32:49.000Z",permalink:"/pages/dc7049/",tags:[null]},regularPath:"/05.llm/14.llm_reasoning.html",relativePath:"05.llm/14.llm_reasoning.md",key:"v-393b6085",path:"/pages/dc7049/",headers:[{level:2,title:"[331] Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",slug:"_331-language-models-dont-always-say-what-they-think-unfaithful-explanations-in-chain-of-thought-prompting",normalizedTitle:"[331] language models dont always say what they think: unfaithful explanations in chain-of-thought prompting",charIndex:4},{level:2,title:"[9831] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",slug:"_9831-chain-of-thought-prompting-elicits-reasoning-in-large-language-models",normalizedTitle:"[9831] chain-of-thought prompting elicits reasoning in large language models",charIndex:117},{level:2,title:"[641] Towards Reasoning in Large Language Models: A Survey",slug:"_641-towards-reasoning-in-large-language-models-a-survey",normalizedTitle:"[641] towards reasoning in large language models: a survey",charIndex:198},{level:3,title:"What is Reasoning?",slug:"what-is-reasoning",normalizedTitle:"what is reasoning?",charIndex:2819},{level:3,title:"Techniques for Enhancing Reasoning in LLMs",slug:"techniques-for-enhancing-reasoning-in-llms",normalizedTitle:"techniques for enhancing reasoning in llms",charIndex:3690},{level:3,title:"Evaluating Reasoning in LLMs",slug:"evaluating-reasoning-in-llms",normalizedTitle:"evaluating reasoning in llms",charIndex:6171},{level:3,title:"Key Findings and Implications",slug:"key-findings-and-implications",normalizedTitle:"key findings and implications",charIndex:6782},{level:3,title:"Open Questions and Future Directions",slug:"open-questions-and-future-directions",normalizedTitle:"open questions and future directions",charIndex:7345},{level:2,title:"4. [2 2025] Self-Training Elicits Concise Reasoning in Large Language Models",slug:"_4-2-2025-self-training-elicits-concise-reasoning-in-large-language-models",normalizedTitle:"4. [2 2025] self-training elicits concise reasoning in large language models",charIndex:258},{level:3,title:"1. Introduction & Background",slug:"_1-introduction-background",normalizedTitle:"1. introduction &amp; background",charIndex:null},{level:3,title:"2. Preliminary Investigation",slug:"_2-preliminary-investigation",normalizedTitle:"2. preliminary investigation",charIndex:8894},{level:3,title:"3. Proposed Methods",slug:"_3-proposed-methods",normalizedTitle:"3. proposed methods",charIndex:9812},{level:3,title:"4. Experimental Setup",slug:"_4-experimental-setup",normalizedTitle:"4. experimental setup",charIndex:10920},{level:3,title:"5. Key Findings & Contributions",slug:"_5-key-findings-contributions",normalizedTitle:"5. key findings &amp; contributions",charIndex:null},{level:3,title:"6. Discussion & Broader Impact",slug:"_6-discussion-broader-impact",normalizedTitle:"6. discussion &amp; broader impact",charIndex:null},{level:3,title:"7. Limitations and Future Directions",slug:"_7-limitations-and-future-directions",normalizedTitle:"7. limitations and future directions",charIndex:12799},{level:3,title:"8. Contributions & Novelty (Summary):",slug:"_8-contributions-novelty-summary",normalizedTitle:"8. contributions &amp; novelty (summary):",charIndex:null},{level:3,title:"9. Practical Implications:",slug:"_9-practical-implications",normalizedTitle:"9. practical implications:",charIndex:13678},{level:3,title:"10. Conclusion",slug:"_10-conclusion",normalizedTitle:"10. conclusion",charIndex:13896}],headersStr:"[331] Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting [9831] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [641] Towards Reasoning in Large Language Models: A Survey What is Reasoning? Techniques for Enhancing Reasoning in LLMs Evaluating Reasoning in LLMs Key Findings and Implications Open Questions and Future Directions 4. [2 2025] Self-Training Elicits Concise Reasoning in Large Language Models 1. Introduction & Background 2. Preliminary Investigation 3. Proposed Methods 4. Experimental Setup 5. Key Findings & Contributions 6. Discussion & Broader Impact 7. Limitations and Future Directions 8. Contributions & Novelty (Summary): 9. Practical Implications: 10. Conclusion",content:' 1. [331] Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting\n 2. [9831] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n 3. [641] Towards Reasoning in Large Language Models: A Survey\n 4. [2 2025] Self-Training Elicits Concise Reasoning in Large Language Models\n\n----------------------------------------\n\n\n# [331] Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting\n\n * Key Findings:\n   * Unfaithful Explanations: CoT explanations can be plausible but systematically unfaithful, failing to reflect the true reasoning process.\n   * Biasing Features: Models are heavily influenced by biasing features (e.g., reordering multiple-choice options), which are not mentioned in explanations.\n   * Accuracy Drop: Biasing models toward incorrect answers leads to a 36% drop in accuracy on BIG-Bench Hard tasks.\n   * Social Bias: Models justify stereotype-aligned answers without acknowledging the influence of social biases.\n   * Counterfactual Simulatability: Models rarely acknowledge biasing features, making explanations systematically unfaithful.\n\nThey instruct llm with bias：\n\n\n\n\n\n----------------------------------------\n\n\n# [9831] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n\nKey findings:\n\n * Chain-of-thought prompting significantly improves the performance of large language models on a variety of reasoning tasks.\n * Chain-of-thought prompting is an emergent property of model scale, meaning that it only provides significant performance gains when used with very large language models (around 100 billion parameters).\n * The improvements from chain-of-thought prompting are robust across different language models, datasets, and annotators\n\nUsage cases:\n\n * Arithmetic reasoning: CoT prompting can help language models solve math word problems that require multiple steps, such as the GSM8K benchmark.\n * Commonsense reasoning: CoT prompting can also improve the performance of language models on tasks that require commonsense reasoning, such as the StrategyQA dataset, which requires models to infer a multi-hop strategy to answer questions.\n * Symbolic reasoning: CoT prompting has also been shown to be effective for symbolic reasoning tasks, such as last letter concatenation, which requires the model to concatenate the last letters of words in a name.\n\n----------------------------------------\n\n\n# [641] Towards Reasoning in Large Language Models: A Survey\n\n\n\nLarge language models (LLMs) have made impressive strides in natural language processing, but their ability to reason remains a hot topic. This blog post delves into the fascinating world of reasoning in LLMs, exploring the techniques, evaluations, and key findings that are shaping this field.\n\n\n# What is Reasoning?\n\nReasoning is the process of using evidence, logic, and past experiences to form conclusions or make decisions. It\'s a fundamental aspect of human intelligence, allowing us to solve problems, think critically, and understand the world around us. There are different types of reasoning, including:\n\n * Deductive reasoning: Drawing a conclusion based on the truth of the premises (e.g., if all mammals have kidneys and all whales are mammals, then all whales have kidneys).\n * Inductive reasoning: Drawing a conclusion based on observations or evidence (e.g., if every winged creature we\'ve seen is a bird, then a new winged creature is likely a bird).\n * Abductive reasoning: Drawing a conclusion based on the best explanation for a set of observations (e.g., if the car won\'t start and there\'s a puddle under it, then the car probably has a leak).\n\n\n# Techniques for Enhancing Reasoning in LLMs\n\nResearchers are constantly developing new techniques to improve or elicit reasoning in LLMs. Some of the most promising methods include:\n\n * Fully supervised fine-tuning: This involves fine-tuning a pre-trained LLM on a dataset containing explicit reasoning examples. For instance, a model could be trained to generate rationales explaining its predictions.\n * Prompting and in-context learning: This approach involves prompting LLMs with a question and a few examples of how to solve similar questions. Chain-of-thought prompting is a popular technique where the examples include intermediate reasoning steps, guiding the LLM to generate its own reasoning process.\n   * Prompting & In-Context Learning: in CoT prompting, ⟨input, output⟩ demonstrations are replaced with ⟨input, chain of thought, output⟩ tripples\n     * manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation\n   * Rationale Engieering： creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by LLMs.\n   * Rationale refinement\n   * complexity-based prompting to create rationales with more reasoning steps. Their experiments show that the performance of LLMs improves with the increased rationale complexity\n   * algorithmic prompting, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations\n   * Rationale exploration: decoding strategy, sampling a divese set of rationale, instead of the greedy one\n   * Rationale verification\n\n\n\n * Hybrid methods: These methods combine techniques like pre-training or fine-tuning LLMs on datasets that include reasoning, along with prompting techniques to elicit reasoning.\n   * LLMs trained on datasets containing scientific and mathematical data can achieve better performance on reasoning tasks like quantitative reasoning problems when using CoT prompting.\n   * bootstrapping & self improving: using LLMs to self-improve their reasoning abilities through a process known as bootstrapping.\n     * Specifically, with CoT prompting, the model first generates initial rationales. And then, the model is finetuned on rationales that lead to correct answers. This process can be repeated, with each iteration resulting in an improved model that can generate better training data.\n\n\n# Evaluating Reasoning in LLMs\n\nEvaluating the reasoning abilities of LLMs is crucial. Researchers use various methods and benchmarks to assess their performance, including:\n\n * End task performance: This involves measuring the accuracy of LLMs on tasks requiring reasoning, such as arithmetic, commonsense, and symbolic reasoning benchmarks.\n * Analysis of reasoning: This approach focuses on directly assessing the reasoning steps taken by LLMs, rather than just the final answer. This can involve analyzing the quality of the generated rationales or using formal metrics to evaluate the reasoning process.\n\n\n# Key Findings and Implications\n\nResearch in reasoning in LLMs has yielded some interesting findings:\n\n * Emergent ability: Reasoning seems to be an emergent ability of LLMs, becoming more pronounced as the models get larger (around 100 billion parameters or more).\n * Chain-of-thought prompting: This technique has been shown to significantly improve the performance of LLMs on various reasoning tasks.\n * Complex reasoning challenges: Despite progress, LLMs still struggle with complex reasoning tasks, suggesting that current benchmarks might be too simple.\n\n\n# Open Questions and Future Directions\n\nThe field of reasoning in LLMs is still evolving, with many open questions and exciting avenues for future research:\n\n * True reasoning or mimicry?: Are LLMs truly capable of reasoning, or are they simply learning to mimic human reasoning through pattern recognition?\n * Improving reasoning capabilities: How can we further enhance the reasoning capabilities of LLMs? This could involve developing new training methods, model architectures, or prompting techniques.\n\nBy addressing these questions and continuing to explore the intricacies of reasoning in LLMs, we can unlock their full potential and pave the way for more intelligent and reliable language-based AI systems.\n\n----------------------------------------\n\n\n# 4. [2 2025] Self-Training Elicits Concise Reasoning in Large Language Models\n\nOur combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy.\n\n\n# 1. Introduction & Background\n\n * Chain-of-Thought (CoT) reasoning significantly boosts LLM performance by breaking complex tasks into intermediate reasoning steps. However, CoT inherently generates redundant tokens that increase inference costs unnecessarily.\n\n * The authors hypothesize that current LLMs inherently possess the latent ability to reason concisely, as evidenced by the presence of shorter correct reasoning paths within their output distributions. Their goal is to explicitly unlock this capability through targeted fine-tuning.\n\n\n\n\n\n\n# 2. Preliminary Investigation\n\nThe authors conducted preliminary studies to support their hypothesis:\n\n# Concise Reasoning Definition:\n\n * Defined as reasoning correctly with fewer output tokens compared to the model’s default (average correct) output length.\n\n# Analysis of Reasoning Length Distribution:\n\n * A kernel density estimation showed significant potential for concise reasoning across various model families (e.g., Llama, DeepSeekMath, Qwen, Gemma).\n * Models frequently generated solutions that were shorter than their default output, suggesting inherent latent potential for conciseness.\n\n# Limitations of Zero-Shot Prompting:\n\n * Popular zero-shot prompts aimed at concise reasoning ("Be Concise," "Fixed Budget," "Estimated Budget," and handcrafted prompts) significantly reduced token count but often compromised accuracy and exhibited inconsistent effects, particularly on math-specialized models.\n\n\n# 3. Proposed Methods\n\nTo reliably elicit concise reasoning without sacrificing accuracy, the authors proposed several fine-tuning methods based on self-training:\n\n# 3.1. Naive Best-of-N (BoN) Sampling:\n\n * Generates multiple reasoning paths (N paths per question) and selects the shortest correct reasoning path for fine-tuning.\n * Effective but sample inefficient as length reduction benefits saturate quickly.\n\n# 3.2. Few-Shot Conditioning for Efficient Reduction:\n\n * Combines few-shot prompting and BoN sampling to enhance concise reasoning.\n * Few-shot exemplars are derived from:\n   * Human annotations (FS-Human)\n   * Proprietary LLMs like GPT-4o (FS-GPT4o)\n   * Self-generated concise examples from the target model itself (FS-Self)\n * Few-shot conditioning significantly improves sample efficiency in eliciting concise reasoning.\n\n# 3.3. Sample Augmentation:\n\n * To improve accuracy retention, they augment the few-shot conditioned data with additional samples generated from naive BoN.\n * Ensures coverage of both easy and difficult questions, preserving accuracy while maintaining conciseness.\n\n\n# 4. Experimental Setup\n\n * Evaluated across two mathematical reasoning datasets : GSM8K and MATH.\n * Models tested include general-purpose (e.g., Llama, Gemma, Qwen2.5) and math-specialized models (e.g., DeepSeekMath, Qwen2.5-Math).\n * Metrics: Accuracy and output length were primary evaluation metrics. Length is measured in tokens.\n\n\n\n\n# 5. Key Findings & Contributions\n\n# 5.1. Main Results\n\n * Naive BoN reduced reasoning length modestly (~12%) without major accuracy loss.\n * Few-shot conditioned methods (especially FS-GPT4o and FS-Human) yielded significant length reductions (~30% average across datasets) while maintaining accuracy.\n\n\n\n * The combined method (FS-GPT4o-BoN ) achieved the greatest reduction in length (average 30%) with minimal accuracy drop.\n * Self-training methods consistently outperformed zero-shot prompting and external fine-tuning baselines in terms of balancing conciseness with accuracy.\n\n# 5.2. In-depth Analysis\n\n * Models adaptively adjusted the length of reasoning paths based on question complexity—shorter paths for easier questions and longer paths for harder ones.\n * Scaling analysis (1B, 3B, and 8B models) showed length reduction consistently improved with larger models.\n * Fine-tuning effectively transferred concise reasoning learned from training to test outputs.\n\n\n# 6. Discussion & Broader Impact\n\n * The default reasoning behavior of current LLMs inherently contains redundancy, likely due to training data and optimization methods not promoting conciseness.\n * The authors suggest that lightweight fine-tuning approaches are sufficient for eliciting concise reasoning, aligning with the "Superficial Alignment Hypothesis" (minimal training can significantly shift model behavior).\n * Emphasized the potential for integrating concise reasoning supervision into standard training pipelines for more efficient LLM inference.\n\n\n# 7. Limitations and Future Directions\n\n * Further exploration of advanced training schemes (reinforcement learning, iterative refinement).\n * Investigating more sophisticated few-shot exemplar selection and prompting strategies.\n * Extending the scalability study beyond 8B parameter models.\n * Generalizing concise reasoning methods to broader, non-mathematical tasks.\n\n\n# 8. Contributions & Novelty (Summary):\n\n * Identified latent concise reasoning capabilities within standard LLMs.\n * Demonstrated ineffectiveness and inconsistency of popular zero-shot concise reasoning methods.\n * Proposed a novel, efficient, and effective fine-tuning approach (few-shot conditioned best-of-N sampling with augmentation) that consistently improves conciseness without significant accuracy degradation.\n * Provided extensive experimental validation across multiple models and datasets.\n\n\n# 9. Practical Implications:\n\n * The authors highlight practical benefits: reduced inference costs, lower latency, and more efficient model deployment in scenarios where reasoning-based inference costs are critical.\n\n\n# 10. Conclusion\n\nThe paper convincingly demonstrates that standard large language models can be fine-tuned to significantly enhance concise reasoning using self-generated concise examples.\n\nThis approach is broadly applicable and beneficial across different LLM architectures and model sizes, offering substantial improvements in inference efficiency without compromising reasoning accuracy.\n\nOverall, this paper contributes significantly to understanding and improving reasoning efficiency in LLMs, providing valuable methodologies for practitioners aiming to optimize inference performance in real-world scenarios.',normalizedContent:' 1. [331] language models dont always say what they think: unfaithful explanations in chain-of-thought prompting\n 2. [9831] chain-of-thought prompting elicits reasoning in large language models\n 3. [641] towards reasoning in large language models: a survey\n 4. [2 2025] self-training elicits concise reasoning in large language models\n\n----------------------------------------\n\n\n# [331] language models dont always say what they think: unfaithful explanations in chain-of-thought prompting\n\n * key findings:\n   * unfaithful explanations: cot explanations can be plausible but systematically unfaithful, failing to reflect the true reasoning process.\n   * biasing features: models are heavily influenced by biasing features (e.g., reordering multiple-choice options), which are not mentioned in explanations.\n   * accuracy drop: biasing models toward incorrect answers leads to a 36% drop in accuracy on big-bench hard tasks.\n   * social bias: models justify stereotype-aligned answers without acknowledging the influence of social biases.\n   * counterfactual simulatability: models rarely acknowledge biasing features, making explanations systematically unfaithful.\n\nthey instruct llm with bias：\n\n\n\n\n\n----------------------------------------\n\n\n# [9831] chain-of-thought prompting elicits reasoning in large language models\n\nkey findings:\n\n * chain-of-thought prompting significantly improves the performance of large language models on a variety of reasoning tasks.\n * chain-of-thought prompting is an emergent property of model scale, meaning that it only provides significant performance gains when used with very large language models (around 100 billion parameters).\n * the improvements from chain-of-thought prompting are robust across different language models, datasets, and annotators\n\nusage cases:\n\n * arithmetic reasoning: cot prompting can help language models solve math word problems that require multiple steps, such as the gsm8k benchmark.\n * commonsense reasoning: cot prompting can also improve the performance of language models on tasks that require commonsense reasoning, such as the strategyqa dataset, which requires models to infer a multi-hop strategy to answer questions.\n * symbolic reasoning: cot prompting has also been shown to be effective for symbolic reasoning tasks, such as last letter concatenation, which requires the model to concatenate the last letters of words in a name.\n\n----------------------------------------\n\n\n# [641] towards reasoning in large language models: a survey\n\n\n\nlarge language models (llms) have made impressive strides in natural language processing, but their ability to reason remains a hot topic. this blog post delves into the fascinating world of reasoning in llms, exploring the techniques, evaluations, and key findings that are shaping this field.\n\n\n# what is reasoning?\n\nreasoning is the process of using evidence, logic, and past experiences to form conclusions or make decisions. it\'s a fundamental aspect of human intelligence, allowing us to solve problems, think critically, and understand the world around us. there are different types of reasoning, including:\n\n * deductive reasoning: drawing a conclusion based on the truth of the premises (e.g., if all mammals have kidneys and all whales are mammals, then all whales have kidneys).\n * inductive reasoning: drawing a conclusion based on observations or evidence (e.g., if every winged creature we\'ve seen is a bird, then a new winged creature is likely a bird).\n * abductive reasoning: drawing a conclusion based on the best explanation for a set of observations (e.g., if the car won\'t start and there\'s a puddle under it, then the car probably has a leak).\n\n\n# techniques for enhancing reasoning in llms\n\nresearchers are constantly developing new techniques to improve or elicit reasoning in llms. some of the most promising methods include:\n\n * fully supervised fine-tuning: this involves fine-tuning a pre-trained llm on a dataset containing explicit reasoning examples. for instance, a model could be trained to generate rationales explaining its predictions.\n * prompting and in-context learning: this approach involves prompting llms with a question and a few examples of how to solve similar questions. chain-of-thought prompting is a popular technique where the examples include intermediate reasoning steps, guiding the llm to generate its own reasoning process.\n   * prompting & in-context learning: in cot prompting, ⟨input, output⟩ demonstrations are replaced with ⟨input, chain of thought, output⟩ tripples\n     * manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation\n   * rationale engieering： creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by llms.\n   * rationale refinement\n   * complexity-based prompting to create rationales with more reasoning steps. their experiments show that the performance of llms improves with the increased rationale complexity\n   * algorithmic prompting, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations\n   * rationale exploration: decoding strategy, sampling a divese set of rationale, instead of the greedy one\n   * rationale verification\n\n\n\n * hybrid methods: these methods combine techniques like pre-training or fine-tuning llms on datasets that include reasoning, along with prompting techniques to elicit reasoning.\n   * llms trained on datasets containing scientific and mathematical data can achieve better performance on reasoning tasks like quantitative reasoning problems when using cot prompting.\n   * bootstrapping & self improving: using llms to self-improve their reasoning abilities through a process known as bootstrapping.\n     * specifically, with cot prompting, the model first generates initial rationales. and then, the model is finetuned on rationales that lead to correct answers. this process can be repeated, with each iteration resulting in an improved model that can generate better training data.\n\n\n# evaluating reasoning in llms\n\nevaluating the reasoning abilities of llms is crucial. researchers use various methods and benchmarks to assess their performance, including:\n\n * end task performance: this involves measuring the accuracy of llms on tasks requiring reasoning, such as arithmetic, commonsense, and symbolic reasoning benchmarks.\n * analysis of reasoning: this approach focuses on directly assessing the reasoning steps taken by llms, rather than just the final answer. this can involve analyzing the quality of the generated rationales or using formal metrics to evaluate the reasoning process.\n\n\n# key findings and implications\n\nresearch in reasoning in llms has yielded some interesting findings:\n\n * emergent ability: reasoning seems to be an emergent ability of llms, becoming more pronounced as the models get larger (around 100 billion parameters or more).\n * chain-of-thought prompting: this technique has been shown to significantly improve the performance of llms on various reasoning tasks.\n * complex reasoning challenges: despite progress, llms still struggle with complex reasoning tasks, suggesting that current benchmarks might be too simple.\n\n\n# open questions and future directions\n\nthe field of reasoning in llms is still evolving, with many open questions and exciting avenues for future research:\n\n * true reasoning or mimicry?: are llms truly capable of reasoning, or are they simply learning to mimic human reasoning through pattern recognition?\n * improving reasoning capabilities: how can we further enhance the reasoning capabilities of llms? this could involve developing new training methods, model architectures, or prompting techniques.\n\nby addressing these questions and continuing to explore the intricacies of reasoning in llms, we can unlock their full potential and pave the way for more intelligent and reliable language-based ai systems.\n\n----------------------------------------\n\n\n# 4. [2 2025] self-training elicits concise reasoning in large language models\n\nour combined method achieves a 30% reduction in output tokens on average, across five model families on gsm8k and math, while maintaining average accuracy.\n\n\n# 1. introduction & background\n\n * chain-of-thought (cot) reasoning significantly boosts llm performance by breaking complex tasks into intermediate reasoning steps. however, cot inherently generates redundant tokens that increase inference costs unnecessarily.\n\n * the authors hypothesize that current llms inherently possess the latent ability to reason concisely, as evidenced by the presence of shorter correct reasoning paths within their output distributions. their goal is to explicitly unlock this capability through targeted fine-tuning.\n\n\n\n\n\n\n# 2. preliminary investigation\n\nthe authors conducted preliminary studies to support their hypothesis:\n\n# concise reasoning definition:\n\n * defined as reasoning correctly with fewer output tokens compared to the model’s default (average correct) output length.\n\n# analysis of reasoning length distribution:\n\n * a kernel density estimation showed significant potential for concise reasoning across various model families (e.g., llama, deepseekmath, qwen, gemma).\n * models frequently generated solutions that were shorter than their default output, suggesting inherent latent potential for conciseness.\n\n# limitations of zero-shot prompting:\n\n * popular zero-shot prompts aimed at concise reasoning ("be concise," "fixed budget," "estimated budget," and handcrafted prompts) significantly reduced token count but often compromised accuracy and exhibited inconsistent effects, particularly on math-specialized models.\n\n\n# 3. proposed methods\n\nto reliably elicit concise reasoning without sacrificing accuracy, the authors proposed several fine-tuning methods based on self-training:\n\n# 3.1. naive best-of-n (bon) sampling:\n\n * generates multiple reasoning paths (n paths per question) and selects the shortest correct reasoning path for fine-tuning.\n * effective but sample inefficient as length reduction benefits saturate quickly.\n\n# 3.2. few-shot conditioning for efficient reduction:\n\n * combines few-shot prompting and bon sampling to enhance concise reasoning.\n * few-shot exemplars are derived from:\n   * human annotations (fs-human)\n   * proprietary llms like gpt-4o (fs-gpt4o)\n   * self-generated concise examples from the target model itself (fs-self)\n * few-shot conditioning significantly improves sample efficiency in eliciting concise reasoning.\n\n# 3.3. sample augmentation:\n\n * to improve accuracy retention, they augment the few-shot conditioned data with additional samples generated from naive bon.\n * ensures coverage of both easy and difficult questions, preserving accuracy while maintaining conciseness.\n\n\n# 4. experimental setup\n\n * evaluated across two mathematical reasoning datasets : gsm8k and math.\n * models tested include general-purpose (e.g., llama, gemma, qwen2.5) and math-specialized models (e.g., deepseekmath, qwen2.5-math).\n * metrics: accuracy and output length were primary evaluation metrics. length is measured in tokens.\n\n\n\n\n# 5. key findings & contributions\n\n# 5.1. main results\n\n * naive bon reduced reasoning length modestly (~12%) without major accuracy loss.\n * few-shot conditioned methods (especially fs-gpt4o and fs-human) yielded significant length reductions (~30% average across datasets) while maintaining accuracy.\n\n\n\n * the combined method (fs-gpt4o-bon ) achieved the greatest reduction in length (average 30%) with minimal accuracy drop.\n * self-training methods consistently outperformed zero-shot prompting and external fine-tuning baselines in terms of balancing conciseness with accuracy.\n\n# 5.2. in-depth analysis\n\n * models adaptively adjusted the length of reasoning paths based on question complexity—shorter paths for easier questions and longer paths for harder ones.\n * scaling analysis (1b, 3b, and 8b models) showed length reduction consistently improved with larger models.\n * fine-tuning effectively transferred concise reasoning learned from training to test outputs.\n\n\n# 6. discussion & broader impact\n\n * the default reasoning behavior of current llms inherently contains redundancy, likely due to training data and optimization methods not promoting conciseness.\n * the authors suggest that lightweight fine-tuning approaches are sufficient for eliciting concise reasoning, aligning with the "superficial alignment hypothesis" (minimal training can significantly shift model behavior).\n * emphasized the potential for integrating concise reasoning supervision into standard training pipelines for more efficient llm inference.\n\n\n# 7. limitations and future directions\n\n * further exploration of advanced training schemes (reinforcement learning, iterative refinement).\n * investigating more sophisticated few-shot exemplar selection and prompting strategies.\n * extending the scalability study beyond 8b parameter models.\n * generalizing concise reasoning methods to broader, non-mathematical tasks.\n\n\n# 8. contributions & novelty (summary):\n\n * identified latent concise reasoning capabilities within standard llms.\n * demonstrated ineffectiveness and inconsistency of popular zero-shot concise reasoning methods.\n * proposed a novel, efficient, and effective fine-tuning approach (few-shot conditioned best-of-n sampling with augmentation) that consistently improves conciseness without significant accuracy degradation.\n * provided extensive experimental validation across multiple models and datasets.\n\n\n# 9. practical implications:\n\n * the authors highlight practical benefits: reduced inference costs, lower latency, and more efficient model deployment in scenarios where reasoning-based inference costs are critical.\n\n\n# 10. conclusion\n\nthe paper convincingly demonstrates that standard large language models can be fine-tuned to significantly enhance concise reasoning using self-generated concise examples.\n\nthis approach is broadly applicable and beneficial across different llm architectures and model sizes, offering substantial improvements in inference efficiency without compromising reasoning accuracy.\n\noverall, this paper contributes significantly to understanding and improving reasoning efficiency in llms, providing valuable methodologies for practitioners aiming to optimize inference performance in real-world scenarios.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM Mixed Precision & Quantization & Outlier",frontmatter:{title:"LLM Mixed Precision & Quantization & Outlier",date:"2025-03-20T23:32:49.000Z",permalink:"/pages/dc7050/",tags:[null]},regularPath:"/05.llm/15.llm_quant.html",relativePath:"05.llm/15.llm_quant.md",key:"v-2bf21845",path:"/pages/dc7050/",headers:[{level:2,title:"1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization",slug:"_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization",normalizedTitle:"1. [139] understanding and overcoming the challenges of efficient transformer quantization",charIndex:2927},{level:3,title:"Problems",slug:"problems",normalizedTitle:"problems",charIndex:3022},{level:3,title:"Solutions",slug:"solutions",normalizedTitle:"solutions",charIndex:5185},{level:2,title:"2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models",slug:"_2-43-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models",normalizedTitle:"2.[43] integer or floating point? new outlooks for low-bit quantization on large language models",charIndex:7069},{level:3,title:"Key Takeaways in Three Sentences",slug:"key-takeaways-in-three-sentences",normalizedTitle:"key takeaways in three sentences",charIndex:7170},{level:3,title:"Abstract",slug:"abstract",normalizedTitle:"abstract",charIndex:7784},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:8165},{level:3,title:"Background and Related Works",slug:"background-and-related-works",normalizedTitle:"background and related works",charIndex:8825},{level:3,title:"Comparative Analysis of INT and FP Formats",slug:"comparative-analysis-of-int-and-fp-formats",normalizedTitle:"comparative analysis of int and fp formats",charIndex:9746},{level:3,title:"Exploiting INT and FP Complementarity",slug:"exploiting-int-and-fp-complementarity",normalizedTitle:"exploiting int and fp complementarity",charIndex:10483},{level:3,title:"Conclusion",slug:"conclusion",normalizedTitle:"conclusion",charIndex:11352},{level:2,title:"5. [26] FP8-LM: Training FP8 Large Language Models",slug:"_5-26-fp8-lm-training-fp8-large-language-models",normalizedTitle:"5. [26] fp8-lm: training fp8 large language models",charIndex:11781},{level:3,title:"Abstract",slug:"abstract-2",normalizedTitle:"abstract",charIndex:7784},{level:3,title:"1. Introduction",slug:"_1-introduction",normalizedTitle:"1. introduction",charIndex:12610},{level:3,title:"2. FP8 LLM Training",slug:"_2-fp8-llm-training",normalizedTitle:"2. fp8 llm training",charIndex:13873},{level:3,title:"3. Experimentation",slug:"_3-experimentation",normalizedTitle:"3. experimentation",charIndex:16094},{level:3,title:"4. Related Work",slug:"_4-related-work",normalizedTitle:"4. related work",charIndex:17655},{level:3,title:"5. Conclusion",slug:"_5-conclusion",normalizedTitle:"5. conclusion",charIndex:18046},{level:3,title:"Key Summary in 3 Sentences*",slug:"key-summary-in-3-sentences",normalizedTitle:"key summary in 3 sentences*",charIndex:18531},{level:2,title:"6. [139] FP8 Formats for Deep Learning",slug:"_6-139-fp8-formats-for-deep-learning",normalizedTitle:"6. [139] fp8 formats for deep learning",charIndex:19197},{level:3,title:"1. Introduction",slug:"_1-introduction-2",normalizedTitle:"1. introduction",charIndex:12610},{level:3,title:"Key contributions:",slug:"key-contributions",normalizedTitle:"key contributions:",charIndex:19510},{level:3,title:"2. Aspects of FP8 Usage in Deep Learning",slug:"_2-aspects-of-fp8-usage-in-deep-learning",normalizedTitle:"2. aspects of fp8 usage in deep learning",charIndex:19885},{level:3,title:"3. FP8 Binary Interchange Format",slug:"_3-fp8-binary-interchange-format",normalizedTitle:"3. fp8 binary interchange format",charIndex:20239},{level:3,title:"4. Empirical Results",slug:"_4-empirical-results",normalizedTitle:"4. empirical results",charIndex:21002},{level:3,title:"5. Conclusions",slug:"_5-conclusions",normalizedTitle:"5. conclusions",charIndex:21966},{level:3,title:"Key Takeaways in Three Sentences",slug:"key-takeaways-in-three-sentences-2",normalizedTitle:"key takeaways in three sentences",charIndex:7170},{level:2,title:"8. [34] Stable and low-precision training for large-scale vision-language models",slug:"_8-34-stable-and-low-precision-training-for-large-scale-vision-language-models",normalizedTitle:"8. [34] stable and low-precision training for large-scale vision-language models",charIndex:22926},{level:3,title:"1. Introduction and Motivation",slug:"_1-introduction-and-motivation",normalizedTitle:"1. introduction and motivation",charIndex:23011},{level:3,title:"2. Eight-Bit (8-bit) Training",slug:"_2-eight-bit-8-bit-training",normalizedTitle:"2. eight-bit (8-bit) training",charIndex:23719},{level:3,title:"3. Stability of Training",slug:"_3-stability-of-training",normalizedTitle:"3. stability of training",charIndex:26525},{level:3,title:"4. Conclusion and Future Directions",slug:"_4-conclusion-and-future-directions",normalizedTitle:"4. conclusion and future directions",charIndex:28676},{level:3,title:"Three-Sentence Core Summary",slug:"three-sentence-core-summary",normalizedTitle:"three-sentence core summary",charIndex:29292},{level:2,title:"9. [Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization",slug:"_9-y2025-paretoq-scaling-laws-in-extremely-low-bit-llm-quantization",normalizedTitle:"9. [y2025] paretoq: scaling laws in extremely low-bit llm quantization",charIndex:29910},{level:3,title:"Abstract",slug:"abstract-3",normalizedTitle:"abstract",charIndex:7784},{level:3,title:"1. Introduction",slug:"_1-introduction-3",normalizedTitle:"1. introduction",charIndex:12610},{level:3,title:"Key Questions:",slug:"key-questions",normalizedTitle:"key questions:",charIndex:31075},{level:3,title:"ParetoQ Approach:",slug:"paretoq-approach",normalizedTitle:"paretoq approach:",charIndex:31314},{level:3,title:"2. A Better QAT Scheduling Strategy for Extreme Low-Bit LLMs",slug:"_2-a-better-qat-scheduling-strategy-for-extreme-low-bit-llms",normalizedTitle:"2. a better qat scheduling strategy for extreme low-bit llms",charIndex:31734},{level:3,title:"3. A Hitchhiker’s Guide to Quantization Method Choices",slug:"_3-a-hitchhiker-s-guide-to-quantization-method-choices",normalizedTitle:"3. a hitchhiker’s guide to quantization method choices",charIndex:32775},{level:3,title:"4. Pareto-Optimality of Extremely Low-Bit LLMs",slug:"_4-pareto-optimality-of-extremely-low-bit-llms",normalizedTitle:"4. pareto-optimality of extremely low-bit llms",charIndex:33580},{level:3,title:"6. Related Work",slug:"_6-related-work",normalizedTitle:"6. related work",charIndex:34221},{level:2,title:"10. [47] Microscaling Data Formats for Deep Learning",slug:"_10-47-microscaling-data-formats-for-deep-learning",normalizedTitle:"10. [47] microscaling data formats for deep learning",charIndex:1047},{level:3,title:"Introduction",slug:"introduction-2",normalizedTitle:"introduction",charIndex:8165},{level:3,title:"Microscaling (MX) Data Formats",slug:"microscaling-mx-data-formats",normalizedTitle:"microscaling (mx) data formats",charIndex:36065},{level:3,title:"Concrete MX Formats",slug:"concrete-mx-formats",normalizedTitle:"concrete mx formats",charIndex:36507},{level:3,title:"Scalar Float to MX Format Conversion",slug:"scalar-float-to-mx-format-conversion",normalizedTitle:"scalar float to mx format conversion",charIndex:37009},{level:3,title:"Compute Flow and Training Pipeline",slug:"compute-flow-and-training-pipeline",normalizedTitle:"compute flow and training pipeline",charIndex:37470},{level:3,title:"Conclusion",slug:"conclusion-2",normalizedTitle:"conclusion",charIndex:11352},{level:3,title:"Key findings:",slug:"key-findings",normalizedTitle:"key findings:",charIndex:40144},{level:3,title:"Three-Sentence Summary",slug:"three-sentence-summary",normalizedTitle:"three-sentence summary",charIndex:40619},{level:2,title:"14. [33] SpinQuant: LLM quantization with learned rotations",slug:"_14-33-spinquant-llm-quantization-with-learned-rotations",normalizedTitle:"14. [33] spinquant: llm quantization with learned rotations",charIndex:41244},{level:3,title:"Abstract & Introduction",slug:"abstract-introduction",normalizedTitle:"abstract &amp; introduction",charIndex:null},{level:3,title:"Motivation & Outlier Reduction",slug:"motivation-outlier-reduction",normalizedTitle:"motivation &amp; outlier reduction",charIndex:null},{level:3,title:"Methodology",slug:"methodology",normalizedTitle:"methodology",charIndex:42716},{level:3,title:"Experiments & Results",slug:"experiments-results",normalizedTitle:"experiments &amp; results",charIndex:null},{level:3,title:"Key findings:",slug:"key-findings-2",normalizedTitle:"key findings:",charIndex:40144},{level:3,title:"Conclusions",slug:"conclusions",normalizedTitle:"conclusions",charIndex:21969},{level:3,title:"Three-Sentence Summary",slug:"three-sentence-summary-2",normalizedTitle:"three-sentence summary",charIndex:40619},{level:2,title:"17. [35] FP8 versus INT8 for efficient deep learning inference",slug:"_17-35-fp8-versus-int8-for-efficient-deep-learning-inference",normalizedTitle:"17. [35] fp8 versus int8 for efficient deep learning inference",charIndex:1615},{level:3,title:"1. Introduction and Motivation",slug:"_1-introduction-and-motivation-2",normalizedTitle:"1. introduction and motivation",charIndex:23011},{level:3,title:"2. Hardware Considerations",slug:"_2-hardware-considerations",normalizedTitle:"2. hardware considerations",charIndex:45564},{level:3,title:"3. Accuracy Comparison",slug:"_3-accuracy-comparison",normalizedTitle:"3. accuracy comparison",charIndex:46183},{level:3,title:"4. Transformer Networks",slug:"_4-transformer-networks",normalizedTitle:"4. transformer networks",charIndex:47003},{level:3,title:"5. Comparison to Other Work",slug:"_5-comparison-to-other-work",normalizedTitle:"5. comparison to other work",charIndex:51388},{level:3,title:"6. FP8 to INT8 Conversion",slug:"_6-fp8-to-int8-conversion",normalizedTitle:"6. fp8 to int8 conversion",charIndex:52017},{level:3,title:"7. INT Quantization Paradigm",slug:"_7-int-quantization-paradigm",normalizedTitle:"7. int quantization paradigm",charIndex:52392},{level:3,title:"8. Conclusion",slug:"_8-conclusion",normalizedTitle:"8. conclusion",charIndex:52780},{level:3,title:"Key Takeaways:",slug:"key-takeaways",normalizedTitle:"key takeaways:",charIndex:53247},{level:2,title:"16.",slug:"_16",normalizedTitle:"16.",charIndex:568},{level:2,title:"17. [434] Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation",slug:"_17-434-integer-quantization-for-deep-learning-inference-principles-and-empirical-evaluation",normalizedTitle:"17. [434] integer quantization for deep learning inference: principles and empirical evaluation",charIndex:54076},{level:3,title:"Key Highlights:",slug:"key-highlights",normalizedTitle:"key highlights:",charIndex:54790},{level:3,title:"Quantization Methods",slug:"quantization-methods",normalizedTitle:"quantization methods",charIndex:55010},{level:3,title:"Post Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)",slug:"post-training-quantization-ptq-vs-quantization-aware-training-qat",normalizedTitle:"post training quantization (ptq) vs. quantization-aware training (qat)",charIndex:55233},{level:3,title:"Calibration Methods",slug:"calibration-methods",normalizedTitle:"calibration methods",charIndex:55681},{level:3,title:"Optimizations to Reduce Accuracy Loss",slug:"optimizations-to-reduce-accuracy-loss",normalizedTitle:"optimizations to reduce accuracy loss",charIndex:55903},{level:3,title:"Recommended Workflow",slug:"recommended-workflow",normalizedTitle:"recommended workflow",charIndex:56356},{level:3,title:"Final 3-Sentence Summary:",slug:"final-3-sentence-summary",normalizedTitle:"final 3-sentence summary:",charIndex:56635},{level:2,title:"20.[73] FP8 Quantization: The Power of the Exponent",slug:"_20-73-fp8-quantization-the-power-of-the-exponent",normalizedTitle:"20.[73] fp8 quantization: the power of the exponent",charIndex:57314},{level:3,title:"Key Takeaways:",slug:"key-takeaways-2",normalizedTitle:"key takeaways:",charIndex:53247},{level:3,title:"Three-Sentence Summary",slug:"three-sentence-summary-3",normalizedTitle:"three-sentence summary",charIndex:40619},{level:2,title:"18. [Y2024]Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs",slug:"_18-y2024-integer-scale-a-free-lunch-for-faster-fine-grained-quantization-of-llms",normalizedTitle:"18. [y2024]integer scale: a free lunch for faster fine-grained quantization of llms",charIndex:1679},{level:3,title:"Overview",slug:"overview",normalizedTitle:"overview",charIndex:59672},{level:3,title:"Key Motivation",slug:"key-motivation",normalizedTitle:"key motivation",charIndex:60346},{level:3,title:"Proposed Method: Integer Scale",slug:"proposed-method-integer-scale",normalizedTitle:"proposed method: integer scale",charIndex:61244},{level:3,title:"Results",slug:"results",normalizedTitle:"results",charIndex:16406},{level:3,title:"Practical Takeaways",slug:"practical-takeaways",normalizedTitle:"practical takeaways",charIndex:63550},{level:3,title:"Key Parts of the Paper in 3 Sentences",slug:"key-parts-of-the-paper-in-3-sentences",normalizedTitle:"key parts of the paper in 3 sentences",charIndex:64326},{level:2,title:"19. [121] Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers",slug:"_19-121-training-high-performance-and-large-scale-deep-neural-networks-with-full-8-bit-integers",normalizedTitle:"19. [121] training high-performance and large-scale deep neural networks with full 8-bit integers",charIndex:64846},{level:3,title:"1. Introduction",slug:"_1-introduction-4",normalizedTitle:"1. introduction",charIndex:12610},{level:3,title:"2. Related Work",slug:"_2-related-work",normalizedTitle:"2. related work",charIndex:66281},{level:3,title:"3. WAGEUBN Framework",slug:"_3-wageubn-framework",normalizedTitle:"3. wageubn framework",charIndex:67559},{level:3,title:"4. Results",slug:"_4-results",normalizedTitle:"4. results",charIndex:69512},{level:3,title:"5. Analysis",slug:"_5-analysis",normalizedTitle:"5. analysis",charIndex:70578},{level:3,title:"6. Conclusion WAGEUBN is the first complete INT8 quantization framework  for training large-scale DNNs. It achieves:",slug:"_6-conclusion-wageubn-is-the-first-complete-int8-quantization-framework-for-training-large-scale-dnns-it-achieves",normalizedTitle:"6. conclusion wageubn is the first complete int8 quantization framework  for training large-scale dnns. it achieves:",charIndex:null},{level:3,title:"Three-Sentence Summary",slug:"three-sentence-summary-4",normalizedTitle:"three-sentence summary",charIndex:40619},{level:2,title:"20. [381] I-BERT: Integer-only BERT Quantization",slug:"_20-381-i-bert-integer-only-bert-quantization",normalizedTitle:"20. [381] i-bert: integer-only bert quantization",charIndex:71945},{level:3,title:"Challenges Addressed",slug:"challenges-addressed",normalizedTitle:"challenges addressed",charIndex:71998},{level:3,title:"Solution - I-BERT Approach:",slug:"solution-i-bert-approach",normalizedTitle:"solution - i-bert approach:",charIndex:72780},{level:3,title:"Results and Impact:",slug:"results-and-impact",normalizedTitle:"results and impact:",charIndex:73494},{level:3,title:"Key Takeaways in 3 Sentences",slug:"key-takeaways-in-3-sentences",normalizedTitle:"key takeaways in 3 sentences",charIndex:73975},{level:2,title:"21. [904] LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",slug:"_21-904-llm-int8-8-bit-matrix-multiplication-for-transformers-at-scale",normalizedTitle:"21. [904] llm.int8(): 8-bit matrix multiplication for transformers at scale",charIndex:74575},{level:3,title:"Key findings include:",slug:"key-findings-include",normalizedTitle:"key findings include:",charIndex:75351},{level:3,title:"LLM.int8 solution",slug:"llm-int8-solution",normalizedTitle:"llm.int8 solution",charIndex:75682},{level:3,title:"Three-Sentence Key Takeaways",slug:"three-sentence-key-takeaways",normalizedTitle:"three-sentence key takeaways",charIndex:76099},{level:2,title:"22.[637] Training Deep Neural Networks with 8-bit Floating Point Numbers",slug:"_22-637-training-deep-neural-networks-with-8-bit-floating-point-numbers",normalizedTitle:"22.[637] training deep neural networks with 8-bit floating point numbers",charIndex:76753},{level:3,title:"1. Introduction",slug:"_1-introduction-5",normalizedTitle:"1. introduction",charIndex:12610},{level:3,title:"2. Challenges in Low-Precision Training",slug:"_2-challenges-in-low-precision-training",normalizedTitle:"2. challenges in low-precision training",charIndex:77365},{level:3,title:"3. Proposed Solutions",slug:"_3-proposed-solutions",normalizedTitle:"3. proposed solutions",charIndex:77755},{level:3,title:"4. Experimental Results",slug:"_4-experimental-results",normalizedTitle:"4. experimental results",charIndex:78345},{level:3,title:"5. Discussion",slug:"_5-discussion",normalizedTitle:"5. discussion",charIndex:78702},{level:3,title:"6. Conclusion",slug:"_6-conclusion",normalizedTitle:"6. conclusion",charIndex:70986},{level:3,title:"Key Takeaways in 3 Sentences",slug:"key-takeaways-in-3-sentences-2",normalizedTitle:"key takeaways in 3 sentences",charIndex:73975},{level:2,title:"[270] Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks",slug:"_270-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks",normalizedTitle:"[270] hybrid 8-bit floating point (hfp8) training and inference for deep neural networks",charIndex:2440},{level:2,title:"28. [Read Y2025 NVIDIA] Coat: Compressing Optimizer States and Activation for Memory-Efficient FP8 Training  :+1:  :+1:  :+1:  :+1:  :+1:",slug:"_28-read-y2025-nvidia-coat-compressing-optimizer-states-and-activation-for-memory-efficient-fp8-training",normalizedTitle:"28. [read y2025 nvidia] coat: compressing optimizer states and activation for memory-efficient fp8 training  👍  👍  👍  👍  👍",charIndex:null}],headersStr:"1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization Problems Solutions 2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models Key Takeaways in Three Sentences Abstract Introduction Background and Related Works Comparative Analysis of INT and FP Formats Exploiting INT and FP Complementarity Conclusion 5. [26] FP8-LM: Training FP8 Large Language Models Abstract 1. Introduction 2. FP8 LLM Training 3. Experimentation 4. Related Work 5. Conclusion Key Summary in 3 Sentences* 6. [139] FP8 Formats for Deep Learning 1. Introduction Key contributions: 2. Aspects of FP8 Usage in Deep Learning 3. FP8 Binary Interchange Format 4. Empirical Results 5. Conclusions Key Takeaways in Three Sentences 8. [34] Stable and low-precision training for large-scale vision-language models 1. Introduction and Motivation 2. Eight-Bit (8-bit) Training 3. Stability of Training 4. Conclusion and Future Directions Three-Sentence Core Summary 9. [Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization Abstract 1. Introduction Key Questions: ParetoQ Approach: 2. A Better QAT Scheduling Strategy for Extreme Low-Bit LLMs 3. A Hitchhiker’s Guide to Quantization Method Choices 4. Pareto-Optimality of Extremely Low-Bit LLMs 6. Related Work 10. [47] Microscaling Data Formats for Deep Learning Introduction Microscaling (MX) Data Formats Concrete MX Formats Scalar Float to MX Format Conversion Compute Flow and Training Pipeline Conclusion Key findings: Three-Sentence Summary 14. [33] SpinQuant: LLM quantization with learned rotations Abstract & Introduction Motivation & Outlier Reduction Methodology Experiments & Results Key findings: Conclusions Three-Sentence Summary 17. [35] FP8 versus INT8 for efficient deep learning inference 1. Introduction and Motivation 2. Hardware Considerations 3. Accuracy Comparison 4. Transformer Networks 5. Comparison to Other Work 6. FP8 to INT8 Conversion 7. INT Quantization Paradigm 8. Conclusion Key Takeaways: 16. 17. [434] Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation Key Highlights: Quantization Methods Post Training Quantization (PTQ) vs. Quantization-Aware Training (QAT) Calibration Methods Optimizations to Reduce Accuracy Loss Recommended Workflow Final 3-Sentence Summary: 20.[73] FP8 Quantization: The Power of the Exponent Key Takeaways: Three-Sentence Summary 18. [Y2024]Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs Overview Key Motivation Proposed Method: Integer Scale Results Practical Takeaways Key Parts of the Paper in 3 Sentences 19. [121] Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers 1. Introduction 2. Related Work 3. WAGEUBN Framework 4. Results 5. Analysis 6. Conclusion WAGEUBN is the first complete INT8 quantization framework  for training large-scale DNNs. It achieves: Three-Sentence Summary 20. [381] I-BERT: Integer-only BERT Quantization Challenges Addressed Solution - I-BERT Approach: Results and Impact: Key Takeaways in 3 Sentences 21. [904] LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale Key findings include: LLM.int8 solution Three-Sentence Key Takeaways 22.[637] Training Deep Neural Networks with 8-bit Floating Point Numbers 1. Introduction 2. Challenges in Low-Precision Training 3. Proposed Solutions 4. Experimental Results 5. Discussion 6. Conclusion Key Takeaways in 3 Sentences [270] Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks 28. [Read Y2025 NVIDIA] Coat: Compressing Optimizer States and Activation for Memory-Efficient FP8 Training  :+1:  :+1:  :+1:  :+1:  :+1:",content:' 1.  [UnRead 2] A Comprehensive Study on Quantization Techniques for Large Language Models :+1：\n 2.  [UnRead 11] A Comprehensive Evaluation of Quantization Strategies for Large Language Models 👍 👍\n 3.  [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization\n 4.  [43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models\n 5.  [26] FP8-LM: Training FP8 Large Language Models 👍 👍 👍 from Microsoft\n     This is discussed that first order in adam could be FP8, but second order in adam should be FP16. and the weight should be reserved a copy of FP32 full-precision or FP16 with tensor scaling..\n 6.  [139] FP8 Formats for Deep Learning\n 7.  [41] With Shared Microexponents, A Little Shifting Goes a Long Way 👍 from Meta, Microsoft 👍\n 8.  [34] Stable and low-precision training for large-scale vision-language models 👍\n     mentioned in Deepseek paper mixed precision training section. Not read yet.\n 9.  [Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization\n 10. [47] Microscaling Data Formats for Deep Learning\n 11. [Unread 2 Y2024] To FP8 and Back Again: Quantifying the Effects of Reducing Precision on LLM Training Stability\n 12. [145] Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model\n 13. [Unread 27]Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n 14. [45] PB-LLM: Partially Binarized Large Language Models\n 15. [55] QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models\n 16. [33] SpinQuant: LLM quantization with learned rotations\n 17. [35] FP8 versus INT8 for efficient deep learning inference\n 18. [Y2024]Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs\n 19. [434] Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation\n 20. [73] FP8 Quantization: The Power of the Exponent\n 21. [UnRead 121] Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers\n 22. [381] I-BERT: Integer-only BERT Quantization\n 23. [637] Training Deep Neural Networks with 8-bit Floating Point Numbers\n 24. [904] LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale\n 25. [UnRead 2568] DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients 👍\n 26. [239] Model Accuracy and Runtime Tradeoff in Distributed Deep Learning: A Systematic Study 👍\n 27. [270] Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks\n     This is the paper where E5M2 and E4M3\n 28. [Read Y2025 NVIDIA] Coat: Compressing Optimizer States and Activation for Memory-Efficient FP8 Training 👍 👍 👍 👍 👍\n     Source code: https://github.com/NVlabs/COAT\n\nOutlier\n\n1.[106] Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling\n\n----------------------------------------\n\n\n# 1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization\n\n\n# Problems\n\nThe problems encountered when attempting to quantize transformer models like BERT, as highlighted in the paper:\n\n * High Dynamic Range of Activations\n   * The authors observed that the activation tensors in transformer models, particularly in the residual connections, exhibit a very high dynamic range.\n   * This means that the values within these tensors vary significantly in magnitude.\n   * Quantization, especially when using low-bit fixed-point formats (like 8-bit integer), struggles to accurately represent such a wide range of values.\n   * Trying to capture both very small and very large values within the limited range of a low-bit format inevitably leads to significant quantization errors.\n * Presence of Structured Outliers\n   * The authors identified the presence of structured outliers within the activation tensors, particularly in the residual connections after the feed-forward network (FFN).\n   * These outliers are not random; they appear to be correlated with specific input tokens and embedding dimensions.\n   * Further analysis revealed that these outliers influence the attention mechanism in the subsequent layers, encouraging the model to attend to specific tokens like the separator token ([SEP]).\n\n> In BERT-like models, an intermediate hidden activation tensor x has a shape (B, T, d), where B is the batch size, T is the sequence length, and d is the number of embedding dimensions (d = 768 for BERT-base, Devlin et al. 2019). In the following figure, you could tell the x-axis is in dimension of 768.\n\n\n\n * While this attention behavior might be beneficial for the model\'s performance, the outliers that cause it also create challenges for quantization.\n * Sensitivity to Quantization Noise\n   * Different parts of the transformer model exhibit varying sensitivities to the noise introduced by quantization.\n   * Some components, like the residual connections and certain attention heads, are particularly sensitive, and quantizing them aggressively can lead to a significant drop in accuracy.\n   * This sensitivity necessitates careful consideration of which parts of the model to quantize and at what bit-width.\n\n\n\n\n# Solutions\n\nsolutions proposed in the paper:\n\n * Mixed-precision PTQ\n   \n   * The authors observed that different parts of the BERT model have varying sensitivities to quantization noise.\n   * To address this, they proposed using a higher bit-width (16-bit) for the more sensitive activation tensors, particularly the residual sum after the feed-forward network (FFN).\n   * This higher bit-width allows for more accurate representation of both the FFN\'s input and output, minimizing potential errors.\n   * Additionally, they explored using low-bit (2-4) quantization for weights and token embeddings, which can significantly reduce model size without much accuracy loss.\n\n * Per-embedding-group PTQ\n   \n   * The authors identified that outliers in the activation tensors primarily reside in a few specific embedding dimensions.\n   * To address this, they proposed a novel per-embedding-group (PEG) quantization scheme, where distinct quantization parameters are used for different embedding dimensions or groups of dimensions.\n   * This method involves splitting the activation tensor into groups along the embedding dimension and applying separate quantization parameters to each group.\n   * To optimize this process, they introduced a range-based permutation step to ensure that all outliers are grouped together.\n   * This approach effectively handles outliers without significantly increasing computational overhead.\n\n * Quantization-aware training (QAT)\n   \n   * The authors also explored QAT, where the model is trained with simulated quantization operations.\n   * This allows the model to adapt to the quantization noise, leading to improved performance compared to post-training quantization.\n   * During QAT, they used learnable ranges for both weights and activations, further enhancing the model\'s adaptability to quantization.\n\n----------------------------------------\n\n\n# 2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models\n\n\n# Key Takeaways in Three Sentences\n\n 1. The study demonstrates that low-bit floating-point formats, particularly FP8, provide superior quantization accuracy for LLMs compared to INT8 , with comparable hardware efficiency at 8-bit precision.\n 2. The Mixture of Formats Quantization (MoFQ) approach optimally selects between INT and FP per layer , improving accuracy without increasing computational overhead.\n 3. MoFQ achieves state-of-the-art results in both W4-only and W8A8 quantization , outperforming existing methods like GPTQ, AWQ, LLM.int8(), and SmoothQuant while maintaining efficient inference speed .\n\n\n# Abstract\n\nThe study finds that optimal quantization formats vary across layers in LLMs, leading to the Mixture of Formats Quantization (MoFQ) approach, which selects the best format per layer.\nMoFQ achieves superior or comparable performance to current quantization methods for weight-only (W-only) and weight-activation (WA) quantization without additional hardware overhead.\n\n\n# Introduction\n\nQuantization minimizes LLMs\' size and inference costs, with prior work focusing on low-bit integer formats.\nHowever, as LLMs grow, integer quantization becomes less effective, requiring optimizations or alternatives. Low-bit floating-point formats have emerged as viable alternatives, with FP8 already supported in NVIDIA’s H100 GPUs.\n\nThe study:\n\n 1. Compares INT and FP formats in terms of hardware efficiency and quantization error.\n 2. Proposes Mixture of Formats Quantization (MoFQ) , selecting the best format per layer.\n 3. Implements an inference system for W-only quantization, maintaining performance parity with INT-based systems.\n\n\n# Background and Related Works\n\nInteger vs. Floating-Point Formats\n\n * Integer (INT) : Uniformly distributed values.\n * Floating-Point (FP) : Non-uniform distribution, allowing higher precision for small values but reduced precision for large values.\n * Hardware efficiency : FP operations typically cost more than INT, but at 8-bit, FP8 and INT8 MAC (Multiply-Accumulate) units have nearly identical area costs.\n\nPost-Training Quantization (PTQ) for LLMs\n\nTwo main PTQ strategies:\n\n 1. Weight-Only (W-only) Quantization: Applies to weights only, e.g., W4A16.\n 2. Weight-Activation (WA) Quantization: Quantizes both weights and activations, e.g., W8A8.\n\nState-of-the-art (SOTA) methods:\n\n * LLM.int8() : Uses mixed precision (INT8+FP16).\n * SmoothQuant : Redistributes quantization difficulty from activations to weights.\n * GPTQ & AWQ : Use second-order information and pre-scaling techniques to improve quantization.\n\n\n# Comparative Analysis of INT and FP Formats\n\nA. Hardware Cost of INT vs. FP MAC Units\n\n * At 8-bit precision, FP8 and INT8 MACs require nearly the same hardware area , aligning with H100 GPU capabilities.\n\nB. Quantization Error Comparison\n\n 1. 4-bit Weight-Only (W4) Quantization (LLaMA-65B model):\n\n * 🔥 Some layers perform better with INT4, while others favor FP4, indicating layer-dependent format preference .\n\n\n\n 2. 8-bit Weight-Activation (W8A8) Quantization :\n\n * 🔥Weights : INT8 generally has lower quantization error.\n * 🔥Activations : FP8 shows better robustness for dynamic activation tensors.\n * Best choice: INT8 for weights, FP8 for activations—but hardware constraints necessitate using the same format per layer.\n\n\n\n\n# Exploiting INT and FP Complementarity\n\nA. Improved Low-Bit FP4 Format\n\n * IEEE floating-point format reserves exponent values for NaN and Inf.\n * Reallocating NaN & Inf to normalized numbers improves FP4 precision by 35%.\n\nB. Mixture of Formats Quantization (MoFQ)\n\n * Selects the best quantization format (INT or FP) per layer based on quantization error.\n * Works for both W-only and WA quantization .\n * Algorithm : Iterates through layers, computes quantization error for INT and FP, and selects the lower-error format.\n\nC. Low-Bit W-Only Inference System\n\n * INT4 and FP4 require conversion to FP16 before computation due to FP16 activations.\n * W8A8 quantization : FP16 activations are converted to FP8 or INT8 based on next-layer format selection.\n * No additional hardware overhead for FP-based or MoFQ-based inference compared to INT-based quantization.\n\n\n\n\n# Conclusion\n\n * Comparative study : INT and FP formats have complementary strengths.\n * Key finding : FP8 and INT8 MAC units have similar hardware costs at low-bit quantization .\n * MoFQ method :\n   * Selects the best quantization format per layer .\n   * Achieves state-of-the-art accuracy in W4-only and W8A8 quantization.\n   * No additional inference latency or hardware overhead .\n\n----------------------------------------\n\n\n# 5. [26] FP8-LM: Training FP8 Large Language Models\n\n\n# Abstract\n\nThe paper explores FP8 low-bit data formats for training large language models (LLMs), significantly reducing memory usage and computation costs while maintaining accuracy.\n\nThe authors introduce an FP8 automatic mixed-precision training framework with three levels of FP8 utilization, improving mixed-precision and distributed parallel training.\n\nKey results show that training the GPT-175B model on an H100 GPU platform using FP8:\n\n * Reduces memory usage by 39%\n * Speeds up training by 75% compared to BF16 (Megatron-LM)\n * Outperforms Nvidia Transformer Engine by 37%\n\nThe FP8 training methodology is generalizable to fine-tuning, instruction tuning, and reinforcement learning with human feedback (RLHF). The framework is open-sourced at aka.ms/MS.AMP .\n\n\n# 1. Introduction\n\nLLMs have demonstrated exceptional performance in various domains but are extremely expensive to train.\nThe cost of training models like GPT-3 (175B) or PaLM (540B) is enormous, requiring thousands of GPUs or TPUs .\nLow-precision training is a promising solution as it:\n\n * Increases speed\n * Reduces memory usage\n * Minimizes communication overhead\n\nMost existing frameworks, such as Megatron-LM, MetaSeq, and Colossal-AI , use FP32, FP16, or BF16 mixed-precision training , but FP8 offers significant efficiency gains :\n\n * 2× speed-up\n * 50%-75% memory and communication savings\n\n# Challenges of FP8 Training\n\n 1. Data underflow/overflow issues due to FP8’s limited dynamic range.\n 2. Numerical instabilities and divergence during training.\n\n# Proposed FP8 Mixed-Precision Framework\n\n * Introduces three levels of FP8 utilization (gradients, optimizer states, and distributed learning).\n * Uses precision decoupling and automatic scaling to mitigate numerical instability.\n * Achieves 29%-39% memory savings and 63%-65% communication cost reductions .\n\n> The resulting FP8 mixed-precision networks are more efficient than their pure FP16 counterparts, but a network that is in full INT8 is expected to be significantly more efficient yet.\n\n\n# 2. FP8 LLM Training\n\n# 2.1 FP8 Gradient and All-Reduce Communication\n\n * Traditional mixed-precision training uses FP16/FP32 for gradients , leading to high communication costs.\n * Applying FP8 directly to gradients results in loss of accuracy due to underflow/overflow.\n * The paper proposes an automatic scaling technique to adapt scaling factors dynamically, preventing numerical instability.\n\n# 2.2 FP8 Optimizer\n\n * The Adam optimizer typically consumes 16 bytes per parameter due to high-precision storage of gradients and optimizer states.\n * The proposed FP8 optimizer stores:\n   * FP8 first-order moment\n   * FP16 master weights (with tensor scaling)\n   * FP16 second-order moment\n * This reduces memory consumption from 16 bytes to 6 bytes per parameter (2.6× savings).\n\n> My main takeaway is that direction of gradient matters, instead of magnitude.\n\n\n\n 1. FP8 master weight induces performance degradation (see the #2a vs. #3 lines in Fig. 8), while FP16 can maintain accuracy as FP32 (see #2a vs. #0 and #1) but requiring using tensor scaling. It reveals that the master weight is precision-sensitive. This can be attributed to the master weight’s role in updating weights, which tend to exhibit small magnitudes, necessitating high precision to maintain accuracy.\n 2. The training loss of BF16 master weight is slightly higher than that of FP16 with a scaling factor because BF16 has fewer mantissa bits, resulting in lower precision (see #2a vs. #2b).\n 3. ❗ The second-order gradient moment is more precision-sensitive than the first-order one, because the ❗square calculation is easy to cause underflow and leads to accuracy degradation. Utilizing FP8 for the second-order gradient moment can lead to divergent training loss (see the #4 dot in Fig. 8).\n\nPlease Notice that FP8 #4 is diverged, not shown in the figure.\n\n\n\n# 2.3 FP8 Distributed Parallel Training\n\n * Tensor Parallelism : Uses FP8 for weight and activation tensors , reducing compute and communication overhead.\n * Sequence Parallelism : Converts activation tensors to FP8 before communication , reducing costs.\n * ZeRO (Zero Redundancy Optimizer) Support : Distributes full tensors across devices while preserving FP8 scaling factors .\n\n\n# 3. Experimentation\n\n# 3.1 Experimental Setup\n\n * Training Dataset : Collected from CommonCrawl, The Pile, C4, OpenWebText, Wikipedia, RedPajama , and other curated sources.\n * Model Configuration : Uses a decoder-only Transformer architecture (like GPT-3), with RoPE embeddings and Flash Attention .\n\n# 3.2 Main Results\n\n# Model Performance\n\n * Loss curves of FP8 models match BF16 models , confirming accuracy preservation\n * Zero-shot evaluations on Lambada, HellaSwag, BoolQ, PIQA, COPA show comparable performance between FP8 and BF16 .\n * Fine-tuning (SFT & RLHF) : FP8 achieves:\n   * 27% faster fine-tuning\n   * 32% reduction in model weight memory\n   * 62% optimizer state memory savings\n\nSystem Performance\n\n * Memory reduction : FP8 achieves 28%-39% lower memory usage than BF16.\n * Training speed improvement :\n   * 75% faster training for GPT-175B\n   * 37% faster than Nvidia Transformer Engine\n * Communication efficiency :\n   * 63%-65% reduction in weight gradient communication\n   * 34% lower activation-related communication costs\n\n# 3.3 Ablation Study\n\n * Gradient Scaling : Automatic scaling reduces underflow/overflow errors , improving training stability.\n * Optimizer Precision :\n   * FP16 master weights outperform FP8 master weights in accuracy preservation.\n   * FP8 first-order gradient moment is viable, but FP8 second-order moment leads to divergence.\n * Parallelism Optimization :\n   * FP8 sequence and tensor parallelism reduce communication costs by 34% .\n   * FP8 ZeRO maintains a balanced GPU memory load while saving memory.\n\n\n# 4. Related Work\n\n * Mixed-Precision Training : Prior work focused on FP16/BF16 , but FP8 remains underexplored .\n * Low-Precision LLM Training :\n   * OPT, Bloom, Gopher, Chinchilla used BF16 for better numerical stability.\n   * FP8 support was limited before Nvidia Hopper GPUs.\n   * This work provides the first systematic FP8 training framework for pre-training and fine-tuning LLMs .\n\n\n# 5. Conclusion\n\n * Introduces a new FP8 mixed-precision training framework with automatic scaling and precision decoupling .\n * Achieves significant reductions in memory, compute, and communication costs .\n * Maintains model accuracy across GPT models from 125M to 175B parameters .\n * Demonstrates versatility in pre-training, instruction tuning, and RLHF.\n * Future work includes scaling to even larger models, training multi-modal models, and deploying FP8 LLMs on edge devices.\n\n\n# Key Summary in 3 Sentences*\n\nThis paper introduces an FP8 mixed-precision training framework that reduces memory consumption by 39% , speeds up training by 75% , and outperforms Nvidia Transformer Engine by 37% while maintaining LLM accuracy.\nThe framework uses automatic scaling and precision decoupling to stabilize training, supports FP8 optimizers and distributed training , and generalizes to fine-tuning and reinforcement learning with human feedback (RLHF) .\nThese findings establish FP8 as the next-generation precision format for training LLMs , significantly lowering costs while preserving model performance.\n\n----------------------------------------\n\n\n# 6. [139] FP8 Formats for Deep Learning\n\n\n# 1. Introduction\n\nDeep learning models require increasing computational resources, necessitating lower-precision formats for efficiency.\n\nFP8 is a natural evolution from FP16 and BF16, reducing compute and memory costs while maintaining accuracy comparable to FP16 .\n\n\n# Key contributions:\n\n * Two FP8 formats:\n   \n   * E4M3 : 4-bit exponent, 3-bit mantissa (for weights and activations).\n   * E5M2 : 5-bit exponent, 2-bit mantissa (for gradients).\n\n * Training and inference in FP8 match FP16/BF16 accuracy across CNNs, RNNs, and Transformers.\n\n * Post-training quantization (PTQ) using FP8 outperforms int8 while preserving model accuracy.\n\n\n# 2. Aspects of FP8 Usage in Deep Learning\n\n * FP8 computations will be performed in higher precision (FP16/FP32) , with final results cast back to FP8.\n * Scaling factors are applied to optimize FP8 precision , similar to loss-scaling in FP16 mixed precision .\n * Handling of special values (NaNs, Infs) is modified in E4M3 to increase dynamic range.\n\n\n# 3. FP8 Binary Interchange Format\n\n\n\nFP8 includes two encodings :\n\n * E4M3 :\n   \n   * Used for weights and activations .\n   * No representation for infinities (max value: 448 ).\n   * Single NaN representation to extend range .\n\n * E5M2 :\n   \n   * Used for gradients .\n   * Standard IEEE-like format , supporting NaNs and infinities .\n   * Larger range (up to 57,344 ).\n\n\n\n# 3.1 Special Value Representations\n\n * E4M3 removes infinities and limits NaNs to a single pattern , extending its dynamic range .\n * E5M2 follows IEEE-754 , allowing straightforward conversion from FP16 .\n\n# 3.2 Exponent Bias\n\n * E4M3 bias = 7, E5M2 bias = 15 (matching IEEE-style representation).\n * Some models require per-tensor scaling rather than a fixed exponent bias (Figure 2).\n\n\n# 4. Empirical Results\n\n# 4.1 Training\n\n * FP8 training achieves accuracy comparable to FP16/BF16 across CNNs, RNNs, and Transformers.\n * Image Classification :\n   * FP8 accuracy is within statistical variation of FP16 for most CNNs (ResNet, MobileNet, VGG, etc.).\n * Language Translation :\n   * FP8 BLEU scores match FP16 for Transformer and GNMT models.\n * NLP Models (Table 4, Figure 1) :\n   * GPT models (126M to 175B parameters) trained in FP8 match FP16 in perplexity .\n\n# 4.2 Inference\n\n * FP8 post-training quantization (PTQ) outperforms int8 , retaining full precision accuracy for:\n   * BERT (F1 score on SQuAD).\n   * GPT-3 (perplexity on Wikitext103).\n * FP8-trained models require no additional quantization steps , simplifying deployment.\n\n# 4.3 Per-Tensor Scaling\n\n * Fixed exponent bias fails when additional tensors (e.g., residuals) are stored in FP8.\n * Per-tensor scaling maintains accuracy , making FP8 viable for expanded use beyond GEMMs .\n\n\n# 5. Conclusions\n\n * FP8 formats (E4M3, E5M2) efficiently reduce training and inference costs while maintaining accuracy .\n * FP8 training is on par with FP16/BF16 , without hyperparameter changes.\n * FP8 simplifies inference by eliminating the need for quantization-aware training (QAT) required for int8.\n * Future work : Expanding FP8 usage to more tensor types and operations beyond matrix multiplications.\n\n\n# Key Takeaways in Three Sentences\n\nFP8 formats (E4M3 for weights/activations, E5M2 for gradients) significantly reduce computation and memory overhead while maintaining accuracy equivalent to FP16/BF16 across CNNs, RNNs, and Transformer models.\n\nPost-training quantization (PTQ) with FP8 outperforms int8 , allowing for simpler and more effective deployment of trained models. The study validates FP8 training up to 175B parameters , proving its scalability for large-scale deep learning applications.\n\n----------------------------------------\n\n\n# 8. [34] Stable and low-precision training for large-scale vision-language models\n\n\n# 1. Introduction and Motivation\n\nThe paper addresses two crucial bottlenecks in large-scale vision-language model training:\n\n * Speed – how to train massive models efficiently despite ballooning compute costs.\n * Stability – how to avoid “loss spikes” that can degrade or derail training.\n\nThey target contrastive language-image pre-training (CLIP) models, which fuse image and text encoders to learn aligned representations.\n\nThese models often involve hundreds of millions to billions of parameters and require large-scale data (like LAION).\n\nBy improving efficiency (low-precision arithmetic) and stability (modified optimizer), the authors hope to sustain further scaling of multimodal architectures.\n\n\n# 2. Eight-Bit (8-bit) Training\n\n# 2.1 Preliminaries & Related Work\n\n16-bit operations (float16/bfloat16) are currently standard for large-scale training.\n\nbfloat16 has broader exponent range than float16, making it more robust at scale, but native hardware support for bfloat16 exists mainly on TPUs or newer GPUs.\n\nMore recently, hardware support for int8 and float8 is emerging (NVIDIA Ampere has int8, Hopper adds float8), enabling potential speedups over 16-bit if quantization can be made accurate and stable.\n\nHowever, at the 1B-parameter scale of CLIP ViT-Huge, quantization can introduce noise that leads to sizable accuracy drops unless carefully managed.\n\n\n\n# 2.2 The SwitchBack Method\n\n 1. Core Idea: In a standard linear layer, three matrix multiplications occur: one in the forward pass and two in backprop (input-grad and weight-grad).\n\nSwitchBack executes only the forward pass and input-grad steps in 8-bit, while reverting the weight-grad step to higher precision (16-bit).\n\n 2. Why This Matters:\n\n * The weight-grad multiply has an extremely large inner dimension (batch size × sequence length), making it more prone to quantization noise.\n * By keeping that specific multiplication in 16-bit, SwitchBack avoids excessive noise accumulation.\n\n 3. Implementation Details:\n\n * Uses row-wise quantization for activation/gradient matrices and tensor-wise quantization for weights.\n * Relies on triton kernels for fused “quantize + transpose” in the backward pass, minimizing overhead from transposing or storing scaled values.\n * Extends easily to float8 (fp8) by simulating exact fp8 values with 16-bit computations.\n\n 4. Experimental Setup:\n\n * Trains multiple CLIP ViT variants (Base, Large, Huge) on LAION-2B with a short schedule (20k iterations, patch dropout 0.5).\n * Evaluates zero-shot ImageNet accuracy using standard CLIP prompts.\n\n\n\n 5. Results & Speedups:\n\n * Accuracy: For CLIP ViT-Huge (∼1B params), SwitchBack stays within 0.1% of the bfloat16 baseline, whereas a simpler baseline (LLM.int8()) underperforms by nearly 6%.\n * Speed: Overall training speeds up by 13–25%. Profiling shows int8 multiplications run ~2× faster than fp16, and quantization overhead is small (<25% of total time in these matmuls).\n\n# 2.3 Float8 via Reduced Feature Magnitudes\n\n * While SwitchBack also supports float8 (fp8), the authors observe that a purely tensor-wise fp8 baseline diverges at large scale.\n * Key Intervention: Zero-initialized layer-scale – each transformer block’s residual path is initially scaled to 0, then learned. This keeps features from growing too large, which can disrupt fp8’s narrower numeric range.\n\nZero-initialized layer-scale\n\n\n\n * With zero-init layer-scale, ViT-Large trains successfully in simulated fp8 (still a small gap from bfloat16, but no divergence).\n\n\n# 3. Stability of Training\n\n# 3.1 Problem: Loss Spikes\n\n * Training very large CLIP models can suffer from fast, sporadic spikes in loss—episodes that slow or degrade convergence.\n * Prior methods like gradient clipping or large warmup sometimes help, but for CLIP ViT-Huge, the authors pinpoint a new mechanism: the AdamW second-moment estimator for the patch embedding layer can become out-of-date.\n\n# 3.2 Observations About Loss Spikes\n\nLoss spike\n\n\n\n\n\n * More Spikes at Larger Scale: Scaling model size, batch size, or learning rate all exacerbate these spikes.\n * AdamW β2 Influence: Reducing β2 from its typical 0.999 helps avoid some spikes but can hamper overall performance.\n * Root Cause: ❗ When gradients suddenly increase for the early patch-embedding parameters, AdamW’s exponentially moving average of squared gradients (the second-moment) underestimates the true magnitude, resulting in an oversized update that triggers a transient loss explosion.\n\nStuck-in-the-past sceneario: historicall gradient magnitude have been historically samll, thus, some parameter is larger for those parameters. Suddenly, if those parameter receives a large gradient signal, the update can be catastrophycally big.\n\n# 3.3 StableAdamW Optimizer\n\n * Based on ideas from AdaFactor, the authors propose update clipping:\n   * Compute an “RMS ratio” = √(E[g² / EMA(g²)]) per layer.\n   * If RMS ratio > 1, automatically downscale the layer’s effective learning rate for that iteration.\n * StableAdamW = AdamW + update clipping. It successfully removes or mitigates these spikes better than gradient norm clipping, sustaining higher accuracy at large scale.\n\n\n\n# 3.4 Connection to Low-Precision Training\n\n * Loss spikes can produce extremely large activations and gradients, risking numerical issues (Inf/NaN) when using narrower floating-point types.\n * The authors show that stabilizing these early-layer explosions goes hand in hand with ensuring safe low-precision training.\n * They also adopt a specialized fp16 “loss scalar” strategy (per-layer updates, fixed scaling) to keep ephemeral spikes in one layer from knocking down the entire network’s scalar.\n\n\n# 4. Conclusion and Future Directions\n\n * SwitchBack plus StableAdamW demonstrates both faster (13–25% speedups) and steadier (fewer loss spikes) training for large CLIP ViTs, outperforming or matching bfloat16 baselines.\n * The authors note limitations: they simulate float8 (rather than use actual fp8 hardware), do not fully explore alternate initialization or width-scaling schemes, and their training runs are shortened for cost reasons. Nonetheless, the methods and open-source code (including Triton kernels) lay groundwork for future improvement in quantizing and stabilizing very large multimodal models.\n\n\n# Three-Sentence Core Summary\n\nThey propose SwitchBack, an 8-bit training approach that leaves weight-gradient computations in higher precision, yielding 13–25% speed improvements over standard 16-bit training while closely matching accuracy.\n\nThey further show that loss spikes in large CLIP models stem from outdated second-moment estimations in AdamW, and propose StableAdamW (AdamW plus adaptive update clipping) as a fix.\n\nBy combining these, the paper demonstrates fast, stable, and memory-efficient low-precision training on billion-parameter vision-language models.\n\n----------------------------------------\n\n\n# 9. [Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization\n\n\n# Abstract\n\nThe study introduces ParetoQ , a unified framework for evaluating extremely low-bit quantization (1-bit to 4-bit) in large language models (LLMs).\nIt identifies a learning transition between 2-bit and 3-bit models, with 3-bit and higher retaining pre-trained distributions, while 2-bit and below undergo drastic representation changes.\nBy optimizing training strategies and quantization functions, ParetoQ achieves state-of-the-art (SOTA) performance across multiple bit-widths.\nNotably, a ternary (1.58-bit) 600M model surpasses a previous 3B ternary model , using only one-fifth of the parameters.\nThe study finds that 2-bit quantization offers a superior balance between accuracy, model size, and hardware efficiency compared to 4-bit and binary quantization.\n\n\n# 1. Introduction\n\nAs models scale, lower-precision computation is gaining traction due to memory savings and computational efficiency .\nPrior studies suggest conflicting optimal quantization levels (1.58-bit, 2-bit, 4-bit, etc.), but no unified framework existed to systematically compare their effectiveness.\n\n\n# Key Questions:\n\n * What is the optimal trade-off between bit-width and model size?\n * How does quantization impact scaling laws in low-bit settings?\n * What training strategies and quantization functions yield Pareto-optimal results ?\n\n\n# ParetoQ Approach:\n\n * Incorporates five key dimensions : model size (N ), token count (D ), quantization precision (P ), training strategy (S ), and quantization function (F ).\n * Identifies bit-specific training schemes and quantization functions .\n * Establishes that binary quantization significantly degrades accuracy , while ternary (1.58-bit), 2-bit, and 3-bit quantization match or exceed 4-bit performance .\n\n\n# 2. A Better QAT Scheduling Strategy for Extreme Low-Bit LLMs\n\n# 2.1 Training Budget Allocation\n\n * Post-Training Quantization (PTQ) is easier to implement but performs poorly below 4-bit.\n * Quantization-Aware Training (QAT) integrates quantization during training, improving low-bit performance .\n * Optimal budget split: 90% full-precision training, 10% QAT fine-tuning .\n * Finding-1: QAT fine-tuning outperforms both PTQ and QAT from scratch , achieving the best trade-off between accuracy and efficiency.\n\n# 2.2 Fine-tuning Characteristics\n\n * Fine-tuning improves accuracy across all bit-widths , including binary and ternary models.\n * Lower-bit models (≤2-bit) require more fine-tuning (30B tokens), while 3-bit and 4-bit saturate at 10B tokens .\n * Finding-2: Bit-width transition effect:\n   * 3-bit & 4-bit recover near full precision with fine-tuning .\n   * 1-bit to 2-bit undergo substantial weight transformations , requiring more tokens.\n   * QAT serves as "compensation" for 3-bit+ but "reconstruction" for ≤2-bit models.\n\n\n# 3. A Hitchhiker’s Guide to Quantization Method Choices\n\n# 3.1 Trade-offs in Low-bit Quantization\n\n 1. Range Clipping : Lower-bit quantization suffers from outlier effects, requiring range clipping or learnable scales .\n 2. Quantization Grids :\n\n * Binary & Ternary require balanced levels.\n * 2-bit prefers symmetric distribution*.\n * 3-bit and 4-bit benefit from including "0" in quantization levels.\n\n# 3.2 Introducing ParetoQ\n\n * Combines the best quantization functions per bit-width :\n   * 1-bit : Elastic Binarization.\n   * 1.58-bit, 2-bit : Stretched Elastic Quant (SEQ) .\n   * 3-bit, 4-bit : Learned Step Size Quantization (LSQ) .\n * Finding-3: No single best function for all bit-widths. Learnable range settings outperform fixed statistical methods , especially for sub-4-bit quantization .\n\n\n# 4. Pareto-Optimality of Extremely Low-Bit LLMs\n\n# 4.1 Accuracy-Compression Trade-off\n\n * Ternary (1.58-bit), 2-bit, and 3-bit outperform 4-bit in accuracy-size efficiency.\n * 2-bit and ternary quantization sit on the Pareto frontier.\n\n# 4.2 Hardware Constraints\n\n * Ternary (1.58-bit) appears efficient but is difficult to implement due to indexing overhead.\n * 2-bit is more hardware-friendly** due to **simpler storage and arithmetic operations.\n\n# 4.3 Accuracy-Speed Trade-off\n\n * 2-bit achieves higher speed at the same accuracy as 4-bit.\n * 2-bit kernels are significantly faster than 4-bit kernels in large matrix multiplications.\n\n\n# 6. Related Work\n\n * Early quantization research focused on 8-bit and 4-bit LLMs .\n * Recent sub-4-bit research explored ternary, 2-bit, and 1-bit models , but lacked a unified comparison framework .\n * ParetoQ is the first study to systematically compare sub-4-bit quantization schemes .\n\n7. Conclusions\n\n * ParetoQ unifies training and quantization schemes across five bit-widths.\n * Ternary (1.58-bit), 2-bit, and 3-bit quantization outperform 4-bit in the accuracy-size trade-off.\n * 2-bit is the most practical choice due to its hardware efficiency.\n * First framework that ensures fair comparisons across different quantization methods.\n\nKey Takeaways (3 Sentences)\n\n 1. ParetoQ introduces a unified framework for extreme low-bit quantization, identifying 2-bit as the most efficient trade-off between accuracy, memory, and computational speed.\n 2. A clear transition exists between 2-bit and 3-bit models, where lower-bit quantization requires substantial fine-tuning to compensate for drastic weight transformations.\n 3. With its optimized quantization functions and training schemes, ParetoQ achieves state-of-the-art performance across all bit-widths, surpassing previous specialized methods.\n\n----------------------------------------\n\n\n# 10. [47] Microscaling Data Formats for Deep Learning\n\n\n\n\n# Introduction\n\nThe rapid advancement of deep learning models has led to increased computational and storage costs.\nOne approach to mitigating these costs is reducing bit-width precision in data formats, moving beyond traditional FP32 to lower-bit formats such as FP16, BFloat16, FP8, and INT8.\nHowever, per-tensor scaling in low-bit-width formats struggles with dynamic range limitations.\nMicroscaling (MX) data formats introduce per-block scaling factors to enhance efficiency, maintain accuracy, and ease deployment in AI hardware.\n\n\n# Microscaling (MX) Data Formats\n\nMX formats encode numerical values in fixed-size blocks , where each block consists of:\n\n * A shared scaling factor (X)\n * Multiple narrow bit-width elements (Pi)\n\nThis approach extends the dynamic range beyond what per-tensor scaling allows, making sub-8-bit computations feasible.\nMX formats are hardware-efficient while minimizing accuracy loss and ensuring seamless adoption in existing AI frameworks.\n\n\n# Concrete MX Formats\n\nMX formats are categorized based on block size, scale format, and element bit-width.\n\nFORMAT   BLOCK SIZE   SCALE DATA FORMAT   SCALE BITS   ELEMENT FORMAT    ELEMENT BIT-WIDTH\nMXFP8    32           E8M0                8            FP8 (E4M3/E5M2)   8\nMXFP6    32           E8M0                8            FP6 (E2M3/E3M2)   6\nMXFP4    32           E8M0                8            FP4 (E2M1)        4\nMXINT8   32           E8M0                8            INT8              8\n\n\n# Scalar Float to MX Format Conversion\n\nTo convert floating-point data to an MX format, the shared scaling factor (X) is computed based on the largest absolute value in a block.\nEach element is then normalized using X and quantized to the desired format. The conversion follows a quantization algorithm that:\n\n 1. Determines the scaling exponent from the maximum value in the block.\n 2. Computes X as a power of two .\n 3. Quantizes elements (Pi) based on X .\n\n\n# Compute Flow and Training Pipeline\n\nFor deep learning workloads, dot-product operations (e.g., matrix multiplication, convolutions) are performed in MX formats , while non-dot operations (e.g., activations, normalization, residual add) remain in higher-precision formats like FP32 or BFloat16 .\nTraining involves keeping a master FP32 copy of weights while performing compute-intensive operations in MX formats.\nQuantization-aware fine-tuning is often required for best accuracy, particularly for lower-bit formats like MXFP6 and MXFP4.\n\nExperimental Results Inference MX data formats were tested across language models, vision transformers, speech recognition, and recommendation models.\n\nDirect-Cast Inference (No Fine-Tuning)\n\n * MXINT8 performs nearly identically to FP32 across all tasks, making it a drop-in replacement .\n * MXFP8 and MXFP6 maintain good accuracy , but MXFP6 requires fine-tuning for best results.\n * MXFP4 suffers from significant accuracy loss , especially in complex models.\n\nPost-Training Quantization (PTQ) with Error Diffusion\n\n * Error diffusion (similar to GPFQ-based post-training quantization ) helps recover accuracy.\n * MXFP6 achieves results close to FP32 after error diffusion.\n * MXFP4 remains significantly worse than FP32, limiting its practical use in inference .\n\nFinetuned Inference\n\n * MXFP6 achieves FP32-level accuracy after fine-tuning .\n * MXFP4 improves slightly but still lags behind .\n * MXINT8 continues to serve as the most effective low-friction alternative to FP32 .\n\nGenerative Model Inference (GPT-3, LLaMA)\n\n * MXINT8 closely matches FP32 performance on GPT-3 and LLaMA.\n * MXFP6 and MXFP8 perform well in most tasks , but some degradation is observed in complex benchmarks.\n * MXFP4 shows noticeable loss , especially in zero-shot settings .\n\nTraining with MX Formats For the first time, MX formats enable sub-8-bit training of large-scale transformers with minimal accuracy loss.\n\nTraining with MXFP6\n\n * MXFP6 (E3M2) trains models with no accuracy drop compared to FP32 .\n * This represents the first demonstration of 6-bit training for large transformer models without modifications to the training recipe.\n * Hyperparameters remain unchanged from FP32 training, making MXFP6 a practical choice.\n\nTraining with MXFP4 + MXFP6\n\n * MXFP4 weights combined with MXFP6 activations/gradients yield slightly worse performance but remain viable for training.\n * Loss curves show only a minor increase in training loss , proving feasibility.\n\n\n# Conclusion\n\nMicroscaling (MX) data formats introduce per-block scaling to reduce bit-width while maintaining high accuracy, hardware efficiency, and seamless integration .\n\n\n# Key findings:\n\n 1. MXINT8 is an effective drop-in replacement for FP32 inference.\n 2. MXFP6 enables sub-8-bit inference and training with minimal accuracy loss.\n 3. MXFP4 combined with MXFP6 remains viable for training but suffers in inference.\n 4. First-ever demonstration of training large generative models using 6-bit weights, activations, and gradients** . MX formats offer a compelling path toward lower precision deep learning without sacrificing model quality.\n\n\n\n\n# Three-Sentence Summary\n\nMicroscaling (MX) data formats introduce per-block scaling factors , improving the efficiency and accuracy of sub-8-bit deep learning computations.\n\nMXINT8 serves as a near-lossless drop-in replacement for FP32 inference , while MXFP6 enables large-scale deep learning models to be trained with sub-8-bit precision without altering training recipes .\n\nThe results demonstrate that MX formats significantly reduce computational and storage costs while maintaining model performance , making them a strong alternative to traditional floating-point formats.\n\n----------------------------------------\n\n\n# 14. [33] SpinQuant: LLM quantization with learned rotations\n\n\n# Abstract & Introduction\n\nSpinQuant is a novel method for post-training quantization (PTQ) of Large Language Models (LLMs) that uses learned rotation matrices to enhance quantization accuracy while maintaining full-precision outputs.\n\nTraditional PTQ struggles with outliers, which widen quantization ranges and reduce effective bit usage. Instead of relying on random rotations, SpinQuant learns rotation matrices that optimize quantization performance.\n\nThe method significantly reduces accuracy gaps in 4-bit quantization across weights, activations, and KV-cache, outperforming previous quantization methods like LLM-QAT, SmoothQuant, and QuaRot.\n\nSpecifically, SpinQuant reduces the zero-shot reasoning accuracy gap on LLaMA-2 7B from 12.1 to 1.6 points, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points.\n\n\n# Motivation & Outlier Reduction\n\nQuantization reduces memory and computation costs but suffers from outliers, which stretch the value distribution.\n\nExisting methods like mixed-precision quantization and weight-activation trade-offs attempt to mitigate this but remain suboptimal.\n\nSpinQuant rotates activation and weight matrices to remove outliers, making them more Gaussian-like, thus improving quantization efficiency.\n\nThe paper demonstrates that some random rotations lead to better results than others, with a 13-point accuracy difference in zero-shot reasoning tasks.\n\n\n# Methodology\n\n\n\nThe method optimizes rotation matrices using Cayley SGD, which maintains orthonormality while minimizing the quantized network loss.\n\nThis process enhances quantization robustness, significantly outperforming random rotations.\n\n\n\n\n# Experiments & Results\n\nExperiments were conducted on LLaMA-2 (7B/13B/70B), LLaMA-3 (1B/3B/8B), and Mistral-7B, evaluated across zero-shot reasoning tasks and perplexity on WikiText2.\n\n\n# Key findings:\n\nSpinQuantₙₒₕₐₑd effectively closes the quantization gap in W4A8KV8 settings, making it comparable to QuIP# and OmniQuant.\n\nSpinQuantₕₐₑd achieves 64.0 average accuracy in W4A4KV4, reducing the accuracy gap to full precision to 2.9 points (LLM-QAT had a 22-point gap).\n\nAgainst QuaRot, SpinQuant improves accuracy by up to 28.6 points in extreme quantization settings.\n\nSpeed analysis shows that SpinQuantₕₐₑd incurs only an 8% latency overhead, making it a practical approach.\n\n\n# Conclusions\n\nSpinQuant is the first PTQ method to optimize learned rotation matrices, offering:\n\n * Significant improvements over existing quantization techniques by reducing outliers.\n * State-of-the-art performance in 4-bit quantization settings.\n * Compatibility with advanced weight quantization methods like GPTQ.\n\nIts learned rotations make quantized LLMs more robust, reducing the performance gap to full-precision models, making low-bit LLM inference practical.\n\n\n# Three-Sentence Summary\n\nSpinQuant introduces learned rotation matrices to mitigate outliers in weight and activation distributions, improving LLM quantization efficiency.\n\nUsing Cayley SGD optimization, it significantly reduces the accuracy gap in 4-bit quantization, outperforming SmoothQuant, LLM-QAT, and QuaRot.\n\nExperiments on LLaMA-2, LLaMA-3, and Mistral-7B show state-of-the-art quantization performance, with SpinQuant narrowing the accuracy gap to full precision by up to 45.1% relative to QuaRot.\n\n----------------------------------------\n\n\n# 17. [35] FP8 versus INT8 for efficient deep learning inference\n\nThis paperexplores the efficiency and accuracy of using FP8 (8-bit floating point) and INT8 (8-bit integer) formats for deep learning inference, particularly on edge devices.\n\nThe key points and findings of the paper are summarized as follows:\n\n\n# 1. Introduction and Motivation\n\nThe paper discusses the growing interest in using FP8 for neural network training, especially with Nvidia\'s introduction of FP8 in their Hopper architecture GPUs.\n\nWhile FP8 is being considered for training, the paper focuses on its implications for efficient inference on edge devices, where INT8 is commonly used due to its efficiency.\n\nThe authors question whether training networks in FP8 and deploying them in the same format could bypass the need for quantization, which is currently required when converting FP32/FP16 models to INT8.\n\n\n# 2. Hardware Considerations\n\nThe paper argues that FP8 is less efficient than INT8 in terms of hardware area and energy consumption. FP8 requires 50% more area and energy compared to INT8, making it less suitable for efficient inference.\n\nFloating-point operations are inherently more complex and costly in hardware compared to integer operations, especially when considering the need for floating-point accumulators.\n\nThe authors highlight that FP8 implementations often involve mixed precision (e.g., FP16 for activations), which can lead to inefficiencies, particularly in networks with large activation tensors.\n\n\n# 3. Accuracy Comparison\n\nThe paper provides a theoretical and empirical comparison of FP8 and INT8 formats in terms of network accuracy.\n\nThe key difference between FP8 and INT8 lies in their ability to handle outliers. FP8, with its exponent bits, can better represent outliers, while INT8 is more efficient for well-behaved, Gaussian-like distributions.\n\n> the only significant difference between the two formats is in their ability to capture outliers.\n\n❗ In post-training quantization (PTQ), INT8 generally performs better for networks without significant outliers, while FP8-E4 (4 exponent bits) is better for networks with outliers, such as ransformers.\n\n❗ In quantization-aware training (QAT), INT8 often outperforms FP8, as training can reduce the impact of outliers, making INT8 more accurate and efficient.\n\n\n# 4. Transformer Networks\n\nTransformer networks, particularly BERT, exhibit significant outliers in certain layers, making FP8-E4 more accurate in PTQ settings.\n\nHowever, the paper argues that these outlier issues can be mitigated with techniques like mixed precision (W8A16) or quantization-aware training, allowing INT8 to achieve similar accuracy without the hardware inefficiencies of FP8.\n\nFP8-E4 vs FP8-E5\n\n❗ It’s all about the outliers. If a distribution has very significant outliers, the FP8-E4/FP8-E5 format is more accurate.\n\nFor well-behaved networks without many outliers, the INT8 format is significantly more accurate in the PTQ setting than FP8-E4 and FP8-E5.\n\nWe also see that FP8-E5 is never the best format for inference; even for the transformer layers with significant outliers, the FP8-E4 format is better.\n\nFor some computer vision networks, INT8 is better; for some networks with significant outliers, the FP8-E4/FP8-E5 formats are better.\n\nPurely taking these results into account, the FP8-E4 format looks comparatively worse than FP8-E2 and FP8-E3.\n\nCombining these findings with the hardware implementation costs, the FP8-E4 format itself looks like it is a worse choice than its lower-exponent bit brethren, which are both cheaper hardware-wise and more accurate.\n\nIf anything, the FP8-E3 format stands out positively in this accuracy analysis compared to other FP formats.\n\nHowever, Deepseek FP8 use E4M4 for both forward and backwards computation.\n\nQAT versus PTQ\n\n\n\nOne surprising trend is that the INT8 results improve more than their PTQ baseline than their FP8 counterparts.\n\nThere is a good reason for this; again, it’s about the outliers.\n\nWhen performing QAT, outliers are clipped and do not receive a gradient.\n\nThe clipped weights/activations then tend towards the clipping threshold due to regularization.\n\nBut most importantly, with the outliers clipped, the network learns weights that still perform well despite the outliers being removed.\n\nwhen training the quantization parameters with a method like LSQ (Esser et al. (2020)), the network can learn to make the ranges smaller so as to find a better trade-off between the clipping and quantization errors.\n\nThe smaller the range, the more accurate your quantized representation will be.\n\nThis is especially the case for INT8, where the sensitivity to the quantization ranges is much larger than the floating-point formats with more exponent bits that are naturally more resistant to outliers.\n\nThis way, the INT8 format benefits significantly more from QAT than the FP formats.\n\nQAT\n\nNumerical representation chosen for training (e.g., FP8-E4) does not dictate how the distributions of weights and activations form.\n\nInstead, these distributions are primarily shaped by the overall training process, including factors like regularization, optimizer settings, and initialization.\n\nQAT Transformer\n\nThere are significant outliers in the summation going into the layer-norm in some of the fully connected modules, especially in the final layers of the network.\n\nThese outliers force the attention mechanism in the next layer to pay attention to some meaningless tokens – like sentence separator tokens, periods, or commas – that occur in the text, causing that specific token to not update significantly.\n\nSimply clipping these outlier reduces accuracy significantly.\n\nOnly a few layers are the best in FP8-E4. The other layers find a lower MSE error with the FP8-E2 and FP8-E3 formats.\n\nIn the most naive PTQ setting, the FP8-E4 format performs better than INT8.\n\nComparison to other work\n\n> The problems with the quantization of gradients put forth in the paper are well-known and have been addressed in many works in the past. (Sun et al. (2019); Gupta et al. (2015)). Many works have shown the necessity of stochastic rounding and proper range setting for the backward pass that alleviate these issues and make INT8 for gradients work just as well (Sun et al. (2019). What is this?\n\nTransformer models can be executed in INT8 as well, both with PTQ and QAT.\n\nFinally, the only comparison with the INT8 format comes in the form of comparing transformer-based language models in the PTQ setting.\n\nHowever, as argued, these problems are easily fixable for transformer networks, making them able to execute entirely in INT8 or in mixed precision with a small number of activations in INT16.\n\n\n# 5. Comparison to Other Work\n\nThe authors compare their findings with other works, such as those from Nvidia, Arm, and Intel, and Graphcore, which also explore FP8 for training.\n\nThey find that their results are consistent with these works but provide a more comprehensive comparison between FP8 and INT8.\n\nThe paper highlights that other works often omit critical comparisons, such as the hardware efficiency of FP8 versus INT8, and the impact of mixed precision on inference performance.\n\nSeveral works have shown that even in the INT8 PTQ setting you can get back your original accuracy with any of several possible tricks\n\n\n# 6. FP8 to INT8 Conversion\n\nThe paper explores the feasibility of converting FP8-trained networks to INT8.\n\nFor networks without significant outliers, the conversion is straightforward and can even improve accuracy.\n\nFor networks with outliers, such as transformers, the conversion to INT8 may degrade accuracy, but this can be mitigated with quantization-aware training.\n\n\n# 7. INT Quantization Paradigm\n\nThe authors advocate for the use of INT8 and INT4 formats for efficient inference, as they offer better hardware efficiency and accuracy for most networks.\n\nThey present a quantization paradigm where INT16 is used for high accuracy, INT8 for most networks, and INT4 for further efficiency, especially in weight-bound networks like large language models.\n\n\n# 8. Conclusion\n\nThe paper concludes that FP8 is not a suitable replacement for INT8 in efficient deep learning inference.\n\nWhile FP8 can handle outliers better in certain cases, the hardware inefficiencies and the availability of techniques to mitigate outlier issues in INT8 make it a less attractive option.\n\nThe authors recommend using INT8 and INT4 formats for efficient on-device inference, as they provide the best trade-off between accuracy and efficiency.\n\n\n# Key Takeaways:\n\nFP8 is less efficient than INT8 in terms of hardware area and energy consumption.\n\nINT8 is more accurate for most networks, especially after quantization-aware training, which can reduce the impact of outliers.\n\nFP8 is only beneficial in specific cases, such as transformer networks with significant outliers, but these issues can be addressed with INT8 using mixed precision or QAT.\n\nINT4 and INT8 are recommended for efficient inference, offering a better balance of accuracy and hardware efficiency compared to FP8.\n\nOverall, the paper provides a comprehensive analysis of the trade-offs between FP8 and INT8, concluding that INT8 remains the superior choice for efficient deep learning inference on edge devices.\n\n----------------------------------------\n\n\n# 16.\n\n----------------------------------------\n\n\n# 17. [434] Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation\n\nThis paper explores integer quantization techniques for deep learning inference, which help reduce model size and improve computational efficiency by leveraging high-throughput integer math pipelines.\n\nThe authors provide a mathematical foundation for different quantization choices and evaluate their empirical performance across multiple deep learning models spanning vision, speech, and language domains.\n\nThe study focuses on 8-bit integer quantization, demonstrating that accuracy loss can be minimized to within 1% of the floating-point baseline, even for challenging models like MobileNets and BERT-large.\n\n\n# Key Highlights:\n\nBenefits of Integer Quantization\n\n * Integer operations offer up to 16× speedup over FP32 on NVIDIA Turing GPUs.\n * Smaller word sizes reduce memory bandwidth pressure and improve cache utilization.\n\n\n# Quantization Methods\n\n * Affine Quantization: Uses a scale factor and zero-point but adds computational overhead.\n * Scale Quantization: More efficient as it avoids additional computations by using only a scale factor.\n\n\n# Post Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)\n\n * PTQ quantizes weights and activations after training but may lead to accuracy degradation for some models.\n * QAT fine-tunes the network with quantization effects included, yielding better results, especially for MobileNets and Transformers.\n\nPer-channel quantization for weights and per-tensor quantization for activations is recommended for accuracy and performance.\n\n\n# Calibration Methods\n\nMax, entropy, and percentile-based (99.9%-99.999%) calibrations are tested.\n\nNo single calibration is best for all networks; entropy and 99.99% percentile provide optimal accuracy for most models.\n\n\n# Optimizations to Reduce Accuracy Loss\n\n * Partial Quantization: Leaves sensitive layers in floating point, improving accuracy while retaining performance benefits.\n * Learning Quantization Parameters: Fine-tuning the quantization ranges (PACT method) slightly improves accuracy but is not always necessary for int8 quantization.\n\nELU Clipping: For BERT, modifying the GELU activation range significantly enhances post-training quantization results.\n\n\n# Recommended Workflow\n\n * Start with PTQ using max, entropy, and percentile calibrations.\n * If accuracy loss is high, use Partial Quantization to leave sensitive layers unquantized.\n * If further accuracy recovery is needed, perform QAT with pre-determined best calibration.\n\n\n# Final 3-Sentence Summary:\n\nThis paper presents a quantization workflow for deep learning inference, balancing speed and accuracy by leveraging int8 quantization with scale quantization for weights and activations.\n\nIt demonstrates that a combination of post-training quantization, partial quantization, and quantization-aware training ensures that accuracy remains within 1% of floating-point models, even for challenging architectures like MobileNets and BERT.\n\nThe proposed methods significantly improve inference efficiency, making integer quantization a practical approach for deploying neural networks on hardware accelerators.\n\n----------------------------------------\n\n\n# 20.[73] FP8 Quantization: The Power of the Exponent\n\nThis paper investigates FP8 quantization for neural network inference, comparing it with traditional INT8 quantization.\n\nThe authors analyze different FP8 format configurations, focusing on the trade-off between exponent and mantissa bits.\n\nThe study includes theoretical analysis, post-training quantization (PTQ), and quantization-aware training (QAT) across various deep learning models.\n\nKey findings indicate that FP8 outperforms INT8 in post-training quantization, particularly for networks with activation outliers, such as Transformers.\n\nHowever, in quantization-aware training, the accuracy difference between INT8 and FP8 diminishes as the network learns to adapt to the quantization scheme.\n\n\n# Key Takeaways:\n\n * FP8 vs. INT8: FP8 offers an additional degree of freedom through exponent bits, which helps manage outliers better than INT8, improving post-training quantization accuracy.\n * Optimal Format Selection: The choice of FP8 format (e.g., 5M2E vs. 4M3E) depends on the severity of outliers in a network, with higher exponent bits benefiting models with significant activation outliers.\n * QAT Reduces Differences: Quantization-aware training helps networks adapt to the quantization format, making INT8 and FP8 perform similarly in trained models.\n\n\n# Three-Sentence Summary\n\nThis paper explores FP8 quantization and its advantages over INT8, showing that FP8 provides better accuracy in post-training quantization due to its ability to handle activation outliers more effectively.\n\nThrough analytical and empirical studies, the authors determine that the optimal FP8 configuration depends on the balance between exponent and mantissa bits, with higher exponent bits benefiting networks with larger outliers.\n\nHowever, in quantization-aware training, the differences between FP8 and INT8 diminish as the network learns to optimize within the quantization scheme.\n\n> We validated the FP8 format for many networks in a post-training quantization setting, showing that generally for neural networks the 5M2E and 4M3E FP8 format works the best, and that for networks with more outliers like transformers increasing the number of exponent bits works best.\n\n----------------------------------------\n\n\n# 18. [Y2024]Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs\n\n\n# Overview\n\nThis paper deals with post-training quantization for large language models (LLMs), focusing on a technique called fine-grained quantization. While fine-grained quantization (group-wise scales) generally retains better accuracy at lower bit-widths than coarse-grained methods, it comes with a major drawback in slower inference. The authors identify that a large part of this overhead comes from repeatedly converting integer multiplication results into floating-point to apply each group’s “float scale.” They therefore propose an Integer Scale approach that removes most of these costly conversions by storing and applying each group’s scale in integer form.\n\n\n# Key Motivation\n\n 1. Accuracy vs. Speed Trade-off:\n\n * Fine-grained quantization (grouping weights or activations into small blocks) achieves higher accuracy than coarse-grained approaches, especially when pushing to lower bit-widths like W4A8 or W4A4 (weights 4-bit, activations 8 or 4-bit).\n * However, the group-wise scaling factors in floating-point form introduce numerous conversions (e.g., INT32 → FP32 → multiply by FP32 scale → back to INT32), which dramatically reduce inference speed.\n\n 2. Integer Scale as a “Free Lunch”:\n\n * By transforming each group’s floating-point scale into an integer, the paper’s method avoids a lot of repetitive float conversions within the GEMM kernels.\n * The authors call it a “free lunch” because it requires no additional calibration or training—just a straightforward conversion of float scales into integer form with a suitable integer “amplifier.”\n\n\n# Proposed Method: Integer Scale\n\n 1. Integer Amplifier (α):\n\n * Since the learned scales from fine-grained quantization typically lie between 0 and 1, the authors multiply them by a power-of-two factor (e.g., 2^10 = 1024) to map them into integer space without introducing big rounding errors.\n * They then store these integer scales and use simple integer multiply operations inside the kernel, followed by one final float conversion for the output—rather than one float conversion per group multiplication.\n\n 2. Modified GEMM Kernel:\n\n * The new kernel accumulates partial sums at integer precision and multiplies by the integer scales on the fly.\n * This leads to far fewer float conversions and yields a noticeable speedup (often well above 1.5× faster) relative to the standard float-scale approach.\n\n 3. Stability & Overflow:\n\n * The paper shows that choosing an amplifier (like 2^10) is sufficient to preserve accuracy while keeping integer operations safely in INT32 range—there are no overflow issues in tested scenarios.\n * The authors also show that per-layer “heuristic search” for the best amplifier can be replaced by a single fixed power-of-two amplifier (e.g., 1024), with negligible difference in accuracy.\n\n\n\n\n# Results\n\n 1. Accuracy Retention:\n\n * On tasks like LAMBADA, C4, WikiText-2, and Common Sense QA, the Integer Scale method achieves nearly the same (or slightly better) accuracy compared to float-scale baselines (GPTQ, AWQ, Omniquant) under the same fine-grained quantization setup.\n * It also helps quantize more challenging models like LLaMA-3 and Mixtral 8×7B at 4-bit weights without the large drop typical of naive methods.\n\n 2. Speed Gains:\n\n * The central benefit is that Integer Scale speeds up fine-grained quantization kernels significantly.\n * In many experiments, the method achieves:\n   * Up to 1.85× acceleration vs. float-scale fine-grained kernels,\n   * Up to 1.17× faster than certain well-optimized W4A16 kernels (e.g., Marlin),\n   * Up to 2.13× faster than the model’s FP16 baseline (depending on batch size and model).\n\n 3. Comparison to Other Libraries:\n\n * Compared to QServe (another W4A8 system) or Marlin’s W4A16, the Integer Scale approach often yields higher or comparable throughput, especially under typical inference batch sizes (32, 64, 128).\n\n\n# Practical Takeaways\n\n * Plug-and-Play for Existing Methods: Integer Scale is designed to be a drop-in replacement for float scales in fine-grained quantization. You keep the same group sizes, bit widths, etc., just multiply the group scales by a power-of-two to store them as integers.\n * No Extra Training: Unlike some other quantization techniques that require knowledge distillation or fine-tuning, Integer Scale only changes how scales are stored and applied. This means no additional compute overhead for calibrating or adjusting the model.\n * Balancing Memory- and Compute-Bound Scenarios: The paper emphasizes that as batch size grows, the gains might change (memory-bound vs. compute-bound). Still, they show consistent improvements in typical inference settings.\n\n\n# Key Parts of the Paper in 3 Sentences\n\n * The authors pinpoint that existing fine-grained quantization is slowed down by many float conversions during inference.\n * They propose “Integer Scale,” which transforms per-group float scales to integer scales (with a power-of-two amplifier), avoiding most of the costly conversions.\n * As a result, they achieve up to 2× speed boost while retaining nearly the same quantization accuracy on a broad range of large language models.\n\n----------------------------------------\n\n\n# 19. [121] Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers\n\n\n# 1. Introduction\n\nDeep neural networks (DNNs) have achieved remarkable success in fields such as image processing, object detection, and natural language processing.\n\nHowever, training these models requires extensive floating-point (FP) operations, leading to high memory, compute, and energy costs.\n\nDNN quantization has been explored as a solution to this problem, primarily focusing on inference quantization (e.g., BWN, XNOR-Net).\n\nRecent advancements extend quantization to training, but existing methods still leave parts of the computation in high-precision floating-point (e.g., FP8, FP16) or do not quantize Batch Normalization (BN).\n\nThe major challenges in achieving full quantization include:\n\n * Incomplete quantization : Some parts of the model remain in floating-point, limiting memory and compute savings.\n * Unquantized Batch Normalization (BN) : BN is critical for training stability but is often left in floating-point.\n * Lack of a unified low-bit training framework : No existing method successfully trains large-scale models with only low-bit integer operations.\n\nThis work introduces WAGEUBN , a unified INT8 training framework that quantizes all major operations, including:\n\n * Weights (W)\n * Activations (A)\n * Gradients (G)\n * Errors (E)\n * Updates (U)\n * Batch Normalization (BN)\n * Momentum optimizer\n\n\n# 2. Related Work\n\n# 2.1. Inference Quantization\n\nInference quantization aims to reduce the memory and compute cost of DNN inference by converting FP operations to bit-wise integer operations. Some key works include:\n\n * BWN (Binary Weight Networks) : Quantizes only weights to {-1,1}.\n * XNOR-Net : Quantizes both weights and activations to binary values.\n * ADMM-based Quantization : Compresses models via alternating direction method of multipliers.\n * FP8/INT16-based Methods : Reduce bit-width to maintain accuracy.\n\nHowever, inference quantization only focuses on the forward pass and does not address the backward pass needed for training.\n\n# 2.2. Training Quantization\n\nTraining quantization extends quantization to the backward pass (gradients and updates). Key approaches include:\n\n * DoReFa-Net : Uses low-bit activations, weights, and gradients but retains FP elements.\n * MP (Mixed Precision) : Uses FP16 for training but is not purely integer-based.\n * FP8 Training : Reduces training precision to FP8 but retains FP operations in BN.\n * QBP2 : Uses 8-bit INT for weights, activations, and errors, but gradients remain FP.\n * WAGE : The most complete prior work, quantizing W, A, G, E, and U, but it lacks BN layers, making it unsuitable for large-scale DNNs.\n\n\n\n\n# 3. WAGEUBN Framework\n\n# 3.1. Key Contributions\n\n * Fully quantizes all training data paths (W, A, G, E, U, BN, and Momentum).\n * Introduces three custom quantization functions for different training components.\n * Quantizes Batch Normalization (BN) for the first time.\n * Applies INT8 quantization to large-scale networks like ResNet on ImageNet.\n\n# 3.2. Straight-Through Estimator (STE)\n\nQuantization introduces a non-differentiability problem , making gradient updates challenging.\n\nSTE is used to approximate gradients during backpropagation: $$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial x_q}$$\n\nThis method allows training to proceed despite non-differentiability.\n\n# 3.3. Quantization Functions\n\nThe framework introduces three quantization functions tailored for different data types:\n\n 1. Direct Quantization : Used for weights, activations, and BN parameters. $$Q(x, k) = \\frac{\\text{round}(x \\cdot 2^{k-1})}{2^{k-1}}$$\n\nThis function approximates floating-point values to the nearest discrete integer.\n\n 2. Constant Quantization : Used for gradients to ensure a fixed update bit-width. $$CQ(x) = \\frac{Sd(x)}{2^{k-1}}$$\n\nThis function scales data dynamically to avoid excessive precision loss.\n\n 3. Shift Quantization : Used for errors , which are typically small-magnitude values. $$SQ(x, k) = R(x) \\cdot \\text{clip}(Q(\\text{Norm}(x), k), -1, 1)$$\n\nThis function ensures errors maintain a meaningful range.\n\n# 3.4. Quantized Training Steps\n\nThe framework quantizes the entire training process:\n\n * Forward Pass:\n   * Quantizes inputs, applies INT8 convolutions, quantizes BN, and applies INT8 activation.\n * Backward Pass:\n   * Uses INT8 gradients and error propagation.\n   * Applies INT8 momentum optimization.\n   * Uses fixed-point updates for weight adjustments.\n * Momentum Quantization:\n   * Conventional optimizers like Adam/Momentum use floating-point accumulations.\n   * WAGEUBN constrains them to fixed-point INT8.\n\n\n# 4. Results\n\n# 4.1. Accuracy Evaluation\n\nThe framework was tested on ResNet18/34/50 with ImageNet. Two versions were evaluated:\n\n 1. Full 8-bit INT : All computations use 8-bit integers.\n 2. 16-bit E2 Variant : Uses 16-bit error gradients to improve convergence. | Model | Vanilla FP32 | WAGEUBN (16-bit E2) | WAGEUBN (Full 8-bit) | | --- | --- | --- | --- | | ResNet18 | 68.70% | 67.40% | 64.79% | | ResNet34 | 71.99% | 68.50% | 67.63% | | ResNet50 | 74.66% | 69.07% | 67.95% |\n\n * Accuracy loss is minimal (~3-5% top-1 accuracy).\n * 16-bit E2 improves accuracy over pure 8-bit training.\n * Comparable accuracy to FP8-based methods.\n\n# 4.2. Efficiency Gains\n\nWAGEUBN significantly reduces hardware overhead compared to FP32:\n\nPRECISION   COMPUTE SPEEDUP   POWER REDUCTION   CIRCUIT AREA REDUCTION\nINT8        3× - 9×           10× - 30×         9× - 30×\nFP8         0.73×             0.31×             0.4×\nFP16        0.58×             0.4×              0.4×\n\n * Memory is reduced by 4× .\n * Computation is up to 9× faster .\n * Power usage is up to 30× lower .\n\n\n# 5. Analysis\n\n * Batch Size Sensitivity : WAGEUBN works best with batch sizes ≥32. Smaller batches lead to higher accuracy loss.\n * Error Gradient Sensitivity : The 8-bit Flag QE2 method significantly improves accuracy over simple 8-bit quantization.\n * Quantization Impact :\n   * BN and Errors are the most sensitive to precision loss .\n   * Weights and activations are more robust to INT8 constraints .\n\n\n# 6. Conclusion WAGEUBN is the first complete INT8 quantization framework for training large-scale DNNs. It achieves:\n\n * End-to-end INT8 training (including BN and optimizers).\n * Competitive accuracy with significant hardware efficiency improvements .\n * Potential for online learning on energy-efficient devices . Future work includes specialized hardware architectures to fully exploit WAGEUBN’s benefits.\n\n\n# Three-Sentence Summary\n\nWAGEUBN is a fully quantized INT8 training framework for large-scale deep learning, covering all data paths (W, A, G, E, U, BN, and Momentum).\n\nBy introducing novel quantization functions and INT8 batch normalization, it reduces memory by 4×, accelerates computation by up to 9×, and cuts power usage by 30× , while achieving comparable accuracy to FP-based models\n\nThis work establishes a scalable and efficient approach for energy-efficient AI hardware and online learning .\n\n----------------------------------------\n\n\n# 20. [381] I-BERT: Integer-only BERT Quantization\n\n\n# Challenges Addressed\n\nInefficiency of Transformer-Based Models: BERT and RoBERTa achieve high accuracy but have high memory, latency, and power costs, making them difficult to deploy on edge devices and data centers.\n\nLimitations of Previous Quantization Approaches: Prior Transformer quantization methods rely on floating-point arithmetic, preventing efficient execution on integer-only hardware like ARM Cortex-M processors and Turing Tensor Cores.\n\nDifficulty in Handling Non-Linear Functions: Existing integer-only quantization techniques are mainly designed for CNNs with piece-wise linear functions like ReLU. Transformers use complex non-linear functions (GELU, Softmax, LayerNorm), which are hard to process using integer arithmetic without significant accuracy loss.\n\n\n\n\n# Solution - I-BERT Approach:\n\nInteger-Only Approximation for Non-Linear Functions:\n\n * GELU: Approximated using a second-order polynomial (i-GELU), avoiding floating-point computation while maintaining accuracy.\n * Softmax: Transformed into a stable integer-friendly form using logarithm and bit-shift operations (i-exp).\n * LayerNorm: Computed using an integer-only square root algorithm.\n\nEnd-to-End Integer Execution:\n\n * MatMul and embeddings are computed using INT8 multiplication and INT32 accumulation.\n * Non-linear operations (GELU, Softmax, LayerNorm) are applied directly to INT32 values and re-quantized to INT8.\n * The entire inference process remains in integer arithmetic without dequantization.\n\n\n# Results and Impact:\n\n * Accuracy: I-BERT achieves comparable or slightly better accuracy than FP32 models on the GLUE benchmark, with an improvement of 0.3 (Base) and 0.5 (Large) in average score.\n * Efficiency: I-BERT provides 2.4× – 4.0× speedup in inference compared to FP32 on NVIDIA T4 GPUs.\n * Deployment Feasibility: Eliminates floating-point dependency, making it ideal for deployment on integer-only hardware like ARM Cortex-M processors and specialized accelerators.\n\n\n# Key Takeaways in 3 Sentences\n\nI-BERT introduces a novel integer-only quantization method for BERT, eliminating floating-point operations and enabling efficient deployment on integer-only hardware.\n\nBy approximating non-linear functions like GELU, Softmax, and LayerNorm with polynomial and integer arithmetic, it maintains high accuracy while significantly improving inference speed.\n\nEvaluation on the GLUE benchmark and hardware tests demonstrate that I-BERT achieves up to 4× speedup while maintaining or slightly improving accuracy over FP32 models.\n\n----------------------------------------\n\n\n# 21. [904] LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale\n\nThe paper introduces LLM.int8(), a quantization method enabling 8-bit matrix multiplication for large transformers without degrading performance.\n\nTraditional 8-bit quantization methods struggle with large-scale models due to systematic outlier features that disrupt quantization precision beyond 6.7B parameters.\n\nTo overcome this, LLM.int8() combines vector-wise quantization (which assigns separate normalization constants per inner product) and mixed-precision decomposition, where outlier dimensions are computed in 16-bit while 99.9% of values remain in 8-bit.\n\nThis approach allows large-scale transformers like OPT-175B and BLOOM to run on a single consumer GPU without accuracy loss.\n\n\n\n\n# Key findings include:\n\n * Emergent outliers: Beyond 6.7B parameters, certain feature dimensions dominate transformer attention and predictive performance, requiring higher precision.\n * Quantization challenge: Existing methods fail due to these outliers, as they occupy only 0.1% of the data but significantly impact accuracy.\n\n\n# LLM.int8 solution\n\nBy isolating outliers in 16-bit operations while keeping most computations in 8-bit, the method retains full-precision inference while cutting memory usage by half.\n\nThe study empirically validates that LLM.int8() maintains 16-bit accuracy across models up to 175B parameters, making LLMs more accessible and practical. The method is open-sourced and integrated with Hugging Face Transformers.\n\n\n# Three-Sentence Key Takeaways\n\nLLM.int8() enables performance-preserving 8-bit quantization for transformers up to 175B parameters by combining vector-wise quantization and mixed-precision decomposition to handle emergent large-magnitude features.\n\nThese systematic outliers, appearing beyond 6.7B parameters, disrupt standard 8-bit quantization but can be isolated in 16-bit precision while keeping over 99.9% of computations in 8-bit, achieving a 2× memory reduction.\n\nThis allows massive models like OPT-175B and BLOOM to run efficiently on consumer GPUs, making large-scale LLM inference more accessible.\n\n----------------------------------------\n\n\n# 22.[637] Training Deep Neural Networks with 8-bit Floating Point Numbers\n\n\n# 1. Introduction\n\nThe paper addresses the challenge of training deep neural networks (DNNs) with reduced precision floating point numbers, specifically using 8-bit floating point (FP8).\n\nWhile inference has been successfully performed with low precision (as low as 2–4 bits), training has traditionally required at least 16-bit precision due to gradient fidelity concerns.\n\nThe paper proposes novel techniques that allow DNN training using FP8 without accuracy loss, promising 2–4× improvements in energy efficiency and throughput.\n\n\n# 2. Challenges in Low-Precision Training\n\nThree major challenges arise when reducing DNN training precision:\n\n * Loss of accuracy when all operands (weights, activations, errors, gradients) are quantized to 8 bits.\n * Reduced accumulation precision (moving from FP32 to FP16) significantly impacts convergence.\n * Weight updates in 16-bit may degrade accuracy unless managed properly.\n\n\n\n\n# 3. Proposed Solutions\n\nThe paper introduces several key innovations:\n\n * Custom FP8 Format: A new FP8 format (1-bit sign, 5-bit exponent, 2-bit mantissa) that effectively represents DNN parameters.\n * Chunk-Based Accumulation: Breaking matrix multiplications into small chunks before accumulation to prevent truncation errors.\n\n\n\n * Floating Point Stochastic Rounding: A rounding method that retains small numerical details to prevent loss of information.\n * Mixed-Precision Computations: Using FP8 for most computations while keeping critical accumulations and weight updates in FP16.\n\n\n# 4. Experimental Results\n\nThe proposed FP8 training method was tested on various models, including ResNet18/50, AlexNet, and CIFAR10-CNN. Results show:\n\n * No significant accuracy loss compared to FP32.\n * Memory savings: Model sizes were reduced by ~50%.\n * Energy-efficient hardware implementation: A prototype chip demonstrated 2–4× efficiency gains.\n\n\n# 5. Discussion\n\n * The first and last layers of DNNs require higher precision (FP16) for better stability.\n * Gradient accumulation in FP16 must be carefully handled with chunk-based summation.\n * Stochastic rounding outperforms nearest rounding in weight updates.\n\n\n# 6. Conclusion\n\nThe paper successfully demonstrates DNN training with FP8 while maintaining accuracy.\n\nThe combination of chunk-based accumulation, stochastic rounding, and mixed-precision strategies opens the door for more efficient hardware training platforms.\n\n\n# Key Takeaways in 3 Sentences\n\nThe paper proposes training deep neural networks using 8-bit floating point numbers by introducing a custom FP8 format, chunk-based accumulation, and stochastic rounding to prevent accuracy loss.\n\nExperiments across multiple models (ResNet, AlexNet) confirm that FP8 training achieves the same accuracy as FP32 while significantly reducing memory and energy costs.\n\nThese innovations enable future hardware architectures with 2–4× improved efficiency, paving the way for practical low-precision DNN training.\n\n----------------------------------------\n\n\n# [270] Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks\n\nThis paper addresses the challenge of training and deploying deep neural networks (DNNs) in 8-bit precision while preserving accuracy.\n\nThe authors note that existing 8-bit floating point (FP8) approaches (e.g., the (1−5-2) format) can train some large-capacity networks like ResNet but struggle with compact models such as MobileNet or NLP-oriented architectures like Transformer.\n\n\n\nBy analyzing the distinct needs of the forward pass (weights and activations) and the backward pass (gradients), they propose a hybrid FP8 (HFP8) scheme:\n\n * Forward Pass\n   * Uses the (1−4−3) format (1 sign bit, 4 exponent bits, 3 mantissa bits) with an exponent bias of 4.\n   * This format gives extra mantissa precision to minimize quantization noise for forward activations and weights.\n   * The exponent bias ensures coverage for smaller values often present in weight distributions.\n * Backward Pass\n   * Uses (1−5−2) precision (1 sign bit, 5 exponent bits, 2 mantissa bits) for gradients and errors.\n   * The wider exponent range captures large gradient magnitudes during backpropagation.\n   * An auto-adjusted loss-scaling technique further prevents underflow or overflow of gradients in 8-bit.\n\nThe paper’s theoretical framework highlights how too few mantissa bits in forward activations create large quantization mismatches, which degrade training.\n\nConversely, reducing exponent bits for backward gradients risks clamping large gradient ranges. By splitting the formats, HFP8 balances mantissa fidelity in the forward path and dynamic range in the backward path.\n\n> The underlying reason for this choice is that forward and backward passes have different optimal balances between range and precision. While tensors in the forward pass prefer higher precision (and lower representational error), gradients in the backward pass prefer a higher dynamic range. We describe our HFP8 training methodology where weights and activations adopt the (1-4-3) format (bias=4) while tensors used in backpropagation continue to be represented using the (1-5-2) format (in combination with loss scaling techniques pioneered by [28]).\n\nAdditional contributions include\n\n * Post-Training FP8 Inference\n   Directly quantizing a full-precision (FP32) model to FP8 often loses accuracy.\n   The authors show that re-tuning Batch Normalization statistics with a small subset of unlabeled training data recovers most or all of the lost accuracy.\n   Furthermore, depthwise convolutions (common in MobileNet-like architectures) and some layers with very small or large magnitudes may remain in slightly higher precision (FP16) to avoid accuracy drops.\n   \n * SoftMax Optimization\n   For tasks like machine translation or speech recognition that rely on large final FC layers, subtracting the max logits before quantization lets the SoftMax function operate in FP8 without collapsing scores.\n   \n * Distributed Training\n   The paper modifies standard ring-based all-reduce to exchange compressed 8-bit weights.\n   A local “round-off residual” (stored in higher precision) ensures these 8-bit weight updates remain stable and converge reliably.\n   The authors’ hardware simulations indicate that adopting HFP8 for both computation and communication can significantly reduce training time.\n   \n\nComprehensive experiments on ImageNet (ResNet, MobileNet, DenseNet, AlexNet), WMT14 En-De (Transformer), large-scale speech (LSTM), and object detection (SSD-Lite, Mask R-CNN) confirm that HFP8 preserves baseline accuracies within a fraction of a percentage point.\n\nThis demonstrates that 8-bit floating point is feasible not only for specialized cases but for a broad range of DNN workloads, offering substantial speedups and energy savings.\n\nThree-Sentence Key Summary\n\n * This paper proposes a hybrid 8-bit floating point (HFP8) approach that uses two different 8-bit formats to match the distinct precision requirements of forward (weights/activations) and backward (gradients/errors) passes.\n * It introduces practical techniques—like Batch Normalization re-tuning, depthwise convolution in FP16, and round-off residual updates—to ensure stable training and inference in 8 bits across diverse models.\n * As a result, HFP8 achieves near-baseline accuracy on tasks ranging from image classification and object detection to machine translation and speech recognition.\n\n----------------------------------------\n\n\n# 28. [Read Y2025 NVIDIA] Coat: Compressing Optimizer States and Activation for Memory-Efficient FP8 Training 👍 👍 👍 👍 👍\n\nPrior studies do not tackle the memory consumption of activations and still leave the optimizer’s second-order momentum in higher precision.\n\nCOAT significantly reduces the overall memory footprint by quantizing optimizer states and activations into FP8.\n\nFor optimizer states, we observe that FP8 format’s representation range is under-utilized when quantizing them, as illustrated in Figure 2(a).\n\nTo address this, we introduce a novel Dynamic Range Expansion method which adjusts the distribution of optimizer states to better fit within the FP8 range, thereby minimizing quantization error.\n\nFor activations, we propose Mixed-Granularity Activation Quantization to achieve efficient and accurate quantization. We apply fine-grained quantization to non-linear layers and apply per-tensor quantization to linear layers.\n\nPer-tensor quantization for matrix multiplications is more efficient and better suited for TensorCores, while fine-grained quantization helps maintain accuracy.\n\nThese two approaches tackle high memory consumption while ensuring minimal performance degradation.\n\n\n\n> Prior studies: FP8-LM (Peng et al., 2023) quantizes the first-order momentum to FP8 while leaving second-order momentum in FP16, which limits the overall memory savings. (Fishman et al., 2024) finds that second-order momentum is more sensitive to quantization, and proposes to quantize it using E5M2 format.\n\n> In addition to quantization, there are other approaches that aim to reduce the memory footprint of the optimizer states (Shazeer & Stern, 2018; Anil et al., 2019; Chen et al., 2024; Zhao et al., 2024), such as low-rank decomposition and optimizer simplification that only store the first-order momentum. These methods are orthogonal to our approach.\n\nTo perform optimizer state quantization, we adopt per-group quantization for both first-order and second-order momentum, following previous works (Dettmers et al., 2021; Li et al., 2024).\n\nEvery consecutive G element forms a group (G is defined as the group size), and each group is quantized independently with its own statistics.\n\nOptimizer states are stored in FP8 precision, while its scaling factor is stored in BF16.\n\nthe dynamic range for first-order momentum is typically less than 1e4, and for second-order momentum, it is usually less than 1e1—both far below the available range of FP8. That is what they mean by second-order momentum is underutilized FP8.\n\n\n\nWe propose to vary the quantization granularity across different layers to balance precision and efficiency in a mixed-granularity manner.\n\nFor non-linear layers, VS-Quant (Dai et al., 2021) or PerBlock Quant (Xi et al., 2024) methods are well-suited due to their fine-grained and precise nature.\n\nFor linear layers, we apply per-tensor quantization to maximize the performance of Tensor Cores.\n\nWe observe that quantizing the input of layernorm across multiple token axes is detrimental to accuracy.\n\nAs illustrated in Figure 4(a), when the number of elements that share a scaling factor is fixed, the quantization error increases significantly when quantization is performed across the token axis.\n\nTherefore instead of using per-block quantization with block size B × B as proposed in (Xi et al., 2024), we propose to use per-group quantization with group size 1 × G, where G = B2 to keep the granularity the same.\n\nThis approach enhances the accuracy of non-linear layers while maintaining efficiency. Our precise FP8 precision flow is visualized in Figure 1(a), where we display the full precision flow for a Llama-style decoder layer, both forward and backward pass.\n\nNote the red color in following graph, shows BF16.\n\n',normalizedContent:' 1.  [unread 2] a comprehensive study on quantization techniques for large language models :+1：\n 2.  [unread 11] a comprehensive evaluation of quantization strategies for large language models 👍 👍\n 3.  [139] understanding and overcoming the challenges of efficient transformer quantization\n 4.  [43] integer or floating point? new outlooks for low-bit quantization on large language models\n 5.  [26] fp8-lm: training fp8 large language models 👍 👍 👍 from microsoft\n     this is discussed that first order in adam could be fp8, but second order in adam should be fp16. and the weight should be reserved a copy of fp32 full-precision or fp16 with tensor scaling..\n 6.  [139] fp8 formats for deep learning\n 7.  [41] with shared microexponents, a little shifting goes a long way 👍 from meta, microsoft 👍\n 8.  [34] stable and low-precision training for large-scale vision-language models 👍\n     mentioned in deepseek paper mixed precision training section. not read yet.\n 9.  [y2025] paretoq: scaling laws in extremely low-bit llm quantization\n 10. [47] microscaling data formats for deep learning\n 11. [unread 2 y2024] to fp8 and back again: quantifying the effects of reducing precision on llm training stability\n 12. [145] efficient 8-bit quantization of transformer neural machine language translation model\n 13. [unread 27]dynamic memory compression: retrofitting llms for accelerated inference\n 14. [45] pb-llm: partially binarized large language models\n 15. [55] qllm: accurate and efficient low-bitwidth quantization for large language models\n 16. [33] spinquant: llm quantization with learned rotations\n 17. [35] fp8 versus int8 for efficient deep learning inference\n 18. [y2024]integer scale: a free lunch for faster fine-grained quantization of llms\n 19. [434] integer quantization for deep learning inference: principles and empirical evaluation\n 20. [73] fp8 quantization: the power of the exponent\n 21. [unread 121] training high-performance and large-scale deep neural networks with full 8-bit integers\n 22. [381] i-bert: integer-only bert quantization\n 23. [637] training deep neural networks with 8-bit floating point numbers\n 24. [904] llm.int8(): 8-bit matrix multiplication for transformers at scale\n 25. [unread 2568] dorefa-net: training low bitwidth convolutional neural networks with low bitwidth gradients 👍\n 26. [239] model accuracy and runtime tradeoff in distributed deep learning: a systematic study 👍\n 27. [270] hybrid 8-bit floating point (hfp8) training and inference for deep neural networks\n     this is the paper where e5m2 and e4m3\n 28. [read y2025 nvidia] coat: compressing optimizer states and activation for memory-efficient fp8 training 👍 👍 👍 👍 👍\n     source code: https://github.com/nvlabs/coat\n\noutlier\n\n1.[106] outlier suppression+: accurate quantization of large language models by equivalent and effective shifting and scaling\n\n----------------------------------------\n\n\n# 1. [139] understanding and overcoming the challenges of efficient transformer quantization\n\n\n# problems\n\nthe problems encountered when attempting to quantize transformer models like bert, as highlighted in the paper:\n\n * high dynamic range of activations\n   * the authors observed that the activation tensors in transformer models, particularly in the residual connections, exhibit a very high dynamic range.\n   * this means that the values within these tensors vary significantly in magnitude.\n   * quantization, especially when using low-bit fixed-point formats (like 8-bit integer), struggles to accurately represent such a wide range of values.\n   * trying to capture both very small and very large values within the limited range of a low-bit format inevitably leads to significant quantization errors.\n * presence of structured outliers\n   * the authors identified the presence of structured outliers within the activation tensors, particularly in the residual connections after the feed-forward network (ffn).\n   * these outliers are not random; they appear to be correlated with specific input tokens and embedding dimensions.\n   * further analysis revealed that these outliers influence the attention mechanism in the subsequent layers, encouraging the model to attend to specific tokens like the separator token ([sep]).\n\n> in bert-like models, an intermediate hidden activation tensor x has a shape (b, t, d), where b is the batch size, t is the sequence length, and d is the number of embedding dimensions (d = 768 for bert-base, devlin et al. 2019). in the following figure, you could tell the x-axis is in dimension of 768.\n\n\n\n * while this attention behavior might be beneficial for the model\'s performance, the outliers that cause it also create challenges for quantization.\n * sensitivity to quantization noise\n   * different parts of the transformer model exhibit varying sensitivities to the noise introduced by quantization.\n   * some components, like the residual connections and certain attention heads, are particularly sensitive, and quantizing them aggressively can lead to a significant drop in accuracy.\n   * this sensitivity necessitates careful consideration of which parts of the model to quantize and at what bit-width.\n\n\n\n\n# solutions\n\nsolutions proposed in the paper:\n\n * mixed-precision ptq\n   \n   * the authors observed that different parts of the bert model have varying sensitivities to quantization noise.\n   * to address this, they proposed using a higher bit-width (16-bit) for the more sensitive activation tensors, particularly the residual sum after the feed-forward network (ffn).\n   * this higher bit-width allows for more accurate representation of both the ffn\'s input and output, minimizing potential errors.\n   * additionally, they explored using low-bit (2-4) quantization for weights and token embeddings, which can significantly reduce model size without much accuracy loss.\n\n * per-embedding-group ptq\n   \n   * the authors identified that outliers in the activation tensors primarily reside in a few specific embedding dimensions.\n   * to address this, they proposed a novel per-embedding-group (peg) quantization scheme, where distinct quantization parameters are used for different embedding dimensions or groups of dimensions.\n   * this method involves splitting the activation tensor into groups along the embedding dimension and applying separate quantization parameters to each group.\n   * to optimize this process, they introduced a range-based permutation step to ensure that all outliers are grouped together.\n   * this approach effectively handles outliers without significantly increasing computational overhead.\n\n * quantization-aware training (qat)\n   \n   * the authors also explored qat, where the model is trained with simulated quantization operations.\n   * this allows the model to adapt to the quantization noise, leading to improved performance compared to post-training quantization.\n   * during qat, they used learnable ranges for both weights and activations, further enhancing the model\'s adaptability to quantization.\n\n----------------------------------------\n\n\n# 2.[43] integer or floating point? new outlooks for low-bit quantization on large language models\n\n\n# key takeaways in three sentences\n\n 1. the study demonstrates that low-bit floating-point formats, particularly fp8, provide superior quantization accuracy for llms compared to int8 , with comparable hardware efficiency at 8-bit precision.\n 2. the mixture of formats quantization (mofq) approach optimally selects between int and fp per layer , improving accuracy without increasing computational overhead.\n 3. mofq achieves state-of-the-art results in both w4-only and w8a8 quantization , outperforming existing methods like gptq, awq, llm.int8(), and smoothquant while maintaining efficient inference speed .\n\n\n# abstract\n\nthe study finds that optimal quantization formats vary across layers in llms, leading to the mixture of formats quantization (mofq) approach, which selects the best format per layer.\nmofq achieves superior or comparable performance to current quantization methods for weight-only (w-only) and weight-activation (wa) quantization without additional hardware overhead.\n\n\n# introduction\n\nquantization minimizes llms\' size and inference costs, with prior work focusing on low-bit integer formats.\nhowever, as llms grow, integer quantization becomes less effective, requiring optimizations or alternatives. low-bit floating-point formats have emerged as viable alternatives, with fp8 already supported in nvidia’s h100 gpus.\n\nthe study:\n\n 1. compares int and fp formats in terms of hardware efficiency and quantization error.\n 2. proposes mixture of formats quantization (mofq) , selecting the best format per layer.\n 3. implements an inference system for w-only quantization, maintaining performance parity with int-based systems.\n\n\n# background and related works\n\ninteger vs. floating-point formats\n\n * integer (int) : uniformly distributed values.\n * floating-point (fp) : non-uniform distribution, allowing higher precision for small values but reduced precision for large values.\n * hardware efficiency : fp operations typically cost more than int, but at 8-bit, fp8 and int8 mac (multiply-accumulate) units have nearly identical area costs.\n\npost-training quantization (ptq) for llms\n\ntwo main ptq strategies:\n\n 1. weight-only (w-only) quantization: applies to weights only, e.g., w4a16.\n 2. weight-activation (wa) quantization: quantizes both weights and activations, e.g., w8a8.\n\nstate-of-the-art (sota) methods:\n\n * llm.int8() : uses mixed precision (int8+fp16).\n * smoothquant : redistributes quantization difficulty from activations to weights.\n * gptq & awq : use second-order information and pre-scaling techniques to improve quantization.\n\n\n# comparative analysis of int and fp formats\n\na. hardware cost of int vs. fp mac units\n\n * at 8-bit precision, fp8 and int8 macs require nearly the same hardware area , aligning with h100 gpu capabilities.\n\nb. quantization error comparison\n\n 1. 4-bit weight-only (w4) quantization (llama-65b model):\n\n * 🔥 some layers perform better with int4, while others favor fp4, indicating layer-dependent format preference .\n\n\n\n 2. 8-bit weight-activation (w8a8) quantization :\n\n * 🔥weights : int8 generally has lower quantization error.\n * 🔥activations : fp8 shows better robustness for dynamic activation tensors.\n * best choice: int8 for weights, fp8 for activations—but hardware constraints necessitate using the same format per layer.\n\n\n\n\n# exploiting int and fp complementarity\n\na. improved low-bit fp4 format\n\n * ieee floating-point format reserves exponent values for nan and inf.\n * reallocating nan & inf to normalized numbers improves fp4 precision by 35%.\n\nb. mixture of formats quantization (mofq)\n\n * selects the best quantization format (int or fp) per layer based on quantization error.\n * works for both w-only and wa quantization .\n * algorithm : iterates through layers, computes quantization error for int and fp, and selects the lower-error format.\n\nc. low-bit w-only inference system\n\n * int4 and fp4 require conversion to fp16 before computation due to fp16 activations.\n * w8a8 quantization : fp16 activations are converted to fp8 or int8 based on next-layer format selection.\n * no additional hardware overhead for fp-based or mofq-based inference compared to int-based quantization.\n\n\n\n\n# conclusion\n\n * comparative study : int and fp formats have complementary strengths.\n * key finding : fp8 and int8 mac units have similar hardware costs at low-bit quantization .\n * mofq method :\n   * selects the best quantization format per layer .\n   * achieves state-of-the-art accuracy in w4-only and w8a8 quantization.\n   * no additional inference latency or hardware overhead .\n\n----------------------------------------\n\n\n# 5. [26] fp8-lm: training fp8 large language models\n\n\n# abstract\n\nthe paper explores fp8 low-bit data formats for training large language models (llms), significantly reducing memory usage and computation costs while maintaining accuracy.\n\nthe authors introduce an fp8 automatic mixed-precision training framework with three levels of fp8 utilization, improving mixed-precision and distributed parallel training.\n\nkey results show that training the gpt-175b model on an h100 gpu platform using fp8:\n\n * reduces memory usage by 39%\n * speeds up training by 75% compared to bf16 (megatron-lm)\n * outperforms nvidia transformer engine by 37%\n\nthe fp8 training methodology is generalizable to fine-tuning, instruction tuning, and reinforcement learning with human feedback (rlhf). the framework is open-sourced at aka.ms/ms.amp .\n\n\n# 1. introduction\n\nllms have demonstrated exceptional performance in various domains but are extremely expensive to train.\nthe cost of training models like gpt-3 (175b) or palm (540b) is enormous, requiring thousands of gpus or tpus .\nlow-precision training is a promising solution as it:\n\n * increases speed\n * reduces memory usage\n * minimizes communication overhead\n\nmost existing frameworks, such as megatron-lm, metaseq, and colossal-ai , use fp32, fp16, or bf16 mixed-precision training , but fp8 offers significant efficiency gains :\n\n * 2× speed-up\n * 50%-75% memory and communication savings\n\n# challenges of fp8 training\n\n 1. data underflow/overflow issues due to fp8’s limited dynamic range.\n 2. numerical instabilities and divergence during training.\n\n# proposed fp8 mixed-precision framework\n\n * introduces three levels of fp8 utilization (gradients, optimizer states, and distributed learning).\n * uses precision decoupling and automatic scaling to mitigate numerical instability.\n * achieves 29%-39% memory savings and 63%-65% communication cost reductions .\n\n> the resulting fp8 mixed-precision networks are more efficient than their pure fp16 counterparts, but a network that is in full int8 is expected to be significantly more efficient yet.\n\n\n# 2. fp8 llm training\n\n# 2.1 fp8 gradient and all-reduce communication\n\n * traditional mixed-precision training uses fp16/fp32 for gradients , leading to high communication costs.\n * applying fp8 directly to gradients results in loss of accuracy due to underflow/overflow.\n * the paper proposes an automatic scaling technique to adapt scaling factors dynamically, preventing numerical instability.\n\n# 2.2 fp8 optimizer\n\n * the adam optimizer typically consumes 16 bytes per parameter due to high-precision storage of gradients and optimizer states.\n * the proposed fp8 optimizer stores:\n   * fp8 first-order moment\n   * fp16 master weights (with tensor scaling)\n   * fp16 second-order moment\n * this reduces memory consumption from 16 bytes to 6 bytes per parameter (2.6× savings).\n\n> my main takeaway is that direction of gradient matters, instead of magnitude.\n\n\n\n 1. fp8 master weight induces performance degradation (see the #2a vs. #3 lines in fig. 8), while fp16 can maintain accuracy as fp32 (see #2a vs. #0 and #1) but requiring using tensor scaling. it reveals that the master weight is precision-sensitive. this can be attributed to the master weight’s role in updating weights, which tend to exhibit small magnitudes, necessitating high precision to maintain accuracy.\n 2. the training loss of bf16 master weight is slightly higher than that of fp16 with a scaling factor because bf16 has fewer mantissa bits, resulting in lower precision (see #2a vs. #2b).\n 3. ❗ the second-order gradient moment is more precision-sensitive than the first-order one, because the ❗square calculation is easy to cause underflow and leads to accuracy degradation. utilizing fp8 for the second-order gradient moment can lead to divergent training loss (see the #4 dot in fig. 8).\n\nplease notice that fp8 #4 is diverged, not shown in the figure.\n\n\n\n# 2.3 fp8 distributed parallel training\n\n * tensor parallelism : uses fp8 for weight and activation tensors , reducing compute and communication overhead.\n * sequence parallelism : converts activation tensors to fp8 before communication , reducing costs.\n * zero (zero redundancy optimizer) support : distributes full tensors across devices while preserving fp8 scaling factors .\n\n\n# 3. experimentation\n\n# 3.1 experimental setup\n\n * training dataset : collected from commoncrawl, the pile, c4, openwebtext, wikipedia, redpajama , and other curated sources.\n * model configuration : uses a decoder-only transformer architecture (like gpt-3), with rope embeddings and flash attention .\n\n# 3.2 main results\n\n# model performance\n\n * loss curves of fp8 models match bf16 models , confirming accuracy preservation\n * zero-shot evaluations on lambada, hellaswag, boolq, piqa, copa show comparable performance between fp8 and bf16 .\n * fine-tuning (sft & rlhf) : fp8 achieves:\n   * 27% faster fine-tuning\n   * 32% reduction in model weight memory\n   * 62% optimizer state memory savings\n\nsystem performance\n\n * memory reduction : fp8 achieves 28%-39% lower memory usage than bf16.\n * training speed improvement :\n   * 75% faster training for gpt-175b\n   * 37% faster than nvidia transformer engine\n * communication efficiency :\n   * 63%-65% reduction in weight gradient communication\n   * 34% lower activation-related communication costs\n\n# 3.3 ablation study\n\n * gradient scaling : automatic scaling reduces underflow/overflow errors , improving training stability.\n * optimizer precision :\n   * fp16 master weights outperform fp8 master weights in accuracy preservation.\n   * fp8 first-order gradient moment is viable, but fp8 second-order moment leads to divergence.\n * parallelism optimization :\n   * fp8 sequence and tensor parallelism reduce communication costs by 34% .\n   * fp8 zero maintains a balanced gpu memory load while saving memory.\n\n\n# 4. related work\n\n * mixed-precision training : prior work focused on fp16/bf16 , but fp8 remains underexplored .\n * low-precision llm training :\n   * opt, bloom, gopher, chinchilla used bf16 for better numerical stability.\n   * fp8 support was limited before nvidia hopper gpus.\n   * this work provides the first systematic fp8 training framework for pre-training and fine-tuning llms .\n\n\n# 5. conclusion\n\n * introduces a new fp8 mixed-precision training framework with automatic scaling and precision decoupling .\n * achieves significant reductions in memory, compute, and communication costs .\n * maintains model accuracy across gpt models from 125m to 175b parameters .\n * demonstrates versatility in pre-training, instruction tuning, and rlhf.\n * future work includes scaling to even larger models, training multi-modal models, and deploying fp8 llms on edge devices.\n\n\n# key summary in 3 sentences*\n\nthis paper introduces an fp8 mixed-precision training framework that reduces memory consumption by 39% , speeds up training by 75% , and outperforms nvidia transformer engine by 37% while maintaining llm accuracy.\nthe framework uses automatic scaling and precision decoupling to stabilize training, supports fp8 optimizers and distributed training , and generalizes to fine-tuning and reinforcement learning with human feedback (rlhf) .\nthese findings establish fp8 as the next-generation precision format for training llms , significantly lowering costs while preserving model performance.\n\n----------------------------------------\n\n\n# 6. [139] fp8 formats for deep learning\n\n\n# 1. introduction\n\ndeep learning models require increasing computational resources, necessitating lower-precision formats for efficiency.\n\nfp8 is a natural evolution from fp16 and bf16, reducing compute and memory costs while maintaining accuracy comparable to fp16 .\n\n\n# key contributions:\n\n * two fp8 formats:\n   \n   * e4m3 : 4-bit exponent, 3-bit mantissa (for weights and activations).\n   * e5m2 : 5-bit exponent, 2-bit mantissa (for gradients).\n\n * training and inference in fp8 match fp16/bf16 accuracy across cnns, rnns, and transformers.\n\n * post-training quantization (ptq) using fp8 outperforms int8 while preserving model accuracy.\n\n\n# 2. aspects of fp8 usage in deep learning\n\n * fp8 computations will be performed in higher precision (fp16/fp32) , with final results cast back to fp8.\n * scaling factors are applied to optimize fp8 precision , similar to loss-scaling in fp16 mixed precision .\n * handling of special values (nans, infs) is modified in e4m3 to increase dynamic range.\n\n\n# 3. fp8 binary interchange format\n\n\n\nfp8 includes two encodings :\n\n * e4m3 :\n   \n   * used for weights and activations .\n   * no representation for infinities (max value: 448 ).\n   * single nan representation to extend range .\n\n * e5m2 :\n   \n   * used for gradients .\n   * standard ieee-like format , supporting nans and infinities .\n   * larger range (up to 57,344 ).\n\n\n\n# 3.1 special value representations\n\n * e4m3 removes infinities and limits nans to a single pattern , extending its dynamic range .\n * e5m2 follows ieee-754 , allowing straightforward conversion from fp16 .\n\n# 3.2 exponent bias\n\n * e4m3 bias = 7, e5m2 bias = 15 (matching ieee-style representation).\n * some models require per-tensor scaling rather than a fixed exponent bias (figure 2).\n\n\n# 4. empirical results\n\n# 4.1 training\n\n * fp8 training achieves accuracy comparable to fp16/bf16 across cnns, rnns, and transformers.\n * image classification :\n   * fp8 accuracy is within statistical variation of fp16 for most cnns (resnet, mobilenet, vgg, etc.).\n * language translation :\n   * fp8 bleu scores match fp16 for transformer and gnmt models.\n * nlp models (table 4, figure 1) :\n   * gpt models (126m to 175b parameters) trained in fp8 match fp16 in perplexity .\n\n# 4.2 inference\n\n * fp8 post-training quantization (ptq) outperforms int8 , retaining full precision accuracy for:\n   * bert (f1 score on squad).\n   * gpt-3 (perplexity on wikitext103).\n * fp8-trained models require no additional quantization steps , simplifying deployment.\n\n# 4.3 per-tensor scaling\n\n * fixed exponent bias fails when additional tensors (e.g., residuals) are stored in fp8.\n * per-tensor scaling maintains accuracy , making fp8 viable for expanded use beyond gemms .\n\n\n# 5. conclusions\n\n * fp8 formats (e4m3, e5m2) efficiently reduce training and inference costs while maintaining accuracy .\n * fp8 training is on par with fp16/bf16 , without hyperparameter changes.\n * fp8 simplifies inference by eliminating the need for quantization-aware training (qat) required for int8.\n * future work : expanding fp8 usage to more tensor types and operations beyond matrix multiplications.\n\n\n# key takeaways in three sentences\n\nfp8 formats (e4m3 for weights/activations, e5m2 for gradients) significantly reduce computation and memory overhead while maintaining accuracy equivalent to fp16/bf16 across cnns, rnns, and transformer models.\n\npost-training quantization (ptq) with fp8 outperforms int8 , allowing for simpler and more effective deployment of trained models. the study validates fp8 training up to 175b parameters , proving its scalability for large-scale deep learning applications.\n\n----------------------------------------\n\n\n# 8. [34] stable and low-precision training for large-scale vision-language models\n\n\n# 1. introduction and motivation\n\nthe paper addresses two crucial bottlenecks in large-scale vision-language model training:\n\n * speed – how to train massive models efficiently despite ballooning compute costs.\n * stability – how to avoid “loss spikes” that can degrade or derail training.\n\nthey target contrastive language-image pre-training (clip) models, which fuse image and text encoders to learn aligned representations.\n\nthese models often involve hundreds of millions to billions of parameters and require large-scale data (like laion).\n\nby improving efficiency (low-precision arithmetic) and stability (modified optimizer), the authors hope to sustain further scaling of multimodal architectures.\n\n\n# 2. eight-bit (8-bit) training\n\n# 2.1 preliminaries & related work\n\n16-bit operations (float16/bfloat16) are currently standard for large-scale training.\n\nbfloat16 has broader exponent range than float16, making it more robust at scale, but native hardware support for bfloat16 exists mainly on tpus or newer gpus.\n\nmore recently, hardware support for int8 and float8 is emerging (nvidia ampere has int8, hopper adds float8), enabling potential speedups over 16-bit if quantization can be made accurate and stable.\n\nhowever, at the 1b-parameter scale of clip vit-huge, quantization can introduce noise that leads to sizable accuracy drops unless carefully managed.\n\n\n\n# 2.2 the switchback method\n\n 1. core idea: in a standard linear layer, three matrix multiplications occur: one in the forward pass and two in backprop (input-grad and weight-grad).\n\nswitchback executes only the forward pass and input-grad steps in 8-bit, while reverting the weight-grad step to higher precision (16-bit).\n\n 2. why this matters:\n\n * the weight-grad multiply has an extremely large inner dimension (batch size × sequence length), making it more prone to quantization noise.\n * by keeping that specific multiplication in 16-bit, switchback avoids excessive noise accumulation.\n\n 3. implementation details:\n\n * uses row-wise quantization for activation/gradient matrices and tensor-wise quantization for weights.\n * relies on triton kernels for fused “quantize + transpose” in the backward pass, minimizing overhead from transposing or storing scaled values.\n * extends easily to float8 (fp8) by simulating exact fp8 values with 16-bit computations.\n\n 4. experimental setup:\n\n * trains multiple clip vit variants (base, large, huge) on laion-2b with a short schedule (20k iterations, patch dropout 0.5).\n * evaluates zero-shot imagenet accuracy using standard clip prompts.\n\n\n\n 5. results & speedups:\n\n * accuracy: for clip vit-huge (∼1b params), switchback stays within 0.1% of the bfloat16 baseline, whereas a simpler baseline (llm.int8()) underperforms by nearly 6%.\n * speed: overall training speeds up by 13–25%. profiling shows int8 multiplications run ~2× faster than fp16, and quantization overhead is small (<25% of total time in these matmuls).\n\n# 2.3 float8 via reduced feature magnitudes\n\n * while switchback also supports float8 (fp8), the authors observe that a purely tensor-wise fp8 baseline diverges at large scale.\n * key intervention: zero-initialized layer-scale – each transformer block’s residual path is initially scaled to 0, then learned. this keeps features from growing too large, which can disrupt fp8’s narrower numeric range.\n\nzero-initialized layer-scale\n\n\n\n * with zero-init layer-scale, vit-large trains successfully in simulated fp8 (still a small gap from bfloat16, but no divergence).\n\n\n# 3. stability of training\n\n# 3.1 problem: loss spikes\n\n * training very large clip models can suffer from fast, sporadic spikes in loss—episodes that slow or degrade convergence.\n * prior methods like gradient clipping or large warmup sometimes help, but for clip vit-huge, the authors pinpoint a new mechanism: the adamw second-moment estimator for the patch embedding layer can become out-of-date.\n\n# 3.2 observations about loss spikes\n\nloss spike\n\n\n\n\n\n * more spikes at larger scale: scaling model size, batch size, or learning rate all exacerbate these spikes.\n * adamw β2 influence: reducing β2 from its typical 0.999 helps avoid some spikes but can hamper overall performance.\n * root cause: ❗ when gradients suddenly increase for the early patch-embedding parameters, adamw’s exponentially moving average of squared gradients (the second-moment) underestimates the true magnitude, resulting in an oversized update that triggers a transient loss explosion.\n\nstuck-in-the-past sceneario: historicall gradient magnitude have been historically samll, thus, some parameter is larger for those parameters. suddenly, if those parameter receives a large gradient signal, the update can be catastrophycally big.\n\n# 3.3 stableadamw optimizer\n\n * based on ideas from adafactor, the authors propose update clipping:\n   * compute an “rms ratio” = √(e[g² / ema(g²)]) per layer.\n   * if rms ratio > 1, automatically downscale the layer’s effective learning rate for that iteration.\n * stableadamw = adamw + update clipping. it successfully removes or mitigates these spikes better than gradient norm clipping, sustaining higher accuracy at large scale.\n\n\n\n# 3.4 connection to low-precision training\n\n * loss spikes can produce extremely large activations and gradients, risking numerical issues (inf/nan) when using narrower floating-point types.\n * the authors show that stabilizing these early-layer explosions goes hand in hand with ensuring safe low-precision training.\n * they also adopt a specialized fp16 “loss scalar” strategy (per-layer updates, fixed scaling) to keep ephemeral spikes in one layer from knocking down the entire network’s scalar.\n\n\n# 4. conclusion and future directions\n\n * switchback plus stableadamw demonstrates both faster (13–25% speedups) and steadier (fewer loss spikes) training for large clip vits, outperforming or matching bfloat16 baselines.\n * the authors note limitations: they simulate float8 (rather than use actual fp8 hardware), do not fully explore alternate initialization or width-scaling schemes, and their training runs are shortened for cost reasons. nonetheless, the methods and open-source code (including triton kernels) lay groundwork for future improvement in quantizing and stabilizing very large multimodal models.\n\n\n# three-sentence core summary\n\nthey propose switchback, an 8-bit training approach that leaves weight-gradient computations in higher precision, yielding 13–25% speed improvements over standard 16-bit training while closely matching accuracy.\n\nthey further show that loss spikes in large clip models stem from outdated second-moment estimations in adamw, and propose stableadamw (adamw plus adaptive update clipping) as a fix.\n\nby combining these, the paper demonstrates fast, stable, and memory-efficient low-precision training on billion-parameter vision-language models.\n\n----------------------------------------\n\n\n# 9. [y2025] paretoq: scaling laws in extremely low-bit llm quantization\n\n\n# abstract\n\nthe study introduces paretoq , a unified framework for evaluating extremely low-bit quantization (1-bit to 4-bit) in large language models (llms).\nit identifies a learning transition between 2-bit and 3-bit models, with 3-bit and higher retaining pre-trained distributions, while 2-bit and below undergo drastic representation changes.\nby optimizing training strategies and quantization functions, paretoq achieves state-of-the-art (sota) performance across multiple bit-widths.\nnotably, a ternary (1.58-bit) 600m model surpasses a previous 3b ternary model , using only one-fifth of the parameters.\nthe study finds that 2-bit quantization offers a superior balance between accuracy, model size, and hardware efficiency compared to 4-bit and binary quantization.\n\n\n# 1. introduction\n\nas models scale, lower-precision computation is gaining traction due to memory savings and computational efficiency .\nprior studies suggest conflicting optimal quantization levels (1.58-bit, 2-bit, 4-bit, etc.), but no unified framework existed to systematically compare their effectiveness.\n\n\n# key questions:\n\n * what is the optimal trade-off between bit-width and model size?\n * how does quantization impact scaling laws in low-bit settings?\n * what training strategies and quantization functions yield pareto-optimal results ?\n\n\n# paretoq approach:\n\n * incorporates five key dimensions : model size (n ), token count (d ), quantization precision (p ), training strategy (s ), and quantization function (f ).\n * identifies bit-specific training schemes and quantization functions .\n * establishes that binary quantization significantly degrades accuracy , while ternary (1.58-bit), 2-bit, and 3-bit quantization match or exceed 4-bit performance .\n\n\n# 2. a better qat scheduling strategy for extreme low-bit llms\n\n# 2.1 training budget allocation\n\n * post-training quantization (ptq) is easier to implement but performs poorly below 4-bit.\n * quantization-aware training (qat) integrates quantization during training, improving low-bit performance .\n * optimal budget split: 90% full-precision training, 10% qat fine-tuning .\n * finding-1: qat fine-tuning outperforms both ptq and qat from scratch , achieving the best trade-off between accuracy and efficiency.\n\n# 2.2 fine-tuning characteristics\n\n * fine-tuning improves accuracy across all bit-widths , including binary and ternary models.\n * lower-bit models (≤2-bit) require more fine-tuning (30b tokens), while 3-bit and 4-bit saturate at 10b tokens .\n * finding-2: bit-width transition effect:\n   * 3-bit & 4-bit recover near full precision with fine-tuning .\n   * 1-bit to 2-bit undergo substantial weight transformations , requiring more tokens.\n   * qat serves as "compensation" for 3-bit+ but "reconstruction" for ≤2-bit models.\n\n\n# 3. a hitchhiker’s guide to quantization method choices\n\n# 3.1 trade-offs in low-bit quantization\n\n 1. range clipping : lower-bit quantization suffers from outlier effects, requiring range clipping or learnable scales .\n 2. quantization grids :\n\n * binary & ternary require balanced levels.\n * 2-bit prefers symmetric distribution*.\n * 3-bit and 4-bit benefit from including "0" in quantization levels.\n\n# 3.2 introducing paretoq\n\n * combines the best quantization functions per bit-width :\n   * 1-bit : elastic binarization.\n   * 1.58-bit, 2-bit : stretched elastic quant (seq) .\n   * 3-bit, 4-bit : learned step size quantization (lsq) .\n * finding-3: no single best function for all bit-widths. learnable range settings outperform fixed statistical methods , especially for sub-4-bit quantization .\n\n\n# 4. pareto-optimality of extremely low-bit llms\n\n# 4.1 accuracy-compression trade-off\n\n * ternary (1.58-bit), 2-bit, and 3-bit outperform 4-bit in accuracy-size efficiency.\n * 2-bit and ternary quantization sit on the pareto frontier.\n\n# 4.2 hardware constraints\n\n * ternary (1.58-bit) appears efficient but is difficult to implement due to indexing overhead.\n * 2-bit is more hardware-friendly** due to **simpler storage and arithmetic operations.\n\n# 4.3 accuracy-speed trade-off\n\n * 2-bit achieves higher speed at the same accuracy as 4-bit.\n * 2-bit kernels are significantly faster than 4-bit kernels in large matrix multiplications.\n\n\n# 6. related work\n\n * early quantization research focused on 8-bit and 4-bit llms .\n * recent sub-4-bit research explored ternary, 2-bit, and 1-bit models , but lacked a unified comparison framework .\n * paretoq is the first study to systematically compare sub-4-bit quantization schemes .\n\n7. conclusions\n\n * paretoq unifies training and quantization schemes across five bit-widths.\n * ternary (1.58-bit), 2-bit, and 3-bit quantization outperform 4-bit in the accuracy-size trade-off.\n * 2-bit is the most practical choice due to its hardware efficiency.\n * first framework that ensures fair comparisons across different quantization methods.\n\nkey takeaways (3 sentences)\n\n 1. paretoq introduces a unified framework for extreme low-bit quantization, identifying 2-bit as the most efficient trade-off between accuracy, memory, and computational speed.\n 2. a clear transition exists between 2-bit and 3-bit models, where lower-bit quantization requires substantial fine-tuning to compensate for drastic weight transformations.\n 3. with its optimized quantization functions and training schemes, paretoq achieves state-of-the-art performance across all bit-widths, surpassing previous specialized methods.\n\n----------------------------------------\n\n\n# 10. [47] microscaling data formats for deep learning\n\n\n\n\n# introduction\n\nthe rapid advancement of deep learning models has led to increased computational and storage costs.\none approach to mitigating these costs is reducing bit-width precision in data formats, moving beyond traditional fp32 to lower-bit formats such as fp16, bfloat16, fp8, and int8.\nhowever, per-tensor scaling in low-bit-width formats struggles with dynamic range limitations.\nmicroscaling (mx) data formats introduce per-block scaling factors to enhance efficiency, maintain accuracy, and ease deployment in ai hardware.\n\n\n# microscaling (mx) data formats\n\nmx formats encode numerical values in fixed-size blocks , where each block consists of:\n\n * a shared scaling factor (x)\n * multiple narrow bit-width elements (pi)\n\nthis approach extends the dynamic range beyond what per-tensor scaling allows, making sub-8-bit computations feasible.\nmx formats are hardware-efficient while minimizing accuracy loss and ensuring seamless adoption in existing ai frameworks.\n\n\n# concrete mx formats\n\nmx formats are categorized based on block size, scale format, and element bit-width.\n\nformat   block size   scale data format   scale bits   element format    element bit-width\nmxfp8    32           e8m0                8            fp8 (e4m3/e5m2)   8\nmxfp6    32           e8m0                8            fp6 (e2m3/e3m2)   6\nmxfp4    32           e8m0                8            fp4 (e2m1)        4\nmxint8   32           e8m0                8            int8              8\n\n\n# scalar float to mx format conversion\n\nto convert floating-point data to an mx format, the shared scaling factor (x) is computed based on the largest absolute value in a block.\neach element is then normalized using x and quantized to the desired format. the conversion follows a quantization algorithm that:\n\n 1. determines the scaling exponent from the maximum value in the block.\n 2. computes x as a power of two .\n 3. quantizes elements (pi) based on x .\n\n\n# compute flow and training pipeline\n\nfor deep learning workloads, dot-product operations (e.g., matrix multiplication, convolutions) are performed in mx formats , while non-dot operations (e.g., activations, normalization, residual add) remain in higher-precision formats like fp32 or bfloat16 .\ntraining involves keeping a master fp32 copy of weights while performing compute-intensive operations in mx formats.\nquantization-aware fine-tuning is often required for best accuracy, particularly for lower-bit formats like mxfp6 and mxfp4.\n\nexperimental results inference mx data formats were tested across language models, vision transformers, speech recognition, and recommendation models.\n\ndirect-cast inference (no fine-tuning)\n\n * mxint8 performs nearly identically to fp32 across all tasks, making it a drop-in replacement .\n * mxfp8 and mxfp6 maintain good accuracy , but mxfp6 requires fine-tuning for best results.\n * mxfp4 suffers from significant accuracy loss , especially in complex models.\n\npost-training quantization (ptq) with error diffusion\n\n * error diffusion (similar to gpfq-based post-training quantization ) helps recover accuracy.\n * mxfp6 achieves results close to fp32 after error diffusion.\n * mxfp4 remains significantly worse than fp32, limiting its practical use in inference .\n\nfinetuned inference\n\n * mxfp6 achieves fp32-level accuracy after fine-tuning .\n * mxfp4 improves slightly but still lags behind .\n * mxint8 continues to serve as the most effective low-friction alternative to fp32 .\n\ngenerative model inference (gpt-3, llama)\n\n * mxint8 closely matches fp32 performance on gpt-3 and llama.\n * mxfp6 and mxfp8 perform well in most tasks , but some degradation is observed in complex benchmarks.\n * mxfp4 shows noticeable loss , especially in zero-shot settings .\n\ntraining with mx formats for the first time, mx formats enable sub-8-bit training of large-scale transformers with minimal accuracy loss.\n\ntraining with mxfp6\n\n * mxfp6 (e3m2) trains models with no accuracy drop compared to fp32 .\n * this represents the first demonstration of 6-bit training for large transformer models without modifications to the training recipe.\n * hyperparameters remain unchanged from fp32 training, making mxfp6 a practical choice.\n\ntraining with mxfp4 + mxfp6\n\n * mxfp4 weights combined with mxfp6 activations/gradients yield slightly worse performance but remain viable for training.\n * loss curves show only a minor increase in training loss , proving feasibility.\n\n\n# conclusion\n\nmicroscaling (mx) data formats introduce per-block scaling to reduce bit-width while maintaining high accuracy, hardware efficiency, and seamless integration .\n\n\n# key findings:\n\n 1. mxint8 is an effective drop-in replacement for fp32 inference.\n 2. mxfp6 enables sub-8-bit inference and training with minimal accuracy loss.\n 3. mxfp4 combined with mxfp6 remains viable for training but suffers in inference.\n 4. first-ever demonstration of training large generative models using 6-bit weights, activations, and gradients** . mx formats offer a compelling path toward lower precision deep learning without sacrificing model quality.\n\n\n\n\n# three-sentence summary\n\nmicroscaling (mx) data formats introduce per-block scaling factors , improving the efficiency and accuracy of sub-8-bit deep learning computations.\n\nmxint8 serves as a near-lossless drop-in replacement for fp32 inference , while mxfp6 enables large-scale deep learning models to be trained with sub-8-bit precision without altering training recipes .\n\nthe results demonstrate that mx formats significantly reduce computational and storage costs while maintaining model performance , making them a strong alternative to traditional floating-point formats.\n\n----------------------------------------\n\n\n# 14. [33] spinquant: llm quantization with learned rotations\n\n\n# abstract & introduction\n\nspinquant is a novel method for post-training quantization (ptq) of large language models (llms) that uses learned rotation matrices to enhance quantization accuracy while maintaining full-precision outputs.\n\ntraditional ptq struggles with outliers, which widen quantization ranges and reduce effective bit usage. instead of relying on random rotations, spinquant learns rotation matrices that optimize quantization performance.\n\nthe method significantly reduces accuracy gaps in 4-bit quantization across weights, activations, and kv-cache, outperforming previous quantization methods like llm-qat, smoothquant, and quarot.\n\nspecifically, spinquant reduces the zero-shot reasoning accuracy gap on llama-2 7b from 12.1 to 1.6 points, surpassing llm-qat by 19.1 points and smoothquant by 25.0 points.\n\n\n# motivation & outlier reduction\n\nquantization reduces memory and computation costs but suffers from outliers, which stretch the value distribution.\n\nexisting methods like mixed-precision quantization and weight-activation trade-offs attempt to mitigate this but remain suboptimal.\n\nspinquant rotates activation and weight matrices to remove outliers, making them more gaussian-like, thus improving quantization efficiency.\n\nthe paper demonstrates that some random rotations lead to better results than others, with a 13-point accuracy difference in zero-shot reasoning tasks.\n\n\n# methodology\n\n\n\nthe method optimizes rotation matrices using cayley sgd, which maintains orthonormality while minimizing the quantized network loss.\n\nthis process enhances quantization robustness, significantly outperforming random rotations.\n\n\n\n\n# experiments & results\n\nexperiments were conducted on llama-2 (7b/13b/70b), llama-3 (1b/3b/8b), and mistral-7b, evaluated across zero-shot reasoning tasks and perplexity on wikitext2.\n\n\n# key findings:\n\nspinquantₙₒₕₐₑd effectively closes the quantization gap in w4a8kv8 settings, making it comparable to quip# and omniquant.\n\nspinquantₕₐₑd achieves 64.0 average accuracy in w4a4kv4, reducing the accuracy gap to full precision to 2.9 points (llm-qat had a 22-point gap).\n\nagainst quarot, spinquant improves accuracy by up to 28.6 points in extreme quantization settings.\n\nspeed analysis shows that spinquantₕₐₑd incurs only an 8% latency overhead, making it a practical approach.\n\n\n# conclusions\n\nspinquant is the first ptq method to optimize learned rotation matrices, offering:\n\n * significant improvements over existing quantization techniques by reducing outliers.\n * state-of-the-art performance in 4-bit quantization settings.\n * compatibility with advanced weight quantization methods like gptq.\n\nits learned rotations make quantized llms more robust, reducing the performance gap to full-precision models, making low-bit llm inference practical.\n\n\n# three-sentence summary\n\nspinquant introduces learned rotation matrices to mitigate outliers in weight and activation distributions, improving llm quantization efficiency.\n\nusing cayley sgd optimization, it significantly reduces the accuracy gap in 4-bit quantization, outperforming smoothquant, llm-qat, and quarot.\n\nexperiments on llama-2, llama-3, and mistral-7b show state-of-the-art quantization performance, with spinquant narrowing the accuracy gap to full precision by up to 45.1% relative to quarot.\n\n----------------------------------------\n\n\n# 17. [35] fp8 versus int8 for efficient deep learning inference\n\nthis paperexplores the efficiency and accuracy of using fp8 (8-bit floating point) and int8 (8-bit integer) formats for deep learning inference, particularly on edge devices.\n\nthe key points and findings of the paper are summarized as follows:\n\n\n# 1. introduction and motivation\n\nthe paper discusses the growing interest in using fp8 for neural network training, especially with nvidia\'s introduction of fp8 in their hopper architecture gpus.\n\nwhile fp8 is being considered for training, the paper focuses on its implications for efficient inference on edge devices, where int8 is commonly used due to its efficiency.\n\nthe authors question whether training networks in fp8 and deploying them in the same format could bypass the need for quantization, which is currently required when converting fp32/fp16 models to int8.\n\n\n# 2. hardware considerations\n\nthe paper argues that fp8 is less efficient than int8 in terms of hardware area and energy consumption. fp8 requires 50% more area and energy compared to int8, making it less suitable for efficient inference.\n\nfloating-point operations are inherently more complex and costly in hardware compared to integer operations, especially when considering the need for floating-point accumulators.\n\nthe authors highlight that fp8 implementations often involve mixed precision (e.g., fp16 for activations), which can lead to inefficiencies, particularly in networks with large activation tensors.\n\n\n# 3. accuracy comparison\n\nthe paper provides a theoretical and empirical comparison of fp8 and int8 formats in terms of network accuracy.\n\nthe key difference between fp8 and int8 lies in their ability to handle outliers. fp8, with its exponent bits, can better represent outliers, while int8 is more efficient for well-behaved, gaussian-like distributions.\n\n> the only significant difference between the two formats is in their ability to capture outliers.\n\n❗ in post-training quantization (ptq), int8 generally performs better for networks without significant outliers, while fp8-e4 (4 exponent bits) is better for networks with outliers, such as ransformers.\n\n❗ in quantization-aware training (qat), int8 often outperforms fp8, as training can reduce the impact of outliers, making int8 more accurate and efficient.\n\n\n# 4. transformer networks\n\ntransformer networks, particularly bert, exhibit significant outliers in certain layers, making fp8-e4 more accurate in ptq settings.\n\nhowever, the paper argues that these outlier issues can be mitigated with techniques like mixed precision (w8a16) or quantization-aware training, allowing int8 to achieve similar accuracy without the hardware inefficiencies of fp8.\n\nfp8-e4 vs fp8-e5\n\n❗ it’s all about the outliers. if a distribution has very significant outliers, the fp8-e4/fp8-e5 format is more accurate.\n\nfor well-behaved networks without many outliers, the int8 format is significantly more accurate in the ptq setting than fp8-e4 and fp8-e5.\n\nwe also see that fp8-e5 is never the best format for inference; even for the transformer layers with significant outliers, the fp8-e4 format is better.\n\nfor some computer vision networks, int8 is better; for some networks with significant outliers, the fp8-e4/fp8-e5 formats are better.\n\npurely taking these results into account, the fp8-e4 format looks comparatively worse than fp8-e2 and fp8-e3.\n\ncombining these findings with the hardware implementation costs, the fp8-e4 format itself looks like it is a worse choice than its lower-exponent bit brethren, which are both cheaper hardware-wise and more accurate.\n\nif anything, the fp8-e3 format stands out positively in this accuracy analysis compared to other fp formats.\n\nhowever, deepseek fp8 use e4m4 for both forward and backwards computation.\n\nqat versus ptq\n\n\n\none surprising trend is that the int8 results improve more than their ptq baseline than their fp8 counterparts.\n\nthere is a good reason for this; again, it’s about the outliers.\n\nwhen performing qat, outliers are clipped and do not receive a gradient.\n\nthe clipped weights/activations then tend towards the clipping threshold due to regularization.\n\nbut most importantly, with the outliers clipped, the network learns weights that still perform well despite the outliers being removed.\n\nwhen training the quantization parameters with a method like lsq (esser et al. (2020)), the network can learn to make the ranges smaller so as to find a better trade-off between the clipping and quantization errors.\n\nthe smaller the range, the more accurate your quantized representation will be.\n\nthis is especially the case for int8, where the sensitivity to the quantization ranges is much larger than the floating-point formats with more exponent bits that are naturally more resistant to outliers.\n\nthis way, the int8 format benefits significantly more from qat than the fp formats.\n\nqat\n\nnumerical representation chosen for training (e.g., fp8-e4) does not dictate how the distributions of weights and activations form.\n\ninstead, these distributions are primarily shaped by the overall training process, including factors like regularization, optimizer settings, and initialization.\n\nqat transformer\n\nthere are significant outliers in the summation going into the layer-norm in some of the fully connected modules, especially in the final layers of the network.\n\nthese outliers force the attention mechanism in the next layer to pay attention to some meaningless tokens – like sentence separator tokens, periods, or commas – that occur in the text, causing that specific token to not update significantly.\n\nsimply clipping these outlier reduces accuracy significantly.\n\nonly a few layers are the best in fp8-e4. the other layers find a lower mse error with the fp8-e2 and fp8-e3 formats.\n\nin the most naive ptq setting, the fp8-e4 format performs better than int8.\n\ncomparison to other work\n\n> the problems with the quantization of gradients put forth in the paper are well-known and have been addressed in many works in the past. (sun et al. (2019); gupta et al. (2015)). many works have shown the necessity of stochastic rounding and proper range setting for the backward pass that alleviate these issues and make int8 for gradients work just as well (sun et al. (2019). what is this?\n\ntransformer models can be executed in int8 as well, both with ptq and qat.\n\nfinally, the only comparison with the int8 format comes in the form of comparing transformer-based language models in the ptq setting.\n\nhowever, as argued, these problems are easily fixable for transformer networks, making them able to execute entirely in int8 or in mixed precision with a small number of activations in int16.\n\n\n# 5. comparison to other work\n\nthe authors compare their findings with other works, such as those from nvidia, arm, and intel, and graphcore, which also explore fp8 for training.\n\nthey find that their results are consistent with these works but provide a more comprehensive comparison between fp8 and int8.\n\nthe paper highlights that other works often omit critical comparisons, such as the hardware efficiency of fp8 versus int8, and the impact of mixed precision on inference performance.\n\nseveral works have shown that even in the int8 ptq setting you can get back your original accuracy with any of several possible tricks\n\n\n# 6. fp8 to int8 conversion\n\nthe paper explores the feasibility of converting fp8-trained networks to int8.\n\nfor networks without significant outliers, the conversion is straightforward and can even improve accuracy.\n\nfor networks with outliers, such as transformers, the conversion to int8 may degrade accuracy, but this can be mitigated with quantization-aware training.\n\n\n# 7. int quantization paradigm\n\nthe authors advocate for the use of int8 and int4 formats for efficient inference, as they offer better hardware efficiency and accuracy for most networks.\n\nthey present a quantization paradigm where int16 is used for high accuracy, int8 for most networks, and int4 for further efficiency, especially in weight-bound networks like large language models.\n\n\n# 8. conclusion\n\nthe paper concludes that fp8 is not a suitable replacement for int8 in efficient deep learning inference.\n\nwhile fp8 can handle outliers better in certain cases, the hardware inefficiencies and the availability of techniques to mitigate outlier issues in int8 make it a less attractive option.\n\nthe authors recommend using int8 and int4 formats for efficient on-device inference, as they provide the best trade-off between accuracy and efficiency.\n\n\n# key takeaways:\n\nfp8 is less efficient than int8 in terms of hardware area and energy consumption.\n\nint8 is more accurate for most networks, especially after quantization-aware training, which can reduce the impact of outliers.\n\nfp8 is only beneficial in specific cases, such as transformer networks with significant outliers, but these issues can be addressed with int8 using mixed precision or qat.\n\nint4 and int8 are recommended for efficient inference, offering a better balance of accuracy and hardware efficiency compared to fp8.\n\noverall, the paper provides a comprehensive analysis of the trade-offs between fp8 and int8, concluding that int8 remains the superior choice for efficient deep learning inference on edge devices.\n\n----------------------------------------\n\n\n# 16.\n\n----------------------------------------\n\n\n# 17. [434] integer quantization for deep learning inference: principles and empirical evaluation\n\nthis paper explores integer quantization techniques for deep learning inference, which help reduce model size and improve computational efficiency by leveraging high-throughput integer math pipelines.\n\nthe authors provide a mathematical foundation for different quantization choices and evaluate their empirical performance across multiple deep learning models spanning vision, speech, and language domains.\n\nthe study focuses on 8-bit integer quantization, demonstrating that accuracy loss can be minimized to within 1% of the floating-point baseline, even for challenging models like mobilenets and bert-large.\n\n\n# key highlights:\n\nbenefits of integer quantization\n\n * integer operations offer up to 16× speedup over fp32 on nvidia turing gpus.\n * smaller word sizes reduce memory bandwidth pressure and improve cache utilization.\n\n\n# quantization methods\n\n * affine quantization: uses a scale factor and zero-point but adds computational overhead.\n * scale quantization: more efficient as it avoids additional computations by using only a scale factor.\n\n\n# post training quantization (ptq) vs. quantization-aware training (qat)\n\n * ptq quantizes weights and activations after training but may lead to accuracy degradation for some models.\n * qat fine-tunes the network with quantization effects included, yielding better results, especially for mobilenets and transformers.\n\nper-channel quantization for weights and per-tensor quantization for activations is recommended for accuracy and performance.\n\n\n# calibration methods\n\nmax, entropy, and percentile-based (99.9%-99.999%) calibrations are tested.\n\nno single calibration is best for all networks; entropy and 99.99% percentile provide optimal accuracy for most models.\n\n\n# optimizations to reduce accuracy loss\n\n * partial quantization: leaves sensitive layers in floating point, improving accuracy while retaining performance benefits.\n * learning quantization parameters: fine-tuning the quantization ranges (pact method) slightly improves accuracy but is not always necessary for int8 quantization.\n\nelu clipping: for bert, modifying the gelu activation range significantly enhances post-training quantization results.\n\n\n# recommended workflow\n\n * start with ptq using max, entropy, and percentile calibrations.\n * if accuracy loss is high, use partial quantization to leave sensitive layers unquantized.\n * if further accuracy recovery is needed, perform qat with pre-determined best calibration.\n\n\n# final 3-sentence summary:\n\nthis paper presents a quantization workflow for deep learning inference, balancing speed and accuracy by leveraging int8 quantization with scale quantization for weights and activations.\n\nit demonstrates that a combination of post-training quantization, partial quantization, and quantization-aware training ensures that accuracy remains within 1% of floating-point models, even for challenging architectures like mobilenets and bert.\n\nthe proposed methods significantly improve inference efficiency, making integer quantization a practical approach for deploying neural networks on hardware accelerators.\n\n----------------------------------------\n\n\n# 20.[73] fp8 quantization: the power of the exponent\n\nthis paper investigates fp8 quantization for neural network inference, comparing it with traditional int8 quantization.\n\nthe authors analyze different fp8 format configurations, focusing on the trade-off between exponent and mantissa bits.\n\nthe study includes theoretical analysis, post-training quantization (ptq), and quantization-aware training (qat) across various deep learning models.\n\nkey findings indicate that fp8 outperforms int8 in post-training quantization, particularly for networks with activation outliers, such as transformers.\n\nhowever, in quantization-aware training, the accuracy difference between int8 and fp8 diminishes as the network learns to adapt to the quantization scheme.\n\n\n# key takeaways:\n\n * fp8 vs. int8: fp8 offers an additional degree of freedom through exponent bits, which helps manage outliers better than int8, improving post-training quantization accuracy.\n * optimal format selection: the choice of fp8 format (e.g., 5m2e vs. 4m3e) depends on the severity of outliers in a network, with higher exponent bits benefiting models with significant activation outliers.\n * qat reduces differences: quantization-aware training helps networks adapt to the quantization format, making int8 and fp8 perform similarly in trained models.\n\n\n# three-sentence summary\n\nthis paper explores fp8 quantization and its advantages over int8, showing that fp8 provides better accuracy in post-training quantization due to its ability to handle activation outliers more effectively.\n\nthrough analytical and empirical studies, the authors determine that the optimal fp8 configuration depends on the balance between exponent and mantissa bits, with higher exponent bits benefiting networks with larger outliers.\n\nhowever, in quantization-aware training, the differences between fp8 and int8 diminish as the network learns to optimize within the quantization scheme.\n\n> we validated the fp8 format for many networks in a post-training quantization setting, showing that generally for neural networks the 5m2e and 4m3e fp8 format works the best, and that for networks with more outliers like transformers increasing the number of exponent bits works best.\n\n----------------------------------------\n\n\n# 18. [y2024]integer scale: a free lunch for faster fine-grained quantization of llms\n\n\n# overview\n\nthis paper deals with post-training quantization for large language models (llms), focusing on a technique called fine-grained quantization. while fine-grained quantization (group-wise scales) generally retains better accuracy at lower bit-widths than coarse-grained methods, it comes with a major drawback in slower inference. the authors identify that a large part of this overhead comes from repeatedly converting integer multiplication results into floating-point to apply each group’s “float scale.” they therefore propose an integer scale approach that removes most of these costly conversions by storing and applying each group’s scale in integer form.\n\n\n# key motivation\n\n 1. accuracy vs. speed trade-off:\n\n * fine-grained quantization (grouping weights or activations into small blocks) achieves higher accuracy than coarse-grained approaches, especially when pushing to lower bit-widths like w4a8 or w4a4 (weights 4-bit, activations 8 or 4-bit).\n * however, the group-wise scaling factors in floating-point form introduce numerous conversions (e.g., int32 → fp32 → multiply by fp32 scale → back to int32), which dramatically reduce inference speed.\n\n 2. integer scale as a “free lunch”:\n\n * by transforming each group’s floating-point scale into an integer, the paper’s method avoids a lot of repetitive float conversions within the gemm kernels.\n * the authors call it a “free lunch” because it requires no additional calibration or training—just a straightforward conversion of float scales into integer form with a suitable integer “amplifier.”\n\n\n# proposed method: integer scale\n\n 1. integer amplifier (α):\n\n * since the learned scales from fine-grained quantization typically lie between 0 and 1, the authors multiply them by a power-of-two factor (e.g., 2^10 = 1024) to map them into integer space without introducing big rounding errors.\n * they then store these integer scales and use simple integer multiply operations inside the kernel, followed by one final float conversion for the output—rather than one float conversion per group multiplication.\n\n 2. modified gemm kernel:\n\n * the new kernel accumulates partial sums at integer precision and multiplies by the integer scales on the fly.\n * this leads to far fewer float conversions and yields a noticeable speedup (often well above 1.5× faster) relative to the standard float-scale approach.\n\n 3. stability & overflow:\n\n * the paper shows that choosing an amplifier (like 2^10) is sufficient to preserve accuracy while keeping integer operations safely in int32 range—there are no overflow issues in tested scenarios.\n * the authors also show that per-layer “heuristic search” for the best amplifier can be replaced by a single fixed power-of-two amplifier (e.g., 1024), with negligible difference in accuracy.\n\n\n\n\n# results\n\n 1. accuracy retention:\n\n * on tasks like lambada, c4, wikitext-2, and common sense qa, the integer scale method achieves nearly the same (or slightly better) accuracy compared to float-scale baselines (gptq, awq, omniquant) under the same fine-grained quantization setup.\n * it also helps quantize more challenging models like llama-3 and mixtral 8×7b at 4-bit weights without the large drop typical of naive methods.\n\n 2. speed gains:\n\n * the central benefit is that integer scale speeds up fine-grained quantization kernels significantly.\n * in many experiments, the method achieves:\n   * up to 1.85× acceleration vs. float-scale fine-grained kernels,\n   * up to 1.17× faster than certain well-optimized w4a16 kernels (e.g., marlin),\n   * up to 2.13× faster than the model’s fp16 baseline (depending on batch size and model).\n\n 3. comparison to other libraries:\n\n * compared to qserve (another w4a8 system) or marlin’s w4a16, the integer scale approach often yields higher or comparable throughput, especially under typical inference batch sizes (32, 64, 128).\n\n\n# practical takeaways\n\n * plug-and-play for existing methods: integer scale is designed to be a drop-in replacement for float scales in fine-grained quantization. you keep the same group sizes, bit widths, etc., just multiply the group scales by a power-of-two to store them as integers.\n * no extra training: unlike some other quantization techniques that require knowledge distillation or fine-tuning, integer scale only changes how scales are stored and applied. this means no additional compute overhead for calibrating or adjusting the model.\n * balancing memory- and compute-bound scenarios: the paper emphasizes that as batch size grows, the gains might change (memory-bound vs. compute-bound). still, they show consistent improvements in typical inference settings.\n\n\n# key parts of the paper in 3 sentences\n\n * the authors pinpoint that existing fine-grained quantization is slowed down by many float conversions during inference.\n * they propose “integer scale,” which transforms per-group float scales to integer scales (with a power-of-two amplifier), avoiding most of the costly conversions.\n * as a result, they achieve up to 2× speed boost while retaining nearly the same quantization accuracy on a broad range of large language models.\n\n----------------------------------------\n\n\n# 19. [121] training high-performance and large-scale deep neural networks with full 8-bit integers\n\n\n# 1. introduction\n\ndeep neural networks (dnns) have achieved remarkable success in fields such as image processing, object detection, and natural language processing.\n\nhowever, training these models requires extensive floating-point (fp) operations, leading to high memory, compute, and energy costs.\n\ndnn quantization has been explored as a solution to this problem, primarily focusing on inference quantization (e.g., bwn, xnor-net).\n\nrecent advancements extend quantization to training, but existing methods still leave parts of the computation in high-precision floating-point (e.g., fp8, fp16) or do not quantize batch normalization (bn).\n\nthe major challenges in achieving full quantization include:\n\n * incomplete quantization : some parts of the model remain in floating-point, limiting memory and compute savings.\n * unquantized batch normalization (bn) : bn is critical for training stability but is often left in floating-point.\n * lack of a unified low-bit training framework : no existing method successfully trains large-scale models with only low-bit integer operations.\n\nthis work introduces wageubn , a unified int8 training framework that quantizes all major operations, including:\n\n * weights (w)\n * activations (a)\n * gradients (g)\n * errors (e)\n * updates (u)\n * batch normalization (bn)\n * momentum optimizer\n\n\n# 2. related work\n\n# 2.1. inference quantization\n\ninference quantization aims to reduce the memory and compute cost of dnn inference by converting fp operations to bit-wise integer operations. some key works include:\n\n * bwn (binary weight networks) : quantizes only weights to {-1,1}.\n * xnor-net : quantizes both weights and activations to binary values.\n * admm-based quantization : compresses models via alternating direction method of multipliers.\n * fp8/int16-based methods : reduce bit-width to maintain accuracy.\n\nhowever, inference quantization only focuses on the forward pass and does not address the backward pass needed for training.\n\n# 2.2. training quantization\n\ntraining quantization extends quantization to the backward pass (gradients and updates). key approaches include:\n\n * dorefa-net : uses low-bit activations, weights, and gradients but retains fp elements.\n * mp (mixed precision) : uses fp16 for training but is not purely integer-based.\n * fp8 training : reduces training precision to fp8 but retains fp operations in bn.\n * qbp2 : uses 8-bit int for weights, activations, and errors, but gradients remain fp.\n * wage : the most complete prior work, quantizing w, a, g, e, and u, but it lacks bn layers, making it unsuitable for large-scale dnns.\n\n\n\n\n# 3. wageubn framework\n\n# 3.1. key contributions\n\n * fully quantizes all training data paths (w, a, g, e, u, bn, and momentum).\n * introduces three custom quantization functions for different training components.\n * quantizes batch normalization (bn) for the first time.\n * applies int8 quantization to large-scale networks like resnet on imagenet.\n\n# 3.2. straight-through estimator (ste)\n\nquantization introduces a non-differentiability problem , making gradient updates challenging.\n\nste is used to approximate gradients during backpropagation: $$\\frac{\\partial l}{\\partial x} = \\frac{\\partial l}{\\partial x_q}$$\n\nthis method allows training to proceed despite non-differentiability.\n\n# 3.3. quantization functions\n\nthe framework introduces three quantization functions tailored for different data types:\n\n 1. direct quantization : used for weights, activations, and bn parameters. $$q(x, k) = \\frac{\\text{round}(x \\cdot 2^{k-1})}{2^{k-1}}$$\n\nthis function approximates floating-point values to the nearest discrete integer.\n\n 2. constant quantization : used for gradients to ensure a fixed update bit-width. $$cq(x) = \\frac{sd(x)}{2^{k-1}}$$\n\nthis function scales data dynamically to avoid excessive precision loss.\n\n 3. shift quantization : used for errors , which are typically small-magnitude values. $$sq(x, k) = r(x) \\cdot \\text{clip}(q(\\text{norm}(x), k), -1, 1)$$\n\nthis function ensures errors maintain a meaningful range.\n\n# 3.4. quantized training steps\n\nthe framework quantizes the entire training process:\n\n * forward pass:\n   * quantizes inputs, applies int8 convolutions, quantizes bn, and applies int8 activation.\n * backward pass:\n   * uses int8 gradients and error propagation.\n   * applies int8 momentum optimization.\n   * uses fixed-point updates for weight adjustments.\n * momentum quantization:\n   * conventional optimizers like adam/momentum use floating-point accumulations.\n   * wageubn constrains them to fixed-point int8.\n\n\n# 4. results\n\n# 4.1. accuracy evaluation\n\nthe framework was tested on resnet18/34/50 with imagenet. two versions were evaluated:\n\n 1. full 8-bit int : all computations use 8-bit integers.\n 2. 16-bit e2 variant : uses 16-bit error gradients to improve convergence. | model | vanilla fp32 | wageubn (16-bit e2) | wageubn (full 8-bit) | | --- | --- | --- | --- | | resnet18 | 68.70% | 67.40% | 64.79% | | resnet34 | 71.99% | 68.50% | 67.63% | | resnet50 | 74.66% | 69.07% | 67.95% |\n\n * accuracy loss is minimal (~3-5% top-1 accuracy).\n * 16-bit e2 improves accuracy over pure 8-bit training.\n * comparable accuracy to fp8-based methods.\n\n# 4.2. efficiency gains\n\nwageubn significantly reduces hardware overhead compared to fp32:\n\nprecision   compute speedup   power reduction   circuit area reduction\nint8        3× - 9×           10× - 30×         9× - 30×\nfp8         0.73×             0.31×             0.4×\nfp16        0.58×             0.4×              0.4×\n\n * memory is reduced by 4× .\n * computation is up to 9× faster .\n * power usage is up to 30× lower .\n\n\n# 5. analysis\n\n * batch size sensitivity : wageubn works best with batch sizes ≥32. smaller batches lead to higher accuracy loss.\n * error gradient sensitivity : the 8-bit flag qe2 method significantly improves accuracy over simple 8-bit quantization.\n * quantization impact :\n   * bn and errors are the most sensitive to precision loss .\n   * weights and activations are more robust to int8 constraints .\n\n\n# 6. conclusion wageubn is the first complete int8 quantization framework for training large-scale dnns. it achieves:\n\n * end-to-end int8 training (including bn and optimizers).\n * competitive accuracy with significant hardware efficiency improvements .\n * potential for online learning on energy-efficient devices . future work includes specialized hardware architectures to fully exploit wageubn’s benefits.\n\n\n# three-sentence summary\n\nwageubn is a fully quantized int8 training framework for large-scale deep learning, covering all data paths (w, a, g, e, u, bn, and momentum).\n\nby introducing novel quantization functions and int8 batch normalization, it reduces memory by 4×, accelerates computation by up to 9×, and cuts power usage by 30× , while achieving comparable accuracy to fp-based models\n\nthis work establishes a scalable and efficient approach for energy-efficient ai hardware and online learning .\n\n----------------------------------------\n\n\n# 20. [381] i-bert: integer-only bert quantization\n\n\n# challenges addressed\n\ninefficiency of transformer-based models: bert and roberta achieve high accuracy but have high memory, latency, and power costs, making them difficult to deploy on edge devices and data centers.\n\nlimitations of previous quantization approaches: prior transformer quantization methods rely on floating-point arithmetic, preventing efficient execution on integer-only hardware like arm cortex-m processors and turing tensor cores.\n\ndifficulty in handling non-linear functions: existing integer-only quantization techniques are mainly designed for cnns with piece-wise linear functions like relu. transformers use complex non-linear functions (gelu, softmax, layernorm), which are hard to process using integer arithmetic without significant accuracy loss.\n\n\n\n\n# solution - i-bert approach:\n\ninteger-only approximation for non-linear functions:\n\n * gelu: approximated using a second-order polynomial (i-gelu), avoiding floating-point computation while maintaining accuracy.\n * softmax: transformed into a stable integer-friendly form using logarithm and bit-shift operations (i-exp).\n * layernorm: computed using an integer-only square root algorithm.\n\nend-to-end integer execution:\n\n * matmul and embeddings are computed using int8 multiplication and int32 accumulation.\n * non-linear operations (gelu, softmax, layernorm) are applied directly to int32 values and re-quantized to int8.\n * the entire inference process remains in integer arithmetic without dequantization.\n\n\n# results and impact:\n\n * accuracy: i-bert achieves comparable or slightly better accuracy than fp32 models on the glue benchmark, with an improvement of 0.3 (base) and 0.5 (large) in average score.\n * efficiency: i-bert provides 2.4× – 4.0× speedup in inference compared to fp32 on nvidia t4 gpus.\n * deployment feasibility: eliminates floating-point dependency, making it ideal for deployment on integer-only hardware like arm cortex-m processors and specialized accelerators.\n\n\n# key takeaways in 3 sentences\n\ni-bert introduces a novel integer-only quantization method for bert, eliminating floating-point operations and enabling efficient deployment on integer-only hardware.\n\nby approximating non-linear functions like gelu, softmax, and layernorm with polynomial and integer arithmetic, it maintains high accuracy while significantly improving inference speed.\n\nevaluation on the glue benchmark and hardware tests demonstrate that i-bert achieves up to 4× speedup while maintaining or slightly improving accuracy over fp32 models.\n\n----------------------------------------\n\n\n# 21. [904] llm.int8(): 8-bit matrix multiplication for transformers at scale\n\nthe paper introduces llm.int8(), a quantization method enabling 8-bit matrix multiplication for large transformers without degrading performance.\n\ntraditional 8-bit quantization methods struggle with large-scale models due to systematic outlier features that disrupt quantization precision beyond 6.7b parameters.\n\nto overcome this, llm.int8() combines vector-wise quantization (which assigns separate normalization constants per inner product) and mixed-precision decomposition, where outlier dimensions are computed in 16-bit while 99.9% of values remain in 8-bit.\n\nthis approach allows large-scale transformers like opt-175b and bloom to run on a single consumer gpu without accuracy loss.\n\n\n\n\n# key findings include:\n\n * emergent outliers: beyond 6.7b parameters, certain feature dimensions dominate transformer attention and predictive performance, requiring higher precision.\n * quantization challenge: existing methods fail due to these outliers, as they occupy only 0.1% of the data but significantly impact accuracy.\n\n\n# llm.int8 solution\n\nby isolating outliers in 16-bit operations while keeping most computations in 8-bit, the method retains full-precision inference while cutting memory usage by half.\n\nthe study empirically validates that llm.int8() maintains 16-bit accuracy across models up to 175b parameters, making llms more accessible and practical. the method is open-sourced and integrated with hugging face transformers.\n\n\n# three-sentence key takeaways\n\nllm.int8() enables performance-preserving 8-bit quantization for transformers up to 175b parameters by combining vector-wise quantization and mixed-precision decomposition to handle emergent large-magnitude features.\n\nthese systematic outliers, appearing beyond 6.7b parameters, disrupt standard 8-bit quantization but can be isolated in 16-bit precision while keeping over 99.9% of computations in 8-bit, achieving a 2× memory reduction.\n\nthis allows massive models like opt-175b and bloom to run efficiently on consumer gpus, making large-scale llm inference more accessible.\n\n----------------------------------------\n\n\n# 22.[637] training deep neural networks with 8-bit floating point numbers\n\n\n# 1. introduction\n\nthe paper addresses the challenge of training deep neural networks (dnns) with reduced precision floating point numbers, specifically using 8-bit floating point (fp8).\n\nwhile inference has been successfully performed with low precision (as low as 2–4 bits), training has traditionally required at least 16-bit precision due to gradient fidelity concerns.\n\nthe paper proposes novel techniques that allow dnn training using fp8 without accuracy loss, promising 2–4× improvements in energy efficiency and throughput.\n\n\n# 2. challenges in low-precision training\n\nthree major challenges arise when reducing dnn training precision:\n\n * loss of accuracy when all operands (weights, activations, errors, gradients) are quantized to 8 bits.\n * reduced accumulation precision (moving from fp32 to fp16) significantly impacts convergence.\n * weight updates in 16-bit may degrade accuracy unless managed properly.\n\n\n\n\n# 3. proposed solutions\n\nthe paper introduces several key innovations:\n\n * custom fp8 format: a new fp8 format (1-bit sign, 5-bit exponent, 2-bit mantissa) that effectively represents dnn parameters.\n * chunk-based accumulation: breaking matrix multiplications into small chunks before accumulation to prevent truncation errors.\n\n\n\n * floating point stochastic rounding: a rounding method that retains small numerical details to prevent loss of information.\n * mixed-precision computations: using fp8 for most computations while keeping critical accumulations and weight updates in fp16.\n\n\n# 4. experimental results\n\nthe proposed fp8 training method was tested on various models, including resnet18/50, alexnet, and cifar10-cnn. results show:\n\n * no significant accuracy loss compared to fp32.\n * memory savings: model sizes were reduced by ~50%.\n * energy-efficient hardware implementation: a prototype chip demonstrated 2–4× efficiency gains.\n\n\n# 5. discussion\n\n * the first and last layers of dnns require higher precision (fp16) for better stability.\n * gradient accumulation in fp16 must be carefully handled with chunk-based summation.\n * stochastic rounding outperforms nearest rounding in weight updates.\n\n\n# 6. conclusion\n\nthe paper successfully demonstrates dnn training with fp8 while maintaining accuracy.\n\nthe combination of chunk-based accumulation, stochastic rounding, and mixed-precision strategies opens the door for more efficient hardware training platforms.\n\n\n# key takeaways in 3 sentences\n\nthe paper proposes training deep neural networks using 8-bit floating point numbers by introducing a custom fp8 format, chunk-based accumulation, and stochastic rounding to prevent accuracy loss.\n\nexperiments across multiple models (resnet, alexnet) confirm that fp8 training achieves the same accuracy as fp32 while significantly reducing memory and energy costs.\n\nthese innovations enable future hardware architectures with 2–4× improved efficiency, paving the way for practical low-precision dnn training.\n\n----------------------------------------\n\n\n# [270] hybrid 8-bit floating point (hfp8) training and inference for deep neural networks\n\nthis paper addresses the challenge of training and deploying deep neural networks (dnns) in 8-bit precision while preserving accuracy.\n\nthe authors note that existing 8-bit floating point (fp8) approaches (e.g., the (1−5-2) format) can train some large-capacity networks like resnet but struggle with compact models such as mobilenet or nlp-oriented architectures like transformer.\n\n\n\nby analyzing the distinct needs of the forward pass (weights and activations) and the backward pass (gradients), they propose a hybrid fp8 (hfp8) scheme:\n\n * forward pass\n   * uses the (1−4−3) format (1 sign bit, 4 exponent bits, 3 mantissa bits) with an exponent bias of 4.\n   * this format gives extra mantissa precision to minimize quantization noise for forward activations and weights.\n   * the exponent bias ensures coverage for smaller values often present in weight distributions.\n * backward pass\n   * uses (1−5−2) precision (1 sign bit, 5 exponent bits, 2 mantissa bits) for gradients and errors.\n   * the wider exponent range captures large gradient magnitudes during backpropagation.\n   * an auto-adjusted loss-scaling technique further prevents underflow or overflow of gradients in 8-bit.\n\nthe paper’s theoretical framework highlights how too few mantissa bits in forward activations create large quantization mismatches, which degrade training.\n\nconversely, reducing exponent bits for backward gradients risks clamping large gradient ranges. by splitting the formats, hfp8 balances mantissa fidelity in the forward path and dynamic range in the backward path.\n\n> the underlying reason for this choice is that forward and backward passes have different optimal balances between range and precision. while tensors in the forward pass prefer higher precision (and lower representational error), gradients in the backward pass prefer a higher dynamic range. we describe our hfp8 training methodology where weights and activations adopt the (1-4-3) format (bias=4) while tensors used in backpropagation continue to be represented using the (1-5-2) format (in combination with loss scaling techniques pioneered by [28]).\n\nadditional contributions include\n\n * post-training fp8 inference\n   directly quantizing a full-precision (fp32) model to fp8 often loses accuracy.\n   the authors show that re-tuning batch normalization statistics with a small subset of unlabeled training data recovers most or all of the lost accuracy.\n   furthermore, depthwise convolutions (common in mobilenet-like architectures) and some layers with very small or large magnitudes may remain in slightly higher precision (fp16) to avoid accuracy drops.\n   \n * softmax optimization\n   for tasks like machine translation or speech recognition that rely on large final fc layers, subtracting the max logits before quantization lets the softmax function operate in fp8 without collapsing scores.\n   \n * distributed training\n   the paper modifies standard ring-based all-reduce to exchange compressed 8-bit weights.\n   a local “round-off residual” (stored in higher precision) ensures these 8-bit weight updates remain stable and converge reliably.\n   the authors’ hardware simulations indicate that adopting hfp8 for both computation and communication can significantly reduce training time.\n   \n\ncomprehensive experiments on imagenet (resnet, mobilenet, densenet, alexnet), wmt14 en-de (transformer), large-scale speech (lstm), and object detection (ssd-lite, mask r-cnn) confirm that hfp8 preserves baseline accuracies within a fraction of a percentage point.\n\nthis demonstrates that 8-bit floating point is feasible not only for specialized cases but for a broad range of dnn workloads, offering substantial speedups and energy savings.\n\nthree-sentence key summary\n\n * this paper proposes a hybrid 8-bit floating point (hfp8) approach that uses two different 8-bit formats to match the distinct precision requirements of forward (weights/activations) and backward (gradients/errors) passes.\n * it introduces practical techniques—like batch normalization re-tuning, depthwise convolution in fp16, and round-off residual updates—to ensure stable training and inference in 8 bits across diverse models.\n * as a result, hfp8 achieves near-baseline accuracy on tasks ranging from image classification and object detection to machine translation and speech recognition.\n\n----------------------------------------\n\n\n# 28. [read y2025 nvidia] coat: compressing optimizer states and activation for memory-efficient fp8 training 👍 👍 👍 👍 👍\n\nprior studies do not tackle the memory consumption of activations and still leave the optimizer’s second-order momentum in higher precision.\n\ncoat significantly reduces the overall memory footprint by quantizing optimizer states and activations into fp8.\n\nfor optimizer states, we observe that fp8 format’s representation range is under-utilized when quantizing them, as illustrated in figure 2(a).\n\nto address this, we introduce a novel dynamic range expansion method which adjusts the distribution of optimizer states to better fit within the fp8 range, thereby minimizing quantization error.\n\nfor activations, we propose mixed-granularity activation quantization to achieve efficient and accurate quantization. we apply fine-grained quantization to non-linear layers and apply per-tensor quantization to linear layers.\n\nper-tensor quantization for matrix multiplications is more efficient and better suited for tensorcores, while fine-grained quantization helps maintain accuracy.\n\nthese two approaches tackle high memory consumption while ensuring minimal performance degradation.\n\n\n\n> prior studies: fp8-lm (peng et al., 2023) quantizes the first-order momentum to fp8 while leaving second-order momentum in fp16, which limits the overall memory savings. (fishman et al., 2024) finds that second-order momentum is more sensitive to quantization, and proposes to quantize it using e5m2 format.\n\n> in addition to quantization, there are other approaches that aim to reduce the memory footprint of the optimizer states (shazeer & stern, 2018; anil et al., 2019; chen et al., 2024; zhao et al., 2024), such as low-rank decomposition and optimizer simplification that only store the first-order momentum. these methods are orthogonal to our approach.\n\nto perform optimizer state quantization, we adopt per-group quantization for both first-order and second-order momentum, following previous works (dettmers et al., 2021; li et al., 2024).\n\nevery consecutive g element forms a group (g is defined as the group size), and each group is quantized independently with its own statistics.\n\noptimizer states are stored in fp8 precision, while its scaling factor is stored in bf16.\n\nthe dynamic range for first-order momentum is typically less than 1e4, and for second-order momentum, it is usually less than 1e1—both far below the available range of fp8. that is what they mean by second-order momentum is underutilized fp8.\n\n\n\nwe propose to vary the quantization granularity across different layers to balance precision and efficiency in a mixed-granularity manner.\n\nfor non-linear layers, vs-quant (dai et al., 2021) or perblock quant (xi et al., 2024) methods are well-suited due to their fine-grained and precise nature.\n\nfor linear layers, we apply per-tensor quantization to maximize the performance of tensor cores.\n\nwe observe that quantizing the input of layernorm across multiple token axes is detrimental to accuracy.\n\nas illustrated in figure 4(a), when the number of elements that share a scaling factor is fixed, the quantization error increases significantly when quantization is performed across the token axis.\n\ntherefore instead of using per-block quantization with block size b × b as proposed in (xi et al., 2024), we propose to use per-group quantization with group size 1 × g, where g = b2 to keep the granularity the same.\n\nthis approach enhances the accuracy of non-linear layers while maintaining efficiency. our precise fp8 precision flow is visualized in figure 1(a), where we display the full precision flow for a llama-style decoder layer, both forward and backward pass.\n\nnote the red color in following graph, shows bf16.\n\n',charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM Sparsity",frontmatter:{title:"LLM Sparsity",date:"2025-02-03T23:32:49.000Z",permalink:"/pages/dc7051/",tags:[null]},regularPath:"/05.llm/16.llm_sparsity.html",relativePath:"05.llm/16.llm_sparsity.md",key:"v-041036ff",path:"/pages/dc7051/",headers:[{level:2,title:"1. [C93 2021] Sparse is Enough in Scaling Transformers",slug:"_1-c93-2021-sparse-is-enough-in-scaling-transformers",normalizedTitle:"1. [c93 2021] sparse is enough in scaling transformers",charIndex:1},{level:2,title:"2. [C78 2023] The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers",slug:"_2-c78-2023-the-lazy-neuron-phenomenon-on-emergence-of-activation-sparsity-in-transformers",normalizedTitle:"2. [c78 2023] the lazy neuron phenomenon: on emergence of activation sparsity in transformers",charIndex:57}],headersStr:"1. [C93 2021] Sparse is Enough in Scaling Transformers 2. [C78 2023] The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers",content:' 1. [C93 2021] Sparse is Enough in Scaling Transformers\n 2. [C78 2023] The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers\n 3. [C1 2024] SDQ: Sparse Decomposed Quantization for LLM Inference\n 4. [C26 2024] HighLight: Efficient and Flexible DNN Acceleration with Hierarchical Structured Sparsity\n 5. [C63] Sparseloop: An Analytical Approach To Sparse Tensor Accelerator Modeling\n 6. [C101] Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights\n\n----------------------------------------\n\n\n# 1. [C93 2021] Sparse is Enough in Scaling Transformers\n\npretrained on C4.\n\nKey Contributions:\n\n 1. Sparse Feedforward Layers:\n    * Uses dynamic sparsity by activating only a subset of weights during inference.\n    * Reduces computational cost while maintaining similar perplexity to dense models.\n\n\n\n 2. Sparse Attention (QKV) Layers:\n    * Uses a multiplicative layer to efficiently represent any permutation of input tokens.\n    * Introduces convolution-based sparsity to further reduce the computational burden.\n\nFollowing figure (a) shows the multiplicative dense layer:\n\n\n\n\n\nPlease notice the "S" in formula and the output of the blue block is S*M dimension.\n\n\n\n> We process this tensor with a two-dimensional convolutional layer, treating the length dimension and number of modules S like height and width of an image. This layer uses M filters and a kernel size of F × F so that each filter looks at F modules (‘S’ axis) of the last F tokens (‘length’ axis).\n\n 3. Speed and Efficiency Gains:\n    * Sparse feedforward and QKV layers achieve a 3.05× speedup in decoding for an 800M parameter model and 20× speedup for a 17B parameter model.\n    * The full Terraformer model (with sparse layers and reversible architecture) achieves 37× speedup in inference for large models.\n\n----------------------------------------\n\n\n# 2. [C78 2023] The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers\n\nSummary of "The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers" This paper explores a surprising phenomenon in Transformer models: activation sparsity . The authors observe that in trained Transformers, only a small fraction of neurons in the feedforward layers are activated for any given input. This emergent sparsity increases as the model size grows, raising questions about computational efficiency, robustness, and generalization.\n\nKey Findings\n\n 1. Activation Sparsity is Ubiquitous\n\n * In trained Transformer models like T5 and ViT , on average, only 3.0% of neurons in T5-Base and 6.3% in ViT-B16 are activated after the ReLU function.\n * Sparsity increases as the model gets deeper and wider .\n * This phenomenon holds across NLP and vision models , for both training and evaluation data .\n\n\n\n 2. Sparsity is Not Explicitly Enforced—It Emerges Naturally\n\n * Unlike traditional sparsity techniques that use explicit regularization, this sparsity arises naturally through training without any special constraints.\n * Similar to biological brains , where only a subset of neurons fire at any given time.\n\n 3. Sparsity Can Reduce Computation Costs\n\n * Since most activations are zero , many FLOPs (floating-point operations) in MLP layers can be skipped , improving efficiency.\n * A Top-k Transformer (where only the k most active neurons are used) further reduces computational overhead without degrading performance .\n\n 4. Sparsity Improves Robustness and Calibration\n    Models with enforced sparsity (e.g., Top-k Transformers) perform better on:\n    * Noisy training data (label corruption)\n    * Adversarial input perturbations\n    * Confidence calibration (better uncertainty estimation)\n\n\n\n\n\n\n\nImplications\n\n * Efficiency Gains : Exploiting this natural sparsity could reduce the computational cost of Transformer inference and training.\n * Robustness : Sparse activations help the model generalize better and resist overfitting.\n * Theoretical Insights : Suggests a link between biological sparsity and deep learning efficiency.\n\n> we emphasize that the sparsity in Top-k Transformers is unstructured and data-dependent, which is not well supported on existing computation hardwares such as TPUs and GPUs. Hence, the results in Figure 6 are for proof-of-concept purposes, and are far from fully realizing the benefit of FLOPs reduction via sparsity. We leave a study of better implementation of sparse computation for obtaining higher wall time reduction to future work.\n\nThe first MLP layer FLOP in the first MLP layer may be reduced to have sublinear complexity in dff.\n\n',normalizedContent:' 1. [c93 2021] sparse is enough in scaling transformers\n 2. [c78 2023] the lazy neuron phenomenon: on emergence of activation sparsity in transformers\n 3. [c1 2024] sdq: sparse decomposed quantization for llm inference\n 4. [c26 2024] highlight: efficient and flexible dnn acceleration with hierarchical structured sparsity\n 5. [c63] sparseloop: an analytical approach to sparse tensor accelerator modeling\n 6. [c101] hardware acceleration of sparse and irregular tensor computations of ml models: a survey and insights\n\n----------------------------------------\n\n\n# 1. [c93 2021] sparse is enough in scaling transformers\n\npretrained on c4.\n\nkey contributions:\n\n 1. sparse feedforward layers:\n    * uses dynamic sparsity by activating only a subset of weights during inference.\n    * reduces computational cost while maintaining similar perplexity to dense models.\n\n\n\n 2. sparse attention (qkv) layers:\n    * uses a multiplicative layer to efficiently represent any permutation of input tokens.\n    * introduces convolution-based sparsity to further reduce the computational burden.\n\nfollowing figure (a) shows the multiplicative dense layer:\n\n\n\n\n\nplease notice the "s" in formula and the output of the blue block is s*m dimension.\n\n\n\n> we process this tensor with a two-dimensional convolutional layer, treating the length dimension and number of modules s like height and width of an image. this layer uses m filters and a kernel size of f × f so that each filter looks at f modules (‘s’ axis) of the last f tokens (‘length’ axis).\n\n 3. speed and efficiency gains:\n    * sparse feedforward and qkv layers achieve a 3.05× speedup in decoding for an 800m parameter model and 20× speedup for a 17b parameter model.\n    * the full terraformer model (with sparse layers and reversible architecture) achieves 37× speedup in inference for large models.\n\n----------------------------------------\n\n\n# 2. [c78 2023] the lazy neuron phenomenon: on emergence of activation sparsity in transformers\n\nsummary of "the lazy neuron phenomenon: on emergence of activation sparsity in transformers" this paper explores a surprising phenomenon in transformer models: activation sparsity . the authors observe that in trained transformers, only a small fraction of neurons in the feedforward layers are activated for any given input. this emergent sparsity increases as the model size grows, raising questions about computational efficiency, robustness, and generalization.\n\nkey findings\n\n 1. activation sparsity is ubiquitous\n\n * in trained transformer models like t5 and vit , on average, only 3.0% of neurons in t5-base and 6.3% in vit-b16 are activated after the relu function.\n * sparsity increases as the model gets deeper and wider .\n * this phenomenon holds across nlp and vision models , for both training and evaluation data .\n\n\n\n 2. sparsity is not explicitly enforced—it emerges naturally\n\n * unlike traditional sparsity techniques that use explicit regularization, this sparsity arises naturally through training without any special constraints.\n * similar to biological brains , where only a subset of neurons fire at any given time.\n\n 3. sparsity can reduce computation costs\n\n * since most activations are zero , many flops (floating-point operations) in mlp layers can be skipped , improving efficiency.\n * a top-k transformer (where only the k most active neurons are used) further reduces computational overhead without degrading performance .\n\n 4. sparsity improves robustness and calibration\n    models with enforced sparsity (e.g., top-k transformers) perform better on:\n    * noisy training data (label corruption)\n    * adversarial input perturbations\n    * confidence calibration (better uncertainty estimation)\n\n\n\n\n\n\n\nimplications\n\n * efficiency gains : exploiting this natural sparsity could reduce the computational cost of transformer inference and training.\n * robustness : sparse activations help the model generalize better and resist overfitting.\n * theoretical insights : suggests a link between biological sparsity and deep learning efficiency.\n\n> we emphasize that the sparsity in top-k transformers is unstructured and data-dependent, which is not well supported on existing computation hardwares such as tpus and gpus. hence, the results in figure 6 are for proof-of-concept purposes, and are far from fully realizing the benefit of flops reduction via sparsity. we leave a study of better implementation of sparse computation for obtaining higher wall time reduction to future work.\n\nthe first mlp layer flop in the first mlp layer may be reduced to have sublinear complexity in dff.\n\n',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM Scaling Law",frontmatter:{title:"LLM Scaling Law",date:"2025-02-03T23:32:49.000Z",permalink:"/pages/dc7052/",tags:[null]},regularPath:"/05.llm/17.llm_scale.html",relativePath:"05.llm/17.llm_scale.md",key:"v-74b8ee65",path:"/pages/dc7052/",headers:[{level:2,title:"[A 2025] s1: Simple test-time scaling :+1:",slug:"a-2025-s1-simple-test-time-scaling",normalizedTitle:"[a 2025] s1: simple test-time scaling 👍",charIndex:4},{level:3,title:"Budget Forcing",slug:"budget-forcing",normalizedTitle:"budget forcing",charIndex:1500},{level:3,title:'Chatgpt Summary of "s1: Simple Test-Time Scaling"',slug:"chatgpt-summary-of-s1-simple-test-time-scaling",normalizedTitle:"chatgpt summary of &quot;s1: simple test-time scaling&quot;",charIndex:null},{level:2,title:"[C13 2024] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",slug:"c13-2024-inference-scaling-laws-an-empirical-analysis-of-compute-optimal-inference-for-llm-problem-solving",normalizedTitle:"[c13 2024] inference scaling laws: an empirical analysis of compute-optimal inference for llm problem-solving",charIndex:49}],headersStr:'[A 2025] s1: Simple test-time scaling :+1: Budget Forcing Chatgpt Summary of "s1: Simple Test-Time Scaling" [C13 2024] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving',content:' 1. [A 2025] s1: Simple test-time scaling 👍\n 2. [C13 2024] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving\n\n----------------------------------------\n\n\n# [A 2025] s1: Simple test-time scaling 👍\n\n * First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality.\n\n * Second, we develop budget forcing to control test-time compute by forcefully terminating the model’s thinking process or lengthening it by appending “Wait” multiple times to the model’s generation when it tries to end.\n\nThis can lead the model to double-check its answer, often fixing incorrect reasoning steps.\n\nAfter supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questionsby up to 27% (MATH and AIME24).\n\n\n\nFinetune train 26 mins on 1,000 carefully curated questions.\n\n(I) If the model generates more thinking tokens than a desired limit, we forcefully end the thinking process by appending an end-of-thinking token delimiter. Ending the thinking this way makes the model transition to generating its answer.\n(II) If we want the model to spend more test-time compute on a problem, we suppress the generation of the end-of-thinking token delimiter and instead append “Wait” to the model’s current reasoning trace to encourage more exploration.\n\n\n\n\n# Budget Forcing\n\nMaximum token\n\nwe enforce a maximum token count by simply appending the end-of-thinking token delimiter and “Final Answer: to early exit the thinking stage and make the model provide its current best answer.\n\nMinimum token\n\nwe suppress the generation of the end-of-thinking token delimiter and optionally append the string “Wait” to the model’s current reasoning trace to encourage the model to reflect on its current generation.\n\n# benchmark\n\n(I) Conditional length-control methods, which rely on telling the model in the prompt how long it should generate for.\nWe group them by granularity into\\\n\n * Token-conditional control: We specify an upper bound of thinking tokens in the prompt;\\\n * Step-conditional control: We specify an upper bound of thinking steps, where each step is around 100 tokens;\\\n * Class-conditional control: We write two generic prompts that tell the model to either think for a short or long amount of time.\n\n(II) Rejection sampling, which samples until a generation fits a predetermined compute budget.\n\n\n\nSequential scaling via Forcing with S1 is better than Parallel scaling.\n\n * Token-conditional control fails without budget forcing, as our model cannot reliably count tokens - even when trained to do so.\n * Under step-conditional control, the model generates a similar total number of tokens when given different step targets, as the model goes from few steps with many tokens per step, to many steps with few tokens in each step. Thus, the model learns to hack its way around the compute constraint making the controllability of this method mediocre.\n * Class-conditional control can work - telling a model to simply think longer can increase its test-time compute and performance, which leads good scaling.\n\nClass-conditional control can work. it controls number of tokens generated, but performance is not good.\n\n\n\nWhy does supervised finetuning on just 1,000 samples lead to such performance gains?\nWe hypothesize that the model is already exposed to large amounts of reasoning data during pretraining which spans trillions of tokens.\nThus, the ability to perform reasoning is already present in our model.\nOur sample-efficient finetuning stage just activates it and we scale it further at test time with budget forcing. This is similar to the "Superficial Alignment Hypothesis".\n\n\n\n\n\n\n# Chatgpt Summary of "s1: Simple Test-Time Scaling"\n\nOverview\n\nThis paper introduces test-time scaling , a method that allows language models to improve their reasoning abilities by using additional computation during inference.\nUnlike traditional approaches that rely on large-scale pretraining and fine-tuning , the authors propose a minimalist approach using a small dataset (1,000 examples) and a technique called budget forcing to control test-time compute.\nThe result is s1-32B , a model that outperforms OpenAI’s o1-preview on math reasoning tasks with significantly fewer training resources.\n\nKey Contributions\n\n 1. Test-Time Scaling Without Large Training Data\n\n * Previous work, including OpenAI’s o1 model , demonstrated that increasing test-time compute leads to better reasoning performance.\n * However, OpenAI did not disclose its methodology, leading to numerous replication attempts.\n * This paper finds the simplest approach to achieve similar results:\n   * Supervised fine-tuning (SFT) on a small, high-quality dataset (s1K, 1,000 examples) .\n   * A new test-time technique called budget forcing to regulate computational effort during inference.\n\n 2. Curating a High-Quality Small Dataset (s1K)\n\n * The authors select 1,000 reasoning questions from an initial 59K dataset using three key principles:\n   * Quality: Ensuring high-quality reasoning traces.\n   * Difficulty: Including challenging problems that require deep reasoning.\n   * Diversity: Covering different subject areas (e.g., math, physics, logic).\n * The dataset is distilled from Google’s Gemini Flash Thinking API to generate reasoning traces.\n\n 3. Budget Forcing: A Simple Test-Time Scaling Strategy\n\n * Budget forcing controls how much computation the model spends per question by:\n   * Forcing early termination when the model has computed enough.\n   * Encouraging extended reasoning by appending "Wait" when the model tries to stop prematurely.\n * This method allows the model to self-correct and double-check its reasoning .\n * Results show that increasing test-time compute through budget forcing improves performance up to 7% on math tasks .\n\n 4. Performance & Efficiency\n\n * s1-32B is trained using only 1,000 examples in 26 minutes on 16 NVIDIA H100 GPUs .\n * Despite this minimal training, it achieves:\n   * +27% improvement over OpenAI’s o1-preview on math benchmarks (MATH, AIME24) .\n   * Near-parity with Gemini 2.0 Thinking Experimental , which has access to massive proprietary data.\n * The model is fully open-source , making it an accessible alternative to closed models.\n\nResults & Ablations\n\n * Test-Time Compute Scaling Works\n   \n   * More compute at inference improves accuracy (Figure 1 in the paper).\n   * Budget forcing extrapolates performance (e.g., from 50% to 57% on AIME24 ).\n\n * Smaller, High-Quality Data Beats Large Random Data\n   \n   * Training on all 59K examples does not outperform using just 1K carefully selected examples.\n   * Random sampling or selecting long reasoning traces performs worse .\n\n * Comparison With Other Scaling Methods\n   \n   * Majority Voting (Parallel Scaling): Generates multiple responses and selects the best.\n   * Rejection Sampling: Generates responses until one meets a length constraint.\n   * Sequential Scaling (Budget Forcing) is the best approach for test-time scaling.\n\nConclusion & Future Directions\n\n * s1-32B proves that strong reasoning abilities can be achieved with minimal data and simple test-time scaling techniques.\n\n * The findings challenge the idea that massive pretraining is necessary for reasoning—small but carefully chosen data works .\n\n * Future research could explore:\n   \n   * Scaling beyond current test-time compute limits.\n   * Combining reinforcement learning with budget forcing.\n   * Parallel reasoning methods like Monte Carlo Tree Search (MCTS) for further improvements.\n\nFinal Thoughts\nThis paper presents a lightweight, effective approach to reasoning with LLMs, demonstrating that sparsity in training data + smart inference techniques can rival expensive large-scale models. It contributes to the ongoing discussion on efficient AI scaling and provides a strong open-source alternative to closed models like OpenAI’s o1 .\n\n----------------------------------------\n\n\n# [C13 2024] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving\n\nAbstract\n\nThe paper investigates compute-optimal inference strategies for large language models (LLMs) , focusing on the trade-offs between model size, inference computation (FLOPs), and accuracy.\nThe study evaluates inference strategies like greedy search, majority voting, best-of-n, weighted voting, and tree search algorithms.\\\n\n> Findings suggest that smaller models (e.g., Llemma-7B) can outperform larger models (Llemma-34B) under the same compute budget when paired with advanced inference algorithms.\n\nThe authors introduce a novel tree search method, REBASE (REward BAlanced SEarch) , which achieves better cost-performance trade-offs than sampling-based methods or Monte Carlo Tree Search (MCTS).\n\nKey Contributions & Findings\n\n1. Understanding Inference Scaling Laws\n\n * Traditional scaling laws analyze LLM performance based on training compute (Kaplan et al., 2020), but this paper focuses on inference compute scaling .\n * The optimal model size depends on the available compute budget . Smaller models initially outperform larger models, but as compute increases, larger models become favorable.\n * Real-world deployment is often compute-limited , making smaller models with effective inference techniques more practical.\n\n2. Comparison of Inference Strategies The paper evaluates different inference strategies:\n\n * Sampling-based methods :\n   \n   * Majority Voting : Selects the most frequent answer from multiple samples.\n   * Best-of-N : Picks the highest-scoring response using a reward model.\n   * Weighted Majority Voting : Assigns weights to answers based on reward scores.\n\n * Tree search methods :\n   \n   * Monte Carlo Tree Search (MCTS) : Often inefficient due to unfinished paths and wasted compute.\n   * REBASE (Proposed Method) : Uses a reward model to guide node expansion, ensuring better trade-offs between accuracy and compute .\n\n> Typically, increasing the compute budget leads to higher accuracy until the accuracy reaches saturation.\n> As the compute budget increases, smaller models initially perform better than larger ones, but once the accuracy of the smaller models saturates, the larger models have favorable performance.\n\n3. REBASE: A New Compute-Optimal Tree Search Method\n\n * Key Idea : Uses a node-quality reward function to prune bad paths early , reducing computational waste.\n * Advantages :\n   * Achieves higher accuracy with fewer FLOPs compared to sampling and MCTS.\n   * Outperforms MCTS , which wastes compute on incomplete solutions.\n   * Scales better with compute , avoiding early saturation seen in sampling.\n\n4. Empirical Results on Math Benchmarks\n\n * Evaluated on GSM8K and MATH500 datasets .\n * Llemma-7B (small model) with REBASE achieves similar or better accuracy than Llemma-34B (large model) with standard voting , while using 2× fewer FLOPs .\n * REBASE consistently outperforms MCTS across all settings.\n\nLlemma-7B model achieves competitive accuracy to Llemma-34B model with lower compute budget.\n\n\n\nConclusions\n\n * Compute-optimal inference is key for practical LLM deployment.\n * Smaller models with sophisticated inference strategies (e.g., REBASE) can match or surpass larger models in compute-limited scenarios .\n * REBASE offers a better cost-performance trade-off than sampling-based methods and MCTS .\n * Future work should explore generalizing these findings beyond mathematical reasoning .\n\nLimitations & Future Directions\n\n * The study is limited to mathematical problem-solving ; applying REBASE to other domains (e.g., coding, commonsense reasoning) is a future goal.\n * Exploring more efficient reward models for better tree search guidance is another area for improvement.\n\nFinal Thoughts\n\nThis paper provides practical insights into LLM inference efficiency , challenging the assumption that larger models always perform better . By optimizing inference strategies, smaller models can be as effective as large ones , making LLM deployment more cost-effective.\n\n----------------------------------------',normalizedContent:' 1. [a 2025] s1: simple test-time scaling 👍\n 2. [c13 2024] inference scaling laws: an empirical analysis of compute-optimal inference for llm problem-solving\n\n----------------------------------------\n\n\n# [a 2025] s1: simple test-time scaling 👍\n\n * first, we curate a small dataset s1k of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality.\n\n * second, we develop budget forcing to control test-time compute by forcefully terminating the model’s thinking process or lengthening it by appending “wait” multiple times to the model’s generation when it tries to end.\n\nthis can lead the model to double-check its answer, often fixing incorrect reasoning steps.\n\nafter supervised finetuning the qwen2.5-32b-instruct language model on s1k and equipping it with budget forcing, our model s1-32b exceeds o1-preview on competition math questionsby up to 27% (math and aime24).\n\n\n\nfinetune train 26 mins on 1,000 carefully curated questions.\n\n(i) if the model generates more thinking tokens than a desired limit, we forcefully end the thinking process by appending an end-of-thinking token delimiter. ending the thinking this way makes the model transition to generating its answer.\n(ii) if we want the model to spend more test-time compute on a problem, we suppress the generation of the end-of-thinking token delimiter and instead append “wait” to the model’s current reasoning trace to encourage more exploration.\n\n\n\n\n# budget forcing\n\nmaximum token\n\nwe enforce a maximum token count by simply appending the end-of-thinking token delimiter and “final answer: to early exit the thinking stage and make the model provide its current best answer.\n\nminimum token\n\nwe suppress the generation of the end-of-thinking token delimiter and optionally append the string “wait” to the model’s current reasoning trace to encourage the model to reflect on its current generation.\n\n# benchmark\n\n(i) conditional length-control methods, which rely on telling the model in the prompt how long it should generate for.\nwe group them by granularity into\\\n\n * token-conditional control: we specify an upper bound of thinking tokens in the prompt;\\\n * step-conditional control: we specify an upper bound of thinking steps, where each step is around 100 tokens;\\\n * class-conditional control: we write two generic prompts that tell the model to either think for a short or long amount of time.\n\n(ii) rejection sampling, which samples until a generation fits a predetermined compute budget.\n\n\n\nsequential scaling via forcing with s1 is better than parallel scaling.\n\n * token-conditional control fails without budget forcing, as our model cannot reliably count tokens - even when trained to do so.\n * under step-conditional control, the model generates a similar total number of tokens when given different step targets, as the model goes from few steps with many tokens per step, to many steps with few tokens in each step. thus, the model learns to hack its way around the compute constraint making the controllability of this method mediocre.\n * class-conditional control can work - telling a model to simply think longer can increase its test-time compute and performance, which leads good scaling.\n\nclass-conditional control can work. it controls number of tokens generated, but performance is not good.\n\n\n\nwhy does supervised finetuning on just 1,000 samples lead to such performance gains?\nwe hypothesize that the model is already exposed to large amounts of reasoning data during pretraining which spans trillions of tokens.\nthus, the ability to perform reasoning is already present in our model.\nour sample-efficient finetuning stage just activates it and we scale it further at test time with budget forcing. this is similar to the "superficial alignment hypothesis".\n\n\n\n\n\n\n# chatgpt summary of "s1: simple test-time scaling"\n\noverview\n\nthis paper introduces test-time scaling , a method that allows language models to improve their reasoning abilities by using additional computation during inference.\nunlike traditional approaches that rely on large-scale pretraining and fine-tuning , the authors propose a minimalist approach using a small dataset (1,000 examples) and a technique called budget forcing to control test-time compute.\nthe result is s1-32b , a model that outperforms openai’s o1-preview on math reasoning tasks with significantly fewer training resources.\n\nkey contributions\n\n 1. test-time scaling without large training data\n\n * previous work, including openai’s o1 model , demonstrated that increasing test-time compute leads to better reasoning performance.\n * however, openai did not disclose its methodology, leading to numerous replication attempts.\n * this paper finds the simplest approach to achieve similar results:\n   * supervised fine-tuning (sft) on a small, high-quality dataset (s1k, 1,000 examples) .\n   * a new test-time technique called budget forcing to regulate computational effort during inference.\n\n 2. curating a high-quality small dataset (s1k)\n\n * the authors select 1,000 reasoning questions from an initial 59k dataset using three key principles:\n   * quality: ensuring high-quality reasoning traces.\n   * difficulty: including challenging problems that require deep reasoning.\n   * diversity: covering different subject areas (e.g., math, physics, logic).\n * the dataset is distilled from google’s gemini flash thinking api to generate reasoning traces.\n\n 3. budget forcing: a simple test-time scaling strategy\n\n * budget forcing controls how much computation the model spends per question by:\n   * forcing early termination when the model has computed enough.\n   * encouraging extended reasoning by appending "wait" when the model tries to stop prematurely.\n * this method allows the model to self-correct and double-check its reasoning .\n * results show that increasing test-time compute through budget forcing improves performance up to 7% on math tasks .\n\n 4. performance & efficiency\n\n * s1-32b is trained using only 1,000 examples in 26 minutes on 16 nvidia h100 gpus .\n * despite this minimal training, it achieves:\n   * +27% improvement over openai’s o1-preview on math benchmarks (math, aime24) .\n   * near-parity with gemini 2.0 thinking experimental , which has access to massive proprietary data.\n * the model is fully open-source , making it an accessible alternative to closed models.\n\nresults & ablations\n\n * test-time compute scaling works\n   \n   * more compute at inference improves accuracy (figure 1 in the paper).\n   * budget forcing extrapolates performance (e.g., from 50% to 57% on aime24 ).\n\n * smaller, high-quality data beats large random data\n   \n   * training on all 59k examples does not outperform using just 1k carefully selected examples.\n   * random sampling or selecting long reasoning traces performs worse .\n\n * comparison with other scaling methods\n   \n   * majority voting (parallel scaling): generates multiple responses and selects the best.\n   * rejection sampling: generates responses until one meets a length constraint.\n   * sequential scaling (budget forcing) is the best approach for test-time scaling.\n\nconclusion & future directions\n\n * s1-32b proves that strong reasoning abilities can be achieved with minimal data and simple test-time scaling techniques.\n\n * the findings challenge the idea that massive pretraining is necessary for reasoning—small but carefully chosen data works .\n\n * future research could explore:\n   \n   * scaling beyond current test-time compute limits.\n   * combining reinforcement learning with budget forcing.\n   * parallel reasoning methods like monte carlo tree search (mcts) for further improvements.\n\nfinal thoughts\nthis paper presents a lightweight, effective approach to reasoning with llms, demonstrating that sparsity in training data + smart inference techniques can rival expensive large-scale models. it contributes to the ongoing discussion on efficient ai scaling and provides a strong open-source alternative to closed models like openai’s o1 .\n\n----------------------------------------\n\n\n# [c13 2024] inference scaling laws: an empirical analysis of compute-optimal inference for llm problem-solving\n\nabstract\n\nthe paper investigates compute-optimal inference strategies for large language models (llms) , focusing on the trade-offs between model size, inference computation (flops), and accuracy.\nthe study evaluates inference strategies like greedy search, majority voting, best-of-n, weighted voting, and tree search algorithms.\\\n\n> findings suggest that smaller models (e.g., llemma-7b) can outperform larger models (llemma-34b) under the same compute budget when paired with advanced inference algorithms.\n\nthe authors introduce a novel tree search method, rebase (reward balanced search) , which achieves better cost-performance trade-offs than sampling-based methods or monte carlo tree search (mcts).\n\nkey contributions & findings\n\n1. understanding inference scaling laws\n\n * traditional scaling laws analyze llm performance based on training compute (kaplan et al., 2020), but this paper focuses on inference compute scaling .\n * the optimal model size depends on the available compute budget . smaller models initially outperform larger models, but as compute increases, larger models become favorable.\n * real-world deployment is often compute-limited , making smaller models with effective inference techniques more practical.\n\n2. comparison of inference strategies the paper evaluates different inference strategies:\n\n * sampling-based methods :\n   \n   * majority voting : selects the most frequent answer from multiple samples.\n   * best-of-n : picks the highest-scoring response using a reward model.\n   * weighted majority voting : assigns weights to answers based on reward scores.\n\n * tree search methods :\n   \n   * monte carlo tree search (mcts) : often inefficient due to unfinished paths and wasted compute.\n   * rebase (proposed method) : uses a reward model to guide node expansion, ensuring better trade-offs between accuracy and compute .\n\n> typically, increasing the compute budget leads to higher accuracy until the accuracy reaches saturation.\n> as the compute budget increases, smaller models initially perform better than larger ones, but once the accuracy of the smaller models saturates, the larger models have favorable performance.\n\n3. rebase: a new compute-optimal tree search method\n\n * key idea : uses a node-quality reward function to prune bad paths early , reducing computational waste.\n * advantages :\n   * achieves higher accuracy with fewer flops compared to sampling and mcts.\n   * outperforms mcts , which wastes compute on incomplete solutions.\n   * scales better with compute , avoiding early saturation seen in sampling.\n\n4. empirical results on math benchmarks\n\n * evaluated on gsm8k and math500 datasets .\n * llemma-7b (small model) with rebase achieves similar or better accuracy than llemma-34b (large model) with standard voting , while using 2× fewer flops .\n * rebase consistently outperforms mcts across all settings.\n\nllemma-7b model achieves competitive accuracy to llemma-34b model with lower compute budget.\n\n\n\nconclusions\n\n * compute-optimal inference is key for practical llm deployment.\n * smaller models with sophisticated inference strategies (e.g., rebase) can match or surpass larger models in compute-limited scenarios .\n * rebase offers a better cost-performance trade-off than sampling-based methods and mcts .\n * future work should explore generalizing these findings beyond mathematical reasoning .\n\nlimitations & future directions\n\n * the study is limited to mathematical problem-solving ; applying rebase to other domains (e.g., coding, commonsense reasoning) is a future goal.\n * exploring more efficient reward models for better tree search guidance is another area for improvement.\n\nfinal thoughts\n\nthis paper provides practical insights into llm inference efficiency , challenging the assumption that larger models always perform better . by optimizing inference strategies, smaller models can be as effective as large ones , making llm deployment more cost-effective.\n\n----------------------------------------',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM KV Cache Management",frontmatter:{title:"LLM KV Cache Management",date:"2025-02-05T23:32:49.000Z",permalink:"/pages/dc7056/",tags:[null]},regularPath:"/05.llm/19.llm_kv_manage.html",relativePath:"05.llm/19.llm_kv_manage.md",key:"v-3c10ac85",path:"/pages/dc7056/",headers:[{level:2,title:"1. [2025 New] Can LLMs Maintain Fundamental Abilities under KV Cache Compression? 👍",slug:"_1-2025-new-can-llms-maintain-fundamental-abilities-under-kv-cache-compression",normalizedTitle:"1. [2025 new] can llms maintain fundamental abilities under kv cache compression? 👍",charIndex:1},{level:3,title:"Key Findings",slug:"key-findings",normalizedTitle:"key findings",charIndex:1976},{level:3,title:"Proposed Method",slug:"proposed-method",normalizedTitle:"proposed method",charIndex:3329},{level:3,title:"Insights:",slug:"insights",normalizedTitle:"insights:",charIndex:3639},{level:3,title:"Conclusion:",slug:"conclusion",normalizedTitle:"conclusion:",charIndex:4638},{level:3,title:"Impact Statement:",slug:"impact-statement",normalizedTitle:"impact statement:",charIndex:5131}],headersStr:"1. [2025 New] Can LLMs Maintain Fundamental Abilities under KV Cache Compression? 👍 Key Findings Proposed Method Insights: Conclusion: Impact Statement:",content:" 1. [2025 New] Can LLMs Maintain Fundamental Abilities under KV Cache Compression? 👍\n 2. [2025 New] ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\n 3. Deep Learning 101: Lesson 30: Understanding Text with Attention Heatmaps\n 4. [136] Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\n 5. [527] SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot 👍\n 6. [77] Token Merging for Fast Stable Diffusion\n 7. [382] Token Merging: Your ViT But Faster\n\n----------------------------------------\n\n\n# 1. [2025 New] Can LLMs Maintain Fundamental Abilities under KV Cache Compression? 👍\n\nThis is a good paper....but no source code\n\nThis paper investigates the impact of Key-Value (KV) cache compression on the fundamental capabilities of Large Language Models (LLMs).\n\nThe authors conduct a comprehensive empirical study to evaluate how different KV cache compression methods affect various tasks, including world knowledge, common-sense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.\n\nThe study reveals that KV cache compression methods exhibit task-specific performance degradation, with arithmetic reasoning tasks being particularly sensitive to aggressive compression.\n\n> The key insight is that the prefill phase KV cache, which contains crucial prompt information, should be compressed once and remain fixed, while the decoding phase KV cache can be dynamically compressed with different strategies. Given a prefill compression ratio rp, we prioritize shots with higher scores while ensuring the total number of preserved tokens does not exceed the KV cache limit. Specifically, shots are ranked by their scores and selected in descending order until reaching the compression budget rp × |KVprefill|. This shot-level selection strategy helps maintain the semantic coherence of important examples while adhering to memory constraints.\n\n\n# Key Findings\n\n * Task-Specific Performance Degradation:\n   \n   Different tasks show varying levels of sensitivity to KV cache compression.\n   \n   Arithmetic reasoning tasks are the most sensitive, with performance drops ranging from 17.4% to 43.3% under aggressive compression.\n   \n   In contrast, tasks like world knowledge and common-sense reasoning are more resilient.\n\n * Robustness of Multi-Step Reasoning Models:\n   \n   Multi-step reasoning LLMs, such as the DeepSeek R1 Distill model, exhibit more robust compression tolerance compared to instruction-tuned models.\n   \n   The DeepSeek R1 Distill model shows only 9.67%-25.53% performance degradation under compression.\n\n * Impact of Prompt Length:\n   \n   Shorter prompts are more vulnerable to compression effects.\n   \n   Performance degradation is more pronounced in one-shot and two-shot scenarios compared to prompts with more examples.\n\n * Chunk-Level Compression:\n   \n   Chunk-level compression strategies are more effective for complex long-context arithmetic reasoning tasks.\n   \n   These strategies help maintain semantic coherence and improve performance under aggressive compression.\n\n * Long-Context Generation Sensitivity:\n   \n   Long-context generation tasks are particularly sensitive to compression, with significant performance degradation observed at low compression ratios.\n\n\n# Proposed Method\n\nShotKV: The authors propose ShotKV, a novel compression approach that distinctly handles the prefill and decoding phases while maintaining shot-level semantic coherence.\n\nShotKV achieves 9%-18% performance improvements on long-context generation tasks under aggressive compression ratios.\n\n\n# Insights:\n\n * Attention Patterns:\n   \n   The study highlights that attention patterns vary significantly across different tasks.\n   \n   Arithmetic reasoning tasks display increased attention sparsity, while safety tasks show concentrated attention on system prompts. These patterns provide insights into how compression affects various tasks.\n\n * Compression Trade-offs:\n   \n   The findings suggest a trade-off between memory efficiency and task performance.\n   \n   While KV cache compression reduces memory usage, it can significantly impact model performance, especially in tasks requiring complex reasoning or extensive knowledge retrieval.\n\n * Model Design Implications: The higher sensitivity of instruction-tuned models to compression raises questions about the relationship between model training objectives and compression robustness.\n   \n   Future research could explore training techniques that optimize for compression resilience while maintaining instruction-following capabilities.\n\n\n# Conclusion:\n\nThe paper provides valuable insights into the impact of KV cache compression on LLMs' core capabilities.\n\nIt highlights the need for task-adaptive compression strategies and suggests that multi-step reasoning models are more robust to compression.\n\nThe proposed ShotKV method demonstrates significant improvements in performance, particularly for long-context generation tasks, making it a promising approach for efficient LLM deployment in resource-constrained environments.\n\n\n# Impact Statement:\n\nThe research has positive implications for reducing computational resources and energy consumption, making AI technology more accessible.\n\nHowever, it also raises concerns about the potential acceleration of LLM adoption and its broader societal impacts, such as employment and privacy. The authors emphasize the importance of responsible deployment and ethical considerations in the application of these technologies.",normalizedContent:" 1. [2025 new] can llms maintain fundamental abilities under kv cache compression? 👍\n 2. [2025 new] chunkkv: semantic-preserving kv cache compression for efficient long-context llm inference\n 3. deep learning 101: lesson 30: understanding text with attention heatmaps\n 4. [136] model tells you what to discard: adaptive kv cache compression for llms\n 5. [527] sparsegpt: massive language models can be accurately pruned in one-shot 👍\n 6. [77] token merging for fast stable diffusion\n 7. [382] token merging: your vit but faster\n\n----------------------------------------\n\n\n# 1. [2025 new] can llms maintain fundamental abilities under kv cache compression? 👍\n\nthis is a good paper....but no source code\n\nthis paper investigates the impact of key-value (kv) cache compression on the fundamental capabilities of large language models (llms).\n\nthe authors conduct a comprehensive empirical study to evaluate how different kv cache compression methods affect various tasks, including world knowledge, common-sense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.\n\nthe study reveals that kv cache compression methods exhibit task-specific performance degradation, with arithmetic reasoning tasks being particularly sensitive to aggressive compression.\n\n> the key insight is that the prefill phase kv cache, which contains crucial prompt information, should be compressed once and remain fixed, while the decoding phase kv cache can be dynamically compressed with different strategies. given a prefill compression ratio rp, we prioritize shots with higher scores while ensuring the total number of preserved tokens does not exceed the kv cache limit. specifically, shots are ranked by their scores and selected in descending order until reaching the compression budget rp × |kvprefill|. this shot-level selection strategy helps maintain the semantic coherence of important examples while adhering to memory constraints.\n\n\n# key findings\n\n * task-specific performance degradation:\n   \n   different tasks show varying levels of sensitivity to kv cache compression.\n   \n   arithmetic reasoning tasks are the most sensitive, with performance drops ranging from 17.4% to 43.3% under aggressive compression.\n   \n   in contrast, tasks like world knowledge and common-sense reasoning are more resilient.\n\n * robustness of multi-step reasoning models:\n   \n   multi-step reasoning llms, such as the deepseek r1 distill model, exhibit more robust compression tolerance compared to instruction-tuned models.\n   \n   the deepseek r1 distill model shows only 9.67%-25.53% performance degradation under compression.\n\n * impact of prompt length:\n   \n   shorter prompts are more vulnerable to compression effects.\n   \n   performance degradation is more pronounced in one-shot and two-shot scenarios compared to prompts with more examples.\n\n * chunk-level compression:\n   \n   chunk-level compression strategies are more effective for complex long-context arithmetic reasoning tasks.\n   \n   these strategies help maintain semantic coherence and improve performance under aggressive compression.\n\n * long-context generation sensitivity:\n   \n   long-context generation tasks are particularly sensitive to compression, with significant performance degradation observed at low compression ratios.\n\n\n# proposed method\n\nshotkv: the authors propose shotkv, a novel compression approach that distinctly handles the prefill and decoding phases while maintaining shot-level semantic coherence.\n\nshotkv achieves 9%-18% performance improvements on long-context generation tasks under aggressive compression ratios.\n\n\n# insights:\n\n * attention patterns:\n   \n   the study highlights that attention patterns vary significantly across different tasks.\n   \n   arithmetic reasoning tasks display increased attention sparsity, while safety tasks show concentrated attention on system prompts. these patterns provide insights into how compression affects various tasks.\n\n * compression trade-offs:\n   \n   the findings suggest a trade-off between memory efficiency and task performance.\n   \n   while kv cache compression reduces memory usage, it can significantly impact model performance, especially in tasks requiring complex reasoning or extensive knowledge retrieval.\n\n * model design implications: the higher sensitivity of instruction-tuned models to compression raises questions about the relationship between model training objectives and compression robustness.\n   \n   future research could explore training techniques that optimize for compression resilience while maintaining instruction-following capabilities.\n\n\n# conclusion:\n\nthe paper provides valuable insights into the impact of kv cache compression on llms' core capabilities.\n\nit highlights the need for task-adaptive compression strategies and suggests that multi-step reasoning models are more robust to compression.\n\nthe proposed shotkv method demonstrates significant improvements in performance, particularly for long-context generation tasks, making it a promising approach for efficient llm deployment in resource-constrained environments.\n\n\n# impact statement:\n\nthe research has positive implications for reducing computational resources and energy consumption, making ai technology more accessible.\n\nhowever, it also raises concerns about the potential acceleration of llm adoption and its broader societal impacts, such as employment and privacy. the authors emphasize the importance of responsible deployment and ethical considerations in the application of these technologies.",charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM Distributed Machine Learning",frontmatter:{title:"LLM Distributed Machine Learning",date:"2025-02-01T23:32:49.000Z",permalink:"/pages/dc7057/",tags:[null]},regularPath:"/05.llm/20.%20llm_distr.html",relativePath:"05.llm/20. llm_distr.md",key:"v-082f1636",path:"/pages/dc7057/",headersStr:null,content:" 1. [1000] A Survey on Distributed Machine Learning",normalizedContent:" 1. [1000] a survey on distributed machine learning",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM Attention",frontmatter:{title:"LLM Attention",date:"2025-02-05T23:32:49.000Z",permalink:"/pages/dc7055/",tags:[null]},regularPath:"/05.llm/18.llm_att.html",relativePath:"05.llm/18.llm_att.md",key:"v-5c1f14b6",path:"/pages/dc7055/",headers:[{level:2,title:"1.[742] A Multiscale Visualization of Attention in the Transformer Model",slug:"_1-742-a-multiscale-visualization-of-attention-in-the-transformer-model",normalizedTitle:"1.[742] a multiscale visualization of attention in the transformer model",charIndex:350},{level:3,title:"Key Findings:",slug:"key-findings",normalizedTitle:"key findings:",charIndex:1030},{level:3,title:"Additional Findings:",slug:"additional-findings",normalizedTitle:"additional findings:",charIndex:2980},{level:3,title:"Future Work:",slug:"future-work",normalizedTitle:"future work:",charIndex:3810},{level:3,title:"Conclusion:",slug:"conclusion",normalizedTitle:"conclusion:",charIndex:4139},{level:2,title:"2.[1910] What Does BERT Look At? An Analysis of BERT’s Attention",slug:"_2-1910-what-does-bert-look-at-an-analysis-of-bert-s-attention",normalizedTitle:"2.[1910] what does bert look at? an analysis of bert’s attention",charIndex:4729},{level:3,title:"Key Findings:",slug:"key-findings-2",normalizedTitle:"key findings:",charIndex:1030},{level:3,title:"Contributions:",slug:"contributions",normalizedTitle:"contributions:",charIndex:7351},{level:3,title:"Conclusion:",slug:"conclusion-2",normalizedTitle:"conclusion:",charIndex:4139},{level:2,title:"3. [626] Transformer Feed-Forward Layers Are Key-Value Memories",slug:"_3-626-transformer-feed-forward-layers-are-key-value-memories",normalizedTitle:"3. [626] transformer feed-forward layers are key-value memories",charIndex:146},{level:3,title:"Key Findings:",slug:"key-findings-3",normalizedTitle:"key findings:",charIndex:1030},{level:3,title:"Contributions:",slug:"contributions-2",normalizedTitle:"contributions:",charIndex:7351},{level:3,title:"Implications:",slug:"implications",normalizedTitle:"implications:",charIndex:11448},{level:3,title:"Conclusion:",slug:"conclusion-3",normalizedTitle:"conclusion:",charIndex:4139}],headersStr:"1.[742] A Multiscale Visualization of Attention in the Transformer Model Key Findings: Additional Findings: Future Work: Conclusion: 2.[1910] What Does BERT Look At? An Analysis of BERT’s Attention Key Findings: Contributions: Conclusion: 3. [626] Transformer Feed-Forward Layers Are Key-Value Memories Key Findings: Contributions: Implications: Conclusion:",content:" 1. [742] A Multiscale Visualization of Attention in the Transformer Model 👍\n 2. [1910] What Does BERT Look At? An Analysis of BERT’s Attention\n 3. [626] Transformer Feed-Forward Layers Are Key-Value Memories 👍\n 4. [102] Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models\n\n----------------------------------------\n\n\n# 1.[742] A Multiscale Visualization of Attention in the Transformer Model\n\nThe paper, \"A Multiscale Visualization of Attention in the Transformer Model\" by Jesse Vig, introduces an open-source tool designed to visualize attention mechanisms in Transformer-based models like BERT and GPT-2.\nThe tool provides three distinct views: Attention-head view, Model view, and Neuron view, each offering a unique perspective on how attention operates at different scales within the model.\nThe tool aims to make the multi-layer, multi-head attention mechanism more interpretable, helping researchers and practitioners understand how the model assigns weights to different input elements.\n\n\n# Key Findings:\n\n# Attention-head View:\n\n * Function: Visualizes attention patterns produced by one or more attention heads in a given layer.\n * Key Insights:\n   * Different heads learn unique attention mechanisms. For example, some heads focus on positional patterns (e.g., attending to the previous word), while others capture lexical patterns (e.g., named entities, subject-verb pairs).\n   * Attention heads can detect syntactic and semantic relations, such as dependency relations and part-of-speech tags.\n   * Use Case: Detecting model bias, such as gender bias in coreference resolution. For instance, the model may associate \"She\" with \"nurse\" and \"He\" with \"doctor,\" indicating potential gender bias.\n\n\n\n# Model View:\n\n * Function: Provides a high-level overview of attention across all layers and heads for a given input.\n * Key Insights:\n   * Attention patterns evolve across layers, with some heads focusing on within-sentence patterns and others on between-sentence patterns.\n   * The model view helps identify relevant heads for specific tasks, such as paraphrase detection, where heads that draw connections between sentences are particularly useful.\n   * Use Case: Locating relevant attention heads for tasks like paraphrase detection, where inter-sentence attention patterns are crucial.\n\n# Neuron View:\n\n * Function: Visualizes how individual neurons in the query and key vectors interact to produce attention.\n * Key Insights:\n   * The neuron view reveals how specific neurons contribute to attention patterns, such as distance-decaying attention (where attention decreases with distance from the source token).\n   * Certain neurons are responsible for specific attention behaviors, and modifying these neurons could control attention patterns (e.g., adjusting the decay rate for different text complexities).\n   * Use Case: Linking neurons to model behavior, allowing for potential interventions to control attention mechanisms.\n\n\n# Additional Findings:\n\n# Attention Patterns:\n\nAttention heads capture a wide range of behaviors, from coarse positional patterns to specific lexical and syntactic patterns. Some heads are specialized for tasks like coreference resolution, while others focus on syntactic structures or semantic relationships.\n\n# Model Bias:\n\nThe tool can help detect and analyze biases in the model, such as gender bias in coreference resolution, by visualizing attention patterns associated with biased predictions.\n\n# Intervention Opportunities:\n\nThe neuron view provides insights into how specific neurons influence attention, offering opportunities for model intervention.\nFor example, modifying neurons responsible for attention decay could adjust the context window size, which might be useful for generating texts of varying complexity.\n\n\n# Future Work:\n\nThe authors plan to develop a unified interface for navigating all three views within the tool.\nThey aim to expose other components of the model, such as value vectors and state activations.\nThe tool could be extended to allow users to manipulate the model by modifying attention or editing individual neurons.\n\n\n# Conclusion:\n\nThe paper presents a powerful visualization tool that enhances the interpretability of Transformer models by providing multiscale views of attention mechanisms.\nThe tool not only helps in understanding how attention works in these models but also aids in detecting biases, locating relevant attention heads, and linking neurons to specific model behaviors.\nThis work opens up new possibilities for model analysis and intervention, contributing to the broader goal of making complex neural models more transparent and controllable.\n\n----------------------------------------\n\n\n# 2.[1910] What Does BERT Look At? An Analysis of BERT’s Attention\n\nThe paper \"What Does BERT Look At? An Analysis of BERT's Attention\" by Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning investigates the attention mechanisms within BERT, a large pre-trained Transformer model, to understand what linguistic features it learns from unlabeled data. The authors propose methods for analyzing BERT's attention maps and apply them to uncover patterns and behaviors of its attention heads.\n\n\n# Key Findings:\n\n# Attention Patterns:\n\nBERT's attention heads exhibit common patterns such as attending to specific positional offsets, delimiter tokens (e.g., [SEP]), or broadly attending over the entire sentence.\n\nA significant amount of attention is focused on the [SEP] token, which the authors hypothesize acts as a \"no-op\" (no operation) when the attention head's function is not applicable.\n\n> qualitative analysis (see Figure 5) shows that heads with specific functions attend to [SEP] when the function is not called for. For example, in head 8-10 direct objects attend to their verbs. For this head, non-nouns mostly attend to [SEP]. Therefore, we speculate that attention over these special tokens might be used as a sort of “no-op” when the attention head’s function is not applicable.\n\n# Linguistic Phenomena:\n\nCertain attention heads correspond well to linguistic notions of syntax and coreference. For example, some heads accurately attend to direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions.\n\nThe authors propose an attention-based probing classifier, which achieves 77% Unlabeled Attachment Score (UAS) in dependency parsing, indicating that BERT's attention captures substantial syntactic information.\n\n# Individual Attention Heads:\n\nIndividual attention heads specialize in specific syntactic relations, such as prepositions attending to their objects or nouns attending to their determiners, with high accuracy.\n\nOne attention head performs well in coreference resolution, particularly for nominal mentions, suggesting that BERT learns some aspects of coreference without explicit supervision.\n\n# Attention Head Combinations:\n\nThe authors propose probing classifiers that combine attention maps from multiple heads, showing that BERT's attention maps collectively capture a thorough representation of English syntax.\n\n# Clustering of Attention Heads:\n\nAttention heads within the same layer tend to behave similarly, and some heads form clear clusters based on their behavior. This redundancy might be due to attention dropout during training.\n\n\n# Contributions:\n\nThe paper provides a detailed analysis of BERT's attention mechanisms, revealing that BERT learns significant syntactic and coreference information through self-supervised training.\n\nThe authors introduce novel methods for probing attention maps, demonstrating that attention-based analysis complements other model analysis techniques.\n\n\n# Conclusion:\n\nThe study shows that BERT's attention mechanisms capture a substantial amount of linguistic knowledge, particularly syntax and coreference, even though the model is not explicitly trained for these tasks.\nThe findings suggest that pre-training on large corpora enables BERT to learn complex linguistic structures indirectly, which contributes to its success in various NLP tasks.\nThe paper also highlights the importance of analyzing attention maps to better understand what neural networks learn about language.\n\n\n\n----------------------------------------\n\n\n# 3. [626] Transformer Feed-Forward Layers Are Key-Value Memories\n\n👍 This is a good paper that deserves further reading.\n\nThe authors argue that these layers, which constitute two-thirds of a transformer's parameters, function as key-value memories, where keys detect specific input patterns and values induce distributions over the output vocabulary.\n\nThe paper provides empirical evidence to support this claim and explores how these memories contribute to the model's predictions.\n\n\n# Key Findings:\n\n# Feed-Forward Layers as Key-Value Memories:\n\nThe feed-forward layers in transformers can be viewed as key-value memories, where the first matrix (keys) detects input patterns, and the second matrix (values) induces distributions over the output vocabulary.\n\nEach key correlates with specific textual patterns in the training data, and the corresponding value represents the distribution of tokens likely to follow that pattern.\n\n# Interpretable Patterns in Keys:\n\nThe keys in feed-forward layers capture human-interpretable patterns in the input. Lower layers tend to detect shallow patterns (e.g., n-grams), while upper layers capture more semantic patterns (e.g., topics or contexts).\n\nExperiments show that the top trigger examples for each key share clear, recognizable patterns, and these patterns become more semantic as the layer depth increases.\n\n# Values as Output Distributions:\n\nThe values in feed-forward layers can be interpreted as distributions over the output vocabulary.\n\nIn upper layers, these distributions often align with the next-token predictions for the patterns detected by the corresponding keys.\n\nThe agreement between the value's top prediction and the next token in the trigger example increases significantly in the upper layers, indicating that these layers are more predictive of the output.\n\n\n\n# Memory Composition and Refinement:\n\nThe output of a feed-forward layer is a composition of multiple memories, where hundreds of active memories contribute to the final distribution.\n\nThe layer's prediction is often different from the predictions of individual memories, suggesting a complex aggregation process.\n\nThe model refines its predictions through residual connections across layers. Lower layers often determine the final prediction, while upper layers fine-tune the distribution.\n\n# Layer-Wise Behavior:\n\nLower layers focus on shallow patterns and have a higher number of active memories, while upper layers focus on semantic patterns and exhibit fewer active memories.\n\n\n\nThe model's predictions are refined sequentially, with the residual connections playing a crucial role in combining and refining the predictions from each layer.\n\n\n\n\n\n\n# Contributions:\n\nThe paper provides a novel interpretation of feed-forward layers in transformers as key-value memories, shedding light on their role in the model's predictions.\n\nIt demonstrates that feed-forward layers capture interpretable patterns in the input and that these patterns evolve from shallow to semantic as the layer depth increases.\n\nThe study shows how the model composes and refines its predictions through the aggregation of memories and residual connections.\n\n\n# Implications:\n\nThe findings open new research directions, including understanding the embedding space transformation across layers, extending the analysis to other transformer models (e.g., BERT), and exploring practical implications for interpretability and data privacy.\n\nThe paper suggests that feed-forward layers play a crucial role in the model's ability to capture and predict linguistic patterns, contributing to the overall success of transformer-based language models.\n\n\n# Conclusion:\n\nThe paper advances our understanding of the inner workings of transformers by revealing that feed-forward layers act as key-value memories that detect input patterns and induce output distributions.\n\nThis work highlights the importance of feed-forward layers in the model's predictions and provides a foundation for further research into the mechanisms of transformer-based models.\n\n----------------------------------------",normalizedContent:" 1. [742] a multiscale visualization of attention in the transformer model 👍\n 2. [1910] what does bert look at? an analysis of bert’s attention\n 3. [626] transformer feed-forward layers are key-value memories 👍\n 4. [102] attention flows: analyzing and comparing attention mechanisms in language models\n\n----------------------------------------\n\n\n# 1.[742] a multiscale visualization of attention in the transformer model\n\nthe paper, \"a multiscale visualization of attention in the transformer model\" by jesse vig, introduces an open-source tool designed to visualize attention mechanisms in transformer-based models like bert and gpt-2.\nthe tool provides three distinct views: attention-head view, model view, and neuron view, each offering a unique perspective on how attention operates at different scales within the model.\nthe tool aims to make the multi-layer, multi-head attention mechanism more interpretable, helping researchers and practitioners understand how the model assigns weights to different input elements.\n\n\n# key findings:\n\n# attention-head view:\n\n * function: visualizes attention patterns produced by one or more attention heads in a given layer.\n * key insights:\n   * different heads learn unique attention mechanisms. for example, some heads focus on positional patterns (e.g., attending to the previous word), while others capture lexical patterns (e.g., named entities, subject-verb pairs).\n   * attention heads can detect syntactic and semantic relations, such as dependency relations and part-of-speech tags.\n   * use case: detecting model bias, such as gender bias in coreference resolution. for instance, the model may associate \"she\" with \"nurse\" and \"he\" with \"doctor,\" indicating potential gender bias.\n\n\n\n# model view:\n\n * function: provides a high-level overview of attention across all layers and heads for a given input.\n * key insights:\n   * attention patterns evolve across layers, with some heads focusing on within-sentence patterns and others on between-sentence patterns.\n   * the model view helps identify relevant heads for specific tasks, such as paraphrase detection, where heads that draw connections between sentences are particularly useful.\n   * use case: locating relevant attention heads for tasks like paraphrase detection, where inter-sentence attention patterns are crucial.\n\n# neuron view:\n\n * function: visualizes how individual neurons in the query and key vectors interact to produce attention.\n * key insights:\n   * the neuron view reveals how specific neurons contribute to attention patterns, such as distance-decaying attention (where attention decreases with distance from the source token).\n   * certain neurons are responsible for specific attention behaviors, and modifying these neurons could control attention patterns (e.g., adjusting the decay rate for different text complexities).\n   * use case: linking neurons to model behavior, allowing for potential interventions to control attention mechanisms.\n\n\n# additional findings:\n\n# attention patterns:\n\nattention heads capture a wide range of behaviors, from coarse positional patterns to specific lexical and syntactic patterns. some heads are specialized for tasks like coreference resolution, while others focus on syntactic structures or semantic relationships.\n\n# model bias:\n\nthe tool can help detect and analyze biases in the model, such as gender bias in coreference resolution, by visualizing attention patterns associated with biased predictions.\n\n# intervention opportunities:\n\nthe neuron view provides insights into how specific neurons influence attention, offering opportunities for model intervention.\nfor example, modifying neurons responsible for attention decay could adjust the context window size, which might be useful for generating texts of varying complexity.\n\n\n# future work:\n\nthe authors plan to develop a unified interface for navigating all three views within the tool.\nthey aim to expose other components of the model, such as value vectors and state activations.\nthe tool could be extended to allow users to manipulate the model by modifying attention or editing individual neurons.\n\n\n# conclusion:\n\nthe paper presents a powerful visualization tool that enhances the interpretability of transformer models by providing multiscale views of attention mechanisms.\nthe tool not only helps in understanding how attention works in these models but also aids in detecting biases, locating relevant attention heads, and linking neurons to specific model behaviors.\nthis work opens up new possibilities for model analysis and intervention, contributing to the broader goal of making complex neural models more transparent and controllable.\n\n----------------------------------------\n\n\n# 2.[1910] what does bert look at? an analysis of bert’s attention\n\nthe paper \"what does bert look at? an analysis of bert's attention\" by kevin clark, urvashi khandelwal, omer levy, and christopher d. manning investigates the attention mechanisms within bert, a large pre-trained transformer model, to understand what linguistic features it learns from unlabeled data. the authors propose methods for analyzing bert's attention maps and apply them to uncover patterns and behaviors of its attention heads.\n\n\n# key findings:\n\n# attention patterns:\n\nbert's attention heads exhibit common patterns such as attending to specific positional offsets, delimiter tokens (e.g., [sep]), or broadly attending over the entire sentence.\n\na significant amount of attention is focused on the [sep] token, which the authors hypothesize acts as a \"no-op\" (no operation) when the attention head's function is not applicable.\n\n> qualitative analysis (see figure 5) shows that heads with specific functions attend to [sep] when the function is not called for. for example, in head 8-10 direct objects attend to their verbs. for this head, non-nouns mostly attend to [sep]. therefore, we speculate that attention over these special tokens might be used as a sort of “no-op” when the attention head’s function is not applicable.\n\n# linguistic phenomena:\n\ncertain attention heads correspond well to linguistic notions of syntax and coreference. for example, some heads accurately attend to direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions.\n\nthe authors propose an attention-based probing classifier, which achieves 77% unlabeled attachment score (uas) in dependency parsing, indicating that bert's attention captures substantial syntactic information.\n\n# individual attention heads:\n\nindividual attention heads specialize in specific syntactic relations, such as prepositions attending to their objects or nouns attending to their determiners, with high accuracy.\n\none attention head performs well in coreference resolution, particularly for nominal mentions, suggesting that bert learns some aspects of coreference without explicit supervision.\n\n# attention head combinations:\n\nthe authors propose probing classifiers that combine attention maps from multiple heads, showing that bert's attention maps collectively capture a thorough representation of english syntax.\n\n# clustering of attention heads:\n\nattention heads within the same layer tend to behave similarly, and some heads form clear clusters based on their behavior. this redundancy might be due to attention dropout during training.\n\n\n# contributions:\n\nthe paper provides a detailed analysis of bert's attention mechanisms, revealing that bert learns significant syntactic and coreference information through self-supervised training.\n\nthe authors introduce novel methods for probing attention maps, demonstrating that attention-based analysis complements other model analysis techniques.\n\n\n# conclusion:\n\nthe study shows that bert's attention mechanisms capture a substantial amount of linguistic knowledge, particularly syntax and coreference, even though the model is not explicitly trained for these tasks.\nthe findings suggest that pre-training on large corpora enables bert to learn complex linguistic structures indirectly, which contributes to its success in various nlp tasks.\nthe paper also highlights the importance of analyzing attention maps to better understand what neural networks learn about language.\n\n\n\n----------------------------------------\n\n\n# 3. [626] transformer feed-forward layers are key-value memories\n\n👍 this is a good paper that deserves further reading.\n\nthe authors argue that these layers, which constitute two-thirds of a transformer's parameters, function as key-value memories, where keys detect specific input patterns and values induce distributions over the output vocabulary.\n\nthe paper provides empirical evidence to support this claim and explores how these memories contribute to the model's predictions.\n\n\n# key findings:\n\n# feed-forward layers as key-value memories:\n\nthe feed-forward layers in transformers can be viewed as key-value memories, where the first matrix (keys) detects input patterns, and the second matrix (values) induces distributions over the output vocabulary.\n\neach key correlates with specific textual patterns in the training data, and the corresponding value represents the distribution of tokens likely to follow that pattern.\n\n# interpretable patterns in keys:\n\nthe keys in feed-forward layers capture human-interpretable patterns in the input. lower layers tend to detect shallow patterns (e.g., n-grams), while upper layers capture more semantic patterns (e.g., topics or contexts).\n\nexperiments show that the top trigger examples for each key share clear, recognizable patterns, and these patterns become more semantic as the layer depth increases.\n\n# values as output distributions:\n\nthe values in feed-forward layers can be interpreted as distributions over the output vocabulary.\n\nin upper layers, these distributions often align with the next-token predictions for the patterns detected by the corresponding keys.\n\nthe agreement between the value's top prediction and the next token in the trigger example increases significantly in the upper layers, indicating that these layers are more predictive of the output.\n\n\n\n# memory composition and refinement:\n\nthe output of a feed-forward layer is a composition of multiple memories, where hundreds of active memories contribute to the final distribution.\n\nthe layer's prediction is often different from the predictions of individual memories, suggesting a complex aggregation process.\n\nthe model refines its predictions through residual connections across layers. lower layers often determine the final prediction, while upper layers fine-tune the distribution.\n\n# layer-wise behavior:\n\nlower layers focus on shallow patterns and have a higher number of active memories, while upper layers focus on semantic patterns and exhibit fewer active memories.\n\n\n\nthe model's predictions are refined sequentially, with the residual connections playing a crucial role in combining and refining the predictions from each layer.\n\n\n\n\n\n\n# contributions:\n\nthe paper provides a novel interpretation of feed-forward layers in transformers as key-value memories, shedding light on their role in the model's predictions.\n\nit demonstrates that feed-forward layers capture interpretable patterns in the input and that these patterns evolve from shallow to semantic as the layer depth increases.\n\nthe study shows how the model composes and refines its predictions through the aggregation of memories and residual connections.\n\n\n# implications:\n\nthe findings open new research directions, including understanding the embedding space transformation across layers, extending the analysis to other transformer models (e.g., bert), and exploring practical implications for interpretability and data privacy.\n\nthe paper suggests that feed-forward layers play a crucial role in the model's ability to capture and predict linguistic patterns, contributing to the overall success of transformer-based language models.\n\n\n# conclusion:\n\nthe paper advances our understanding of the inner workings of transformers by revealing that feed-forward layers act as key-value memories that detect input patterns and induce output distributions.\n\nthis work highlights the importance of feed-forward layers in the model's predictions and provides a foundation for further research into the mechanisms of transformer-based models.\n\n----------------------------------------",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM Internals",frontmatter:{title:"LLM Internals",date:"2025-02-11T23:32:49.000Z",permalink:"/pages/dc7059/",tags:[null]},regularPath:"/05.llm/21.llm_internals.html",relativePath:"05.llm/21.llm_internals.md",key:"v-915e75f6",path:"/pages/dc7059/",headers:[{level:2,title:"1. [C606 2021] Transformer Feed-Forward Layers Are Key-Value Memories",slug:"_1-c606-2021-transformer-feed-forward-layers-are-key-value-memories",normalizedTitle:"1. [c606 2021] transformer feed-forward layers are key-value memories",charIndex:1},{level:3,title:"Overview",slug:"overview",normalizedTitle:"overview",charIndex:191},{level:3,title:"Key Contributions",slug:"key-contributions",normalizedTitle:"key contributions",charIndex:970},{level:3,title:"Key Insights and Findings",slug:"key-insights-and-findings",normalizedTitle:"key insights and findings",charIndex:2529},{level:3,title:"Unintuitive Findings",slug:"unintuitive-findings",normalizedTitle:"unintuitive findings",charIndex:4295},{level:3,title:"Practical Implications",slug:"practical-implications",normalizedTitle:"practical implications",charIndex:5144},{level:3,title:"Future Research Directions",slug:"future-research-directions",normalizedTitle:"future research directions",charIndex:5525},{level:3,title:"Conclusion",slug:"conclusion",normalizedTitle:"conclusion",charIndex:5954}],headersStr:"1. [C606 2021] Transformer Feed-Forward Layers Are Key-Value Memories Overview Key Contributions Key Insights and Findings Unintuitive Findings Practical Implications Future Research Directions Conclusion",content:' 1. [C606 2021] Transformer Feed-Forward Layers Are Key-Value Memories\n\n----------------------------------------\n\n\n# 1. [C606 2021] Transformer Feed-Forward Layers Are Key-Value Memories\n\n\n# Overview\n\nThe paper investigates the under-explored role of feed-forward layers in transformer-based language models.\n\nWhile self-attention has been extensively studied, feed-forward layers contain two-thirds of a transformer\'s parameters.\n\nThe authors propose that these layers function as key-value (KV) memories , where:\n\n * Keys capture textual patterns in training examples.\n * Values induce distributions over the output vocabulary .\n\nThrough experiments, the paper demonstrates that:\n\n * Lower-layer keys capture shallow syntactic patterns , while upper layers learn semantic patterns .\n * Values predict next-token distributions , aligning more strongly in upper layers.\n * The final model prediction is an aggregation of memories , refined by residual connections .\n\n\n# Key Contributions\n\n 1. Feed-forward layers operate as key-value memories\n\n * The first parameter matrix (The first parameter matrix (keys, $K$ ) interacts with inputs to compute a weighted sum of the second parameter matrix (The first parameter matrix (The first parameter matrix (keys, $K$ ) interacts with inputs to compute a weighted sum of the second parameter matrix (values, $V$ ).\n * This mirrors neural memory models like those from Sukhbaatar et al. (2015, 2019) .\n\n 2. Keys capture human-interpretable textual patterns\n\n * Keys correspond to n-grams, phrase structures, and semantic themes .\n * Patterns vary across layers:\n   * Lower layers → Shallow linguistic patterns (e.g., common words, syntactic structures)\n   * Upper layers → Semantic relationships (e.g., “a part of,” “military base”)\n * Removing the last word in a sentence affects activations more than removing the first word , indicating that later words are more salient.\n\n 3. Values store next-token probability distributions\n\n * Values represent un-normalized probability distributions over the output vocabulary.\n * Agreement between keys and values increases in upper layers , meaning values encode likely next words based on key patterns.\n\n 4. Prediction refinement across layers\n\n * The final output results from an aggregation of multiple memories at each layer.\n * Residual connections act as a refinement mechanism , tuning predictions layer-by-layer.\n * In lower layers, residuals dominate the prediction, while in upper layers , feed-forward layers play a bigger role.\n\n\n# Key Insights and Findings\n\n# 1. Feed-Forward Layers as Unnormalized Key-Value Memories\n\n * Each feed-forward layer resembles a key-value memory network .\n * The equation governing a feed-forward layer: $$FF(x) = f(x \\cdot K^T) \\cdot V$$\n\nclosely resembles memory-based networks: $$MN(x) = \\text{softmax}(x \\cdot K^T) \\cdot V$$\n\n * The primary difference is the absence of softmax normalization in transformer feed-forward layers.\n\n# 2. Keys Detect Patterns in Training Data\n\n * Experiments show that keys correlate with specific input patterns .\n * Methodology :\n   * Trained a 16-layer transformer (Baevski & Auli, 2019) on WikiText-103 .\n   * Extracted the most activating input sentences for each key .\n   * Human annotators identified patterns.\n * Findings :\n   * Lower-layer keys detect surface-level syntax (e.g., "words ending in -ing").\n   * Upper-layer keys capture deeper semantics (e.g., “military bases,” “a part of” relationships).\n   * Removing the last word in a sentence impacts activations more than removing the first .\n\n# 3. Values Represent Next-Token Distributions\n\n * Values store distributions over output words.\n * In lower layers, values are uncorrelated with key patterns .\n * In upper layers, values strongly correlate with likely next tokens .\n * Agreement rate between key activations and value-predicted tokens rises in deeper layers .\n\n# 4. Aggregation of Memories Produces Final Prediction\n\n * Each layer combines multiple key-value pairs , producing a distribution different from any individual memory .\n * Prediction refinement occurs layer-by-layer via residual connections.\n * At least 30% of predictions are already determined in the lower layers .\n * Upper layers refine predictions, sometimes vetoing earlier predictions .\n\n\n# Unintuitive Findings\n\n 1. Lower layers do not predict tokens well, but upper layers do\n\n * Unlike self-attention, which is useful throughout the model, feed-forward layers do not contribute much to token prediction in early layers .\n * Instead, early layers store patterns that are later utilized in upper layers .\n\n 2. Memories do not act independently but are composed layer-wise\n\n * Individual key-value memories are not direct predictors.\n * Instead, the model aggregates multiple memories at each layer to form a final output distribution.\n\n 3. Residual connections mostly retain earlier predictions rather than overriding them\n\n * In most cases, the residual connection output remains the same through layers, with minor refinements.\n * Occasionally, feed-forward layers override residual outputs , leading to large shifts in predictions.\n\n\n# Practical Implications\n\n * Better model interpretability:\n   * Automating pattern detection in keys could improve transparency in transformers.\n * Memory efficiency:\n   * Understanding how transformers use feed-forward layers could enable parameter reduction techniques .\n * Security concerns:\n   * Key-value memories store training data patterns , posing data leakage risks .\n\n\n# Future Research Directions\n\n 1. Embedding Space Transformations Across Layers\n\n * How does the representation space evolve through layers?\n * Why does token prediction improve only in upper layers?\n\n 2. Extending to Other Transformer Architectures\n\n * Do BERT, T5, or vision transformers exhibit the same key-value behavior?\n\n 3. Improving Transformer Efficiency\n\n * Can we prune redundant keys while preserving performance?\n\n\n# Conclusion\n\nThis paper demystifies the function of feed-forward layers in transformers by revealing their role as key-value memories.\n\nThe insights gained provide a foundation for:\n\n * Understanding how transformers make predictions.\n * Optimizing transformer architectures for efficiency.\n * Developing interpretability and security mechanisms for language models.\n\nFinal Thoughts This paper is particularly valuable for researchers exploring transformer internals. Its key contribution is a paradigm shift in understanding feed-forward layers—not just as nonlinear projections, but as dynamic memory components that store patterns and predict next-token distributions .\n\n----------------------------------------',normalizedContent:' 1. [c606 2021] transformer feed-forward layers are key-value memories\n\n----------------------------------------\n\n\n# 1. [c606 2021] transformer feed-forward layers are key-value memories\n\n\n# overview\n\nthe paper investigates the under-explored role of feed-forward layers in transformer-based language models.\n\nwhile self-attention has been extensively studied, feed-forward layers contain two-thirds of a transformer\'s parameters.\n\nthe authors propose that these layers function as key-value (kv) memories , where:\n\n * keys capture textual patterns in training examples.\n * values induce distributions over the output vocabulary .\n\nthrough experiments, the paper demonstrates that:\n\n * lower-layer keys capture shallow syntactic patterns , while upper layers learn semantic patterns .\n * values predict next-token distributions , aligning more strongly in upper layers.\n * the final model prediction is an aggregation of memories , refined by residual connections .\n\n\n# key contributions\n\n 1. feed-forward layers operate as key-value memories\n\n * the first parameter matrix (the first parameter matrix (keys, $k$ ) interacts with inputs to compute a weighted sum of the second parameter matrix (the first parameter matrix (the first parameter matrix (keys, $k$ ) interacts with inputs to compute a weighted sum of the second parameter matrix (values, $v$ ).\n * this mirrors neural memory models like those from sukhbaatar et al. (2015, 2019) .\n\n 2. keys capture human-interpretable textual patterns\n\n * keys correspond to n-grams, phrase structures, and semantic themes .\n * patterns vary across layers:\n   * lower layers → shallow linguistic patterns (e.g., common words, syntactic structures)\n   * upper layers → semantic relationships (e.g., “a part of,” “military base”)\n * removing the last word in a sentence affects activations more than removing the first word , indicating that later words are more salient.\n\n 3. values store next-token probability distributions\n\n * values represent un-normalized probability distributions over the output vocabulary.\n * agreement between keys and values increases in upper layers , meaning values encode likely next words based on key patterns.\n\n 4. prediction refinement across layers\n\n * the final output results from an aggregation of multiple memories at each layer.\n * residual connections act as a refinement mechanism , tuning predictions layer-by-layer.\n * in lower layers, residuals dominate the prediction, while in upper layers , feed-forward layers play a bigger role.\n\n\n# key insights and findings\n\n# 1. feed-forward layers as unnormalized key-value memories\n\n * each feed-forward layer resembles a key-value memory network .\n * the equation governing a feed-forward layer: $$ff(x) = f(x \\cdot k^t) \\cdot v$$\n\nclosely resembles memory-based networks: $$mn(x) = \\text{softmax}(x \\cdot k^t) \\cdot v$$\n\n * the primary difference is the absence of softmax normalization in transformer feed-forward layers.\n\n# 2. keys detect patterns in training data\n\n * experiments show that keys correlate with specific input patterns .\n * methodology :\n   * trained a 16-layer transformer (baevski & auli, 2019) on wikitext-103 .\n   * extracted the most activating input sentences for each key .\n   * human annotators identified patterns.\n * findings :\n   * lower-layer keys detect surface-level syntax (e.g., "words ending in -ing").\n   * upper-layer keys capture deeper semantics (e.g., “military bases,” “a part of” relationships).\n   * removing the last word in a sentence impacts activations more than removing the first .\n\n# 3. values represent next-token distributions\n\n * values store distributions over output words.\n * in lower layers, values are uncorrelated with key patterns .\n * in upper layers, values strongly correlate with likely next tokens .\n * agreement rate between key activations and value-predicted tokens rises in deeper layers .\n\n# 4. aggregation of memories produces final prediction\n\n * each layer combines multiple key-value pairs , producing a distribution different from any individual memory .\n * prediction refinement occurs layer-by-layer via residual connections.\n * at least 30% of predictions are already determined in the lower layers .\n * upper layers refine predictions, sometimes vetoing earlier predictions .\n\n\n# unintuitive findings\n\n 1. lower layers do not predict tokens well, but upper layers do\n\n * unlike self-attention, which is useful throughout the model, feed-forward layers do not contribute much to token prediction in early layers .\n * instead, early layers store patterns that are later utilized in upper layers .\n\n 2. memories do not act independently but are composed layer-wise\n\n * individual key-value memories are not direct predictors.\n * instead, the model aggregates multiple memories at each layer to form a final output distribution.\n\n 3. residual connections mostly retain earlier predictions rather than overriding them\n\n * in most cases, the residual connection output remains the same through layers, with minor refinements.\n * occasionally, feed-forward layers override residual outputs , leading to large shifts in predictions.\n\n\n# practical implications\n\n * better model interpretability:\n   * automating pattern detection in keys could improve transparency in transformers.\n * memory efficiency:\n   * understanding how transformers use feed-forward layers could enable parameter reduction techniques .\n * security concerns:\n   * key-value memories store training data patterns , posing data leakage risks .\n\n\n# future research directions\n\n 1. embedding space transformations across layers\n\n * how does the representation space evolve through layers?\n * why does token prediction improve only in upper layers?\n\n 2. extending to other transformer architectures\n\n * do bert, t5, or vision transformers exhibit the same key-value behavior?\n\n 3. improving transformer efficiency\n\n * can we prune redundant keys while preserving performance?\n\n\n# conclusion\n\nthis paper demystifies the function of feed-forward layers in transformers by revealing their role as key-value memories.\n\nthe insights gained provide a foundation for:\n\n * understanding how transformers make predictions.\n * optimizing transformer architectures for efficiency.\n * developing interpretability and security mechanisms for language models.\n\nfinal thoughts this paper is particularly valuable for researchers exploring transformer internals. its key contribution is a paradigm shift in understanding feed-forward layers—not just as nonlinear projections, but as dynamic memory components that store patterns and predict next-token distributions .\n\n----------------------------------------',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM Compression",frontmatter:{title:"LLM Compression",date:"2025-04-01T12:32:49.000Z",permalink:"/pages/dc7061/",tags:[null]},regularPath:"/05.llm/24.llm_comp.html",relativePath:"05.llm/24.llm_comp.md",key:"v-77ef8fad",path:"/pages/dc7061/",headers:[{level:2,title:"[35] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models",slug:"_35-minicache-kv-cache-compression-in-depth-dimension-for-large-language-models",normalizedTitle:"[35] minicache: kv cache compression in depth dimension for large language models",charIndex:4},{level:3,title:"Background and Motivation",slug:"background-and-motivation",normalizedTitle:"background and motivation",charIndex:747},{level:3,title:"Key Insights",slug:"key-insights",normalizedTitle:"key insights",charIndex:1295},{level:3,title:"Main Contributions",slug:"main-contributions",normalizedTitle:"main contributions",charIndex:1921},{level:3,title:"Methodology",slug:"methodology",normalizedTitle:"methodology",charIndex:2686},{level:3,title:"Experimental Evaluation",slug:"experimental-evaluation",normalizedTitle:"experimental evaluation",charIndex:3644},{level:3,title:"Results and Findings:",slug:"results-and-findings",normalizedTitle:"results and findings:",charIndex:4068},{level:3,title:"Ablation Studies:",slug:"ablation-studies",normalizedTitle:"ablation studies:",charIndex:4562},{level:3,title:"Contributions Summarized",slug:"contributions-summarized",normalizedTitle:"contributions summarized",charIndex:4829},{level:3,title:"Limitations and Future Work",slug:"limitations-and-future-work",normalizedTitle:"limitations and future work",charIndex:5303},{level:3,title:"Conclusion",slug:"conclusion",normalizedTitle:"conclusion",charIndex:5630}],headersStr:"[35] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models Background and Motivation Key Insights Main Contributions Methodology Experimental Evaluation Results and Findings: Ablation Studies: Contributions Summarized Limitations and Future Work Conclusion",content:' 1. [35] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models\n\n----------------------------------------\n\n\n# [35] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models\n\nThe paper "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models" addresses the challenge of efficiently deploying large language models (LLMs) by proposing an innovative method for compressing the key-value (KV) cache.\n\nBelow is a detailed explanation of the paper, including key insights, contributions, and findings.\n\nAdditionally, since MiniCache is orthogonal to existing quantization techniques, it can achieve a compression ratio of up to 5.02× when combined with the 4-bit quantization technique.\n\n\n# Background and Motivation\n\nLarge Language Models (LLMs) like GPT and LLaMA rely heavily on autoregressive generation, where previously computed tokens\' key-value pairs are cached to minimize redundant computation during inference.\n\nHowever, the KV cache grows linearly with sequence length, becoming a substantial memory burden for long context tasks.\nThe authors identify an overlooked dimension—cross-layer redundancy , highlighting high similarities between the KV states of adjacent layers, particularly in middle-to-deep layers of LLMs.\n\n\n\n\n# Key Insights\n\n * Cross-Layer Redundancy\n   KV cache states across adjacent layers share significant redundancy, especially in deeper layers.\n * Unequal Mergeability\n   Not all KV cache state pairs between adjacent layers are equally suitable for merging—some tokens show distinct semantic differences and thus should not be merged indiscriminately.\n * Reparameterization Approach\n   Separating state vectors into magnitude and directional components allows effective merging via interpolation in polar coordinates, preserving crucial information and performance.\n\n> Please notice this consine similarity in tokens index\n\n\n\n\n# Main Contributions\n\n 1. MiniCache Framework\n    Introduces a novel cross-layer compression strategy that merges KV caches from adjacent layers starting from the middle layers of the LLM.\n\n\n\n 2. Reparameterization-based Cache Merging\n    Uses Spherical Linear Interpolation (SLERP) to merge the direction component of the KV cache vectors, preserving the original magnitude for minimal information loss.\n\n 3. Token Retention Strategy\n    Identifies and retains critical, distinct token pairs to prevent semantic degradation during merging, ensuring accuracy with minimal overhead.\n\n 4. Orthogonality to Existing Methods\n    MiniCache complements existing compression methods (e.g., quantization and sparsity), achieving superior compression rates when combined.\n\n\n# Methodology\n\nThe method involves two main components:\n\n# Cross-Layer Compression\n\n * Identifying Optimal Layers Merging begins at the midpoint of LLM layers, justified by observed higher redundancy at deeper layers.\n * Merge Function KV pairs from adjacent layers are merged via a carefully designed function leveraging SLERP, preserving semantic integrity and directional properties.\n\n# Cache Merging and Restoration\n\n * Reparameterization KV caches are decomposed into directional vectors (normalized) and magnitudes. The directional component undergoes merging using SLERP, ensuring geometrically coherent interpolation.\n * Token Retention Identifies outliers—distinct KV pairs unsuitable for merging—based on angular distance, selectively retaining these tokens to minimize performance loss.\n * Restoration Process Merged caches are restored by scaling merged directions with their original magnitudes and reintegrating the retained distinct tokens.\n\n\n# Experimental Evaluation\n\nThe authors evaluate MiniCache extensively across several popular LLMs:\n\n * Models LLaMA-2, LLaMA-3, Mixtral, Phi-3.\n * Datasets Evaluations include GSM8K (math problems), COQA (conversational Q&A), TruthfulQA, and LongBench (long-context tasks).\n * Baselines Comparisons include FP16 baseline (no compression), quantization-based methods (e.g., KIVI, SmoothQuant), and sparsity-driven methods.\n\n\n# Results and Findings:\n\n * Memory Efficiency MiniCache achieves up to 41% memory reduction and a compression ratio up to 5.02× when combined with 4-bit quantization.\n * Throughput Enhancement Improves inference throughput by approximately 5× compared to the FP16 baseline due to reduced memory footprint, enabling larger batch sizes and faster generation.\n * Minimal Performance Drop Compression with MiniCache leads to near-lossless performance, even under aggressive compression settings.\n\n\n# Ablation Studies:\n\n * Interpolation Parameter A critical hyperparameter determining the balance in merging adjacent KV pairs—optimal around t=0.6.\n * Retention Threshold Optimal token retention (γ=0.05) strikes the best balance between accuracy and memory usage.\n\n\n# Contributions Summarized\n\n * The paper proposes a novel depth-wise KV cache compression method, MiniCache.\n * It identifies cross-layer KV cache redundancy as a previously unexplored yet crucial dimension.\n * It introduces robust merging via SLERP interpolation and a targeted retention strategy for distinct tokens.\n * Experimental validation highlights substantial efficiency improvements, minimal accuracy degradation, and strong compatibility with existing methods.\n\n\n# Limitations and Future Work\n\n * The SLERP-based merging function is currently limited to pairwise merging. Future extensions could explore simultaneous merging across multiple layers.\n * Further exploration into adaptive interpolation parameters based on the relative magnitude ratio of vectors is identified as promising.\n\n\n# Conclusion\n\n"MiniCache" successfully identifies and exploits an important new dimension—depth-wise redundancy in KV caches of LLMs.\nIts combination of reparameterization-based merging and selective token retention provides significant improvements in memory efficiency and inference throughput, establishing a new direction for research and practical optimization in deploying large-scale language models.',normalizedContent:' 1. [35] minicache: kv cache compression in depth dimension for large language models\n\n----------------------------------------\n\n\n# [35] minicache: kv cache compression in depth dimension for large language models\n\nthe paper "minicache: kv cache compression in depth dimension for large language models" addresses the challenge of efficiently deploying large language models (llms) by proposing an innovative method for compressing the key-value (kv) cache.\n\nbelow is a detailed explanation of the paper, including key insights, contributions, and findings.\n\nadditionally, since minicache is orthogonal to existing quantization techniques, it can achieve a compression ratio of up to 5.02× when combined with the 4-bit quantization technique.\n\n\n# background and motivation\n\nlarge language models (llms) like gpt and llama rely heavily on autoregressive generation, where previously computed tokens\' key-value pairs are cached to minimize redundant computation during inference.\n\nhowever, the kv cache grows linearly with sequence length, becoming a substantial memory burden for long context tasks.\nthe authors identify an overlooked dimension—cross-layer redundancy , highlighting high similarities between the kv states of adjacent layers, particularly in middle-to-deep layers of llms.\n\n\n\n\n# key insights\n\n * cross-layer redundancy\n   kv cache states across adjacent layers share significant redundancy, especially in deeper layers.\n * unequal mergeability\n   not all kv cache state pairs between adjacent layers are equally suitable for merging—some tokens show distinct semantic differences and thus should not be merged indiscriminately.\n * reparameterization approach\n   separating state vectors into magnitude and directional components allows effective merging via interpolation in polar coordinates, preserving crucial information and performance.\n\n> please notice this consine similarity in tokens index\n\n\n\n\n# main contributions\n\n 1. minicache framework\n    introduces a novel cross-layer compression strategy that merges kv caches from adjacent layers starting from the middle layers of the llm.\n\n\n\n 2. reparameterization-based cache merging\n    uses spherical linear interpolation (slerp) to merge the direction component of the kv cache vectors, preserving the original magnitude for minimal information loss.\n\n 3. token retention strategy\n    identifies and retains critical, distinct token pairs to prevent semantic degradation during merging, ensuring accuracy with minimal overhead.\n\n 4. orthogonality to existing methods\n    minicache complements existing compression methods (e.g., quantization and sparsity), achieving superior compression rates when combined.\n\n\n# methodology\n\nthe method involves two main components:\n\n# cross-layer compression\n\n * identifying optimal layers merging begins at the midpoint of llm layers, justified by observed higher redundancy at deeper layers.\n * merge function kv pairs from adjacent layers are merged via a carefully designed function leveraging slerp, preserving semantic integrity and directional properties.\n\n# cache merging and restoration\n\n * reparameterization kv caches are decomposed into directional vectors (normalized) and magnitudes. the directional component undergoes merging using slerp, ensuring geometrically coherent interpolation.\n * token retention identifies outliers—distinct kv pairs unsuitable for merging—based on angular distance, selectively retaining these tokens to minimize performance loss.\n * restoration process merged caches are restored by scaling merged directions with their original magnitudes and reintegrating the retained distinct tokens.\n\n\n# experimental evaluation\n\nthe authors evaluate minicache extensively across several popular llms:\n\n * models llama-2, llama-3, mixtral, phi-3.\n * datasets evaluations include gsm8k (math problems), coqa (conversational q&a), truthfulqa, and longbench (long-context tasks).\n * baselines comparisons include fp16 baseline (no compression), quantization-based methods (e.g., kivi, smoothquant), and sparsity-driven methods.\n\n\n# results and findings:\n\n * memory efficiency minicache achieves up to 41% memory reduction and a compression ratio up to 5.02× when combined with 4-bit quantization.\n * throughput enhancement improves inference throughput by approximately 5× compared to the fp16 baseline due to reduced memory footprint, enabling larger batch sizes and faster generation.\n * minimal performance drop compression with minicache leads to near-lossless performance, even under aggressive compression settings.\n\n\n# ablation studies:\n\n * interpolation parameter a critical hyperparameter determining the balance in merging adjacent kv pairs—optimal around t=0.6.\n * retention threshold optimal token retention (γ=0.05) strikes the best balance between accuracy and memory usage.\n\n\n# contributions summarized\n\n * the paper proposes a novel depth-wise kv cache compression method, minicache.\n * it identifies cross-layer kv cache redundancy as a previously unexplored yet crucial dimension.\n * it introduces robust merging via slerp interpolation and a targeted retention strategy for distinct tokens.\n * experimental validation highlights substantial efficiency improvements, minimal accuracy degradation, and strong compatibility with existing methods.\n\n\n# limitations and future work\n\n * the slerp-based merging function is currently limited to pairwise merging. future extensions could explore simultaneous merging across multiple layers.\n * further exploration into adaptive interpolation parameters based on the relative magnitude ratio of vectors is identified as promising.\n\n\n# conclusion\n\n"minicache" successfully identifies and exploits an important new dimension—depth-wise redundancy in kv caches of llms.\nits combination of reparameterization-based merging and selective token retention provides significant improvements in memory efficiency and inference throughput, establishing a new direction for research and practical optimization in deploying large-scale language models.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM Posttraining/Finetuning",frontmatter:{title:"LLM Posttraining/Finetuning",date:"2025-02-05T23:32:49.000Z",permalink:"/pages/dc7058/",tags:[null]},regularPath:"/05.llm/22.llm_post.html",relativePath:"05.llm/22.llm_post.md",key:"v-33912107",path:"/pages/dc7058/",headers:[{level:2,title:"1.[1171] Towards a Human-like Open-Domain Chatbot",slug:"_1-1171-towards-a-human-like-open-domain-chatbot",normalizedTitle:"1.[1171] towards a human-like open-domain chatbot",charIndex:403},{level:3,title:'Key Insights from "Towards a Human-like Open-Domain Chatbot"',slug:"key-insights-from-towards-a-human-like-open-domain-chatbot",normalizedTitle:"key insights from &quot;towards a human-like open-domain chatbot&quot;",charIndex:null},{level:3,title:"1. Main Contributions",slug:"_1-main-contributions",normalizedTitle:"1. main contributions",charIndex:885},{level:3,title:"2. Key Findings",slug:"_2-key-findings",normalizedTitle:"2. key findings",charIndex:2069},{level:3,title:"3. Major Improvements Over Prior Work",slug:"_3-major-improvements-over-prior-work",normalizedTitle:"3. major improvements over prior work",charIndex:2998},{level:3,title:"4. Surprising & Unintuitive Findings",slug:"_4-surprising-unintuitive-findings",normalizedTitle:"4. surprising &amp; unintuitive findings",charIndex:null},{level:3,title:"5. Future Research & Limitations",slug:"_5-future-research-limitations",normalizedTitle:"5. future research &amp; limitations",charIndex:null},{level:3,title:"6. Summary & Takeaways",slug:"_6-summary-takeaways",normalizedTitle:"6. summary &amp; takeaways",charIndex:null},{level:2,title:"3. [Y2025] o3-mini vs DeepSeek-R1: Which One is Safer?",slug:"_3-y2025-o3-mini-vs-deepseek-r1-which-one-is-safer",normalizedTitle:"3. [y2025] o3-mini vs deepseek-r1: which one is safer?",charIndex:147}],headersStr:'1.[1171] Towards a Human-like Open-Domain Chatbot Key Insights from "Towards a Human-like Open-Domain Chatbot" 1. Main Contributions 2. Key Findings 3. Major Improvements Over Prior Work 4. Surprising & Unintuitive Findings 5. Future Research & Limitations 6. Summary & Takeaways 3. [Y2025] o3-mini vs DeepSeek-R1: Which One is Safer?',content:' 1. [1171 Google] Towards a Human-like Open-Domain Chatbot\n 2. [10596 OpenAI] Training language models to follow instructions with human feedback\n 3. [Y2025] o3-mini vs DeepSeek-R1: Which One is Safer?\n 4. [659] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers 👍\n 5. [2183] QLORA: Efficient Finetuning of Quantized LLMs 👍\n\n----------------------------------------\n\n\n# 1.[1171] Towards a Human-like Open-Domain Chatbot\n\n\n# Key Insights from "Towards a Human-like Open-Domain Chatbot"\n\nThis paper presents Meena , a 2.6B parameter open-domain chatbot developed by Google Research, trained end-to-end on 40B words mined from public-domain social media conversations.\n\nThe key contributions include the introduction of a novel evaluation metric (SSA) and demonstrating that perplexity correlates with chatbot quality . Below are the major takeaways:\n\n\n# 1. Main Contributions\n\n# 1.1. Sensibleness and Specificity Average (SSA)\n\n * SSA measures conversational quality using two fundamental aspects:\n   1. Sensibleness – whether a response makes sense in the context.\n   2. Specificity – whether the response is meaningful rather than generic (e.g., avoiding vague replies like "I don\'t know").\n * Findings: SSA strongly correlates with perplexity , meaning that training models to minimize perplexity improves conversation quality.\n\n# 1.2. Correlation Between Perplexity and SSA\n\n * Perplexity (how well the model predicts the next token) is a strong predictor of chatbot quality.\n * R² = 0.93 correlation between perplexity and SSA in multi-turn evaluations.\n * Suggests that improving perplexity will lead to near-human conversation quality (human SSA = 86%, Meena = 79%).\n\n# 1.3. Large-Scale End-to-End Training\n\n * Uses an Evolved Transformer architecture (a variant of the Transformer optimized via Neural Architecture Search).\n * 2.6B parameters , trained with Adafactor optimizer on a TPU-v3 Pod .\n * Unlike retrieval-based or hand-crafted chatbots (like Mitsuku, Cleverbot), Meena is purely generative and trained end-to-end .\n\n\n# 2. Key Findings\n\n# 2.1. Meena Outperforms Other Chatbots\n\n * SSA Scores (higher is better):\n   * Human: 86%\n   * Meena: 79% (72% base model, 79% after improvements)\n   * Mitsuku: 56%\n   * DialoGPT: 48%\n   * Cleverbot: 44%\n   * XiaoIce: 31%\n * Meena is closer to human-level conversations than any previous chatbot.\n\n# 2.2. Model Scaling & Data Quality Matter More Than Architecture\n\n * Training data plays a major role in performance.\n * A large dataset with diverse conversations improves chatbot quality significantly.\n * End-to-end models can outperform complex rule-based systems when trained at scale.\n\n# 2.3. Static vs. Interactive Evaluation\n\n * Static Evaluation: Measured SSA over a fixed dataset of 1,477 multi-turn conversations .\n * Interactive Evaluation: Humans freely chatted with the chatbot, with responses evaluated by crowd workers .\n * Findings: Both evaluations showed strong perplexity-SSA correlation.\n\n\n# 3. Major Improvements Over Prior Work\n\n# 3.1. Better Decoding Strategy: Sample-and-Rank\n\nSample and rank might be the most important contribution in this paper. 👍\n\n * Standard Beam Search often produces generic, repetitive responses.\n * Meena uses Sample-and-Rank:\n   \n   1. Generate N=20 diverse responses using temperature-controlled sampling .\n   \n   2. Rank them based on likelihood and select the most contextually appropriate one .\n\n * This significantly increases specificity.\n\n# 3.2. Filtering Out Repetitions\n\n * Common failure mode: Chatbots repeat responses across turns.\n * Fix: Apply a cross-turn repetition filter , which improved SSA by 5% .\n\n# 3.3. Tuning Perplexity Further\n\n * Further reducing perplexity could push SSA beyond 86% , reaching human-like conversational quality.\n\n\n# 4. Surprising & Unintuitive Findings\n\n# 4.1. Higher Perplexity = Worse Conversations\n\n * Unlike BLEU or ROUGE (which are poor correlates of chatbot quality ), perplexity is a strong predictor of human-like conversations .\n * The best-performing model had a perplexity of 10.2 .\n\n# 4.2. Human Sensibleness vs. Specificity\n\n * Humans are nearly always sensible (~97%) but often fail on specificity (~75%).\n * Meena\'s specificity (70%) is closer to humans than previous chatbots.\n\n# 4.3. Some Rule-Based Chatbots (e.g., Mitsuku, Cleverbot) Still Compete\n\n * While Meena outperforms them, some rule-based chatbots still generate sensible responses in specific scenarios .\n * Mitsuku scored 56% SSA , better than many neural models.\n\n\n# 5. Future Research & Limitations\n\n# 5.1. Remaining Challenges\n\n * Does not handle long-term memory well.\n * Still produces some inconsistent or repetitive responses.\n * Fails on deep reasoning, knowledge discussion, humor, and empathy.\n * Cannot fact-check or validate external knowledge.\n * SSA does not measure other aspects of human conversation (e.g., empathy, humor, reasoning).\n\n# 5.2. What’s Next?\n\n * Scaling the model further could improve performance beyond 86% SSA .\n * Integrating retrieval mechanisms (e.g., knowledge-grounded chatbots).\n * Expanding evaluation metrics beyond SSA (e.g., humor, empathy, coherence).\n\n\n# 6. Summary & Takeaways\n\n * ✅ Introduces Meena , a 2.6B parameter chatbot , trained on 40B words .\n * ✅ SSA metric (Sensibleness + Specificity) shows strong correlation with Perplexity.\n * ✅ Meena outperforms previous chatbots, reaching 79% SSA (human-level: 86%).\n * ✅ Uses "Sample-and-Rank" decoding, outperforming Beam Search.\n * ✅ Higher perplexity = lower conversational quality (counterintuitive finding).\n * ✅ Future improvements could bring it to human-like levels. This paper is a landmark step towards human-like chatbots , demonstrating that scaling data and reducing perplexity significantly improve conversation quality .\n\n----------------------------------------\n\n\n# 3. [Y2025] o3-mini vs DeepSeek-R1: Which One is Safer?\n\nDeepSeek-R1 is not safer compared with o3-mini.',normalizedContent:' 1. [1171 google] towards a human-like open-domain chatbot\n 2. [10596 openai] training language models to follow instructions with human feedback\n 3. [y2025] o3-mini vs deepseek-r1: which one is safer?\n 4. [659] gptq: accurate post-training quantization for generative pre-trained transformers 👍\n 5. [2183] qlora: efficient finetuning of quantized llms 👍\n\n----------------------------------------\n\n\n# 1.[1171] towards a human-like open-domain chatbot\n\n\n# key insights from "towards a human-like open-domain chatbot"\n\nthis paper presents meena , a 2.6b parameter open-domain chatbot developed by google research, trained end-to-end on 40b words mined from public-domain social media conversations.\n\nthe key contributions include the introduction of a novel evaluation metric (ssa) and demonstrating that perplexity correlates with chatbot quality . below are the major takeaways:\n\n\n# 1. main contributions\n\n# 1.1. sensibleness and specificity average (ssa)\n\n * ssa measures conversational quality using two fundamental aspects:\n   1. sensibleness – whether a response makes sense in the context.\n   2. specificity – whether the response is meaningful rather than generic (e.g., avoiding vague replies like "i don\'t know").\n * findings: ssa strongly correlates with perplexity , meaning that training models to minimize perplexity improves conversation quality.\n\n# 1.2. correlation between perplexity and ssa\n\n * perplexity (how well the model predicts the next token) is a strong predictor of chatbot quality.\n * r² = 0.93 correlation between perplexity and ssa in multi-turn evaluations.\n * suggests that improving perplexity will lead to near-human conversation quality (human ssa = 86%, meena = 79%).\n\n# 1.3. large-scale end-to-end training\n\n * uses an evolved transformer architecture (a variant of the transformer optimized via neural architecture search).\n * 2.6b parameters , trained with adafactor optimizer on a tpu-v3 pod .\n * unlike retrieval-based or hand-crafted chatbots (like mitsuku, cleverbot), meena is purely generative and trained end-to-end .\n\n\n# 2. key findings\n\n# 2.1. meena outperforms other chatbots\n\n * ssa scores (higher is better):\n   * human: 86%\n   * meena: 79% (72% base model, 79% after improvements)\n   * mitsuku: 56%\n   * dialogpt: 48%\n   * cleverbot: 44%\n   * xiaoice: 31%\n * meena is closer to human-level conversations than any previous chatbot.\n\n# 2.2. model scaling & data quality matter more than architecture\n\n * training data plays a major role in performance.\n * a large dataset with diverse conversations improves chatbot quality significantly.\n * end-to-end models can outperform complex rule-based systems when trained at scale.\n\n# 2.3. static vs. interactive evaluation\n\n * static evaluation: measured ssa over a fixed dataset of 1,477 multi-turn conversations .\n * interactive evaluation: humans freely chatted with the chatbot, with responses evaluated by crowd workers .\n * findings: both evaluations showed strong perplexity-ssa correlation.\n\n\n# 3. major improvements over prior work\n\n# 3.1. better decoding strategy: sample-and-rank\n\nsample and rank might be the most important contribution in this paper. 👍\n\n * standard beam search often produces generic, repetitive responses.\n * meena uses sample-and-rank:\n   \n   1. generate n=20 diverse responses using temperature-controlled sampling .\n   \n   2. rank them based on likelihood and select the most contextually appropriate one .\n\n * this significantly increases specificity.\n\n# 3.2. filtering out repetitions\n\n * common failure mode: chatbots repeat responses across turns.\n * fix: apply a cross-turn repetition filter , which improved ssa by 5% .\n\n# 3.3. tuning perplexity further\n\n * further reducing perplexity could push ssa beyond 86% , reaching human-like conversational quality.\n\n\n# 4. surprising & unintuitive findings\n\n# 4.1. higher perplexity = worse conversations\n\n * unlike bleu or rouge (which are poor correlates of chatbot quality ), perplexity is a strong predictor of human-like conversations .\n * the best-performing model had a perplexity of 10.2 .\n\n# 4.2. human sensibleness vs. specificity\n\n * humans are nearly always sensible (~97%) but often fail on specificity (~75%).\n * meena\'s specificity (70%) is closer to humans than previous chatbots.\n\n# 4.3. some rule-based chatbots (e.g., mitsuku, cleverbot) still compete\n\n * while meena outperforms them, some rule-based chatbots still generate sensible responses in specific scenarios .\n * mitsuku scored 56% ssa , better than many neural models.\n\n\n# 5. future research & limitations\n\n# 5.1. remaining challenges\n\n * does not handle long-term memory well.\n * still produces some inconsistent or repetitive responses.\n * fails on deep reasoning, knowledge discussion, humor, and empathy.\n * cannot fact-check or validate external knowledge.\n * ssa does not measure other aspects of human conversation (e.g., empathy, humor, reasoning).\n\n# 5.2. what’s next?\n\n * scaling the model further could improve performance beyond 86% ssa .\n * integrating retrieval mechanisms (e.g., knowledge-grounded chatbots).\n * expanding evaluation metrics beyond ssa (e.g., humor, empathy, coherence).\n\n\n# 6. summary & takeaways\n\n * ✅ introduces meena , a 2.6b parameter chatbot , trained on 40b words .\n * ✅ ssa metric (sensibleness + specificity) shows strong correlation with perplexity.\n * ✅ meena outperforms previous chatbots, reaching 79% ssa (human-level: 86%).\n * ✅ uses "sample-and-rank" decoding, outperforming beam search.\n * ✅ higher perplexity = lower conversational quality (counterintuitive finding).\n * ✅ future improvements could bring it to human-like levels. this paper is a landmark step towards human-like chatbots , demonstrating that scaling data and reducing perplexity significantly improve conversation quality .\n\n----------------------------------------\n\n\n# 3. [y2025] o3-mini vs deepseek-r1: which one is safer?\n\ndeepseek-r1 is not safer compared with o3-mini.',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM Compression",frontmatter:{title:"LLM Compression",date:"2025-04-02T12:32:49.000Z",permalink:"/pages/dc7062/",tags:[null]},regularPath:"/05.llm/25.llm_optimizer.html",relativePath:"05.llm/25.llm_optimizer.md",key:"v-66fc8ee5",path:"/pages/dc7062/",headers:[{level:2,title:"1. [1] BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models",slug:"_1-1-badam-a-memory-efficient-full-parameter-optimization-method-for-large-language-models",normalizedTitle:"1. [1] badam: a memory efficient full parameter optimization method for large language models",charIndex:1},{level:2,title:"2. [1075] Adafactor Adaptive Learning Rates with Sublinear Memory Cost",slug:"_2-1075-adafactor-adaptive-learning-rates-with-sublinear-memory-cost",normalizedTitle:"2. [1075] adafactor adaptive learning rates with sublinear memory cost",charIndex:96},{level:2,title:"[3] Adam Accumulation to Reduce Memory Footprints of both Activations and Gradients for Large-scale DNN Training",slug:"_3-adam-accumulation-to-reduce-memory-footprints-of-both-activations-and-gradients-for-large-scale-dnn-training",normalizedTitle:"[3] adam accumulation to reduce memory footprints of both activations and gradients for large-scale dnn training",charIndex:171},{level:3,title:"Gradient Accumulation & Gradient Release",slug:"gradient-accumulation-gradient-release",normalizedTitle:"gradient accumulation &amp; gradient release",charIndex:null},{level:3,title:"Adam Accumulation (AdamA)",slug:"adam-accumulation-adama",normalizedTitle:"adam accumulation (adama)",charIndex:5867},{level:3,title:"Elaboration",slug:"elaboration",normalizedTitle:"elaboration",charIndex:6557},{level:3,title:"Mechanism",slug:"mechanism",normalizedTitle:"mechanism",charIndex:7178}],headersStr:"1. [1] BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models 2. [1075] Adafactor Adaptive Learning Rates with Sublinear Memory Cost [3] Adam Accumulation to Reduce Memory Footprints of both Activations and Gradients for Large-scale DNN Training Gradient Accumulation & Gradient Release Adam Accumulation (AdamA) Elaboration Mechanism",content:" 1. [1] BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models\n 2. [1075] Adafactor Adaptive Learning Rates with Sublinear Memory Cost\n 3. [3] Adam Accumulation to Reduce Memory Footprints of both Activations and Gradients for Large-scale DNN Training\n\n----------------------------------------\n\n\n# 1. [1] BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models\n\nIt change the finetuing into block coordinate descent (BCD)-type optimization which I dont understand.\n\nFor instance, to finetune an LLM with M billion parameters, Adam [23] necessitates roughly 18M GB of GPU memory for successful training, and this estimate does not even account for the storage of activations used in the backpropagation (BP) process.\n\nDespite the success of PEFT methods, finetuning within a substantially lower-dimensional subspace may potentially limit downstream performance.\n\n\n\nWe first analyze the memory cost of Adam with mixed precision training.\n\nOne needs to store the FP16 model parameters for the BP process, which costs 2M memory.\n\nFor a more precise update, the optimizer also maintains a master copy of a FP32 model, which costs 4M memory.\n\nThen, it comes to store the gradient (converted to FP32), momentum, and second moment in FP32 precision, costing 4M + 4M + 4M = 12M memory.\n\nIn total, Adam needs roughly 18M memory.\n\nIn terms of BAdam, it needs to store the up-to-date model parameters (see Figure 1) in FP16 precision, which costs 2M memory. Importantly, since BAdam only updates the active block at one time, we can store the model parameters, gradient, momentum, and second moment only for the active block θπi in FP32 precision, where the FP32 model parameters and gradient of the active block can be obtained by transforming their FP16 versions to the FP32 versions.\n\nLet us consider the simple case where the partitioned D blocks are equal-sized. Then, BAdam only needs in total\n\n\n\n----------------------------------------\n\n\n# 2. [1075] Adafactor Adaptive Learning Rates with Sublinear Memory Cost\n\nThis paper introduces Adafactor , an optimization algorithm designed to provide adaptive learning rates with significantly reduced memory overhead compared to traditional methods like Adam.\n\nThe core innovation is replacing full per-parameter second-moment estimators (used to scale gradients) with factored approximations based on the row and column sums of squared gradients.\n\nSimilar to LORA, using only the per-row and percolumn sums of these moving averages, and estimating the per-parameter second moments based on these sums.\n\nThis change reduces memory usage from O(nm) to O(n + m) for matrix-shaped parameters.\n\nKey contributions and details include:\n\n * Factored Second Moment Estimation :\n   \n   * Adafactor leverages a rank-1 approximation using the generalized Kullback-Leibler divergence to estimate the second moment of gradients.\n   * Instead of storing full-size accumulators, it maintains exponential moving averages of row and column sums, achieving memory savings while preserving empirical performance.\n   * A closed-form solution for the rank-1 approximation ensures computational efficiency and compatibility with exponential smoothing.\n\n * Removing Momentum :\n   \n   * Momentum (first moment) is omitted (β₁ = 0) to further reduce memory cost. This change initially causes instability, especially without learning rate warmup.\n\n * Stabilizing Updates :\n   \n   * Update Clipping : Caps the RMS of unscaled updates to avoid large, destabilizing parameter jumps due to outdated second-moment estimators.\n   * Increasing β₂ Schedule : Proposes a decay schedule like β̂₂ₜ = 1 − t^(-c), which adapts over time and avoids the need for bias correction.\n   * These methods independently and jointly stabilize training in the absence of momentum and warmup.\n\n * Relative Step Sizes :\n   \n   * Instead of fixed absolute learning rates, Adafactor uses parameter-relative step sizes , scaling updates based on the parameter norm, making it more resilient to differing parameter magnitudes (e.g., in embeddings).\n\n * Experiments :\n   \n   * Conducted on Transformer models for WMT’14 En→De machine translation.\n   * Adafactor with factored moments, no momentum, update clipping, increasing decay rate, and relative step sizes performs comparably to Adam while using less memory.\n   * Results show robustness to poor parameter initialization and scaling, unlike Adam.\n\n * Practical Use :\n   \n   * Adafactor enables training larger models on memory-constrained hardware.\n   * Implementation is available in the Tensor2Tensor library.\n\nThree-Sentence Summary:\n\nThe paper proposes Adafactor , a memory-efficient optimizer that approximates second-moment estimators using factored row and column sums, drastically reducing auxiliary memory usage from O(nm) to O(n + m).\n\nTo address instability caused by omitting momentum and slow-decaying estimators, the authors introduce update clipping and an increasing decay schedule , both of which stabilize training.\n\nCombined with relative step sizes , Adafactor achieves comparable performance to Adam on large-scale Transformer tasks while enabling significantly larger models on limited hardware.\n\n----------------------------------------\n\n\n# [3] Adam Accumulation to Reduce Memory Footprints of both Activations and Gradients for Large-scale DNN Training\n\n\n# Gradient Accumulation & Gradient Release\n\nGradient accumulation reduces the activation memory by splitting a mini-batch into a sequence of micro batches and accumulating the gradients of all micro-batches.\n\nGradient accumulation must preserve accumulated value of gradients until the last micro-batch.\n\nGradient release reduces the gradient memory by freeing up the gradient-occupied space in a layer-by-layer manner.\n\nGradient release releases the gradients immediately after use.\n\n\n# Adam Accumulation (AdamA)\n\nSpecifically, instead of accumulating gradients, AdamA integrates gradients into optimizer states (m and v in Adam) immediately after the gradients are produced, and accumulates optimizer states sequentially over micro-batches.\n\nThis subtle change of directly integrating gradients to optimizer states makes the memory space for whole model gradients no longer needed, eliminating the aforementioned contradiction between preserving gradients and releasing gradients.\n\nConsequently, AdamA can reduce the gradient memory to 1/M of the original (M is the number of layers), and the activation memory to 1/N of the original (N is the number of micro-batches).\n\n\n\n\n# Elaboration\n\nThe key idea behind gradient accumulation is to split a mini-batch into several micro-batches.\n\nThis method computes the gradients of micro-batches sequentially and accumulates them to reduce the memory footprint of activations as well as to keep the same convergence properties as the original mini-batch.\n\nGradient release executes the backward process in a layer-by-layer manner, which immediately releases the gradient-occupied memory after the weight updating is finished, so that the memory allocated for gradients can be reduced from the size of whole model size to the size of the maximum layer.\n\n\n# Mechanism\n\nIntuitively, as gradients are eventually used to update the optimizer states (m and v in Adam), if we can integrate gradients into optimizer states in advance, the gradients memory can be released, thus resolving this dilemma.\n\nInspired by this insight, we for the first time propose an optimizer accumulation method, namely AdamA, that integrates gradients into optimizer states immediately after produced and then accumulates optimizer states sequentially over micro-batches.",normalizedContent:" 1. [1] badam: a memory efficient full parameter optimization method for large language models\n 2. [1075] adafactor adaptive learning rates with sublinear memory cost\n 3. [3] adam accumulation to reduce memory footprints of both activations and gradients for large-scale dnn training\n\n----------------------------------------\n\n\n# 1. [1] badam: a memory efficient full parameter optimization method for large language models\n\nit change the finetuing into block coordinate descent (bcd)-type optimization which i dont understand.\n\nfor instance, to finetune an llm with m billion parameters, adam [23] necessitates roughly 18m gb of gpu memory for successful training, and this estimate does not even account for the storage of activations used in the backpropagation (bp) process.\n\ndespite the success of peft methods, finetuning within a substantially lower-dimensional subspace may potentially limit downstream performance.\n\n\n\nwe first analyze the memory cost of adam with mixed precision training.\n\none needs to store the fp16 model parameters for the bp process, which costs 2m memory.\n\nfor a more precise update, the optimizer also maintains a master copy of a fp32 model, which costs 4m memory.\n\nthen, it comes to store the gradient (converted to fp32), momentum, and second moment in fp32 precision, costing 4m + 4m + 4m = 12m memory.\n\nin total, adam needs roughly 18m memory.\n\nin terms of badam, it needs to store the up-to-date model parameters (see figure 1) in fp16 precision, which costs 2m memory. importantly, since badam only updates the active block at one time, we can store the model parameters, gradient, momentum, and second moment only for the active block θπi in fp32 precision, where the fp32 model parameters and gradient of the active block can be obtained by transforming their fp16 versions to the fp32 versions.\n\nlet us consider the simple case where the partitioned d blocks are equal-sized. then, badam only needs in total\n\n\n\n----------------------------------------\n\n\n# 2. [1075] adafactor adaptive learning rates with sublinear memory cost\n\nthis paper introduces adafactor , an optimization algorithm designed to provide adaptive learning rates with significantly reduced memory overhead compared to traditional methods like adam.\n\nthe core innovation is replacing full per-parameter second-moment estimators (used to scale gradients) with factored approximations based on the row and column sums of squared gradients.\n\nsimilar to lora, using only the per-row and percolumn sums of these moving averages, and estimating the per-parameter second moments based on these sums.\n\nthis change reduces memory usage from o(nm) to o(n + m) for matrix-shaped parameters.\n\nkey contributions and details include:\n\n * factored second moment estimation :\n   \n   * adafactor leverages a rank-1 approximation using the generalized kullback-leibler divergence to estimate the second moment of gradients.\n   * instead of storing full-size accumulators, it maintains exponential moving averages of row and column sums, achieving memory savings while preserving empirical performance.\n   * a closed-form solution for the rank-1 approximation ensures computational efficiency and compatibility with exponential smoothing.\n\n * removing momentum :\n   \n   * momentum (first moment) is omitted (β₁ = 0) to further reduce memory cost. this change initially causes instability, especially without learning rate warmup.\n\n * stabilizing updates :\n   \n   * update clipping : caps the rms of unscaled updates to avoid large, destabilizing parameter jumps due to outdated second-moment estimators.\n   * increasing β₂ schedule : proposes a decay schedule like β₂ₜ = 1 − t^(-c), which adapts over time and avoids the need for bias correction.\n   * these methods independently and jointly stabilize training in the absence of momentum and warmup.\n\n * relative step sizes :\n   \n   * instead of fixed absolute learning rates, adafactor uses parameter-relative step sizes , scaling updates based on the parameter norm, making it more resilient to differing parameter magnitudes (e.g., in embeddings).\n\n * experiments :\n   \n   * conducted on transformer models for wmt’14 en→de machine translation.\n   * adafactor with factored moments, no momentum, update clipping, increasing decay rate, and relative step sizes performs comparably to adam while using less memory.\n   * results show robustness to poor parameter initialization and scaling, unlike adam.\n\n * practical use :\n   \n   * adafactor enables training larger models on memory-constrained hardware.\n   * implementation is available in the tensor2tensor library.\n\nthree-sentence summary:\n\nthe paper proposes adafactor , a memory-efficient optimizer that approximates second-moment estimators using factored row and column sums, drastically reducing auxiliary memory usage from o(nm) to o(n + m).\n\nto address instability caused by omitting momentum and slow-decaying estimators, the authors introduce update clipping and an increasing decay schedule , both of which stabilize training.\n\ncombined with relative step sizes , adafactor achieves comparable performance to adam on large-scale transformer tasks while enabling significantly larger models on limited hardware.\n\n----------------------------------------\n\n\n# [3] adam accumulation to reduce memory footprints of both activations and gradients for large-scale dnn training\n\n\n# gradient accumulation & gradient release\n\ngradient accumulation reduces the activation memory by splitting a mini-batch into a sequence of micro batches and accumulating the gradients of all micro-batches.\n\ngradient accumulation must preserve accumulated value of gradients until the last micro-batch.\n\ngradient release reduces the gradient memory by freeing up the gradient-occupied space in a layer-by-layer manner.\n\ngradient release releases the gradients immediately after use.\n\n\n# adam accumulation (adama)\n\nspecifically, instead of accumulating gradients, adama integrates gradients into optimizer states (m and v in adam) immediately after the gradients are produced, and accumulates optimizer states sequentially over micro-batches.\n\nthis subtle change of directly integrating gradients to optimizer states makes the memory space for whole model gradients no longer needed, eliminating the aforementioned contradiction between preserving gradients and releasing gradients.\n\nconsequently, adama can reduce the gradient memory to 1/m of the original (m is the number of layers), and the activation memory to 1/n of the original (n is the number of micro-batches).\n\n\n\n\n# elaboration\n\nthe key idea behind gradient accumulation is to split a mini-batch into several micro-batches.\n\nthis method computes the gradients of micro-batches sequentially and accumulates them to reduce the memory footprint of activations as well as to keep the same convergence properties as the original mini-batch.\n\ngradient release executes the backward process in a layer-by-layer manner, which immediately releases the gradient-occupied memory after the weight updating is finished, so that the memory allocated for gradients can be reduced from the size of whole model size to the size of the maximum layer.\n\n\n# mechanism\n\nintuitively, as gradients are eventually used to update the optimizer states (m and v in adam), if we can integrate gradients into optimizer states in advance, the gradients memory can be released, thus resolving this dilemma.\n\ninspired by this insight, we for the first time propose an optimizer accumulation method, namely adama, that integrates gradients into optimizer states immediately after produced and then accumulates optimizer states sequentially over micro-batches.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"LLM MOE Inference",frontmatter:{title:"LLM MOE Inference",date:"2025-03-23T23:32:49.000Z",permalink:"/pages/dc7060/",tags:[null]},regularPath:"/05.llm/23.llm_moe.html",relativePath:"05.llm/23.llm_moe.md",key:"v-5e4b7fb6",path:"/pages/dc7060/",headers:[{level:2,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:126},{level:2,title:"Expert Routing and Load Balancing",slug:"expert-routing-and-load-balancing",normalizedTitle:"expert routing and load balancing",charIndex:2460},{level:2,title:"Latency and Parallelism Bottlenecks",slug:"latency-and-parallelism-bottlenecks",normalizedTitle:"latency and parallelism bottlenecks",charIndex:7764},{level:2,title:"Memory Usage and Bandwidth Constraints",slug:"memory-usage-and-bandwidth-constraints",normalizedTitle:"memory usage and bandwidth constraints",charIndex:15710},{level:2,title:"Hardware Compatibility (TPUs, GPUs, and Systems)",slug:"hardware-compatibility-tpus-gpus-and-systems",normalizedTitle:"hardware compatibility (tpus, gpus, and systems)",charIndex:25128},{level:2,title:"Expert Sparsity and Activation Imbalance",slug:"expert-sparsity-and-activation-imbalance",normalizedTitle:"expert sparsity and activation imbalance",charIndex:32604},{level:2,title:"Real-Time and Streaming Inference",slug:"real-time-and-streaming-inference",normalizedTitle:"real-time and streaming inference",charIndex:40554},{level:2,title:"Conclusion",slug:"conclusion",normalizedTitle:"conclusion",charIndex:48322}],headersStr:"Introduction Expert Routing and Load Balancing Latency and Parallelism Bottlenecks Memory Usage and Bandwidth Constraints Hardware Compatibility (TPUs, GPUs, and Systems) Expert Sparsity and Activation Imbalance Real-Time and Streaming Inference Conclusion",content:"> This is deepresearch report generated from ChatGPT.\n\n\n# Inference-Time Challenges in Mixture-of-Experts Models for LLMs\n\n\n# Introduction\n\nMixture-of-Experts (MoE) architectures have enabled trillion-parameter scale language models while keeping inference computation per token relatively low.\n\nUnlike dense Transformers where every layer’s feed-forward network (FFN) is applied to every input, MoE models activate only a few “expert” FFN sub-models per input token SEAS.UPENN.EDU.\n\nFor example, Google’s 1.2 trillion-parameter GLaM model routes each token to 2 out of 64 experts, achieving better accuracy than GPT-3 (175B) while using about 50% fewer FLOPs per token AR5IV.ORG AR5IV.ORG.\n\nSimilarly, a Vision MoE (V-MoE) matched the performance of state-of-the-art vision models using half the inference compute of a dense network ARXIV.ORG .\n\nThese sparse models demonstrate that enormous parameter counts can be leveraged without proportional increases in computation.\n\nHowever, inference-time performance of MoEs often falls short of these theoretical gains.\n\nAs Fedus et al. note, adoption of MoE models has been hindered by added system complexity and communication overhead on current hardware JMLR.ORG.\n\nIn practice, a MoE’s massive parameter size and dynamic token routing can introduce new bottlenecks.\n\nServing a MoE at scale requires careful orchestration:\n\n * experts are typically distributed across devices (e.g. via Google’s GShard sharding framework SEAS.UPENN.EDU )\n * extra networking is needed to route tokens and gather results.\n\nThese factors can lead to higher latency, memory usage, and engineering challenges during inference.\n\nIn fact, Microsoft found that without special optimizations, a quality-equivalent MoE can be 15× slower in inference latency than a dense model despite similar FLOPs SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nThis survey reviews the key inference-time challenges of MoE-based large language models – focusing on cost and performance issues (not training instabilities) – and summarizes notable findings and solutions from both research literature and industry implementations (Google’s Switch Transformer and GLaM, Meta’s NLLB MoE, Microsoft’s DeepSpeed-MoE, Alibaba’s M6, etc.).\n\nWe structure the discussion by major challenge areas:\n\n * expert routing and load balancing\n * latency and parallelism\n * memory and bandwidth\n * hardware compatibility\n * expert sparsity/imbalance\n * real-time streaming inference\n\n\n# Expert Routing and Load Balancing\n\nRouting tokens to experts is at the heart of MoE inference.\n\nA learned gating network computes scores for each expert and selects the top-$k$ experts for each token (typically $k=1$ or $k=2$) SEAS.UPENN.EDU SEAS.UPENN.EDU .\n\nThis conditional routing can lead to load imbalance : some experts may receive far more tokens than others in a given inference batch.\n\nIf one expert is overloaded, it becomes a bottleneck while other experts sit idle.\n\nTo mitigate this, MoE training regimes include an auxiliary load-balancing loss that nudges the gate toward a uniform token-to-expert distribution JMLR.ORG JMLR.ORG.\n\nFor instance, the Switch Transformer used a gating capacity factor (often 1.2× the average load) and would drop excess tokens if an expert’s capacity was exceeded JMLR.ORG JMLR.ORG.\n\nIn practice, with well-tuned capacity, only a small fraction of tokens (<1%) are dropped during training/inference in Switch Transformer JMLR.ORG JMLR.ORG.\n\nThis ensures each expert processes roughly similar numbers of tokens, avoiding stragglers that slow down the batch.\n\nDespite such measures, runtime imbalances still occur.\n\nThe distribution of tokens at inference can differ from training averages – e.g. certain prompts might trigger one expert almost exclusively, violating load assumptions SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nEmpirical studies have observed that in a language model MoE, a few “hot” experts consistently receive a large share of tokens while many others get very few or even none for a given workload SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nThis activation imbalance was seen across domains: in one experiment, some experts were always heavily used for Wikipedia text but idle for code (and vice versa) SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nAn imbalanced gate not only under-utilizes model capacity but can also cause latency spikes if a single expert (and its host device) must handle a disproportionate amount of work.\n\nProposed solutions to routing imbalance include improved gating algorithms and coarse-grained routing.\n\nBase Layers (Lewis et al. 2021) formulated expert assignment as an optimal transport problem to ensure each expert gets an equal number of tokens PAPERS.NEURIPS.CC ARXIV.ORG, thereby achieving balanced routing without an auxiliary loss.\n\nAlternatively, hash-based routing replaces the learned gate with a deterministic hash function that assigns tokens to experts uniformly at random PROCEEDINGS.NEURIPS.CC OPENREVIEW.NET.\n\nThis removes gating overhead and guarantees balance, though at a potential cost to quality (since routing is no longer learned).\n\nOn the industry side, Google explored task-level routing to avoid token-level variance: Kudugunta et al. (2021) route entire sentences or tasks to fixed experts, effectively partitioning the model by task ARXIV.ORG ARXIV.ORG.\n\nIn multilingual machine translation, this task-MoE approach allowed extracting a single-expert subnetwork per language pair for inference.\n\nIt preserved all the quality gains of the full MoE while improving peak throughput by 1.9× over token-level MoE, since each inference uses only one expert with no on-the-fly switching ARXIV.ORG ARXIV.ORG .\n\nThe trade-off is reduced flexibility – expert allocation must be decided at the granularity of a sentence or user query – but it guarantees balanced loads and simpler scheduling.\n\nMore dynamically, researchers have proposed adjusting the gate during inference based on observed loads: one recent system tracks the running load on each expert/GPU and redistributes some tokens to less-busy experts to avoid overloads SEAS.UPENN.EDU SEAS.UPENN.EDU . While this may deviate from the gate’s first choice, it can prevent worst-case slowdowns and out-of-memory errors.\n\n> In summary, robust MoE inference requires not just a smart gate, but possibly a capacity-aware router that ensures no expert becomes a throughput sink.\n\nAnother aspect of routing is the overhead of the gating function itself.\n\nThe gating computation and token-to-expert mapping add extra steps to each inference.\n\nFor large MoEs (e.g. hundreds of experts), computing the softmax scores and selecting top-$k$ experts for every token can be non-trivial.\n\nMore significantly, many frameworks historically implemented token routing with inefficient operations (e.g. constructing sparse masks or using scatter/gather on CPUs).\n\nA recent analysis found that the gating operations and associated data shuffling were a major contributor to MoE inference latency and memory use SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nTo address this, researchers introduced Dynamic Gating that more tightly couples the routing decisions with the available capacity.\n\nInstead of always padding each expert’s batch to a fixed size (which wastes computation on “empty” slots), dynamic gating only allocates exactly as many slots as needed per expert, reducing unnecessary computation on placeholder tokens SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nThis optimization was shown to cut gating overhead significantly, enabling inference with larger batch sizes or fewer devices than before.\n\nIn essence, efficient MoE serving demands that the routing mechanism be lightweight and balanced , so that the cost of deciding “which expert?” does not outweigh the benefit of skipping computations.\n\n\n# Latency and Parallelism Bottlenecks\n\nServing a MoE model efficiently requires navigating a complex parallelism hierarchy.\n\nIn a multi-GPU or TPU setup, different experts reside on different devices (this distribution is necessary to fit the huge model in memory, as discussed later).\n\nThus, each inference step involves a pattern of communication and parallel computation: tokens assigned to various experts must be sent to the appropriate devices, processed in parallel, and then the results merged in the original sequence order.\n\nThis all-to-all exchange is a key throughput limiter.\n\nLepikhin et al.’s GShard paper noted that while spreading experts across $N$ devices cuts compute per device, it incurs an all-to-all communication to route tokens and collect outputs SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nFor small batches or many experts, the communication overhead can dominate.\n\nIn fact, early MoE benchmarks attributed the longer inference latency of MoEs primarily to the cost of frequent all-to-all communication between devices SEAS.UPENN.EDU SEAS.UPENN.EDU .\n\nParallelism bottlenecks manifest especially when the batch or sequence length is not large enough to fully utilize all experts concurrently.\n\nIn an ideal scenario, if we have as many tokens as experts, each expert/device can handle one slice of the batch concurrently – achieving near-linear speedups.\n\nThis is the best-case view described by Microsoft’s DeepSpeed-MoE team: for a 52B MoE model with 128 experts of 1.3B each, each token only activates a 1.3B subnetwork, so distributing those experts across 128 GPUs means each device handles 1/128 of the model per token (1.3B parameters) MICROSOFT.COM MICROSOFT.COM.\n\nIn theory, with perfect parallelization and no comm overhead, such a model could run 5× faster per token than a dense 6.7B model that offers similar quality MICROSOFT.COM MICROSOFT.COM.\n\nIn practice, however, achieving this requires careful coordination.\n\nThe worst-case is that a group of tokens collectively needs every expert, meaning the system ends up touching the full model parameters within a single inference step MICROSOFT.COM MICROSOFT.COM.\n\nIn that regime, latency and throughput suffer.\n\nModern MoE serving systems therefore try to “steer performance toward the best-case” via smart parallelism and scheduling MICROSOFT.COM MICROSOFT.COM .\n\nMicrosoft’s DeepSpeed-MoE inference design, for example, combines multiple parallelism strategies : it uses expert parallelism (different GPUs host different experts and process different token groups concurrently) and traditional tensor or data parallelism for the non-expert layers MICROSOFT.COM MICROSOFT.COM.\n\nBy grouping tokens that share the same expert path on the same device, they minimize redundant weight access and maximize each GPU’s utilization MICROSOFT.COM MICROSOFT.COM.\n\nThis design effectively minimizes each device’s critical path, so that no single GPU has to load all experts’ weights.\n\nExperiments showed that with expert parallelism equal to the number of experts (one expert per GPU), the sequential work per device is dramatically smaller than in an equivalent dense model – in their example, each GPU saw 5× fewer parameters per token than a dense baseline MICROSOFT.COM MICROSOFT.COM.\n\nThe remaining challenge is communication: naive all-to-all algorithms scale poorly as the number of devices grows, since latency increases linearly with devices in typical networks MICROSOFT.COM MICROSOFT.COM.\n\nTo tackle this, DeepSpeed-MoE introduced a custom communication stack with optimizations like hierarchical all-to-all.\n\nInstead of one global shuffle, they first perform intra-node exchange of tokens, then a smaller inter-node exchange MICROSOFT.COM MICROSOFT.COM . This reduced communication hops from $O(p)$ to $O(G + p/G)$ (for $p$ total devices and $G$ devices per node) MICROSOFT.COM MICROSOFT.COM , improving latency for small batches on large clusters.\n\nThey further coordinated communications for expert and data parallel steps to avoid redundant transfers MICROSOFT.COM MICROSOFT.COM.\n\nWith these system optimizations, DeepSpeed reported up to 7.3× lower latency and 4.5× faster inference throughput for MoEs compared to prior systems MICROSOFT.COM MICROSOFT.COM – bringing MoE latency in line with (or better than) dense models of similar quality.\n\nOther industry efforts likewise targeted the all-to-all bottleneck.\n\nMicrosoft’s open-source Tutel library focuses on accelerating MoE inference/training on GPU clusters.\n\nTutel optimized low-level CUDA kernels and NCCL communication, achieving an 8.5× speedup for a single MoE layer on 8 GPUs, and about 40% end-to-end latency reduction for a 1.1-trillion-parameter MoE model spread over 512 GPUs MICROSOFT.COM.\n\nThis was measured against the baseline MoE implementation in Meta’s Fairseq, highlighting how much overhead could be trimmed with custom all-to-all routines.\n\nGoogle’s Switch Transformer team similarly noted that existing hardware and libraries were geared toward dense matrix ops, so they simplified MoE routing (using $k=1$ expert) to reduce overhead JMLR.ORG JMLR.ORG.\n\nBy doing so, they attained 7× faster pre-training than a dense T5-XXL with the same compute budget JMLR.ORG JMLR.ORG – and also found that even with as few as 2 experts (on 2 cores) the MoE provided speedups JMLR.ORG JMLR.ORG.\n\nThis implies MoEs can benefit even “small” deployments if the parallelism is carefully exploited.\n\nDespite these advances, a fundamental issue remains: MoE inference efficiency tends to scale well with throughput (batch size or concurrent requests) but not as well with single-input latency.\n\nIf only one sequence is processed at a time (e.g. an interactive user query), many experts will be idle.\n\nIn a Switch-style MoE, each token is sent to only one expert – so out of (say) 64 experts, 63 do nothing for that token.\n\nThe next token might hit a different expert, but sequentially you’re not utilizing the full model’s parallel potential.\n\nThis is one reason MoE inference can have inconsistent latency.\n\nIn fact, DeepSpeed explicitly notes that inference usually runs at small batch sizes , meaning the time to load model weights from memory often dominates, and lesser compute doesn’t automatically translate to faster inference MICROSOFT.COM MICROSOFT.COM.\n\nSystems mitigate this by packing multiple requests together or running large batches of tokens – but that adds latency or requires enough traffic to batch.\n\nIn real-time streaming, one approach is to use multithreading or coroutines to serve many generation streams on the MoE model simultaneously, effectively faking a larger batch and keeping all experts busy.\n\nAnother approach is the coarse routing mentioned earlier: if a whole session or task can be pinned to a subset of experts, the model effectively reduces to a smaller dense model for that session, avoiding frequent cross-expert communication. This was shown to improve throughput 2–3× in multilingual MoE inference ARXIV.ORG ARXIV.ORG . The trade-off is that it forgoes token-level adaptability in favor of steadier, more predictable usage of experts.\n\nIn summary, latency optimization for MoEs revolves around using parallelism to the fullest and minimizing coordination costs.\n\nTechniques like expert parallelism, optimized collectives, and request batching ensure that the sparse activations translate into actual wall-clock speedups.\n\nWith these in place, MoE models can serve significantly more tokens per second – even outperforming dense models of comparable quality on throughput MICROSOFT.COM MICROSOFT.COM.\n\nBut without such optimizations, an MoE can be slower than a dense model, as the overhead of many small computations and communications overwhelms the FLOP savings SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nAchieving low tail latency for single examples remains challenging, requiring careful scheduling or fallback to distilled models when ultra-low latency is needed.\n\n\n# Memory Usage and Bandwidth Constraints\n\nMoE models trade compute for size – they introduce a huge number of parameters (the experts) that must be stored and accessed efficiently at inference.\n\nA standard Transformer might have billions of parameters, whereas MoE models can scale to trillions of parameters by replicating FFN layers into many experts. For example, Google’s Switch-C Transformer reached 1.6 trillion parameters with 2048 experts HUGGINGFACE.CO HUGGINGFACE.CO , and Alibaba’s M6-T model reportedly pushed to 10 trillion parameters using MoE techniques ALIBABACLOUD.COM ALIBABACLOUD.COM.\n\nIn inference, even though only a fraction of those weights are used per token, all experts generally need to be loaded in memory so that any token can be routed to the appropriate ones HUGGINGFACE.CO HUGGINGFACE.CO.\n\nThis creates a memory footprint far larger than dense models.\n\nAs the Hugging Face team succinctly put it, MoEs *“require high VRAM as all experts are loaded in memory”* HUGGINGFACE.CO HUGGINGFACE.CO . For instance, an 8×7B MoE (Mistral’s Mixtral model) totals 47B parameters loaded (~8× more than a single expert) to allow any combination of 2 experts to be active HUGGINGFACE.CO HUGGINGFACE.CO.\n\nRiquelme et al. (2021) noted a similar overhead in V-MoE vision models: the sparse model needed additional memory compared to a dense one of similar throughput, due to storing many experts that are each rarely used ARXIV.ORG ARXIV.ORG .\n\nBeyond just capacity, memory bandwidth becomes a limiting factor.\n\nIn modern accelerators, reading weights from GPU memory (or across a network) can bottleneck throughput if the working set is large.\n\nDeepSpeed’s developers point out that in MoE inference, the time to load model parameters from memory often outweighs arithmetic costs MICROSOFT.COM MICROSOFT.COM.\n\nThey identify overall model size and achievable memory bandwidth as the main determinants of MoE latency, rather than FLOPs MICROSOFT.COM MICROSOFT.COM.\n\nIn their analysis, a MoE’s “critical path” per token is limited to the few experts selected, but the aggregate parameters needed for a batch could be as large as the entire model MICROSOFT.COM MICROSOFT.COM.\n\nThis means if tokens all go to different experts, the system might still have to touch most of the 52B weights (in their example) within a short time, stressing memory bandwidth.\n\nIndeed, Huang et al. (2023) found that a dense model requiring X GB of memory per GPU might require 3×–9× more memory when converted to a sparsely-activated MoE of similar accuracy SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nThey measured an 8.6× increase in memory usage for a language model MoE versus a dense model (2.2 GB -> 18.9 GB per GPU), due to both the larger parameter set and extra activation buffers for gating SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nSuch demands can easily exhaust device memory or lead to frequent swapping of weights from host memory, which kills inference performance.\n\nTo manage this, model sharding is used extensively.\n\nGShard (Lepikhin et al. 2020) demonstrated that distributing experts across devices allows the MoE’s memory load to be split, albeit at the cost of communication SEAS.UPENN.EDU SEAS.UPENN.EDU .\n\nTypically, each GPU holds a subset of the experts’ weights and a full copy of the shared weights (e.g. attention layers, embeddings) SEAS.UPENN.EDU SEAS.UPENN.EDU . This way, no single GPU needs to store all $E$ experts, reducing per-device memory.\n\nFor example, if you have 8 GPUs and 64 experts, each GPU might store 8 experts.\n\nThe trade-off is that when an input token needs an expert not on its current GPU, the token’s data must be sent to the GPU owning that expert (as described in the previous section). Still, for very large MoEs, sharding is the only way to even fit the model in a cluster’s combined memory. Meta’s 1.1T-parameter MoE in Fairseq was spread across 64+ GPUs, and Microsoft noted that each GPU then only loads ~1/64 of the model weights , enabling the model to be served at all MICROSOFT.COM .\n\nEven with sharding, the total memory footprint can be problematic, especially if multiple models or other workloads share the hardware.\n\nOne recent idea to address this is expert buffering or on-demand loading.\n\nHuang et al. implemented a mechanism that keeps only a small number of “hot” experts in GPU memory and leaves the rest in CPU memory, swapping them in when needed SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nThey observed that MoE workloads often have temporal locality – the same expert tends to be reused for several batches in a row, especially in translation or sequential tasks SEAS.UPENN.EDU SEAS.UPENN.EDU .\n\nExploiting this, their system could significantly cut GPU memory usage (by 5× or more) with minimal performance loss, by dynamically loading experts that haven’t been used recently from CPU to GPU just-in-time SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nThis is somewhat analogous to a cache for expert weights.\n\nOf course, it relies on a fast interconnect and careful pre-fetching to not stall the pipeline.\n\nAnother approach is model compression or merging : for serving, one can reduce MoE model size via weight consolidation.\n\nResearchers have experimented with merging the weights of multiple experts into a single expert or smaller set (“aggregation of experts”) to shrink the model for inference HUGGINGFACE.CO HUGGINGFACE.CO.\n\nThis essentially turns a sparse model into a semi-dense one by averaging or otherwise combining experts, attempting to preserve performance while cutting memory needs.\n\nAlternatively, distillation can be used: train the MoE for quality, then distill its knowledge into a smaller dense model for deployment HUGGINGFACE.CO HUGGINGFACE.CO.\n\nThe Switch Transformer authors showed that by distilling a 7B MoE into a 1.6B dense model, they retained ~30–40% of the MoE’s quality gain over a baseline HUGGINGFACE.CO HUGGINGFACE.CO.\n\nIn other words, a distilled model can outperform a same-size model trained from scratch, thanks to the MoE teacher, and it is much easier to serve (no multi-expert routing).\n\nCompanies like Google and OpenAI often use distillation or weight pruning on giant research models to get something more tractable for production.\n\nThat said, distillation sacrifices some peak quality – for example, the task-MoE approach by Kudugunta et al. preserved 100% of the MoE performance in a slimmed model, whereas distilling their token-level MoE to a dense model kept only 32% of the BLEU improvement ARXIV.ORG ARXIV.ORG .\n\nThus, there is interest in directly optimizing MoE memory efficiency so the full sparse model can be used in production.\n\nOne insight from recent work is that a lot of memory waste in MoEs comes from padding and under-utilized capacity.\n\nBecause frameworks often allocate a fixed-size buffer for each expert’s input (equal to the capacity factor times batch size), an expert that only gets a few tokens will still consume memory as if it got the maximum.\n\nHuang et al. quantified this waste: in their tests, static gating with a generous capacity led to 12.8× more allocated slots than actual tokens for LM inference in the worst case SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nTechniques like the aforementioned dynamic gating and custom kernels aim to eliminate this padding.\n\nThe MegaBlocks system from Stanford reformulated MoE computations as block-sparse matrix ops , allowing irregular batch sizes per expert to be handled without padding HUGGINGFACE.CO HUGGINGFACE.CO.\n\nBy treating the expert weight matrix as blocks and only computing the blocks needed for the non-zero inputs, they could “never drop tokens” and efficiently handle imbalanced loads HUGGINGFACE.CO HUGGINGFACE.CO.\n\nThis improved hardware utilization and cut memory overhead, since an expert with 3 tokens would only produce 3 token’s worth of intermediate activations instead of, say, 32.\n\nIn fact, MegaBlocks reported significant speedups and memory savings on GPU hardware by this method HUGGINGFACE.CO HUGGINGFACE.CO.\n\nSuch custom kernels and memory optimizations are crucial for MoE inference on GPUs, which otherwise might waste VRAM on empty activations.\n\nIn summary, MoE inference is memory-intensive : the model’s many experts must be stored and accessed, and naive implementations incur extra memory overhead from padding and duplicated parameters.\n\nSolutions like expert sharding, intelligent weight offloading (keeping only active experts on device), and reducing padding waste are key to making MoEs fit in available memory and bandwidth budgets.\n\nFurthermore, utilizing the aggregate memory bandwidth of multiple devices is critical – DeepSpeed’s design explicitly achieved speedups by leveraging distributed GPU memory to increase effective bandwidth MICROSOFT.COM MICROSOFT.COM.\n\nWith enough GPUs each handling a slice of the weights, the model can effectively stream parameters from many memory pools in parallel, overcoming the bottleneck of a single memory channel.\n\nThis approach enabled serving models that would otherwise be “bandwidth-bound.”\n\nFor instance, Alibaba’s 10-trillion M6 model could only be trained/inferred by splitting it across 512 GPUs, effectively dividing the enormous parameter matrix into manageable chunks ALIBABACLOUD.COM.\n\nThe bottom line is that to deploy MoEs at scale, one must treat memory as a first-class constraint – optimize what is stored, where it’s stored, and how fast it can be transferred to compute.\n\n\n# Hardware Compatibility (TPUs, GPUs, and Systems)\n\nThe challenges above often interplay with the specifics of the hardware platform.\n\nEarly MoE successes came on Google’s TPU clusters, which provided high-bandwidth interconnects and specialized support for collective operations.\n\nThe TPU software stack (XLA) also required static shapes for all operations. As a result, Google’s MoE implementations (e.g. GShard, Switch Transformer) had to make the routing computation statically allocatable – hence the use of a fixed expert capacity and dropping overflow tokens to maintain a consistent tensor shape JMLR.ORG JMLR.ORG.\n\nThis static allocation simplified compilation on TPU but introduced the padding inefficiencies discussed.\n\nMeanwhile, GPUs offer more flexible programming but traditionally lacked built-in support for dynamic sparse computation.\n\nOff-the-shelf deep learning libraries in 2020–2021 were heavily optimized for dense matrix multiplication and did not cater to dynamic sparsity JMLR.ORG JMLR.ORG.\n\nRunning MoEs on GPU efficiently therefore required considerable engineering.\n\nFacebook (Meta) researchers integrated MoE layers into PyTorch/Fairseq in 2021, but they observed suboptimal scaling until improvements like Tutel came along MICROSOFT.COM MICROSOFT.COM.\n\nThe default NCCL all-to-all, for instance, had high latency beyond a single node MICROSOFT.COM MICROSOFT.COM , prompting custom libraries (Microsoft’s MSCCL and SCCL) to implement faster patterns MICROSOFT.COM MICROSOFT.COM.\n\nHardware topology also affects MoE performance.\n\nOn a TPU pod, each TPU core has direct high-speed links to others in a 2D torus.\n\nThis makes all-to-all communication relatively efficient up to a certain scale, and Google’s MoE models often scaled to thousands of TPU cores (Switch Transformer used 2048 TPU v3 cores for its largest model HUGGINGFACE.CO HUGGINGFACE.CO ).\n\nOn GPU clusters, inter-node communication can be more of a bottleneck (limited by InfiniBand or Ethernet speeds).\n\nThe DeepSpeed team explicitly addressed this by splitting communication into intra-node (on NVLink/NVSwitch) and inter-node phases MICROSOFT.COM MICROSOFT.COM.\n\nThey found that at small message sizes (typical when splitting a batch of tokens), latency dominates bandwidth, so reducing the number of network hops was vital MICROSOFT.COM MICROSOFT.COM . This is a reminder that a MoE that is efficient on one hardware setup might not be on another.\n\nIn fact, Fedus et al. noted that their Switch Transformer was developed with TPUs in mind, but that MoEs “should be similarly trained on GPU clusters” with the right mapping of experts to devices JMLR.ORG JMLR.ORG . Ensuring hardware compatibility often means adapting the parallelism strategy: e.g., using more intra-GPU parallel threads for small expert computations, or merging multiple small matrix ops to better utilize GPU cores (an optimization Tutel performs).\n\nAnother compatibility issue is support for fast dispatching of variable workloads.\n\nEach token in an MoE may go to a different expert, leading to a scatter of work.\n\nTraditional GPU execution likes large, uniform batches for efficiency.\n\nIf each expert only gets a handful of tokens, the GPU kernel launch overhead for each small matmul can hurt performance.\n\nLibraries like Tutel addressed this by fusing the computation of all experts on a device into one kernel where possible, to amortize overhead MICROSOFT.COM MICROSOFT.COM.\n\nSimilarly, the MegaBlocks approach of using block-sparse operations is about aligning the computation with GPU-friendly primitives HUGGINGFACE.CO HUGGINGFACE.CO.\n\nAs hardware evolves, we may see more native support for sparsity (for example, NVIDIA’s Hopper GPUs introduced a form of dynamic programming sparsity, though not specifically for MoEs). Custom ASICs or future accelerators might even include routing logic on-chip to eliminate the general-purpose overhead of gating.\n\nOne more aspect is that MoE models are complex distributed systems.\n\nDeploying them requires a sophisticated runtime to handle networking, memory management, and load balancing.\n\nThis blurs the line between model design and system design.\n\nGoogle’s “Pathways” system (2022) was an effort to build a general infrastructure for expert models that route data across multiple model pieces.\n\nWhile details are scarce, the goal was to seamlessly support models that conditionally use different subnetworks – essentially MoEs on a larger scale.\n\nNvidia and Microsoft have also worked on inference managers that can host massive models across clusters (e.g. NVIDIA’s Trinity or Microsoft’s ZeRO-Inference MUG.MVAPICH.CSE.OHIO-STATE.EDU DEEPSPEED.AI ).\n\nThese frameworks must be hardware-aware : they might schedule MoE computations to minimize data transfer (e.g. by preferring local experts) or dynamically adjust parallelism depending on available resources.\n\nIn practice, TPUs excel at large all-to-all due to their high-bandwidth mesh, whereas GPUs (prior to NVSwitch/H100 advances) needed more tuning.\n\nBut recent results show GPUs can handle trillion-parameter MoEs efficiently with the right software.\n\nDeepSpeed-MoE demonstrated 25 ms inference latency for a 1.5 trillion param MoE across hundreds of GPUs ARXIV.ORG PROCEEDINGS.MLR.PRESS – an impressive feat made possible by co-optimizing the model and hardware usage.\n\nMeanwhile, on TPUs, Google reported strong scaling of Switch Transformer; they even scaled down to smaller TPU slices to show MoE benefits “with only a few cores” JMLR.ORG JMLR.ORG , highlighting that the approach wasn’t limited to enormous pods.\n\nA final consideration is hardware-specific limits like memory per core. TPU v3 has 16 GB per chip, which constrained the size of experts that could be hosted per core. GLaM (1.2T) and Switch (1.6T) both navigated this by using many experts with relatively smaller size (e.g. 64 experts each of ~/≍ 10B parameters across many cores).\n\nGPU A100s offer 40–80 GB, allowing larger experts per device, but then fewer devices (which can increase communication per token).\n\nThese trade-offs mean the optimal MoE configuration (number of experts, expert size, distribution) can be different between hardware platforms.\n\nSoftware libraries have begun to abstract this: for example, DeepSpeed’s MoE support can automatically partition experts across GPUs and even offload some layers to CPU if needed SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nThe goal is to make MoE deployment “hardware-agnostic,” but in reality, achieving peak performance still requires tuning to the strengths of the hardware (e.g. using tensor parallelism on GPU for big matmuls vs. relying on TPU systolic array utilization, etc.).\n\nIn summary, MoE inference pushes the envelope of what our hardware and frameworks were initially designed for.\n\nOver the past few years, a synergy of model research and system engineering has emerged: new routing algorithms are designed with hardware limitations in mind (e.g. favor local experts to reduce network traffic HUGGINGFACE.CO HUGGINGFACE.CO ), and systems are built to better accommodate dynamic, sparse computation.\n\nEnsuring that an MoE model runs efficiently on a given platform often means bridging the gap between model parallelism and the hardware’s communication topology. The advancements from projects like GShard, Tutel, and DeepSpeed-MoE reflect a broad consensus that to serve giant MoEs at low latency, one must co-optimize the model and the hardware execution plan.\n\n\n# Expert Sparsity and Activation Imbalance\n\nMoE models are deliberately sparse – at any given layer, only a small fraction of the model’s neurons “activate” for an input token.\n\nThis sparsity is what gives MoEs their compute advantage, but it also leads to uneven utilization of the model’s capacity.\n\nWe touched on load imbalance earlier from a performance perspective; here we consider the implications of expert sparsity on model behavior and inference outcomes.\n\nIn an ideal scenario, each expert in an MoE would specialize in different aspects of language and be utilized regularly when those aspects are present in the input.\n\nIn practice, MoEs often develop skewed specializations : some experts become “generalists” that fire on a wide range of tokens, while others become niche experts that rarely activate. For example, the ST-MoE model analysis found that certain experts specialized in punctuation or rare words HUGGINGFACE.CO.\n\nIf the input doesn’t contain those patterns, those experts stay dormant.\n\nHuang et al. (2023) confirmed this phenomenon at scale: in their MoE language model, they observed multiple hot experts that handled a large portion of tokens consistently, whereas many other experts received only a trickle of tokens or none at all SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nImportantly, which experts are hot can vary by domain (e.g. some experts are hot for programming text, others for Wikipedia), but within a given domain or task, the imbalance persists SEAS.UPENN.EDU SEAS.UPENN.EDU .\n\nThis activation imbalance means a lot of parameters are effectively under-utilized during inference.\n\nIf 2–3 experts handle, say, 50% of all tokens, then the remaining experts contribute little to the throughput or real-time decision-making.\n\nThey may hold knowledge that’s rarely needed, which is inefficient from a deployment standpoint – we’re dedicating memory and maintenance to experts that rarely contribute.\n\nMoreover, when an expert is rarely used, its weights might not be well-fine-tuned (if the distribution shift from training causes it to activate less, it may perform suboptimally the few times it is chosen).\n\nOn the flip side, the over-used experts can become points of contention as described, and also single points of failure if they saturate.\n\nResearchers have proposed various strategies to handle this sparsity/imbalance issue.\n\nOne concept is expert rejuvenation or pruning : periodically identify experts that receive almost no traffic and either retrain them on new data or remove/merge them.\n\nThis is analogous to pruning neurons in a dense model that never fire.\n\nThere has been less published on pruning experts post-training, but it’s a plausible avenue for model compression – essentially, if an expert is never selected, one could drop it to save memory (with a fallback that the gating network might choose the next best expert instead). Another strategy, as mentioned, is dynamic load balancing at inference SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nThis doesn’t eliminate sparsity but ensures it doesn’t harm performance: if one expert is “hotter” than others in a particular batch, we can spread some of its load to other experts.\n\nThis approach was shown to improve system robustness significantly SEAS.UPENN.EDU.\n\nIt effectively reduces activation imbalance per batch even if the model’s inherent gating is imbalanced.\n\nHowever, it does change the model’s computations slightly (some tokens go to a second-choice expert), which could affect output quality marginally.\n\nThe authors reported that this technique avoids OOM errors and keeps latency manageable in pathological cases SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nAn intriguing finding by Google researchers was that expert utilization can drop during fine-tuning if the load-balancing loss is turned off HUGGINGFACE.CO.\n\nThey found that even when up to 11% of tokens were dropped (due to capacity limits) during fine-tuning, the model’s quality wasn’t significantly hurt – suggesting that many experts were not critical for those tokens HUGGINGFACE.CO HUGGINGFACE.CO.\n\nThis hints that there is redundancy in expert knowledge: some tokens can afford to skip their top expert and still be processed well by another.\n\nThat redundancy might be exploited at inference: one could imagine an MoE serving system that, under heavy load, temporarily routes tokens to fewer experts to save compute, relying on the redundancy to maintain output quality. This would be a form of graceful degradation using sparsity.\n\nThe sparsity itself (using only a few experts per token) also has implications for output variance.\n\nIf an MoE uses $k=2$ experts with weighted combination, it is essentially ensembling two expert opinions per token. Many MoEs (like GLaM) do this, which can stabilize outputs at the cost of more compute AR5IV.ORG AR5IV.ORG.\n\nSwitch Transformer opted for $k=1$ (only one expert per token) to simplify and speed up inference JMLR.ORG.\n\nThis increases sparsity but means each token’s representation is determined by a single expert’s weights.\n\nThat can lead to more variance in token outputs – if an expert has a peculiar behavior, the token’s output will reflect it strongly.\n\nIn critical applications, one might prefer $k=2$ routing despite the extra cost, for a form of local ensemble averaging.\n\nIn fact, some recent MoE variants allow adjusting $k$ at inference (trading off quality vs. speed) HUGGINGFACE.CO HUGGINGFACE.CO.\n\nFor example, one can train with $k=2$ but use only $k=1$ for faster inference, accepting a small quality drop HUGGINGFACE.CO HUGGINGFACE.CO.\n\nThis is another way to use sparsity as a tunable knob at runtime.\n\nFinally, knowledge concentration in experts has pros and cons for inference.\n\nOn one hand, having experts specialize (e.g. an expert for legal text, one for code, one for conversation) is useful – it means when such content appears, that expert can handle it well.\n\nIndeed, MoEs are partly motivated by the idea of capturing diverse patterns or modalities in different experts.\n\nOn the other hand, if a specialization is too narrow, the expert might remain idle most of the time.\n\nMeta’s team encountered this in their multilingual MoE for No Language Left Behind: some language experts would be mostly unused unless that language was being translated.\n\nTheir solution was task-based routing as discussed, or training the gating to be language-aware ARXIV.ORG.\n\nIn general, there’s a design choice: should experts be purely data-driven in what they specialize in, or explicitly structured (by language, by topic, etc.)?\n\nIf structured, one can ensure each expert has a role and gets used when that role is needed (like a module).\n\nThis can avoid wasted experts but requires knowing the axes of specialization ahead of time.\n\nData-driven specializations may yield unexpected or overlapping expert roles, some of which might not be worth the cost at inference.\n\nIn summary, the sparse activation nature of MoEs yields uneven expert usage, which raises the question of how to maximize the value of all those parameters.\n\nTechniques to balance expert load, both at training (auxiliary losses) and at inference (dynamic token redistribution), have been successful in preventing any expert from becoming a throughput liability JMLR.ORG JMLR.ORG SEAS.UPENN.EDU SEAS.UPENN.EDU.\n\nMeanwhile, acknowledging that some experts will inevitably be lightly used, researchers are exploring ways to compress or remove them without losing much model capacity.\n\nAs MoEs enter production settings, we may see more adaptive inference mechanisms – for instance, a system might monitor expert utilization over time and unload an expert from GPU if it hasn’t been used for a while (similar to expert buffering).\n\nThe sparsity that is a blessing for compute is a curse for utilization , but through careful design, MoEs can ensure that even “cold” experts have their moment to shine when needed, and do not impede the model when they are not.\n\n\n# Real-Time and Streaming Inference\n\nOne of the ultimate goals for deploying large language models is to serve interactive applications – e.g. chatbots, real-time translation, or personalized assistants – with low latency per user. MoE models pose unique challenges in these scenarios.\n\nAs discussed, MoEs thrive on throughput: the hardware can be kept busy if there are many tokens to process in parallel.\n\nBut in a streaming inference setting (generating one token at a time for a user, then immediately using it before generating the next), we often operate at batch size 1.\n\nThis emphasizes the per-token overheads of MoEs.\n\nIf a model is outputting text token by token, each step involves running the MoE layer routing.\n\nThe overhead of gating and cross-device communication on every generation step can introduce noticeable latency.\n\nFor example, if it takes a dense model 50 ms to generate a token and a MoE model 150 ms (due to extra coordination), the user will experience slower response despite the MoE’s theoretical efficiency.\n\nIn fact, without optimizations, MoE layers can make auto-regressive generation significantly slower per token than a dense model.\n\nThis was highlighted by Nvidia in the context of small batches: they note that sequential token generation limits throughput because you cannot parallelize the timeline of generation DEVELOPER.NVIDIA.COM.\n\nIn MoEs, this is exacerbated by the fact that not all GPU cores are used for each token (since only some experts run). Essentially, MoEs trade off some single-stream speed for multi-stream scalability.\n\nThere are a few strategies to handle real-time usage of MoEs:\n\n * Concurrent streams : Run multiple independent sequences through the MoE model in interleaved fashion.\n   This way, while one sequence is waiting for its next token’s computation, another sequence’s token can use other experts.\n   If done cleverly, this can fill the gaps and use all experts efficiently.\n   Many inference servers do this by batching requests that arrive within a short window.\n   For MoEs, you might maintain a rolling batch of tokens from different users.\n   The DeepSpeed-MoE system explicitly allows combining data parallelism (different inputs on different devices) with expert-slicing MICROSOFT.COM MICROSOFT.COM , which could be leveraged to serve multiple requests at once.\n   The downside is added latency variance – a user’s query might wait a few extra milliseconds to be grouped with another’s.\n   There’s a trade-off between latency and throughput here.\n\n * Quality-vs-latency tuning : As mentioned, one can reduce the number of experts used at inference to speed up each token, at some cost to output quality HUGGINGFACE.CO HUGGINGFACE.CO.\n   For example, switching from top-2 to top-1 gating at generation time roughly halves the expert compute and communication.\n   If the model was trained with $k=2$, this might degrade quality slightly, but perhaps not dramatically (especially if the gating network often had one dominant expert anyway).\n   This is a way to adapt a MoE for faster interactive use – essentially operating it in a more sparse mode when needed.\n\n * Distillation fallback : A practical approach used in industry is to maintain a smaller dense model as a fallback for real-time.\n   The MoE might be used offline or for batch tasks where latency is less critical, whereas a distilled version serves the online traffic.\n   Google’s early experiments with Switch Transformer indeed suggested distilling the MoE into a dense model for production serving HUGGINGFACE.CO HUGGINGFACE.CO.\n   They managed to compress about 30–40% of the MoE’s performance gain into the dense model. For streaming applications, that dense model can generate faster and with more predictable timing. The trade-off is losing some of the MoE’s peak accuracy.\n\n * Streaming-friendly routing : Research is underway on making routing algorithms more cognizant of temporal context.\n   For instance, one could enforce that once a conversation is in progress, the gating tries to reuse the same experts it used for previous tokens (assuming topic/linguistic context persists). This could improve locality and perhaps allow caching of those experts on certain devices (reducing repetitive loading).\n   It’s a bit speculative, but one could imagine a gating that says “for this dialogue turn, stick to expert 5 and 9 unless a very out-of-context token appears.”\n   This starts to resemble the task-level routing, but at a finer session granularity. The benefit would be smoother latency (since subsequent tokens hit the same devices) and possible reuse of intermediate states.\n\n * Real-time load balancing : In an interactive system, load can fluctuate (many users at peak hours, few at others).\n   MoEs can actually shine in high-concurrency situations by scaling gracefully with load – more incoming tokens naturally utilize more experts, spreading out work.\n   But in low-load conditions, the cluster running a MoE might be underutilized.\n   One solution is to dynamically scale down the number of active devices when load is low (parking some GPUs).\n   However, cold-starting them when load increases could be slow.\n   It’s a general infrastructure problem not unique to MoE, though MoE’s conditional activation makes it tricky to pin which devices can sleep.\n   If certain experts aren’t likely to be used, those could be temporarily paged out to save power, as long as the system can page them in on demand (similar to expert buffering but for power/availability scaling).\n\nThe consistency of inference is also a consideration in real-time.\n\nBecause MoE outputs can be nondeterministic if there's any randomness in gating or ties broken arbitrarily, one might see small variations in outputs between runs.\n\nMost MoEs are deterministic given the same model and input, but if load-balancing decisions reroute some tokens differently under pressure, outputs could change run-to-run.\n\nFor applications like translation or medical text generation, consistent outputs are important.\n\nThus, any adaptive inference tricks have to be carefully evaluated for their effect on the text output, not just latency.\n\nFinally, streaming MoEs for things like continuous speech recognition or video processing face the challenge of state.\n\nA streaming ASR model might process audio frame by frame; if it were an MoE, each frame routes independently. It might be beneficial to have the same expert handle a contiguous segment of audio to accumulate context.\n\nThis is a frontier area: combining MoE with recurrent or stateful models for streaming inputs.\n\nIn summary, real-time deployment of MoE models involves careful engineering to get the best of both worlds: the MoE’s efficiency during heavy utilization and acceptable latency for single-stream cases.\n\nSome notable trade-offs and findings include: (1) MoEs can deliver higher throughput under load, but at the cost of higher per-token latency at low loads MICROSOFT.COM MICROSOFT.COM ; (2) batching and concurrency are key to mitigating this – in one experiment, routing by entire tasks improved throughput ~2× while preserving quality, essentially by turning the MoE into multiple smaller models used in parallel ARXIV.ORG ARXIV.ORG; (3) when latency is paramount, techniques like model compression or gating simplification may be necessary to meet strict SLAs.\n\nAs the field progresses, we expect to see MoEs become more elastic – adapting on the fly to different latency/throughput regimes.\n\nThe end-game is a system where the MoE’s sparsity can be dialed up or down in real-time: using lots of experts for tough, batch tasks and fewer experts for quick, interactive responses.\n\nAchieving that will require advances in both algorithms and serving infrastructure.\n\n\n# Conclusion\n\nMixture-of-Experts models represent a powerful approach to scaling up language models, offering unprecedented parameter counts and the potential for improved quality at lower inference cost.\n\nRealizing these benefits in practice, however, demands surmounting several inference-time challenges. We have surveyed the major hurdles: from expert routing overhead and load imbalance , to latency/throughput bottlenecks , memory and bandwidth demands , hardware integration issues , uneven expert utilization , and serving in real-time contexts.\n\nAcross the literature and industry implementations, a few common themes emerge:\n\n * Load Balancing and Routing : Effective MoE inference hinges on keeping the workload evenly spread among experts and minimizing routing overhead.\n   Auxiliary training losses and novel gating algorithms (e.g. BASE Layers, hash routing) help distribute tokens, while dynamic routing adjustments at runtime can further smooth out imbalances JMLR.ORG SEAS.UPENN.EDU.\n   Coarse routing at the level of tasks or sequences is a viable strategy to simplify inference, essentially breaking a giant MoE into manageable sub-models per request ARXIV.ORG ARXIV.ORG .\n\n * System and Parallelism Optimizations : To achieve low latency, MoE models must leverage parallelism across experts while avoiding excessive communication.\n   Designs like DeepSpeed-MoE and Tutel show that with tailored all-to-all protocols and a mix of parallelism strategies, MoEs can reach 4–8× throughput gains over naive implementations MICROSOFT.COM MICROSOFT.COM.\n   Hierarchical communication, fused kernels, and scheduling that groups tokens by expert are key techniques MICROSOFT.COM MICROSOFT.COM.\n   These optimizations turn the theoretical compute savings of sparsity into real wall-clock improvements.\n\n * Memory Management : The massive scale of MoEs means memory (not compute) often becomes the primary bottleneck MICROSOFT.COM . Techniques such as expert sharding (as in GShard SEAS.UPENN.EDU ), memory-aware gating (to reduce padding SEAS.UPENN.EDU ), and offloading inactive experts SEAS.UPENN.EDU all serve to bring memory usage under control.\n   Research prototypes like expert buffering and block-sparse execution demonstrate that it’s possible to cut memory overhead dramatically without sacrificing much accuracy SEAS.UPENN.EDU HUGGINGFACE.CO.\n   In practice, a combination of hardware memory capacity, distributed memory bandwidth, and software caching is used to accommodate trillion-parameter models.\n\n * Hardware Adaptation : MoE inference performance is deeply tied to hardware capabilities.\n   Early MoEs leveraged Google’s TPU architecture for scale HUGGINGFACE.CO , while recent efforts focus on optimizing for GPU clusters with high-speed interconnects MICROSOFT.COM.\n   Custom communication libraries (NCCL tweaks, MSCCL) and compiler support for dynamic shapes (as in Meta’s FMI and Megablocks) are bridging the gap.\n   The community has learned that dense-to-sparse migration isn’t plug-and-play – it requires co-designing the model and the system.\n   But with each generation of hardware and software (e.g. NVIDIA’s Hopper GPUs, new networking technologies), MoEs become more feasible to serve at scale.\n\n * Quality vs. Performance Trade-offs : Many solutions involve a trade-off. Using fewer experts per token, merging experts, or distilling to dense models – all these can ease deployment but may reduce model accuracy HUGGINGFACE.CO.\n   The optimal balance depends on the application. Notably, some recent works achieved full quality retention with faster inference: e.g. task-MoE kept all gains while boosting speed 2× ARXIV.ORG , and Mixtral 8×7B matched or beat a dense 70B model with much faster inference HUGGINGFACE.CO HUGGINGFACE.CO.\n   These results are encouraging, suggesting that clever routing or expert combination can give the best of both worlds.\n\nIn conclusion, the inference-time challenges of MoE models are being actively tackled on multiple fronts.\n\nAcademia and industry have proposed a spectrum of solutions – from algorithmic innovations like balanced routing and adaptive gating, to system-level improvements in kernels and networking, to pragmatic approaches like model compression for serving.\n\nThe most notable finding across the board is that sparsity in large models offers real advantages but requires a sophisticated runtime to harness.\n\nWhen properly managed, MoE models can achieve superior performance-per-compute and even faster inference than dense models of equivalent quality MICROSOFT.COM MICROSOFT.COM.\n\nBut without careful design, they can suffer from bottlenecks that erase their theoretical benefits.\n\nThe trade-offs often come down to flexibility vs. efficiency: a highly dynamic MoE (routing every token optimally) might maximize quality but be harder to optimize, whereas a more structured or restricted MoE (routing by task or with fixed patterns) might be easier to serve but less adaptive.\n\nWe see this reflected in designs like Switch (simpler routing, easier scaling) versus GLaM (more complex routing, higher peak quality).\n\nLooking forward, proposed solutions such as more intelligent compilers for MoE (compiling sparse operations into dense blocks), hardware with native support for conditional computation, and hybrid models (sparse layers combined with dense backbones) will further mitigate inference challenges.\n\nCompanies like Google and Meta continue to refine MoE deployments for multilingual and multitask models, indicating that the approach remains promising if the serving cost can be kept in check.\n\nAs of 2024, MoEs are not yet the de facto standard for all LLM deployments (many productions use dense models like GPT-4 or LLaMA due to maturity and simplicity), but the gap is closing.\n\nWith projects like DeepSpeed-MoE demonstrating sub-30ms latency for trillion-param models ARXIV.ORG , it’s clear that MoEs can be made to work in real-time with enough engineering.\n\nIn summary, Mixture-of-Experts models introduce unique inference-time challenges, but research has shown these are surmountable.\n\nThrough balanced expert utilization, parallelism-aware system design, memory optimizations, and strategic simplifications, MoEs can live up to their promise of “maximum parameters, minimal compute” JMLR.ORG JMLR.ORG.\n\nThe ongoing convergence of model architecture and infrastructure suggests that future large-scale AI systems may routinely use expert mixtures to deliver both scale and speed – dynamically allocating computation only where needed, and thereby achieving new levels of efficiency in large language model inference.",normalizedContent:"> this is deepresearch report generated from chatgpt.\n\n\n# inference-time challenges in mixture-of-experts models for llms\n\n\n# introduction\n\nmixture-of-experts (moe) architectures have enabled trillion-parameter scale language models while keeping inference computation per token relatively low.\n\nunlike dense transformers where every layer’s feed-forward network (ffn) is applied to every input, moe models activate only a few “expert” ffn sub-models per input token seas.upenn.edu.\n\nfor example, google’s 1.2 trillion-parameter glam model routes each token to 2 out of 64 experts, achieving better accuracy than gpt-3 (175b) while using about 50% fewer flops per token ar5iv.org ar5iv.org.\n\nsimilarly, a vision moe (v-moe) matched the performance of state-of-the-art vision models using half the inference compute of a dense network arxiv.org .\n\nthese sparse models demonstrate that enormous parameter counts can be leveraged without proportional increases in computation.\n\nhowever, inference-time performance of moes often falls short of these theoretical gains.\n\nas fedus et al. note, adoption of moe models has been hindered by added system complexity and communication overhead on current hardware jmlr.org.\n\nin practice, a moe’s massive parameter size and dynamic token routing can introduce new bottlenecks.\n\nserving a moe at scale requires careful orchestration:\n\n * experts are typically distributed across devices (e.g. via google’s gshard sharding framework seas.upenn.edu )\n * extra networking is needed to route tokens and gather results.\n\nthese factors can lead to higher latency, memory usage, and engineering challenges during inference.\n\nin fact, microsoft found that without special optimizations, a quality-equivalent moe can be 15× slower in inference latency than a dense model despite similar flops seas.upenn.edu seas.upenn.edu.\n\nthis survey reviews the key inference-time challenges of moe-based large language models – focusing on cost and performance issues (not training instabilities) – and summarizes notable findings and solutions from both research literature and industry implementations (google’s switch transformer and glam, meta’s nllb moe, microsoft’s deepspeed-moe, alibaba’s m6, etc.).\n\nwe structure the discussion by major challenge areas:\n\n * expert routing and load balancing\n * latency and parallelism\n * memory and bandwidth\n * hardware compatibility\n * expert sparsity/imbalance\n * real-time streaming inference\n\n\n# expert routing and load balancing\n\nrouting tokens to experts is at the heart of moe inference.\n\na learned gating network computes scores for each expert and selects the top-$k$ experts for each token (typically $k=1$ or $k=2$) seas.upenn.edu seas.upenn.edu .\n\nthis conditional routing can lead to load imbalance : some experts may receive far more tokens than others in a given inference batch.\n\nif one expert is overloaded, it becomes a bottleneck while other experts sit idle.\n\nto mitigate this, moe training regimes include an auxiliary load-balancing loss that nudges the gate toward a uniform token-to-expert distribution jmlr.org jmlr.org.\n\nfor instance, the switch transformer used a gating capacity factor (often 1.2× the average load) and would drop excess tokens if an expert’s capacity was exceeded jmlr.org jmlr.org.\n\nin practice, with well-tuned capacity, only a small fraction of tokens (<1%) are dropped during training/inference in switch transformer jmlr.org jmlr.org.\n\nthis ensures each expert processes roughly similar numbers of tokens, avoiding stragglers that slow down the batch.\n\ndespite such measures, runtime imbalances still occur.\n\nthe distribution of tokens at inference can differ from training averages – e.g. certain prompts might trigger one expert almost exclusively, violating load assumptions seas.upenn.edu seas.upenn.edu.\n\nempirical studies have observed that in a language model moe, a few “hot” experts consistently receive a large share of tokens while many others get very few or even none for a given workload seas.upenn.edu seas.upenn.edu.\n\nthis activation imbalance was seen across domains: in one experiment, some experts were always heavily used for wikipedia text but idle for code (and vice versa) seas.upenn.edu seas.upenn.edu.\n\nan imbalanced gate not only under-utilizes model capacity but can also cause latency spikes if a single expert (and its host device) must handle a disproportionate amount of work.\n\nproposed solutions to routing imbalance include improved gating algorithms and coarse-grained routing.\n\nbase layers (lewis et al. 2021) formulated expert assignment as an optimal transport problem to ensure each expert gets an equal number of tokens papers.neurips.cc arxiv.org, thereby achieving balanced routing without an auxiliary loss.\n\nalternatively, hash-based routing replaces the learned gate with a deterministic hash function that assigns tokens to experts uniformly at random proceedings.neurips.cc openreview.net.\n\nthis removes gating overhead and guarantees balance, though at a potential cost to quality (since routing is no longer learned).\n\non the industry side, google explored task-level routing to avoid token-level variance: kudugunta et al. (2021) route entire sentences or tasks to fixed experts, effectively partitioning the model by task arxiv.org arxiv.org.\n\nin multilingual machine translation, this task-moe approach allowed extracting a single-expert subnetwork per language pair for inference.\n\nit preserved all the quality gains of the full moe while improving peak throughput by 1.9× over token-level moe, since each inference uses only one expert with no on-the-fly switching arxiv.org arxiv.org .\n\nthe trade-off is reduced flexibility – expert allocation must be decided at the granularity of a sentence or user query – but it guarantees balanced loads and simpler scheduling.\n\nmore dynamically, researchers have proposed adjusting the gate during inference based on observed loads: one recent system tracks the running load on each expert/gpu and redistributes some tokens to less-busy experts to avoid overloads seas.upenn.edu seas.upenn.edu . while this may deviate from the gate’s first choice, it can prevent worst-case slowdowns and out-of-memory errors.\n\n> in summary, robust moe inference requires not just a smart gate, but possibly a capacity-aware router that ensures no expert becomes a throughput sink.\n\nanother aspect of routing is the overhead of the gating function itself.\n\nthe gating computation and token-to-expert mapping add extra steps to each inference.\n\nfor large moes (e.g. hundreds of experts), computing the softmax scores and selecting top-$k$ experts for every token can be non-trivial.\n\nmore significantly, many frameworks historically implemented token routing with inefficient operations (e.g. constructing sparse masks or using scatter/gather on cpus).\n\na recent analysis found that the gating operations and associated data shuffling were a major contributor to moe inference latency and memory use seas.upenn.edu seas.upenn.edu.\n\nto address this, researchers introduced dynamic gating that more tightly couples the routing decisions with the available capacity.\n\ninstead of always padding each expert’s batch to a fixed size (which wastes computation on “empty” slots), dynamic gating only allocates exactly as many slots as needed per expert, reducing unnecessary computation on placeholder tokens seas.upenn.edu seas.upenn.edu.\n\nthis optimization was shown to cut gating overhead significantly, enabling inference with larger batch sizes or fewer devices than before.\n\nin essence, efficient moe serving demands that the routing mechanism be lightweight and balanced , so that the cost of deciding “which expert?” does not outweigh the benefit of skipping computations.\n\n\n# latency and parallelism bottlenecks\n\nserving a moe model efficiently requires navigating a complex parallelism hierarchy.\n\nin a multi-gpu or tpu setup, different experts reside on different devices (this distribution is necessary to fit the huge model in memory, as discussed later).\n\nthus, each inference step involves a pattern of communication and parallel computation: tokens assigned to various experts must be sent to the appropriate devices, processed in parallel, and then the results merged in the original sequence order.\n\nthis all-to-all exchange is a key throughput limiter.\n\nlepikhin et al.’s gshard paper noted that while spreading experts across $n$ devices cuts compute per device, it incurs an all-to-all communication to route tokens and collect outputs seas.upenn.edu seas.upenn.edu.\n\nfor small batches or many experts, the communication overhead can dominate.\n\nin fact, early moe benchmarks attributed the longer inference latency of moes primarily to the cost of frequent all-to-all communication between devices seas.upenn.edu seas.upenn.edu .\n\nparallelism bottlenecks manifest especially when the batch or sequence length is not large enough to fully utilize all experts concurrently.\n\nin an ideal scenario, if we have as many tokens as experts, each expert/device can handle one slice of the batch concurrently – achieving near-linear speedups.\n\nthis is the best-case view described by microsoft’s deepspeed-moe team: for a 52b moe model with 128 experts of 1.3b each, each token only activates a 1.3b subnetwork, so distributing those experts across 128 gpus means each device handles 1/128 of the model per token (1.3b parameters) microsoft.com microsoft.com.\n\nin theory, with perfect parallelization and no comm overhead, such a model could run 5× faster per token than a dense 6.7b model that offers similar quality microsoft.com microsoft.com.\n\nin practice, however, achieving this requires careful coordination.\n\nthe worst-case is that a group of tokens collectively needs every expert, meaning the system ends up touching the full model parameters within a single inference step microsoft.com microsoft.com.\n\nin that regime, latency and throughput suffer.\n\nmodern moe serving systems therefore try to “steer performance toward the best-case” via smart parallelism and scheduling microsoft.com microsoft.com .\n\nmicrosoft’s deepspeed-moe inference design, for example, combines multiple parallelism strategies : it uses expert parallelism (different gpus host different experts and process different token groups concurrently) and traditional tensor or data parallelism for the non-expert layers microsoft.com microsoft.com.\n\nby grouping tokens that share the same expert path on the same device, they minimize redundant weight access and maximize each gpu’s utilization microsoft.com microsoft.com.\n\nthis design effectively minimizes each device’s critical path, so that no single gpu has to load all experts’ weights.\n\nexperiments showed that with expert parallelism equal to the number of experts (one expert per gpu), the sequential work per device is dramatically smaller than in an equivalent dense model – in their example, each gpu saw 5× fewer parameters per token than a dense baseline microsoft.com microsoft.com.\n\nthe remaining challenge is communication: naive all-to-all algorithms scale poorly as the number of devices grows, since latency increases linearly with devices in typical networks microsoft.com microsoft.com.\n\nto tackle this, deepspeed-moe introduced a custom communication stack with optimizations like hierarchical all-to-all.\n\ninstead of one global shuffle, they first perform intra-node exchange of tokens, then a smaller inter-node exchange microsoft.com microsoft.com . this reduced communication hops from $o(p)$ to $o(g + p/g)$ (for $p$ total devices and $g$ devices per node) microsoft.com microsoft.com , improving latency for small batches on large clusters.\n\nthey further coordinated communications for expert and data parallel steps to avoid redundant transfers microsoft.com microsoft.com.\n\nwith these system optimizations, deepspeed reported up to 7.3× lower latency and 4.5× faster inference throughput for moes compared to prior systems microsoft.com microsoft.com – bringing moe latency in line with (or better than) dense models of similar quality.\n\nother industry efforts likewise targeted the all-to-all bottleneck.\n\nmicrosoft’s open-source tutel library focuses on accelerating moe inference/training on gpu clusters.\n\ntutel optimized low-level cuda kernels and nccl communication, achieving an 8.5× speedup for a single moe layer on 8 gpus, and about 40% end-to-end latency reduction for a 1.1-trillion-parameter moe model spread over 512 gpus microsoft.com.\n\nthis was measured against the baseline moe implementation in meta’s fairseq, highlighting how much overhead could be trimmed with custom all-to-all routines.\n\ngoogle’s switch transformer team similarly noted that existing hardware and libraries were geared toward dense matrix ops, so they simplified moe routing (using $k=1$ expert) to reduce overhead jmlr.org jmlr.org.\n\nby doing so, they attained 7× faster pre-training than a dense t5-xxl with the same compute budget jmlr.org jmlr.org – and also found that even with as few as 2 experts (on 2 cores) the moe provided speedups jmlr.org jmlr.org.\n\nthis implies moes can benefit even “small” deployments if the parallelism is carefully exploited.\n\ndespite these advances, a fundamental issue remains: moe inference efficiency tends to scale well with throughput (batch size or concurrent requests) but not as well with single-input latency.\n\nif only one sequence is processed at a time (e.g. an interactive user query), many experts will be idle.\n\nin a switch-style moe, each token is sent to only one expert – so out of (say) 64 experts, 63 do nothing for that token.\n\nthe next token might hit a different expert, but sequentially you’re not utilizing the full model’s parallel potential.\n\nthis is one reason moe inference can have inconsistent latency.\n\nin fact, deepspeed explicitly notes that inference usually runs at small batch sizes , meaning the time to load model weights from memory often dominates, and lesser compute doesn’t automatically translate to faster inference microsoft.com microsoft.com.\n\nsystems mitigate this by packing multiple requests together or running large batches of tokens – but that adds latency or requires enough traffic to batch.\n\nin real-time streaming, one approach is to use multithreading or coroutines to serve many generation streams on the moe model simultaneously, effectively faking a larger batch and keeping all experts busy.\n\nanother approach is the coarse routing mentioned earlier: if a whole session or task can be pinned to a subset of experts, the model effectively reduces to a smaller dense model for that session, avoiding frequent cross-expert communication. this was shown to improve throughput 2–3× in multilingual moe inference arxiv.org arxiv.org . the trade-off is that it forgoes token-level adaptability in favor of steadier, more predictable usage of experts.\n\nin summary, latency optimization for moes revolves around using parallelism to the fullest and minimizing coordination costs.\n\ntechniques like expert parallelism, optimized collectives, and request batching ensure that the sparse activations translate into actual wall-clock speedups.\n\nwith these in place, moe models can serve significantly more tokens per second – even outperforming dense models of comparable quality on throughput microsoft.com microsoft.com.\n\nbut without such optimizations, an moe can be slower than a dense model, as the overhead of many small computations and communications overwhelms the flop savings seas.upenn.edu seas.upenn.edu.\n\nachieving low tail latency for single examples remains challenging, requiring careful scheduling or fallback to distilled models when ultra-low latency is needed.\n\n\n# memory usage and bandwidth constraints\n\nmoe models trade compute for size – they introduce a huge number of parameters (the experts) that must be stored and accessed efficiently at inference.\n\na standard transformer might have billions of parameters, whereas moe models can scale to trillions of parameters by replicating ffn layers into many experts. for example, google’s switch-c transformer reached 1.6 trillion parameters with 2048 experts huggingface.co huggingface.co , and alibaba’s m6-t model reportedly pushed to 10 trillion parameters using moe techniques alibabacloud.com alibabacloud.com.\n\nin inference, even though only a fraction of those weights are used per token, all experts generally need to be loaded in memory so that any token can be routed to the appropriate ones huggingface.co huggingface.co.\n\nthis creates a memory footprint far larger than dense models.\n\nas the hugging face team succinctly put it, moes *“require high vram as all experts are loaded in memory”* huggingface.co huggingface.co . for instance, an 8×7b moe (mistral’s mixtral model) totals 47b parameters loaded (~8× more than a single expert) to allow any combination of 2 experts to be active huggingface.co huggingface.co.\n\nriquelme et al. (2021) noted a similar overhead in v-moe vision models: the sparse model needed additional memory compared to a dense one of similar throughput, due to storing many experts that are each rarely used arxiv.org arxiv.org .\n\nbeyond just capacity, memory bandwidth becomes a limiting factor.\n\nin modern accelerators, reading weights from gpu memory (or across a network) can bottleneck throughput if the working set is large.\n\ndeepspeed’s developers point out that in moe inference, the time to load model parameters from memory often outweighs arithmetic costs microsoft.com microsoft.com.\n\nthey identify overall model size and achievable memory bandwidth as the main determinants of moe latency, rather than flops microsoft.com microsoft.com.\n\nin their analysis, a moe’s “critical path” per token is limited to the few experts selected, but the aggregate parameters needed for a batch could be as large as the entire model microsoft.com microsoft.com.\n\nthis means if tokens all go to different experts, the system might still have to touch most of the 52b weights (in their example) within a short time, stressing memory bandwidth.\n\nindeed, huang et al. (2023) found that a dense model requiring x gb of memory per gpu might require 3×–9× more memory when converted to a sparsely-activated moe of similar accuracy seas.upenn.edu seas.upenn.edu.\n\nthey measured an 8.6× increase in memory usage for a language model moe versus a dense model (2.2 gb -> 18.9 gb per gpu), due to both the larger parameter set and extra activation buffers for gating seas.upenn.edu seas.upenn.edu.\n\nsuch demands can easily exhaust device memory or lead to frequent swapping of weights from host memory, which kills inference performance.\n\nto manage this, model sharding is used extensively.\n\ngshard (lepikhin et al. 2020) demonstrated that distributing experts across devices allows the moe’s memory load to be split, albeit at the cost of communication seas.upenn.edu seas.upenn.edu .\n\ntypically, each gpu holds a subset of the experts’ weights and a full copy of the shared weights (e.g. attention layers, embeddings) seas.upenn.edu seas.upenn.edu . this way, no single gpu needs to store all $e$ experts, reducing per-device memory.\n\nfor example, if you have 8 gpus and 64 experts, each gpu might store 8 experts.\n\nthe trade-off is that when an input token needs an expert not on its current gpu, the token’s data must be sent to the gpu owning that expert (as described in the previous section). still, for very large moes, sharding is the only way to even fit the model in a cluster’s combined memory. meta’s 1.1t-parameter moe in fairseq was spread across 64+ gpus, and microsoft noted that each gpu then only loads ~1/64 of the model weights , enabling the model to be served at all microsoft.com .\n\neven with sharding, the total memory footprint can be problematic, especially if multiple models or other workloads share the hardware.\n\none recent idea to address this is expert buffering or on-demand loading.\n\nhuang et al. implemented a mechanism that keeps only a small number of “hot” experts in gpu memory and leaves the rest in cpu memory, swapping them in when needed seas.upenn.edu seas.upenn.edu.\n\nthey observed that moe workloads often have temporal locality – the same expert tends to be reused for several batches in a row, especially in translation or sequential tasks seas.upenn.edu seas.upenn.edu .\n\nexploiting this, their system could significantly cut gpu memory usage (by 5× or more) with minimal performance loss, by dynamically loading experts that haven’t been used recently from cpu to gpu just-in-time seas.upenn.edu seas.upenn.edu.\n\nthis is somewhat analogous to a cache for expert weights.\n\nof course, it relies on a fast interconnect and careful pre-fetching to not stall the pipeline.\n\nanother approach is model compression or merging : for serving, one can reduce moe model size via weight consolidation.\n\nresearchers have experimented with merging the weights of multiple experts into a single expert or smaller set (“aggregation of experts”) to shrink the model for inference huggingface.co huggingface.co.\n\nthis essentially turns a sparse model into a semi-dense one by averaging or otherwise combining experts, attempting to preserve performance while cutting memory needs.\n\nalternatively, distillation can be used: train the moe for quality, then distill its knowledge into a smaller dense model for deployment huggingface.co huggingface.co.\n\nthe switch transformer authors showed that by distilling a 7b moe into a 1.6b dense model, they retained ~30–40% of the moe’s quality gain over a baseline huggingface.co huggingface.co.\n\nin other words, a distilled model can outperform a same-size model trained from scratch, thanks to the moe teacher, and it is much easier to serve (no multi-expert routing).\n\ncompanies like google and openai often use distillation or weight pruning on giant research models to get something more tractable for production.\n\nthat said, distillation sacrifices some peak quality – for example, the task-moe approach by kudugunta et al. preserved 100% of the moe performance in a slimmed model, whereas distilling their token-level moe to a dense model kept only 32% of the bleu improvement arxiv.org arxiv.org .\n\nthus, there is interest in directly optimizing moe memory efficiency so the full sparse model can be used in production.\n\none insight from recent work is that a lot of memory waste in moes comes from padding and under-utilized capacity.\n\nbecause frameworks often allocate a fixed-size buffer for each expert’s input (equal to the capacity factor times batch size), an expert that only gets a few tokens will still consume memory as if it got the maximum.\n\nhuang et al. quantified this waste: in their tests, static gating with a generous capacity led to 12.8× more allocated slots than actual tokens for lm inference in the worst case seas.upenn.edu seas.upenn.edu.\n\ntechniques like the aforementioned dynamic gating and custom kernels aim to eliminate this padding.\n\nthe megablocks system from stanford reformulated moe computations as block-sparse matrix ops , allowing irregular batch sizes per expert to be handled without padding huggingface.co huggingface.co.\n\nby treating the expert weight matrix as blocks and only computing the blocks needed for the non-zero inputs, they could “never drop tokens” and efficiently handle imbalanced loads huggingface.co huggingface.co.\n\nthis improved hardware utilization and cut memory overhead, since an expert with 3 tokens would only produce 3 token’s worth of intermediate activations instead of, say, 32.\n\nin fact, megablocks reported significant speedups and memory savings on gpu hardware by this method huggingface.co huggingface.co.\n\nsuch custom kernels and memory optimizations are crucial for moe inference on gpus, which otherwise might waste vram on empty activations.\n\nin summary, moe inference is memory-intensive : the model’s many experts must be stored and accessed, and naive implementations incur extra memory overhead from padding and duplicated parameters.\n\nsolutions like expert sharding, intelligent weight offloading (keeping only active experts on device), and reducing padding waste are key to making moes fit in available memory and bandwidth budgets.\n\nfurthermore, utilizing the aggregate memory bandwidth of multiple devices is critical – deepspeed’s design explicitly achieved speedups by leveraging distributed gpu memory to increase effective bandwidth microsoft.com microsoft.com.\n\nwith enough gpus each handling a slice of the weights, the model can effectively stream parameters from many memory pools in parallel, overcoming the bottleneck of a single memory channel.\n\nthis approach enabled serving models that would otherwise be “bandwidth-bound.”\n\nfor instance, alibaba’s 10-trillion m6 model could only be trained/inferred by splitting it across 512 gpus, effectively dividing the enormous parameter matrix into manageable chunks alibabacloud.com.\n\nthe bottom line is that to deploy moes at scale, one must treat memory as a first-class constraint – optimize what is stored, where it’s stored, and how fast it can be transferred to compute.\n\n\n# hardware compatibility (tpus, gpus, and systems)\n\nthe challenges above often interplay with the specifics of the hardware platform.\n\nearly moe successes came on google’s tpu clusters, which provided high-bandwidth interconnects and specialized support for collective operations.\n\nthe tpu software stack (xla) also required static shapes for all operations. as a result, google’s moe implementations (e.g. gshard, switch transformer) had to make the routing computation statically allocatable – hence the use of a fixed expert capacity and dropping overflow tokens to maintain a consistent tensor shape jmlr.org jmlr.org.\n\nthis static allocation simplified compilation on tpu but introduced the padding inefficiencies discussed.\n\nmeanwhile, gpus offer more flexible programming but traditionally lacked built-in support for dynamic sparse computation.\n\noff-the-shelf deep learning libraries in 2020–2021 were heavily optimized for dense matrix multiplication and did not cater to dynamic sparsity jmlr.org jmlr.org.\n\nrunning moes on gpu efficiently therefore required considerable engineering.\n\nfacebook (meta) researchers integrated moe layers into pytorch/fairseq in 2021, but they observed suboptimal scaling until improvements like tutel came along microsoft.com microsoft.com.\n\nthe default nccl all-to-all, for instance, had high latency beyond a single node microsoft.com microsoft.com , prompting custom libraries (microsoft’s msccl and sccl) to implement faster patterns microsoft.com microsoft.com.\n\nhardware topology also affects moe performance.\n\non a tpu pod, each tpu core has direct high-speed links to others in a 2d torus.\n\nthis makes all-to-all communication relatively efficient up to a certain scale, and google’s moe models often scaled to thousands of tpu cores (switch transformer used 2048 tpu v3 cores for its largest model huggingface.co huggingface.co ).\n\non gpu clusters, inter-node communication can be more of a bottleneck (limited by infiniband or ethernet speeds).\n\nthe deepspeed team explicitly addressed this by splitting communication into intra-node (on nvlink/nvswitch) and inter-node phases microsoft.com microsoft.com.\n\nthey found that at small message sizes (typical when splitting a batch of tokens), latency dominates bandwidth, so reducing the number of network hops was vital microsoft.com microsoft.com . this is a reminder that a moe that is efficient on one hardware setup might not be on another.\n\nin fact, fedus et al. noted that their switch transformer was developed with tpus in mind, but that moes “should be similarly trained on gpu clusters” with the right mapping of experts to devices jmlr.org jmlr.org . ensuring hardware compatibility often means adapting the parallelism strategy: e.g., using more intra-gpu parallel threads for small expert computations, or merging multiple small matrix ops to better utilize gpu cores (an optimization tutel performs).\n\nanother compatibility issue is support for fast dispatching of variable workloads.\n\neach token in an moe may go to a different expert, leading to a scatter of work.\n\ntraditional gpu execution likes large, uniform batches for efficiency.\n\nif each expert only gets a handful of tokens, the gpu kernel launch overhead for each small matmul can hurt performance.\n\nlibraries like tutel addressed this by fusing the computation of all experts on a device into one kernel where possible, to amortize overhead microsoft.com microsoft.com.\n\nsimilarly, the megablocks approach of using block-sparse operations is about aligning the computation with gpu-friendly primitives huggingface.co huggingface.co.\n\nas hardware evolves, we may see more native support for sparsity (for example, nvidia’s hopper gpus introduced a form of dynamic programming sparsity, though not specifically for moes). custom asics or future accelerators might even include routing logic on-chip to eliminate the general-purpose overhead of gating.\n\none more aspect is that moe models are complex distributed systems.\n\ndeploying them requires a sophisticated runtime to handle networking, memory management, and load balancing.\n\nthis blurs the line between model design and system design.\n\ngoogle’s “pathways” system (2022) was an effort to build a general infrastructure for expert models that route data across multiple model pieces.\n\nwhile details are scarce, the goal was to seamlessly support models that conditionally use different subnetworks – essentially moes on a larger scale.\n\nnvidia and microsoft have also worked on inference managers that can host massive models across clusters (e.g. nvidia’s trinity or microsoft’s zero-inference mug.mvapich.cse.ohio-state.edu deepspeed.ai ).\n\nthese frameworks must be hardware-aware : they might schedule moe computations to minimize data transfer (e.g. by preferring local experts) or dynamically adjust parallelism depending on available resources.\n\nin practice, tpus excel at large all-to-all due to their high-bandwidth mesh, whereas gpus (prior to nvswitch/h100 advances) needed more tuning.\n\nbut recent results show gpus can handle trillion-parameter moes efficiently with the right software.\n\ndeepspeed-moe demonstrated 25 ms inference latency for a 1.5 trillion param moe across hundreds of gpus arxiv.org proceedings.mlr.press – an impressive feat made possible by co-optimizing the model and hardware usage.\n\nmeanwhile, on tpus, google reported strong scaling of switch transformer; they even scaled down to smaller tpu slices to show moe benefits “with only a few cores” jmlr.org jmlr.org , highlighting that the approach wasn’t limited to enormous pods.\n\na final consideration is hardware-specific limits like memory per core. tpu v3 has 16 gb per chip, which constrained the size of experts that could be hosted per core. glam (1.2t) and switch (1.6t) both navigated this by using many experts with relatively smaller size (e.g. 64 experts each of ~/≍ 10b parameters across many cores).\n\ngpu a100s offer 40–80 gb, allowing larger experts per device, but then fewer devices (which can increase communication per token).\n\nthese trade-offs mean the optimal moe configuration (number of experts, expert size, distribution) can be different between hardware platforms.\n\nsoftware libraries have begun to abstract this: for example, deepspeed’s moe support can automatically partition experts across gpus and even offload some layers to cpu if needed seas.upenn.edu seas.upenn.edu.\n\nthe goal is to make moe deployment “hardware-agnostic,” but in reality, achieving peak performance still requires tuning to the strengths of the hardware (e.g. using tensor parallelism on gpu for big matmuls vs. relying on tpu systolic array utilization, etc.).\n\nin summary, moe inference pushes the envelope of what our hardware and frameworks were initially designed for.\n\nover the past few years, a synergy of model research and system engineering has emerged: new routing algorithms are designed with hardware limitations in mind (e.g. favor local experts to reduce network traffic huggingface.co huggingface.co ), and systems are built to better accommodate dynamic, sparse computation.\n\nensuring that an moe model runs efficiently on a given platform often means bridging the gap between model parallelism and the hardware’s communication topology. the advancements from projects like gshard, tutel, and deepspeed-moe reflect a broad consensus that to serve giant moes at low latency, one must co-optimize the model and the hardware execution plan.\n\n\n# expert sparsity and activation imbalance\n\nmoe models are deliberately sparse – at any given layer, only a small fraction of the model’s neurons “activate” for an input token.\n\nthis sparsity is what gives moes their compute advantage, but it also leads to uneven utilization of the model’s capacity.\n\nwe touched on load imbalance earlier from a performance perspective; here we consider the implications of expert sparsity on model behavior and inference outcomes.\n\nin an ideal scenario, each expert in an moe would specialize in different aspects of language and be utilized regularly when those aspects are present in the input.\n\nin practice, moes often develop skewed specializations : some experts become “generalists” that fire on a wide range of tokens, while others become niche experts that rarely activate. for example, the st-moe model analysis found that certain experts specialized in punctuation or rare words huggingface.co.\n\nif the input doesn’t contain those patterns, those experts stay dormant.\n\nhuang et al. (2023) confirmed this phenomenon at scale: in their moe language model, they observed multiple hot experts that handled a large portion of tokens consistently, whereas many other experts received only a trickle of tokens or none at all seas.upenn.edu seas.upenn.edu.\n\nimportantly, which experts are hot can vary by domain (e.g. some experts are hot for programming text, others for wikipedia), but within a given domain or task, the imbalance persists seas.upenn.edu seas.upenn.edu .\n\nthis activation imbalance means a lot of parameters are effectively under-utilized during inference.\n\nif 2–3 experts handle, say, 50% of all tokens, then the remaining experts contribute little to the throughput or real-time decision-making.\n\nthey may hold knowledge that’s rarely needed, which is inefficient from a deployment standpoint – we’re dedicating memory and maintenance to experts that rarely contribute.\n\nmoreover, when an expert is rarely used, its weights might not be well-fine-tuned (if the distribution shift from training causes it to activate less, it may perform suboptimally the few times it is chosen).\n\non the flip side, the over-used experts can become points of contention as described, and also single points of failure if they saturate.\n\nresearchers have proposed various strategies to handle this sparsity/imbalance issue.\n\none concept is expert rejuvenation or pruning : periodically identify experts that receive almost no traffic and either retrain them on new data or remove/merge them.\n\nthis is analogous to pruning neurons in a dense model that never fire.\n\nthere has been less published on pruning experts post-training, but it’s a plausible avenue for model compression – essentially, if an expert is never selected, one could drop it to save memory (with a fallback that the gating network might choose the next best expert instead). another strategy, as mentioned, is dynamic load balancing at inference seas.upenn.edu seas.upenn.edu.\n\nthis doesn’t eliminate sparsity but ensures it doesn’t harm performance: if one expert is “hotter” than others in a particular batch, we can spread some of its load to other experts.\n\nthis approach was shown to improve system robustness significantly seas.upenn.edu.\n\nit effectively reduces activation imbalance per batch even if the model’s inherent gating is imbalanced.\n\nhowever, it does change the model’s computations slightly (some tokens go to a second-choice expert), which could affect output quality marginally.\n\nthe authors reported that this technique avoids oom errors and keeps latency manageable in pathological cases seas.upenn.edu seas.upenn.edu.\n\nan intriguing finding by google researchers was that expert utilization can drop during fine-tuning if the load-balancing loss is turned off huggingface.co.\n\nthey found that even when up to 11% of tokens were dropped (due to capacity limits) during fine-tuning, the model’s quality wasn’t significantly hurt – suggesting that many experts were not critical for those tokens huggingface.co huggingface.co.\n\nthis hints that there is redundancy in expert knowledge: some tokens can afford to skip their top expert and still be processed well by another.\n\nthat redundancy might be exploited at inference: one could imagine an moe serving system that, under heavy load, temporarily routes tokens to fewer experts to save compute, relying on the redundancy to maintain output quality. this would be a form of graceful degradation using sparsity.\n\nthe sparsity itself (using only a few experts per token) also has implications for output variance.\n\nif an moe uses $k=2$ experts with weighted combination, it is essentially ensembling two expert opinions per token. many moes (like glam) do this, which can stabilize outputs at the cost of more compute ar5iv.org ar5iv.org.\n\nswitch transformer opted for $k=1$ (only one expert per token) to simplify and speed up inference jmlr.org.\n\nthis increases sparsity but means each token’s representation is determined by a single expert’s weights.\n\nthat can lead to more variance in token outputs – if an expert has a peculiar behavior, the token’s output will reflect it strongly.\n\nin critical applications, one might prefer $k=2$ routing despite the extra cost, for a form of local ensemble averaging.\n\nin fact, some recent moe variants allow adjusting $k$ at inference (trading off quality vs. speed) huggingface.co huggingface.co.\n\nfor example, one can train with $k=2$ but use only $k=1$ for faster inference, accepting a small quality drop huggingface.co huggingface.co.\n\nthis is another way to use sparsity as a tunable knob at runtime.\n\nfinally, knowledge concentration in experts has pros and cons for inference.\n\non one hand, having experts specialize (e.g. an expert for legal text, one for code, one for conversation) is useful – it means when such content appears, that expert can handle it well.\n\nindeed, moes are partly motivated by the idea of capturing diverse patterns or modalities in different experts.\n\non the other hand, if a specialization is too narrow, the expert might remain idle most of the time.\n\nmeta’s team encountered this in their multilingual moe for no language left behind: some language experts would be mostly unused unless that language was being translated.\n\ntheir solution was task-based routing as discussed, or training the gating to be language-aware arxiv.org.\n\nin general, there’s a design choice: should experts be purely data-driven in what they specialize in, or explicitly structured (by language, by topic, etc.)?\n\nif structured, one can ensure each expert has a role and gets used when that role is needed (like a module).\n\nthis can avoid wasted experts but requires knowing the axes of specialization ahead of time.\n\ndata-driven specializations may yield unexpected or overlapping expert roles, some of which might not be worth the cost at inference.\n\nin summary, the sparse activation nature of moes yields uneven expert usage, which raises the question of how to maximize the value of all those parameters.\n\ntechniques to balance expert load, both at training (auxiliary losses) and at inference (dynamic token redistribution), have been successful in preventing any expert from becoming a throughput liability jmlr.org jmlr.org seas.upenn.edu seas.upenn.edu.\n\nmeanwhile, acknowledging that some experts will inevitably be lightly used, researchers are exploring ways to compress or remove them without losing much model capacity.\n\nas moes enter production settings, we may see more adaptive inference mechanisms – for instance, a system might monitor expert utilization over time and unload an expert from gpu if it hasn’t been used for a while (similar to expert buffering).\n\nthe sparsity that is a blessing for compute is a curse for utilization , but through careful design, moes can ensure that even “cold” experts have their moment to shine when needed, and do not impede the model when they are not.\n\n\n# real-time and streaming inference\n\none of the ultimate goals for deploying large language models is to serve interactive applications – e.g. chatbots, real-time translation, or personalized assistants – with low latency per user. moe models pose unique challenges in these scenarios.\n\nas discussed, moes thrive on throughput: the hardware can be kept busy if there are many tokens to process in parallel.\n\nbut in a streaming inference setting (generating one token at a time for a user, then immediately using it before generating the next), we often operate at batch size 1.\n\nthis emphasizes the per-token overheads of moes.\n\nif a model is outputting text token by token, each step involves running the moe layer routing.\n\nthe overhead of gating and cross-device communication on every generation step can introduce noticeable latency.\n\nfor example, if it takes a dense model 50 ms to generate a token and a moe model 150 ms (due to extra coordination), the user will experience slower response despite the moe’s theoretical efficiency.\n\nin fact, without optimizations, moe layers can make auto-regressive generation significantly slower per token than a dense model.\n\nthis was highlighted by nvidia in the context of small batches: they note that sequential token generation limits throughput because you cannot parallelize the timeline of generation developer.nvidia.com.\n\nin moes, this is exacerbated by the fact that not all gpu cores are used for each token (since only some experts run). essentially, moes trade off some single-stream speed for multi-stream scalability.\n\nthere are a few strategies to handle real-time usage of moes:\n\n * concurrent streams : run multiple independent sequences through the moe model in interleaved fashion.\n   this way, while one sequence is waiting for its next token’s computation, another sequence’s token can use other experts.\n   if done cleverly, this can fill the gaps and use all experts efficiently.\n   many inference servers do this by batching requests that arrive within a short window.\n   for moes, you might maintain a rolling batch of tokens from different users.\n   the deepspeed-moe system explicitly allows combining data parallelism (different inputs on different devices) with expert-slicing microsoft.com microsoft.com , which could be leveraged to serve multiple requests at once.\n   the downside is added latency variance – a user’s query might wait a few extra milliseconds to be grouped with another’s.\n   there’s a trade-off between latency and throughput here.\n\n * quality-vs-latency tuning : as mentioned, one can reduce the number of experts used at inference to speed up each token, at some cost to output quality huggingface.co huggingface.co.\n   for example, switching from top-2 to top-1 gating at generation time roughly halves the expert compute and communication.\n   if the model was trained with $k=2$, this might degrade quality slightly, but perhaps not dramatically (especially if the gating network often had one dominant expert anyway).\n   this is a way to adapt a moe for faster interactive use – essentially operating it in a more sparse mode when needed.\n\n * distillation fallback : a practical approach used in industry is to maintain a smaller dense model as a fallback for real-time.\n   the moe might be used offline or for batch tasks where latency is less critical, whereas a distilled version serves the online traffic.\n   google’s early experiments with switch transformer indeed suggested distilling the moe into a dense model for production serving huggingface.co huggingface.co.\n   they managed to compress about 30–40% of the moe’s performance gain into the dense model. for streaming applications, that dense model can generate faster and with more predictable timing. the trade-off is losing some of the moe’s peak accuracy.\n\n * streaming-friendly routing : research is underway on making routing algorithms more cognizant of temporal context.\n   for instance, one could enforce that once a conversation is in progress, the gating tries to reuse the same experts it used for previous tokens (assuming topic/linguistic context persists). this could improve locality and perhaps allow caching of those experts on certain devices (reducing repetitive loading).\n   it’s a bit speculative, but one could imagine a gating that says “for this dialogue turn, stick to expert 5 and 9 unless a very out-of-context token appears.”\n   this starts to resemble the task-level routing, but at a finer session granularity. the benefit would be smoother latency (since subsequent tokens hit the same devices) and possible reuse of intermediate states.\n\n * real-time load balancing : in an interactive system, load can fluctuate (many users at peak hours, few at others).\n   moes can actually shine in high-concurrency situations by scaling gracefully with load – more incoming tokens naturally utilize more experts, spreading out work.\n   but in low-load conditions, the cluster running a moe might be underutilized.\n   one solution is to dynamically scale down the number of active devices when load is low (parking some gpus).\n   however, cold-starting them when load increases could be slow.\n   it’s a general infrastructure problem not unique to moe, though moe’s conditional activation makes it tricky to pin which devices can sleep.\n   if certain experts aren’t likely to be used, those could be temporarily paged out to save power, as long as the system can page them in on demand (similar to expert buffering but for power/availability scaling).\n\nthe consistency of inference is also a consideration in real-time.\n\nbecause moe outputs can be nondeterministic if there's any randomness in gating or ties broken arbitrarily, one might see small variations in outputs between runs.\n\nmost moes are deterministic given the same model and input, but if load-balancing decisions reroute some tokens differently under pressure, outputs could change run-to-run.\n\nfor applications like translation or medical text generation, consistent outputs are important.\n\nthus, any adaptive inference tricks have to be carefully evaluated for their effect on the text output, not just latency.\n\nfinally, streaming moes for things like continuous speech recognition or video processing face the challenge of state.\n\na streaming asr model might process audio frame by frame; if it were an moe, each frame routes independently. it might be beneficial to have the same expert handle a contiguous segment of audio to accumulate context.\n\nthis is a frontier area: combining moe with recurrent or stateful models for streaming inputs.\n\nin summary, real-time deployment of moe models involves careful engineering to get the best of both worlds: the moe’s efficiency during heavy utilization and acceptable latency for single-stream cases.\n\nsome notable trade-offs and findings include: (1) moes can deliver higher throughput under load, but at the cost of higher per-token latency at low loads microsoft.com microsoft.com ; (2) batching and concurrency are key to mitigating this – in one experiment, routing by entire tasks improved throughput ~2× while preserving quality, essentially by turning the moe into multiple smaller models used in parallel arxiv.org arxiv.org; (3) when latency is paramount, techniques like model compression or gating simplification may be necessary to meet strict slas.\n\nas the field progresses, we expect to see moes become more elastic – adapting on the fly to different latency/throughput regimes.\n\nthe end-game is a system where the moe’s sparsity can be dialed up or down in real-time: using lots of experts for tough, batch tasks and fewer experts for quick, interactive responses.\n\nachieving that will require advances in both algorithms and serving infrastructure.\n\n\n# conclusion\n\nmixture-of-experts models represent a powerful approach to scaling up language models, offering unprecedented parameter counts and the potential for improved quality at lower inference cost.\n\nrealizing these benefits in practice, however, demands surmounting several inference-time challenges. we have surveyed the major hurdles: from expert routing overhead and load imbalance , to latency/throughput bottlenecks , memory and bandwidth demands , hardware integration issues , uneven expert utilization , and serving in real-time contexts.\n\nacross the literature and industry implementations, a few common themes emerge:\n\n * load balancing and routing : effective moe inference hinges on keeping the workload evenly spread among experts and minimizing routing overhead.\n   auxiliary training losses and novel gating algorithms (e.g. base layers, hash routing) help distribute tokens, while dynamic routing adjustments at runtime can further smooth out imbalances jmlr.org seas.upenn.edu.\n   coarse routing at the level of tasks or sequences is a viable strategy to simplify inference, essentially breaking a giant moe into manageable sub-models per request arxiv.org arxiv.org .\n\n * system and parallelism optimizations : to achieve low latency, moe models must leverage parallelism across experts while avoiding excessive communication.\n   designs like deepspeed-moe and tutel show that with tailored all-to-all protocols and a mix of parallelism strategies, moes can reach 4–8× throughput gains over naive implementations microsoft.com microsoft.com.\n   hierarchical communication, fused kernels, and scheduling that groups tokens by expert are key techniques microsoft.com microsoft.com.\n   these optimizations turn the theoretical compute savings of sparsity into real wall-clock improvements.\n\n * memory management : the massive scale of moes means memory (not compute) often becomes the primary bottleneck microsoft.com . techniques such as expert sharding (as in gshard seas.upenn.edu ), memory-aware gating (to reduce padding seas.upenn.edu ), and offloading inactive experts seas.upenn.edu all serve to bring memory usage under control.\n   research prototypes like expert buffering and block-sparse execution demonstrate that it’s possible to cut memory overhead dramatically without sacrificing much accuracy seas.upenn.edu huggingface.co.\n   in practice, a combination of hardware memory capacity, distributed memory bandwidth, and software caching is used to accommodate trillion-parameter models.\n\n * hardware adaptation : moe inference performance is deeply tied to hardware capabilities.\n   early moes leveraged google’s tpu architecture for scale huggingface.co , while recent efforts focus on optimizing for gpu clusters with high-speed interconnects microsoft.com.\n   custom communication libraries (nccl tweaks, msccl) and compiler support for dynamic shapes (as in meta’s fmi and megablocks) are bridging the gap.\n   the community has learned that dense-to-sparse migration isn’t plug-and-play – it requires co-designing the model and the system.\n   but with each generation of hardware and software (e.g. nvidia’s hopper gpus, new networking technologies), moes become more feasible to serve at scale.\n\n * quality vs. performance trade-offs : many solutions involve a trade-off. using fewer experts per token, merging experts, or distilling to dense models – all these can ease deployment but may reduce model accuracy huggingface.co.\n   the optimal balance depends on the application. notably, some recent works achieved full quality retention with faster inference: e.g. task-moe kept all gains while boosting speed 2× arxiv.org , and mixtral 8×7b matched or beat a dense 70b model with much faster inference huggingface.co huggingface.co.\n   these results are encouraging, suggesting that clever routing or expert combination can give the best of both worlds.\n\nin conclusion, the inference-time challenges of moe models are being actively tackled on multiple fronts.\n\nacademia and industry have proposed a spectrum of solutions – from algorithmic innovations like balanced routing and adaptive gating, to system-level improvements in kernels and networking, to pragmatic approaches like model compression for serving.\n\nthe most notable finding across the board is that sparsity in large models offers real advantages but requires a sophisticated runtime to harness.\n\nwhen properly managed, moe models can achieve superior performance-per-compute and even faster inference than dense models of equivalent quality microsoft.com microsoft.com.\n\nbut without careful design, they can suffer from bottlenecks that erase their theoretical benefits.\n\nthe trade-offs often come down to flexibility vs. efficiency: a highly dynamic moe (routing every token optimally) might maximize quality but be harder to optimize, whereas a more structured or restricted moe (routing by task or with fixed patterns) might be easier to serve but less adaptive.\n\nwe see this reflected in designs like switch (simpler routing, easier scaling) versus glam (more complex routing, higher peak quality).\n\nlooking forward, proposed solutions such as more intelligent compilers for moe (compiling sparse operations into dense blocks), hardware with native support for conditional computation, and hybrid models (sparse layers combined with dense backbones) will further mitigate inference challenges.\n\ncompanies like google and meta continue to refine moe deployments for multilingual and multitask models, indicating that the approach remains promising if the serving cost can be kept in check.\n\nas of 2024, moes are not yet the de facto standard for all llm deployments (many productions use dense models like gpt-4 or llama due to maturity and simplicity), but the gap is closing.\n\nwith projects like deepspeed-moe demonstrating sub-30ms latency for trillion-param models arxiv.org , it’s clear that moes can be made to work in real-time with enough engineering.\n\nin summary, mixture-of-experts models introduce unique inference-time challenges, but research has shown these are surmountable.\n\nthrough balanced expert utilization, parallelism-aware system design, memory optimizations, and strategic simplifications, moes can live up to their promise of “maximum parameters, minimal compute” jmlr.org jmlr.org.\n\nthe ongoing convergence of model architecture and infrastructure suggests that future large-scale ai systems may routinely use expert mixtures to deliver both scale and speed – dynamically allocating computation only where needed, and thereby achieving new levels of efficiency in large language model inference.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"how malloc works",frontmatter:{title:"how malloc works",date:"2024-01-02T15:32:49.000Z",permalink:"/pages/ec7035/",tags:[null]},regularPath:"/06.unix/01.malloc.html",relativePath:"06.unix/01.malloc.md",key:"v-e114a476",path:"/pages/ec7035/",headersStr:null,content:"malloc\n\n1 brk and mmap\nhttps://people.kth.se/~johanmon/courses/id2206/lectures/management-handout.pdf\n\n\nMalloc will invoke brk or mmap systemcall. The difference is based on\nFocus on Size:\n\nThe primary factor influencing malloc's choice is the requested memory size.\nbrk for Smaller Allocations:\nFor smaller allocations (often configurable through a threshold), malloc will likely use brk. brk is a system call that adjusts the program's data segment boundary. It's a relatively fast operation for requesting contiguous memory from the heap.\n\nmmap for Larger Allocations:\nWhen the requested memory size exceeds a certain threshold (often set by mallopt function), malloc might use mmap instead.\n\n2. Memory.\n\n\nEvery time you call sbrk，it will increase the brk.\n\n\n\n\n\n3. Code, Lib, Systemcall\n\n\n\n\n\n\n4. The Object in memory is organized by metadata and then data.\nThe metadata is size and bit.\n\n\nThe metadata is aligned by 16Byte. ------- This needs to be proved.\n\n\n\n\n\n5. Create Hooks for malloc.\nhttps://www.gnu.org/software/libc/manual/html_node/Hooks-for-Malloc.html\n\n/* Prototypes for __malloc_hook, __free_hook */\n#include <malloc.h>\n\n/* Prototypes for our hooks.  */\nstatic void my_init_hook (void);\nstatic void *my_malloc_hook (size_t, const void *);\nstatic void my_free_hook (void*, const void *);\n\nstatic void\nmy_init (void)\n{\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n}\n\nstatic void *\nmy_malloc_hook (size_t size, const void *caller)\n{\n  void *result;\n  /* Restore all old hooks */\n  __malloc_hook = old_malloc_hook;\n  __free_hook = old_free_hook;\n  /* Call recursively */\n  result = malloc (size);\n  /* Save underlying hooks */\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  /* printf might call malloc, so protect it too. */\n  printf (\"malloc (%u) returns %p\\n\", (unsigned int) size, result);\n  /* Restore our own hooks */\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n  return result;\n}\n\nstatic void\nmy_free_hook (void *ptr, const void *caller)\n{\n  /* Restore all old hooks */\n  __malloc_hook = old_malloc_hook;\n  __free_hook = old_free_hook;\n  /* Call recursively */\n  free (ptr);\n  /* Save underlying hooks */\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  /* printf might call free, so protect it too. */\n  printf (\"freed pointer %p\\n\", ptr);\n  /* Restore our own hooks */\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n}\n\nmain ()\n{\n  my_init ();\n  …\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n6. CppCon 2017 Memory Alloc”\n\nCppCon 2017: John Lakos “Local ('Arena') Memory Allocators (part 1 of 2)”",normalizedContent:"malloc\n\n1 brk and mmap\nhttps://people.kth.se/~johanmon/courses/id2206/lectures/management-handout.pdf\n\n\nmalloc will invoke brk or mmap systemcall. the difference is based on\nfocus on size:\n\nthe primary factor influencing malloc's choice is the requested memory size.\nbrk for smaller allocations:\nfor smaller allocations (often configurable through a threshold), malloc will likely use brk. brk is a system call that adjusts the program's data segment boundary. it's a relatively fast operation for requesting contiguous memory from the heap.\n\nmmap for larger allocations:\nwhen the requested memory size exceeds a certain threshold (often set by mallopt function), malloc might use mmap instead.\n\n2. memory.\n\n\nevery time you call sbrk，it will increase the brk.\n\n\n\n\n\n3. code, lib, systemcall\n\n\n\n\n\n\n4. the object in memory is organized by metadata and then data.\nthe metadata is size and bit.\n\n\nthe metadata is aligned by 16byte. ------- this needs to be proved.\n\n\n\n\n\n5. create hooks for malloc.\nhttps://www.gnu.org/software/libc/manual/html_node/hooks-for-malloc.html\n\n/* prototypes for __malloc_hook, __free_hook */\n#include <malloc.h>\n\n/* prototypes for our hooks.  */\nstatic void my_init_hook (void);\nstatic void *my_malloc_hook (size_t, const void *);\nstatic void my_free_hook (void*, const void *);\n\nstatic void\nmy_init (void)\n{\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n}\n\nstatic void *\nmy_malloc_hook (size_t size, const void *caller)\n{\n  void *result;\n  /* restore all old hooks */\n  __malloc_hook = old_malloc_hook;\n  __free_hook = old_free_hook;\n  /* call recursively */\n  result = malloc (size);\n  /* save underlying hooks */\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  /* printf might call malloc, so protect it too. */\n  printf (\"malloc (%u) returns %p\\n\", (unsigned int) size, result);\n  /* restore our own hooks */\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n  return result;\n}\n\nstatic void\nmy_free_hook (void *ptr, const void *caller)\n{\n  /* restore all old hooks */\n  __malloc_hook = old_malloc_hook;\n  __free_hook = old_free_hook;\n  /* call recursively */\n  free (ptr);\n  /* save underlying hooks */\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  /* printf might call free, so protect it too. */\n  printf (\"freed pointer %p\\n\", ptr);\n  /* restore our own hooks */\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n}\n\nmain ()\n{\n  my_init ();\n  …\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n6. cppcon 2017 memory alloc”\n\ncppcon 2017: john lakos “local ('arena') memory allocators (part 1 of 2)”",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"No More Asking",frontmatter:{title:"No More Asking",date:"2024-01-02T15:32:49.000Z",permalink:"/pages/ec7036/",tags:[null]},regularPath:"/06.unix/02.op.html",relativePath:"06.unix/02.op.md",key:"v-0d7764c5",path:"/pages/ec7036/",headers:[{level:2,title:"git",slug:"git",normalizedTitle:"git",charIndex:2},{level:3,title:"add keys",slug:"add-keys",normalizedTitle:"add keys",charIndex:10},{level:3,title:"create repo",slug:"create-repo",normalizedTitle:"create repo",charIndex:229}],headersStr:"git add keys create repo",content:'# git\n\n\n# add keys\n\nssh-keygen -t ed25519 -C "*@gmail.com"\n# in the above procedure, you could change path, rename\n\nssh-add ~/.ssh/new_key\n# then we could cat ~/.ssh/new_key.pub and add into github setting keys.\n\n\n1\n2\n3\n4\n5\n\n\n\n# create repo\n\ngit init\ngit add README.md\ngit commit -m ""\ngit branch -M main\ngit remote set-url origin git@github.com-second:username/*.git\ngit config user.name "*"\ngit config user.email "*@gmail.com"\n\n\n1\n2\n3\n4\n5\n6\n7\n',normalizedContent:'# git\n\n\n# add keys\n\nssh-keygen -t ed25519 -c "*@gmail.com"\n# in the above procedure, you could change path, rename\n\nssh-add ~/.ssh/new_key\n# then we could cat ~/.ssh/new_key.pub and add into github setting keys.\n\n\n1\n2\n3\n4\n5\n\n\n\n# create repo\n\ngit init\ngit add readme.md\ngit commit -m ""\ngit branch -m main\ngit remote set-url origin git@github.com-second:username/*.git\ngit config user.name "*"\ngit config user.email "*@gmail.com"\n\n\n1\n2\n3\n4\n5\n6\n7\n',charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"leakage current moores law meets static power",frontmatter:{title:"leakage current moores law meets static power",date:"2024-08-06T15:32:49.000Z",permalink:"/pages/f00000/",tags:[null]},regularPath:"/10.mix/01.leakagecurrent.html",relativePath:"10.mix/01.leakagecurrent.md",key:"v-23bbae21",path:"/pages/f00000/",headers:[{level:3,title:"Leakge Current Moore's Law Meets Static Power",slug:"leakge-current-moore-s-law-meets-static-power",normalizedTitle:"leakge current moore's law meets static power",charIndex:2},{level:3,title:"Power Basics",slug:"power-basics",normalizedTitle:"power basics",charIndex:837},{level:3,title:"Overall Power Consumption",slug:"overall-power-consumption",normalizedTitle:"overall power consumption",charIndex:1129},{level:3,title:"Leakage current",slug:"leakage-current",normalizedTitle:"leakage current",charIndex:1966},{level:2,title:"REDUCING STATIC POWER CONSUMPTION",slug:"reducing-static-power-consumption",normalizedTitle:"reducing static power consumption",charIndex:3589},{level:3,title:"Retention flip-flops",slug:"retention-flip-flops",normalizedTitle:"retention flip-flops",charIndex:3627},{level:3,title:"Controlling memory leakage",slug:"controlling-memory-leakage",normalizedTitle:"controlling memory leakage",charIndex:4215},{level:2,title:"TECHNOLOGY TRENDS AND CHALLENGES",slug:"technology-trends-and-challenges",normalizedTitle:"technology trends and challenges",charIndex:5431},{level:3,title:"Multiple threshold voltages",slug:"multiple-threshold-voltages",normalizedTitle:"multiple threshold voltages",charIndex:6549},{level:3,title:"Gate length",slug:"gate-length",normalizedTitle:"gate length",charIndex:6581},{level:3,title:"Oxide tunneling",slug:"oxide-tunneling",normalizedTitle:"oxide tunneling",charIndex:6597}],headersStr:"Leakge Current Moore's Law Meets Static Power Power Basics Overall Power Consumption Leakage current REDUCING STATIC POWER CONSUMPTION Retention flip-flops Controlling memory leakage TECHNOLOGY TRENDS AND CHALLENGES Multiple threshold voltages Gate length Oxide tunneling",content:"# Leakge Current Moore's Law Meets Static Power\n\nOff-state leakage is static power, current that leaks through transistors even when they are turned off.\n\n * It is one of two principal sources of power dissipation in today’s microprocessors.\n * The other is dynamic power, which arises from the repeated capacitance charge and discharge on the output of the hundreds of millions of gates in today’s chips.\n   * subthreshold leakage, a weak inversion current across the device; and\n   * gate leakage, a tunneling current through the gate oxide insulation.\n\nDynamic power is proportional to the square of supply voltage, so reducing the voltage significantly reduces power consumption.\n\nUnfortunately, smaller geometries exacerbate leakage, so static power begins to dominate the power consumption equation in microprocessor design.\n\n\n\n\n# Power Basics\n\n# Operating Frequency and volatage\n\n\n\n\n\nwe see that f = 0 corresponds to Vnorm = Vth / Vmax, which for today’s technology is approximately 0.3.\n\nReducing the operating frequency by a particular percentage from fmax will reduce the operating voltage by a smaller percentage.\n\n\n# Overall Power Consumption\n\n\n\n * The first term is the dynamic power lost from charging and discharging the processor’s capacitive loads: A is the fraction of gates actively switching and C is the total capacitance load of all gates.\n * The second term models the static power lost due to leakage current, Ileak.\n\nIn fact, halving the voltage will reduce the power consumption by a factor of four. But Equation 2 shows that halving the voltage will reduce the processor’s maximum operating frequency by more than half.\n\nTo compensate for this performance loss, we can use either parallel or pipelined implementations.\n\nIf the implementation runs the original serial computation as two parallel subtasks or as two pipelined subtasks, the dynamic power consumption can decrease by more than a factor of two compared to the serial case.\n\n\n# Leakage current\n\nAs noted, leakage current, the source of static power consumption, is a combination of subthreshold and gate-oxide leakage: Ileak = Isub + Iox.\n\n# Subthreshold power leakage\n\n\n\nHow to reduce Isub\n\n * First, we could turn off the supply voltage—that is, set V to zero so that the factor in parentheses also becomes zero.\n * Second, we could increase the threshold voltage, which—because it appears as a negative exponent—can have a dramatic effect in even small increments. On the other hand, we know from Equation 1 that increasing Vth will reduce speed.\n\nThe problem with the first approach is loss of state;\n\nThe problem with the second approach is the loss of performance.\n\nGate width W is the other contributor to subthreshold leakage in a particular transistor. Designers often use the combined widths of all the processor’s transistors as a convenient measure of total subthreshold leakage.\n\n# Gate-oxide power leakage\n\n\n\nK2 and α are experimentally derived. The term of interest is oxide thickness, Tox.\n\nTox will reduce gate leakage. Unfortunately, it also degrades the transistor’s effectiveness because Tox must decrease proportionally with process scaling to avoid short channel effects.\n\n# Low-power architectural options\n\nBecause subthreshold and oxide leakage both depend on total gate width or, approximately, gate count.\n\nPipelined implementations can run at a lower voltage, which can reduce power consumption for both dynamic and static power compared to the serial case.\n\nParallel implementations can also run at a lower voltage, but only by roughly doubling the amount of hardware.\n\n\n# REDUCING STATIC POWER CONSUMPTION\n\n\n# Retention flip-flops\n\nFor shorter inactive periods, researchers have developed “balloon” logic, also called retention flip-flops. The idea is to use highVth latches to duplicate those latches that must preserve state.\n\nUsing doping techniques or applying a bias voltage to the substrate can increase threshold voltage by 100 mV. This in turn reduces leakage by a factor of about 10, but it increases switching time by about 15 percent.\n\nThus, lowleakage retention flops are only useful in saving state energy efficiently—their use on the processor’s critical path would slow it down.\n\n\n# Controlling memory leakage\n\nIn fact, leakage is projected to account for 70 percent of the cache power budget in 70-nm technology.\n\nBoth bitline and cell leakage result from subthreshold conduction—current flowing from the source to drain even when gate-source voltage is below the threshold voltage.\n\n# Circuit techniques.\n\n * State-destructive\n\n * State-preserving techniques vary. Drowsy caches multiplex supply voltages according to the state of each cache line or block. Waking up the drowsy cache lines is treated as a pseudo cache miss and incurs one additional cycle overhead.\n   \n   Moreover, while state-preserving techniques can only reduce leakage by about a factor of 10, compared to more than a factor of 1,000 for destructive techniques, the net difference in power consumed by the two techniques is less than 10 percent.\n\n# Control techniques.\n\n * application-sensitive controls, based on runtime performance feedback,9,13 and\n * application-insensitive controls, which periodically turn off cache lines.\n\n# Compiler techniques\n\n * Using compiler directives might make it possible to keep some loops within bank boundaries.\n * The compiler can also provide application-sensitive leakage control.\n\n\n# TECHNOLOGY TRENDS AND CHALLENGES\n\nOne approach to reducing subthreshold leakage is to actively refrigerate the chip.\n\nWhile this option seems promising for controlling the subthreshold leakage point, it does not address gateoxide leakage.\n\nDesigners assign a low threshold voltage to a few performance-critical transistors and a high threshold voltage to the majority of less timing-critical transistors.\n\nThis approach incurs a high subthreshold leakage current for the performance-critical transistors, but it can significantly reduce the overall leakage.\n\nfuture technologies are likely to offer three threshold voltages—low, high, and extra high—or even more.\n\nThis opens the way to new leakage optimizations within different portions of a cache or at different levels of its hierarchy.\n\nthe address-decoder and bus-driver circuits in a cache consume a significant portion of total access time, so a designer could construct them from high-Vth transistors, while constructing the more numerous bit cells from extra-high-Vth devices and reserving low-Vth devices for speed-critical parts of the processor core.\n\n\n# Multiple threshold voltages\n\n\n# Gate length\n\n\n# Oxide tunneling",normalizedContent:"# leakge current moore's law meets static power\n\noff-state leakage is static power, current that leaks through transistors even when they are turned off.\n\n * it is one of two principal sources of power dissipation in today’s microprocessors.\n * the other is dynamic power, which arises from the repeated capacitance charge and discharge on the output of the hundreds of millions of gates in today’s chips.\n   * subthreshold leakage, a weak inversion current across the device; and\n   * gate leakage, a tunneling current through the gate oxide insulation.\n\ndynamic power is proportional to the square of supply voltage, so reducing the voltage significantly reduces power consumption.\n\nunfortunately, smaller geometries exacerbate leakage, so static power begins to dominate the power consumption equation in microprocessor design.\n\n\n\n\n# power basics\n\n# operating frequency and volatage\n\n\n\n\n\nwe see that f = 0 corresponds to vnorm = vth / vmax, which for today’s technology is approximately 0.3.\n\nreducing the operating frequency by a particular percentage from fmax will reduce the operating voltage by a smaller percentage.\n\n\n# overall power consumption\n\n\n\n * the first term is the dynamic power lost from charging and discharging the processor’s capacitive loads: a is the fraction of gates actively switching and c is the total capacitance load of all gates.\n * the second term models the static power lost due to leakage current, ileak.\n\nin fact, halving the voltage will reduce the power consumption by a factor of four. but equation 2 shows that halving the voltage will reduce the processor’s maximum operating frequency by more than half.\n\nto compensate for this performance loss, we can use either parallel or pipelined implementations.\n\nif the implementation runs the original serial computation as two parallel subtasks or as two pipelined subtasks, the dynamic power consumption can decrease by more than a factor of two compared to the serial case.\n\n\n# leakage current\n\nas noted, leakage current, the source of static power consumption, is a combination of subthreshold and gate-oxide leakage: ileak = isub + iox.\n\n# subthreshold power leakage\n\n\n\nhow to reduce isub\n\n * first, we could turn off the supply voltage—that is, set v to zero so that the factor in parentheses also becomes zero.\n * second, we could increase the threshold voltage, which—because it appears as a negative exponent—can have a dramatic effect in even small increments. on the other hand, we know from equation 1 that increasing vth will reduce speed.\n\nthe problem with the first approach is loss of state;\n\nthe problem with the second approach is the loss of performance.\n\ngate width w is the other contributor to subthreshold leakage in a particular transistor. designers often use the combined widths of all the processor’s transistors as a convenient measure of total subthreshold leakage.\n\n# gate-oxide power leakage\n\n\n\nk2 and α are experimentally derived. the term of interest is oxide thickness, tox.\n\ntox will reduce gate leakage. unfortunately, it also degrades the transistor’s effectiveness because tox must decrease proportionally with process scaling to avoid short channel effects.\n\n# low-power architectural options\n\nbecause subthreshold and oxide leakage both depend on total gate width or, approximately, gate count.\n\npipelined implementations can run at a lower voltage, which can reduce power consumption for both dynamic and static power compared to the serial case.\n\nparallel implementations can also run at a lower voltage, but only by roughly doubling the amount of hardware.\n\n\n# reducing static power consumption\n\n\n# retention flip-flops\n\nfor shorter inactive periods, researchers have developed “balloon” logic, also called retention flip-flops. the idea is to use highvth latches to duplicate those latches that must preserve state.\n\nusing doping techniques or applying a bias voltage to the substrate can increase threshold voltage by 100 mv. this in turn reduces leakage by a factor of about 10, but it increases switching time by about 15 percent.\n\nthus, lowleakage retention flops are only useful in saving state energy efficiently—their use on the processor’s critical path would slow it down.\n\n\n# controlling memory leakage\n\nin fact, leakage is projected to account for 70 percent of the cache power budget in 70-nm technology.\n\nboth bitline and cell leakage result from subthreshold conduction—current flowing from the source to drain even when gate-source voltage is below the threshold voltage.\n\n# circuit techniques.\n\n * state-destructive\n\n * state-preserving techniques vary. drowsy caches multiplex supply voltages according to the state of each cache line or block. waking up the drowsy cache lines is treated as a pseudo cache miss and incurs one additional cycle overhead.\n   \n   moreover, while state-preserving techniques can only reduce leakage by about a factor of 10, compared to more than a factor of 1,000 for destructive techniques, the net difference in power consumed by the two techniques is less than 10 percent.\n\n# control techniques.\n\n * application-sensitive controls, based on runtime performance feedback,9,13 and\n * application-insensitive controls, which periodically turn off cache lines.\n\n# compiler techniques\n\n * using compiler directives might make it possible to keep some loops within bank boundaries.\n * the compiler can also provide application-sensitive leakage control.\n\n\n# technology trends and challenges\n\none approach to reducing subthreshold leakage is to actively refrigerate the chip.\n\nwhile this option seems promising for controlling the subthreshold leakage point, it does not address gateoxide leakage.\n\ndesigners assign a low threshold voltage to a few performance-critical transistors and a high threshold voltage to the majority of less timing-critical transistors.\n\nthis approach incurs a high subthreshold leakage current for the performance-critical transistors, but it can significantly reduce the overall leakage.\n\nfuture technologies are likely to offer three threshold voltages—low, high, and extra high—or even more.\n\nthis opens the way to new leakage optimizations within different portions of a cache or at different levels of its hierarchy.\n\nthe address-decoder and bus-driver circuits in a cache consume a significant portion of total access time, so a designer could construct them from high-vth transistors, while constructing the more numerous bit cells from extra-high-vth devices and reserving low-vth devices for speed-critical parts of the processor core.\n\n\n# multiple threshold voltages\n\n\n# gate length\n\n\n# oxide tunneling",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"template",frontmatter:{title:"template",date:"2020-04-05T10:38:46.000Z",permalink:"/pages/ee5bf2/",tags:[null]},regularPath:"/09.nine/02.template.html",relativePath:"09.nine/02.template.md",key:"v-bb1ed876",path:"/pages/ee5bf2/",headers:[{level:2,title:"Contents",slug:"contents",normalizedTitle:"contents",charIndex:28},{level:2,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:42},{level:2,title:"List of Papers",slug:"list-of-papers",normalizedTitle:"list of papers",charIndex:59},{level:2,title:"Paper Reviews",slug:"paper-reviews",normalizedTitle:"paper reviews",charIndex:5},{level:3,title:"Paper 1: Title of Paper 1",slug:"paper-1-title-of-paper-1",normalizedTitle:"paper 1: title of paper 1",charIndex:98},{level:3,title:"Paper 2: Title of Paper 2",slug:"paper-2-title-of-paper-2",normalizedTitle:"paper 2: title of paper 2",charIndex:130},{level:3,title:"Paper 3: Title of Paper 3",slug:"paper-3-title-of-paper-3",normalizedTitle:"paper 3: title of paper 3",charIndex:162}],headersStr:"Contents Introduction List of Papers Paper Reviews Paper 1: Title of Paper 1 Paper 2: Title of Paper 2 Paper 3: Title of Paper 3",content:"# 📚 Paper Reviews Blog\n\n\n# Contents\n\n 1. Introduction\n 2. List of Papers\n 3. Paper Reviews\n    * Paper 1: Title of Paper 1\n    * Paper 2: Title of Paper 2\n    * Paper 3: Title of Paper 3\n\n----------------------------------------\n\n\n# Introduction\n\nWelcome to my paper reviews blog! Here, I delve into various academic papers and provide insights, critiques, and summaries. Each paper offers a unique perspective on its subject matter, contributing to the ever-evolving landscape of research. Let's explore the latest findings and discussions in the academic world.\n\n\n# List of Papers\n\n * Title of Paper 1\n * Title of Paper 2\n * Title of Paper 3\n\n----------------------------------------\n\n\n# Paper Reviews\n\n\n# Paper 1: Title of Paper 1\n\n# Authors: Author Names\n\nAbstract: Brief summary of the paper's abstract.\n\nKeywords: Keywords, Keywords, Keywords\n\nIntroduction: Provide a summary of the introduction section.\n\nMethodology: Discuss the methodology used in the research.\n\nResults: Highlight the key findings of the study.\n\nDiscussion: Offer your thoughts and critiques on the paper.\n\n----------------------------------------\n\n\n# Paper 2: Title of Paper 2\n\n# Authors: Author Names\n\nAbstract: Brief summary of the paper's abstract.\n\nKeywords: Keywords, Keywords, Keywords\n\nIntroduction: Provide a summary of the introduction section.\n\nMethodology: Discuss the methodology used in the research.\n\nResults: Highlight the key findings of the study.\n\nDiscussion: Offer your thoughts and critiques on the paper.\n\n----------------------------------------\n\n\n# Paper 3: Title of Paper 3\n\n# Authors: Author Names\n\nAbstract: Brief summary of the paper's abstract.\n\nKeywords: Keywords, Keywords, Keywords\n\nIntroduction: Provide a summary of the introduction section.\n\nMethodology: Discuss the methodology used in the research.\n\nResults: Highlight the key findings of the study.\n\nDiscussion: Offer your thoughts and critiques on the paper.\n\n----------------------------------------",normalizedContent:"# 📚 paper reviews blog\n\n\n# contents\n\n 1. introduction\n 2. list of papers\n 3. paper reviews\n    * paper 1: title of paper 1\n    * paper 2: title of paper 2\n    * paper 3: title of paper 3\n\n----------------------------------------\n\n\n# introduction\n\nwelcome to my paper reviews blog! here, i delve into various academic papers and provide insights, critiques, and summaries. each paper offers a unique perspective on its subject matter, contributing to the ever-evolving landscape of research. let's explore the latest findings and discussions in the academic world.\n\n\n# list of papers\n\n * title of paper 1\n * title of paper 2\n * title of paper 3\n\n----------------------------------------\n\n\n# paper reviews\n\n\n# paper 1: title of paper 1\n\n# authors: author names\n\nabstract: brief summary of the paper's abstract.\n\nkeywords: keywords, keywords, keywords\n\nintroduction: provide a summary of the introduction section.\n\nmethodology: discuss the methodology used in the research.\n\nresults: highlight the key findings of the study.\n\ndiscussion: offer your thoughts and critiques on the paper.\n\n----------------------------------------\n\n\n# paper 2: title of paper 2\n\n# authors: author names\n\nabstract: brief summary of the paper's abstract.\n\nkeywords: keywords, keywords, keywords\n\nintroduction: provide a summary of the introduction section.\n\nmethodology: discuss the methodology used in the research.\n\nresults: highlight the key findings of the study.\n\ndiscussion: offer your thoughts and critiques on the paper.\n\n----------------------------------------\n\n\n# paper 3: title of paper 3\n\n# authors: author names\n\nabstract: brief summary of the paper's abstract.\n\nkeywords: keywords, keywords, keywords\n\nintroduction: provide a summary of the introduction section.\n\nmethodology: discuss the methodology used in the research.\n\nresults: highlight the key findings of the study.\n\ndiscussion: offer your thoughts and critiques on the paper.\n\n----------------------------------------",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Blogs to be read",frontmatter:{title:"Blogs to be read",date:"2024-12-26T15:32:49.000Z",permalink:"/pages/f00001/",tags:[null]},regularPath:"/10.mix/02.cuda_blogs.html",relativePath:"10.mix/02.cuda_blogs.md",key:"v-4a87ebca",path:"/pages/f00001/",headers:[{level:2,title:"Pytorch Code Analysis",slug:"pytorch-code-analysis",normalizedTitle:"pytorch code analysis",charIndex:20},{level:2,title:"CUDA Optimization",slug:"cuda-optimization",normalizedTitle:"cuda optimization",charIndex:855},{level:3,title:"CUDA Warp-level Primitives",slug:"cuda-warp-level-primitives",normalizedTitle:"cuda warp-level primitives",charIndex:877},{level:3,title:"CUDA Kernel Optimization",slug:"cuda-kernel-optimization",normalizedTitle:"cuda kernel optimization",charIndex:1091},{level:3,title:"Optimization Papers",slug:"optimization-papers",normalizedTitle:"optimization papers",charIndex:1964},{level:3,title:"Softmax",slug:"softmax",normalizedTitle:"softmax",charIndex:2122},{level:2,title:"Papers to be read",slug:"papers-to-be-read",normalizedTitle:"papers to be read",charIndex:2434},{level:2,title:"LLM Related",slug:"llm-related",normalizedTitle:"llm related",charIndex:3272}],headersStr:"Pytorch Code Analysis CUDA Optimization CUDA Warp-level Primitives CUDA Kernel Optimization Optimization Papers Softmax Papers to be read LLM Related",content:"# Blogs watcher\n\n\n# Pytorch Code Analysis\n\n👍 👍 👍 👍\n\n 1. https://hurray0.com/menu/151/\n\n 2. https://hurray0.com/menu/152/\n\n 3. https://www.cnblogs.com/int-me-X/category/2371391.html\n\n 4. https://cloud.tencent.com/developer/article/2346580\n\n 5. https://github.com/search?q=repo%3Akeithyin%2Fread-pytorch-source-code%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0&type=code\n\n 6. https://mlgdg.github.io/2019/12/05/Pytorch%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/\n\n👍 👍 👍 👍 https://www.52coding.com.cn/2019/05/05/PyTorch0/\n\n👍 👍 👍 👍 Paper: SURVEY AND EVALUATION OF CONVERGING ARCHITECTURE IN LLMS BASED ON FOOTSTEPS OF OPERATIONS\n\n👍 👍 👍 👍 PyTorch – Internal Architecture Tour\n\n👍 👍 👍 👍 PyData Montreal slides for the talk: PyTorch under the hood\n\n👍 👍 👍 👍 PyTorch 2 Internals – Talk\n\nllama.cpp source code analysis\n\nllama.cpp source code analysis\n\n\n# CUDA Optimization\n\n\n# CUDA Warp-level Primitives\n\nhttps://developer.nvidia.com/blog/using-cuda-warp-level-primitives/\n\nhttps://developer.nvidia.com/blog/cooperative-groups/\n\nhttps://blog.csdn.net/kunhe0512/article/details/125492263\n\n\n# CUDA Kernel Optimization\n\n# GEMM\n\n👍 👍 👍 👍 How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog\n\nProgramming tensor cores using nvcuda-wmma\n\nhttps://github.com/hitqshao/NVIDIA_SGEMM_PRACTICE\n\nhttps://netfiles.pw/cuda-matrix-multiplication-performance-optimization-guide/\n\nhttps://leimao.github.io/blog/NVIDIA-Tensor-Core-Programming/\n\nhttps://leimao.github.io/blog/Row-Major-VS-Column-Major/\n\nNVIDIA-developer-blog code-samples\n\n[KAUST] https://vccvisualization.org/teaching/CS380/CS380_fall2021_lecture_26.pdf\n\nhttps://0mean1sigma.com/\n\n * Step2 Global Memory Calescing\n * Step3 GPU Shared Memory\n * Step4 1D Thread Coarsening using GPU Registers\n * Step5 2D Thread Coarsening using GPU Registers\n * Step6 Vectorized Memory Accesses\n\n# Kernel Optimization\n\nPart V - 1D Convolution in CUDA (Optimized)\n\nPart II - CUDA Kernel Optimization Tips\n\n\n# Optimization Papers\n\nThis paper mention a bit about float4 memory accessing performance. CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization\n\n\n# Softmax\n\nDemo of diy softmax and import in pytorch https://github.com/fattorib/CudaSoftmax\n\n👍 👍 👍 👍 ICS Paper on using tensor core to accelerate Reduction and Scan. https://arxiv.org/pdf/1811.09736\n\nVolta Tensor Core： https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Tensor-Cores.pdf\n\n\n# Papers to be read\n\n * Optimization Principles and Application Performance Evaluation of a Multithreaded GPU Using CUDA\n * Performance Study of GPU applications using SYCL and CUDA on Tesla V100 GPU\n * A performance prediction model for the CUDA GPGPU platform\n * 3.5-D Blocking Optimization for Stencil Computations on Modern CPUs and GPUs\n * Benchmarking Optimization Algorithms for Auto-Tuning GPU Kernels\n * [316] Auto-tuning a high-level language targeted to GPU codes\n * [91] Kernel Tuner: A search-optimizing GPU code auto-tuner\n * Meta-programming and auto-tuning in the search for high performance GPU code\n * [79] Autotuning in High-Performance Computing Applications\n * [105] Optimizing CUDA code by kernel fusion: application on BLAS\n * [24] A review of CUDA optimization techniques and tools for structured grid computing\n\n\n# LLM Related\n\n * Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n\n * LLMs相关知识及面试题\n\n * Gating Dropout: Communication-efficient Regularization for Sparsely Activated Transformers This paper discuss about token routing to expert in another node. All-to-all communication.\n\n * PipeDream: inter-batch pipeline parallelism\n\n * BPIPE: Memory-Balanced Pipeline Parallelism for Training Large Language Models\n\n * SCALING FP8 TRAINING TO TRILLION-TOKEN LLMS\n\n * FP8-LM: Training FP8 Large Language Models",normalizedContent:"# blogs watcher\n\n\n# pytorch code analysis\n\n👍 👍 👍 👍\n\n 1. https://hurray0.com/menu/151/\n\n 2. https://hurray0.com/menu/152/\n\n 3. https://www.cnblogs.com/int-me-x/category/2371391.html\n\n 4. https://cloud.tencent.com/developer/article/2346580\n\n 5. https://github.com/search?q=repo%3akeithyin%2fread-pytorch-source-code%20%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0&type=code\n\n 6. https://mlgdg.github.io/2019/12/05/pytorch%e6%ba%90%e7%a0%81%e5%ad%a6%e4%b9%a0/\n\n👍 👍 👍 👍 https://www.52coding.com.cn/2019/05/05/pytorch0/\n\n👍 👍 👍 👍 paper: survey and evaluation of converging architecture in llms based on footsteps of operations\n\n👍 👍 👍 👍 pytorch – internal architecture tour\n\n👍 👍 👍 👍 pydata montreal slides for the talk: pytorch under the hood\n\n👍 👍 👍 👍 pytorch 2 internals – talk\n\nllama.cpp source code analysis\n\nllama.cpp source code analysis\n\n\n# cuda optimization\n\n\n# cuda warp-level primitives\n\nhttps://developer.nvidia.com/blog/using-cuda-warp-level-primitives/\n\nhttps://developer.nvidia.com/blog/cooperative-groups/\n\nhttps://blog.csdn.net/kunhe0512/article/details/125492263\n\n\n# cuda kernel optimization\n\n# gemm\n\n👍 👍 👍 👍 how to optimize a cuda matmul kernel for cublas-like performance: a worklog\n\nprogramming tensor cores using nvcuda-wmma\n\nhttps://github.com/hitqshao/nvidia_sgemm_practice\n\nhttps://netfiles.pw/cuda-matrix-multiplication-performance-optimization-guide/\n\nhttps://leimao.github.io/blog/nvidia-tensor-core-programming/\n\nhttps://leimao.github.io/blog/row-major-vs-column-major/\n\nnvidia-developer-blog code-samples\n\n[kaust] https://vccvisualization.org/teaching/cs380/cs380_fall2021_lecture_26.pdf\n\nhttps://0mean1sigma.com/\n\n * step2 global memory calescing\n * step3 gpu shared memory\n * step4 1d thread coarsening using gpu registers\n * step5 2d thread coarsening using gpu registers\n * step6 vectorized memory accesses\n\n# kernel optimization\n\npart v - 1d convolution in cuda (optimized)\n\npart ii - cuda kernel optimization tips\n\n\n# optimization papers\n\nthis paper mention a bit about float4 memory accessing performance. cudadma: optimizing gpu memory bandwidth via warp specialization\n\n\n# softmax\n\ndemo of diy softmax and import in pytorch https://github.com/fattorib/cudasoftmax\n\n👍 👍 👍 👍 ics paper on using tensor core to accelerate reduction and scan. https://arxiv.org/pdf/1811.09736\n\nvolta tensor core： https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_tensor-cores.pdf\n\n\n# papers to be read\n\n * optimization principles and application performance evaluation of a multithreaded gpu using cuda\n * performance study of gpu applications using sycl and cuda on tesla v100 gpu\n * a performance prediction model for the cuda gpgpu platform\n * 3.5-d blocking optimization for stencil computations on modern cpus and gpus\n * benchmarking optimization algorithms for auto-tuning gpu kernels\n * [316] auto-tuning a high-level language targeted to gpu codes\n * [91] kernel tuner: a search-optimizing gpu code auto-tuner\n * meta-programming and auto-tuning in the search for high performance gpu code\n * [79] autotuning in high-performance computing applications\n * [105] optimizing cuda code by kernel fusion: application on blas\n * [24] a review of cuda optimization techniques and tools for structured grid computing\n\n\n# llm related\n\n * switch transformers: scaling to trillion parameter models with simple and efficient sparsity\n\n * llms相关知识及面试题\n\n * gating dropout: communication-efficient regularization for sparsely activated transformers this paper discuss about token routing to expert in another node. all-to-all communication.\n\n * pipedream: inter-batch pipeline parallelism\n\n * bpipe: memory-balanced pipeline parallelism for training large language models\n\n * scaling fp8 training to trillion-token llms\n\n * fp8-lm: training fp8 large language models",charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"留言板",frontmatter:{title:"留言板",date:"2022-07-12T10:38:46.000Z",permalink:"/message-board",tags:[null]},regularPath:"/09.nine/01.%E7%95%99%E8%A8%80%E6%9D%BF.html",relativePath:"09.nine/01.留言板.md",key:"v-2e715932",path:"/message-board/",headersStr:null,content:"你可以在这里留下想说的内容。",normalizedContent:"你可以在这里留下想说的内容。",charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"Understanding Pytorch Source Code",frontmatter:{title:"Understanding Pytorch Source Code",date:"2024-10-15T15:32:49.000Z",permalink:"/pages/f00002/",tags:[null]},regularPath:"/10.mix/03.learn_pytorch_source.html",relativePath:"10.mix/03.learn_pytorch_source.md",key:"v-e225a34a",path:"/pages/f00002/",headers:[{level:2,title:"./aten/src/ATen/core/dispatch",slug:"aten-src-aten-core-dispatch",normalizedTitle:"./aten/src/aten/core/dispatch",charIndex:1592},{level:2,title:"core",slug:"core",normalizedTitle:"core",charIndex:59},{level:2,title:"csrc",slug:"csrc",normalizedTitle:"csrc",charIndex:6048},{level:3,title:"autograd",slug:"autograd",normalizedTitle:"autograd",charIndex:6057}],headersStr:"./aten/src/ATen/core/dispatch core csrc autograd",content:"# aten\n\nPurpose ATen (short for \"A Tensor Library\") is the core tensor library in PyTorch. It provides the following:\n\n * Tensor Abstraction: Defines the Tensor class and provides fundamental tensor operations.\n * Backend Management: Handles the execution of operations on various backends (e.g., CPU, CUDA).\n * Dispatcher Integration: Works with the dispatcher to route tensor operations to the correct implementation.\n\nKey Roles\n\n * Tensor Definition:\n   \n   * Provides the Tensor class, which is the foundational data structure in PyTorch.\n   * Source: aten/src/ATen/Tensor.h.\n\n * Tensor Operations:\n   \n   * Implements a wide variety of tensor operations, such as matrix multiplication, element-wise addition, and reductions.\n   * Operations are defined in:\n     * aten/src/ATen/native/ (backend-specific implementations like cpu, cuda, or quantized).\n     * Example: aten/src/ATen/native/LinearAlgebra.cpp (matrix-related ops).\n\n * Dispatcher Entry Points:\n   \n   * Provides entry points for the PyTorch dispatcher system, which routes tensor operations to their specific implementations.\n   * Function signatures for tensor operations are declared in aten/src/ATen/Functions.h and implemented in backend-specific files.\n\n * Backend Management:\n   \n   * Supports multiple device backends, such as CPU, CUDA, ROCm, XLA, and others.\n   * Backend-specific code is implemented in native directories (e.g., aten/src/ATen/native/cuda/ for CUDA).\n\n * Default Fallbacks:\n   \n   * Provides fallback implementations for some operations when a specific backend implementation is not available.\n\n\n# ./aten/src/ATen/core/dispatch\n\nDispatcher\n\nCode\n\n  /**\n   * Register a new operator schema.\n   *\n   * If a schema with the same operator name and overload name already exists,\n   * this function will check that both schemas are exactly identical.\n   */\n  RegistrationHandleRAII registerDef(FunctionSchema schema, std::string debug, std::vector<at::Tag> tags = {});\n\n  /**\n   * Register a kernel to the dispatch table for an operator.\n   * If dispatch_key is nullopt, then this registers a fallback kernel.\n   *\n   * @return A RAII object that manages the lifetime of the registration.\n   *         Once that object is destructed, the kernel will be deregistered.\n   */\n  // NB: steals the inferred function schema, as we may need to hold on to\n  // it for a bit until the real schema turns up\n  RegistrationHandleRAII registerImpl(OperatorName op_name, c10::optional<DispatchKey> dispatch_key, KernelFunction kernel, c10::optional<impl::CppSignature> cpp_signature, std::unique_ptr<FunctionSchema> inferred_function_schema, std::string debug);\n\n  /**\n   * Register a new operator by name.\n   */\n  RegistrationHandleRAII registerName(OperatorName op_name);\n\n  /**\n   * Register a fallback kernel for a backend.\n   * If an operator is called but there is no concrete kernel for the dispatch\n   * key of the given operator arguments, it will check if there is such a\n   * fallback kernel for the given dispatch key and, if yes, call that one.\n   */\n  RegistrationHandleRAII registerFallback(DispatchKey dispatch_key, KernelFunction kernel, std::string debug);\n\n  /**\n   * Use to register whenever we had a TORCH_LIBRARY declaration in the frontend\n   * API.  These invocations are only permitted once per program, so we raise\n   * an error if this is called again for the same namespace.\n   */\n  RegistrationHandleRAII registerLibrary(std::string ns, std::string debug);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n\n# C10\n\nPurpose C10 (short for \"Caffe 2.0\") is a lower-level, foundational library within PyTorch. It provides the following:\n\n * Device Abstraction: Defines device and tensor metadata.\n * Dispatcher System: Implements the core dispatch mechanism for routing tensor operations.\n * Type Management: Handles scalar types, tensor data types, and other utilities.\n * Utilities: Provides logging, threading, and memory management.\n\nKey Roles\n\n * Dispatcher:\n   \n   * C10's dispatcher is the routing mechanism for PyTorch tensor operations.\n   * Routes function calls (like torch.add) to backend-specific implementations.\n   * Files:\n     * c10/Dispatcher.h: Core dispatcher implementation.\n     * c10/core/KernelFunction.h: Manages kernel function registration.\n\n * Device Abstraction:\n   \n   * Provides c10::Device and c10::DeviceType to manage devices like CPU, CUDA, XLA, etc.\n   * Files:\n     * c10/core/Device.h: Device abstraction.\n     * c10/core/DeviceType.h: Enumerates device types.\n\n * Type Management:\n   \n   * Handles scalar and tensor data types.\n   * Provides abstractions for c10::ScalarType (e.g., float, int, bool).\n   * Files:\n     * c10/core/ScalarType.h: Scalar type definitions.\n     * c10/core/TensorTypeId.h: Tensor type identifiers.\n\n * Memory Management:\n   \n   * Abstracts memory allocation across devices.\n   * Supports custom allocators for tensors (e.g., caching allocators for CUDA).\n   * Files:\n     * c10/core/Allocator.h: Base allocator interface.\n     * c10/cuda/CUDACachingAllocator.cpp: CUDA memory management.\n\n * Threading:\n   \n   * Provides utilities for multi-threaded execution.\n   * Files:\n     * c10/util/ThreadPool.h: Thread pool implementation.\n     * c10/util/ThreadLocal.h: Thread-local storage utilities.\n\n * Utilities:\n   \n   * Provides logging, error handling, and common utilities.\n   * Files:\n     * c10/util/Logging.h: Logging macros.\n     * c10/util/Exception.h: Exception handling utilities.\n\n\n# core\n\nGeneratorImpl.h\n\nRandom Number Generator\n\n> A Pseudo Random Number Generator (PRNG) is an engine that uses an algorithm to generate a seemingly random sequence of numbers, that may be later be used in creating a random distribution.\n> Such an engine almost always maintains a state and requires a seed to start off the creation of random numbers.\n> Often times, users have found it beneficial to be able to explicitly create, retain, and destroy PRNG states and also be able to have control over the seed value.\n\n\n# torch\n\n\n# csrc\n\n\n# autograd\n\nvariable.h\n\nVariable\n\nA Variable augments a Tensor with the ability to interact in our autograd machinery.\nConceptually, Variables travel along Edges between Nodes in the autograd graph.\nA Variable can either be a leaf, like a weight in a neural network, or an interior variable, when it is the result of an operation between variables. Every Variable also stores another Variable called its grad (gradient).\nIf the variable is a leaf, its gradient will be accumulated into this variable.\n\nEvery Tensor is a Variable, but sometimes we colloquially refer to Variables that don't require gradients as Tensors (since none of the autograd machinery for Variables applies).\nHistorically, Variables and Tensors were separate concepts, but now they are exactly the same (i.e. we have using Variable = at::Tensor`).\n\nGradient Edges\n\nFurthermore, Variables have the notion of a gradient_edge, which is the edge in the autograd graph that connects the variable to a particular input of the gradient function that will be invoked with the variable during the backward pass.\n\nMore precisely, this gradient function can be one of two things:\n\n * 1. A grad_fn, if the variable is in the interior of the graph. This is the gradient of the function that produced the variable.\n * 2. A grad_accumulator, if the variable is a leaf, which accumulates a scalar gradient value into its grad variable.\n\nVersioning\n\nAnother major feature of Variables are versions.\nVersions are incremented when an in-place mutation of a variable occurs.\nVersions are useful when constructing SavedVariables, which take a snapshot of a Variable at a certain version.\nYou can retrieve a Variable's version through its current_version() method.\n\nViews\n\nIt is possible for a Variable to be a view of another Variable, in which case it tracks that Variable's data and autograd history. Beyond construction, the interface of a view is identical to that of a regular Variable.\nYou can determine whether Variable is in fact a view by probing its is_view() method.\nNote that the view semantics are only meaningful for Variable relations that are relevant to autograd.",normalizedContent:"# aten\n\npurpose aten (short for \"a tensor library\") is the core tensor library in pytorch. it provides the following:\n\n * tensor abstraction: defines the tensor class and provides fundamental tensor operations.\n * backend management: handles the execution of operations on various backends (e.g., cpu, cuda).\n * dispatcher integration: works with the dispatcher to route tensor operations to the correct implementation.\n\nkey roles\n\n * tensor definition:\n   \n   * provides the tensor class, which is the foundational data structure in pytorch.\n   * source: aten/src/aten/tensor.h.\n\n * tensor operations:\n   \n   * implements a wide variety of tensor operations, such as matrix multiplication, element-wise addition, and reductions.\n   * operations are defined in:\n     * aten/src/aten/native/ (backend-specific implementations like cpu, cuda, or quantized).\n     * example: aten/src/aten/native/linearalgebra.cpp (matrix-related ops).\n\n * dispatcher entry points:\n   \n   * provides entry points for the pytorch dispatcher system, which routes tensor operations to their specific implementations.\n   * function signatures for tensor operations are declared in aten/src/aten/functions.h and implemented in backend-specific files.\n\n * backend management:\n   \n   * supports multiple device backends, such as cpu, cuda, rocm, xla, and others.\n   * backend-specific code is implemented in native directories (e.g., aten/src/aten/native/cuda/ for cuda).\n\n * default fallbacks:\n   \n   * provides fallback implementations for some operations when a specific backend implementation is not available.\n\n\n# ./aten/src/aten/core/dispatch\n\ndispatcher\n\ncode\n\n  /**\n   * register a new operator schema.\n   *\n   * if a schema with the same operator name and overload name already exists,\n   * this function will check that both schemas are exactly identical.\n   */\n  registrationhandleraii registerdef(functionschema schema, std::string debug, std::vector<at::tag> tags = {});\n\n  /**\n   * register a kernel to the dispatch table for an operator.\n   * if dispatch_key is nullopt, then this registers a fallback kernel.\n   *\n   * @return a raii object that manages the lifetime of the registration.\n   *         once that object is destructed, the kernel will be deregistered.\n   */\n  // nb: steals the inferred function schema, as we may need to hold on to\n  // it for a bit until the real schema turns up\n  registrationhandleraii registerimpl(operatorname op_name, c10::optional<dispatchkey> dispatch_key, kernelfunction kernel, c10::optional<impl::cppsignature> cpp_signature, std::unique_ptr<functionschema> inferred_function_schema, std::string debug);\n\n  /**\n   * register a new operator by name.\n   */\n  registrationhandleraii registername(operatorname op_name);\n\n  /**\n   * register a fallback kernel for a backend.\n   * if an operator is called but there is no concrete kernel for the dispatch\n   * key of the given operator arguments, it will check if there is such a\n   * fallback kernel for the given dispatch key and, if yes, call that one.\n   */\n  registrationhandleraii registerfallback(dispatchkey dispatch_key, kernelfunction kernel, std::string debug);\n\n  /**\n   * use to register whenever we had a torch_library declaration in the frontend\n   * api.  these invocations are only permitted once per program, so we raise\n   * an error if this is called again for the same namespace.\n   */\n  registrationhandleraii registerlibrary(std::string ns, std::string debug);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n\n# c10\n\npurpose c10 (short for \"caffe 2.0\") is a lower-level, foundational library within pytorch. it provides the following:\n\n * device abstraction: defines device and tensor metadata.\n * dispatcher system: implements the core dispatch mechanism for routing tensor operations.\n * type management: handles scalar types, tensor data types, and other utilities.\n * utilities: provides logging, threading, and memory management.\n\nkey roles\n\n * dispatcher:\n   \n   * c10's dispatcher is the routing mechanism for pytorch tensor operations.\n   * routes function calls (like torch.add) to backend-specific implementations.\n   * files:\n     * c10/dispatcher.h: core dispatcher implementation.\n     * c10/core/kernelfunction.h: manages kernel function registration.\n\n * device abstraction:\n   \n   * provides c10::device and c10::devicetype to manage devices like cpu, cuda, xla, etc.\n   * files:\n     * c10/core/device.h: device abstraction.\n     * c10/core/devicetype.h: enumerates device types.\n\n * type management:\n   \n   * handles scalar and tensor data types.\n   * provides abstractions for c10::scalartype (e.g., float, int, bool).\n   * files:\n     * c10/core/scalartype.h: scalar type definitions.\n     * c10/core/tensortypeid.h: tensor type identifiers.\n\n * memory management:\n   \n   * abstracts memory allocation across devices.\n   * supports custom allocators for tensors (e.g., caching allocators for cuda).\n   * files:\n     * c10/core/allocator.h: base allocator interface.\n     * c10/cuda/cudacachingallocator.cpp: cuda memory management.\n\n * threading:\n   \n   * provides utilities for multi-threaded execution.\n   * files:\n     * c10/util/threadpool.h: thread pool implementation.\n     * c10/util/threadlocal.h: thread-local storage utilities.\n\n * utilities:\n   \n   * provides logging, error handling, and common utilities.\n   * files:\n     * c10/util/logging.h: logging macros.\n     * c10/util/exception.h: exception handling utilities.\n\n\n# core\n\ngeneratorimpl.h\n\nrandom number generator\n\n> a pseudo random number generator (prng) is an engine that uses an algorithm to generate a seemingly random sequence of numbers, that may be later be used in creating a random distribution.\n> such an engine almost always maintains a state and requires a seed to start off the creation of random numbers.\n> often times, users have found it beneficial to be able to explicitly create, retain, and destroy prng states and also be able to have control over the seed value.\n\n\n# torch\n\n\n# csrc\n\n\n# autograd\n\nvariable.h\n\nvariable\n\na variable augments a tensor with the ability to interact in our autograd machinery.\nconceptually, variables travel along edges between nodes in the autograd graph.\na variable can either be a leaf, like a weight in a neural network, or an interior variable, when it is the result of an operation between variables. every variable also stores another variable called its grad (gradient).\nif the variable is a leaf, its gradient will be accumulated into this variable.\n\nevery tensor is a variable, but sometimes we colloquially refer to variables that don't require gradients as tensors (since none of the autograd machinery for variables applies).\nhistorically, variables and tensors were separate concepts, but now they are exactly the same (i.e. we have using variable = at::tensor`).\n\ngradient edges\n\nfurthermore, variables have the notion of a gradient_edge, which is the edge in the autograd graph that connects the variable to a particular input of the gradient function that will be invoked with the variable during the backward pass.\n\nmore precisely, this gradient function can be one of two things:\n\n * 1. a grad_fn, if the variable is in the interior of the graph. this is the gradient of the function that produced the variable.\n * 2. a grad_accumulator, if the variable is a leaf, which accumulates a scalar gradient value into its grad variable.\n\nversioning\n\nanother major feature of variables are versions.\nversions are incremented when an in-place mutation of a variable occurs.\nversions are useful when constructing savedvariables, which take a snapshot of a variable at a certain version.\nyou can retrieve a variable's version through its current_version() method.\n\nviews\n\nit is possible for a variable to be a view of another variable, in which case it tracks that variable's data and autograd history. beyond construction, the interface of a view is identical to that of a regular variable.\nyou can determine whether variable is in fact a view by probing its is_view() method.\nnote that the view semantics are only meaningful for variable relations that are relevant to autograd.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"CUDA Merge",frontmatter:{title:"CUDA Merge",date:"2024-12-27T15:32:49.000Z",permalink:"/pages/f00004/",tags:[null]},regularPath:"/10.mix/05.cuda_merge.html",relativePath:"10.mix/05.cuda_merge.md",key:"v-7b8d66da",path:"/pages/f00004/",headers:[{level:2,title:"CUDA Merge",slug:"cuda-merge",normalizedTitle:"cuda merge",charIndex:2}],headersStr:"CUDA Merge",content:"# CUDA Merge\n\nDifficulty: 👍 👍 👍 👍 👍\n\nThe merge algorithm is much more complex than matrix multiplication and sorting, maybe even more complex than prefix-sum reduction.\n\nLets illustrate how this works.\n\nThe problem:\n\ngiven ordered array A and ordered array B, merge the array.\n\n\n\nThis algorthm is output-centric.\n\n\n\nEach thread-block is responsible for merging a tile of C.\n\nEach thread is responsible for merging several consective elements of C.\n\nSince we want to use parallel algorthm to solve this problem, we want to divide and solve the C.\n\nIf given any position C, we could find its startpoint of A and startpoint of B, when we could just assign each thread with position, by some magic algorithm like 😾:\n\n * thread0, C Pos 0, finds where to start in A and B by thread0 itself\n * thread1, C Pos 8, finds where to start in A and B by thread1 itself\n * thread2, C Pos 12, finds where to start in A and B by thread2 itself\n * thread3, C Pos 16, finds where to start in A and B by thread3 itself\n\nThen we could solve the question.\n\nAnother idea deserved to be mention is that since A and B are consective, we could load them into share memory, so threads could use memory coalescing to access them.\n\nThose above two idea are the core idea of merge.\n\nThen, the only left question is how to implement the algorithm. 😾\n\nIt follows the basic idea that:\n\n 1. if we have two ordered array a[i-1] a[i]\n\nb[j-1] b[j]\n\nthe start point we want to find for A and B must follows:\n\na[i-1] <= b[j]\n\nb[j-1] < a[i]\n\nThus if we know the start point of A and B, we could just use binary search to gradually get the start of A and B\n\n 2. \n\nAnother rule is if we want to calculate for C Pos X, elements before C contains number of I elements in A and number of j elements in B.\n\nThis helps to understand.\n\nIf each thread is assigned by 4 elements and for thread 2, it want to know where to start from A and B.\n\nThe expected position is 5th in A and 3th in B.\n\nIt follows the above two rules.\n\n",normalizedContent:"# cuda merge\n\ndifficulty: 👍 👍 👍 👍 👍\n\nthe merge algorithm is much more complex than matrix multiplication and sorting, maybe even more complex than prefix-sum reduction.\n\nlets illustrate how this works.\n\nthe problem:\n\ngiven ordered array a and ordered array b, merge the array.\n\n\n\nthis algorthm is output-centric.\n\n\n\neach thread-block is responsible for merging a tile of c.\n\neach thread is responsible for merging several consective elements of c.\n\nsince we want to use parallel algorthm to solve this problem, we want to divide and solve the c.\n\nif given any position c, we could find its startpoint of a and startpoint of b, when we could just assign each thread with position, by some magic algorithm like 😾:\n\n * thread0, c pos 0, finds where to start in a and b by thread0 itself\n * thread1, c pos 8, finds where to start in a and b by thread1 itself\n * thread2, c pos 12, finds where to start in a and b by thread2 itself\n * thread3, c pos 16, finds where to start in a and b by thread3 itself\n\nthen we could solve the question.\n\nanother idea deserved to be mention is that since a and b are consective, we could load them into share memory, so threads could use memory coalescing to access them.\n\nthose above two idea are the core idea of merge.\n\nthen, the only left question is how to implement the algorithm. 😾\n\nit follows the basic idea that:\n\n 1. if we have two ordered array a[i-1] a[i]\n\nb[j-1] b[j]\n\nthe start point we want to find for a and b must follows:\n\na[i-1] <= b[j]\n\nb[j-1] < a[i]\n\nthus if we know the start point of a and b, we could just use binary search to gradually get the start of a and b\n\n 2. \n\nanother rule is if we want to calculate for c pos x, elements before c contains number of i elements in a and number of j elements in b.\n\nthis helps to understand.\n\nif each thread is assigned by 4 elements and for thread 2, it want to know where to start from a and b.\n\nthe expected position is 5th in a and 3th in b.\n\nit follows the above two rules.\n\n",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Understanding Pytorch Source Code AOT - Inductor IR - Codegen",frontmatter:{title:"Understanding Pytorch Source Code AOT - Inductor IR - Codegen",date:"2024-12-15T15:32:49.000Z",permalink:"/pages/f00003/",tags:[null]},regularPath:"/10.mix/04.learn_pytorch_source_aot.html",relativePath:"10.mix/04.learn_pytorch_source_aot.md",key:"v-ac6a65e2",path:"/pages/f00003/",headers:[{level:2,title:"aot dispatch create joint graph",slug:"aot-dispatch-create-joint-graph",normalizedTitle:"aot dispatch create joint graph",charIndex:2},{level:3,title:"aotdispatchautograd_graph",slug:"aot-dispatch-autograd-graph",normalizedTitle:"aotdispatchautograd_graph",charIndex:null},{level:2,title:"TorchInductor",slug:"torchinductor",normalizedTitle:"torchinductor",charIndex:8081},{level:3,title:"recursicvpostgradpasses()",slug:"recursicv-post-grad-passes",normalizedTitle:"recursicvpostgradpasses()",charIndex:null},{level:3,title:"GraphLowering",slug:"graphlowering",normalizedTitle:"graphlowering",charIndex:9270},{level:3,title:"GraphLowering.compiletofn()",slug:"graphlowering-compile-to-fn",normalizedTitle:"graphlowering.compiletofn()",charIndex:null}],headersStr:"aot dispatch create joint graph aotdispatchautograd_graph TorchInductor recursicvpostgradpasses() GraphLowering GraphLowering.compiletofn()",content:"# aot dispatch create joint graph\n\n./torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py.\n\naot_dispatch_autograd\n\nThe aot_dispatch_autograd function is responsible for tracing, partitioning, and compiling a given function for automatic differentiation using Ahead-Of-Time (AOT) Autograd.\n\nIt handles the creation of forward and backward graphs, manages metadata, and ensures that the compiled function can be executed efficiently with support for gradient computation.\n\nInputs\n\n * flat_fn: The original function to be traced and compiled.\n * flat_args: A list of arguments to be passed to the function.\n * aot_config: Configuration for AOT Autograd, which includes settings for partitioning, logging, and compilation.\n * fw_metadata: Metadata about the function's inputs and outputs, including information about views and mutations.\n\nOutputs\n\n * compiled_function: A compiled version of the original function that includes both the forward and backward passes, optimized for execution with support for gradient computation.\n\nMajor Functions in aot_dispatch_autograd\n\naot_dispatch_autograd_graph\n\n * Purpose: Traces the original function and creates a joint forward-backward FX graph.\n * Steps: Calls aot_dispatch_autograd_graph to trace the function and generate the FX graph.\n   * Returns the FX graph, joint inputs, and subclass metadata (if any).\n\npartition_fn:\n\n * Purpose: Partitions the joint FX graph into separate forward and backward graphs.\n * Steps: Uses the partition function specified in aot_config to split the FX graph into forward and backward modules.\n   * Returns the forward and backward modules.\n   * min_cut_rematerialization_partition 👍 👍\n\nfw_compiler and bw_compiler:\n\n * Purpose: Compiles the forward and backward FX graphs into executable functions.\n * Steps: Uses the forward and backward compilers specified in aot_config to compile the FX modules.\n   * Returns the compiled forward and backward functions.\n\nCompiledFunction:\n\n * Purpose: A custom autograd function that wraps the compiled forward and backward functions.\n * Steps: Defines the forward and backward static methods to handle the execution of the compiled functions.\n   * Manages the saving and restoring of tensors and symbolic integers for gradient computation.\n\ncreate_runtime_wrapper\n\n * Purpose: Creates a runtime wrapper for the compiled function to handle input mutations and other runtime considerations.\n * Steps: Wraps the CompiledFunction.apply method with additional logic for handling input mutations and AMP (Automatic Mixed Precision) settings.\n   * Returns the wrapped function.\n\nCode\n\ndef aot_dispatch_autograd(...)\n    fx_g, joint_inputs, maybe_subclass_meta = aot_dispatch_autograd_graph(  # type: ignore[misc]\n        flat_fn, flat_args, aot_config, fw_metadata=fw_metadata\n    )\n    ...\n    fw_module, bw_module = aot_config.partition_fn(\n        fx_g, joint_inputs, num_fwd_outputs=num_inner_fwd_outputs\n    )\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# aot_dispatch_autograd_graph\n\nThe aot_dispatch_autograd_graph function is responsible for preparing and tracing a given function (flat_fn) with its arguments (flat_args) for automatic differentiation using AOT (Ahead-Of-Time) Autograd.\nIt processes the function to handle input mutations, creates a joint forward-backward function, and generates an FX graph for the function.\nThe function ensures that the graph is functional (i.e., free of in-place operations) and can handle tensor subclasses if necessary.\n\n * pytree.tree_map: This function processes the traced_tangents to ensure they are detached and contiguous if they are tensors, preparing them for tracing.\n * fn_prepped_for_autograd: Prepares the original function for autograd by incorporating metadata about views and mutations, ensuring correct handling of these aspects during tracing.\n * create_joint: Creates a joint forward-backward function that traces both the forward and backward passes together, enabling efficient autograd processing.\n * create_functionalized_fn: Converts the joint function into a functional form, handling input mutations and tracing the joint structure, ensuring compatibility with autograd.\n * aot_dispatch_subclass: Handles tracing for tensor subclasses, ensuring that the autograd process can correctly handle these specialized tensor types.\n * _create_graph: Creates an FX graph from the joint function and its inputs, providing a lower-level representation of the function for optimization and execution.\n * fx_g.graph.eliminate_dead_code: Eliminates any dead code from the FX graph to optimize it, improving performance and reducing unnecessary computations.\n * fx_g.recompile: Recompiles the FX graph after eliminating dead code, ensuring that the graph is up-to-date and optimized for execution.\n\nCode\n\n    ### dispatch_and_compile_graph.py\n    fn_prepared_for_autograd = fn_prepped_for_autograd(\n        flat_fn,\n        fw_metadata,\n    )\n    joint_fn_to_trace = create_joint(fn_prepared_for_autograd, aot_config=aot_config)\n\n    joint_fn_to_trace, updated_joint_inputs = create_functionalized_fn(\n        joint_fn_to_trace,\n        joint_inputs,\n        meta=fw_metadata,\n        aot_config=aot_config,\n        trace_joint=True,\n    )\n\n    subclass_tracing_info = aot_dispatch_subclass(\n        joint_fn_to_trace,\n        updated_joint_inputs,\n        is_joint_structure=True,\n        meta=fw_metadata,\n        fw_only=flat_fn,\n    )\n    ...\n    fx_g = _create_graph(joint_fn_to_trace, updated_joint_inputs, aot_config=aot_config)\n    ...\n    fx_g.graph.eliminate_dead_code()\n    fx_g.recompile()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n# create_joint\n\nThe create_joint function is designed to create a joint forward-backward function for automatic differentiation.\nIt ensures that the function can be traced and differentiated correctly, handling the computation of gradients and preserving the necessary metadata.\n\n * Inputs\n\n * fn: A callable function that returns a tuple of (outputs, mask). The mask indicates which outputs require tangents.\n * aot_config: Configuration for AOT (Ahead-Of-Time) Autograd, which includes settings like whether tangents are needed.\n\n * Outputs\n\n * return a tuple of (outs, mask), where mask tells us which outputs are meant to have tangents.\n * compute tangents for every output that requires grad.\n\ninner_fn\n\nThis is the core function that computes the forward pass, identifies the outputs that require gradients, and performs the backward pass to compute the gradients.\n\n * Calls the original function fn with the primal inputs to get the outputs and a mask indicating which outputs require tangents.\n * Identifies the inputs and outputs that need gradients.\n * Sets up stack trace preservation hooks for the gradient functions. setup_stacktrace_preservation_hooks\n * Calls torch.autograd.grad to compute the gradients of the needed outputs with respect to the inputs that require gradients.\n * Returns the original outputs and the computed gradients.\n\n# _create_graph\n\n_create_graph wraps make_fx.\n\nThe make_fx function is a utility in PyTorch that traces a given function f and its inputs to produce an FX graph.\nThis graph represents the operations performed by the function in a way that can be further analyzed, transformed, and optimized.\nThe function supports different tracing modes (real, fake, symbolic) and can handle decomposition of complex operations into simpler ones.\n\n * tracing_mode Handling: Determines the mode of tracing (real, fake, symbolic) and sets up the appropriate context for each mode.\n * ShapeEnv: Manages symbolic shapes during tracing, especially in symbolic mode.\n * FakeTensorMode: Creates fake tensors to simulate tensor operations without actual computation, used in fake and symbolic modes.\n * ProxyTorchDispatchMode: Sets up a proxy mode to intercept and record tensor operations during tracing.\n * wrap_fake: Wraps input tensors as fake tensors or symbolic integers based on the tracing mode.\n * dispatch_trace: Performs the actual tracing of the function, recording the operations into an FX graph.\n\n\n# TorchInductor\n\nTorchInductor is the backend of pytorch, converting compute graph into target-specific code.\nIt also utilize optimization techiniques, memory optimization, parallelism and low-level codegen.\n\nIn previous part, after aot_dispatch_autograd() obtain forward/backward FX graph, it will compile the graph with forward/backward compiler.\n\ninductor calls compile_fx_inner() by default. THe kernel function is fx_codegen_and_compile(),\nwhich optimizes FX graph optimization and generate code.\n\nThe fx_codegen_and_compile function is responsible for generating and compiling a Torch FX (Functional Transformations) graph module.\nIt performs several steps to optimize and prepare the graph for execution, including handling tensor shapes, converting operations, and compiling the graph into an executable function.\nIt supports various modes such as AOT (Ahead-Of-Time) compilation and inference.\n\nFile location: ./torch/_inductor/compile_fx.py\n\n * _recursive_post_grad_passes: Applies post-gradient passes to the graph, optimizing it for inference or training.\n * split_const_gm: Splits the graph module into constant and non-constant parts if runtime constant folding is enabled.\n * GraphLowering: Lowers the FX graph to a lower-level representation, preparing it for code generation and execution.\n * graph.run:Executes the lowered graph with the provided example inputs.\n * graph.compile_to_fn: Compiles the lowered graph into an executable function.\n * CompiledFxGraph: Creates a compiled FX graph object that includes the compiled function, graph metadata, and metrics.\n\n\n# _recursicv_post_grad_passes()\n\nGraph optimization.\n\n# group_batch_fusion_passes()\n\noperator fusion.\n\n# remove_noops_ops()\n\nremove aten.clone/alias operations.\n\n# fuse_ddp_communication\n\n# decompose_auto_functionalized\n\nsplit high-level oprations\n\n\n# GraphLowering\n\nLower FX graph into Inductor IR(lower-level IR)\n\n\n# GraphLowering.compile_to_fn()\n\nsource code: compile_fx.py\n\ngenerate target-specific codes\n\nsource code: graph.py\n\ncompile_to_fn -> compile_to_module() -> codegen_with_cpp_wrapper\n\ncodegen_with_cpp_wrapper\n\n * CPU one pass\n * GPU two pass.\n   * JIT-compile the model with python wrapper code and run it to generate autotuned kernel binaries in the first pass;\n   * and then generate cpp wrapper code and compile it to a dynamic library in the second pass.\n\nGPU\n\n 1. First pass: compiled = self.compile_to_module().call; compiled(real_inputs)\n 2. Second pass: codegen()\n\nCode\n\n    def codegen(self):\n        from .scheduler import Scheduler\n\n        self.init_wrapper_code()\n\n        self.scheduler = Scheduler(self.buffers)\n        V.debug.draw_orig_fx_graph(self.orig_gm, self.scheduler.nodes)\n\n        self.scheduler.codegen()\n        return self.wrapper_code.generate(self.is_inference)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nFrom above code, we can see that it instantiate scheduler and use codegen function is scheduler.\n\nscheduler.py\n\nCode\n\nclass Scheduler:\n    @dynamo_timed\n    def __init__(self, nodes):\n        self.compute_dependencies()\n        self.topological_sort_schedule()\n        self.dead_node_elimination()\n        if config.reorder_for_compute_comm_overlap:\n            comms.decide_global_ordering_of_comms(self.nodes)\n        self.compute_ancestors()\n        metrics.ir_nodes_pre_fusion += len(self.nodes)\n        V.debug.ir_pre_fusion(self.nodes)\n        self.num_orig_nodes = len(self.nodes)\n        self.name_to_fused_node = {n.get_name(): n for n in self.nodes}\n        self.create_foreach_nodes()\n        self.topological_sort_schedule()\n        self.logged_slow_fusion = set()\n        self.fuse_nodes()\n        if config.reorder_for_compute_comm_overlap:\n            # Refresh node_users and inverse_users to reflect fused nodes\n            self.compute_node_users()\n            self.nodes = comms.reorder_compute_and_comm_for_overlap(self.nodes)\n        self.compute_last_usage()\n        V.debug.ir_post_fusion(self.nodes)\n        V.debug.graph_diagram(self.nodes)\n        self.debug_draw_graph()\n\n        # used during codegen:\n        self.current_device: torch.device = None  # type: ignore[assignment]\n        self.buffer_names_to_free = set()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# scheduler.init()\n\nkernel fusion optimization\n\n# compute_dependencies()\n\nCreate dependency edges between nodes, handling aliasing and mutation properly.\n\n# fuse_nodes()\n\nMutates self.nodes to combine nodes into FusedSchedulerNodes.\n\nThis relies on two key functions to control the logic:\n\n * self.can_fuse(): checks if a fusion is legal\n * self.score_fusion(): assigns priority to a given fusion\n\nfuse_nodes_once\n\n * get_possibble_fusions()\n   * can_fuse and not will_fusion_create_cycle\n   * speedup_by_fusion\n   * fused_nodes.remove(node1/node2)\n   * fused_nodes.add(node3)\n * topological_sort_schedule()\n * self.prune_redundant_deps()\n\n# Scheduler.codegen()\n\nget_backend(device).codegen_node(node)\n\nThis will call specific backend, such as gpu to generate code for nodes.\n\nThere is a cuda_cpp_scheduling.py defines class CUDACPPScheduling with codegen_template method.\n\nI am still confused how pytorch knows the mapping between IR and CUDA code.\n\nIt might be like this:\n\n * native_functions.yaml\n\nCode\n\n- func: _foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()\n  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices\n  variants: function\n  dispatch:\n    CPU: foreach_tensor_sub_scalarlist_kernel_slow_\n    CUDA: foreach_tensor_sub_scalarlist_kernel_cuda_\n  autogen: _foreach_sub.ScalarList_out\n\n- func: _foreach_mul.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]\n  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices\n  variants: function\n  dispatch:\n    CPU: foreach_tensor_mul_scalar_kernel_slow\n    CUDA: foreach_tensor_mul_scalar_kernel_cuda\n\n- func: _foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()\n  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices\n  variants: function\n  dispatch:\n    CPU: foreach_tensor_mul_scalar_kernel_slow_\n    CUDA: foreach_tensor_mul_scalar_kernel_cuda_\n  autogen: _foreach_mul.Scalar_out\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\nRegisterCUDA.cpp\n\nCode\n\nnamespace {\nvoid wrapper_CUDA_Scalar__foreach_mul_(at::TensorList self, const at::Scalar & scalar) {\n    // No device check\n  const OptionalDeviceGuard device_guard(device_of(self));\n  return at::native::foreach_tensor_mul_scalar_kernel_cuda_(self, scalar);\n}\n} // anonymous namespace\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * ./aten/src/ATen/Native/cuda/ForeachBinaryOp*.cu",normalizedContent:"# aot dispatch create joint graph\n\n./torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py.\n\naot_dispatch_autograd\n\nthe aot_dispatch_autograd function is responsible for tracing, partitioning, and compiling a given function for automatic differentiation using ahead-of-time (aot) autograd.\n\nit handles the creation of forward and backward graphs, manages metadata, and ensures that the compiled function can be executed efficiently with support for gradient computation.\n\ninputs\n\n * flat_fn: the original function to be traced and compiled.\n * flat_args: a list of arguments to be passed to the function.\n * aot_config: configuration for aot autograd, which includes settings for partitioning, logging, and compilation.\n * fw_metadata: metadata about the function's inputs and outputs, including information about views and mutations.\n\noutputs\n\n * compiled_function: a compiled version of the original function that includes both the forward and backward passes, optimized for execution with support for gradient computation.\n\nmajor functions in aot_dispatch_autograd\n\naot_dispatch_autograd_graph\n\n * purpose: traces the original function and creates a joint forward-backward fx graph.\n * steps: calls aot_dispatch_autograd_graph to trace the function and generate the fx graph.\n   * returns the fx graph, joint inputs, and subclass metadata (if any).\n\npartition_fn:\n\n * purpose: partitions the joint fx graph into separate forward and backward graphs.\n * steps: uses the partition function specified in aot_config to split the fx graph into forward and backward modules.\n   * returns the forward and backward modules.\n   * min_cut_rematerialization_partition 👍 👍\n\nfw_compiler and bw_compiler:\n\n * purpose: compiles the forward and backward fx graphs into executable functions.\n * steps: uses the forward and backward compilers specified in aot_config to compile the fx modules.\n   * returns the compiled forward and backward functions.\n\ncompiledfunction:\n\n * purpose: a custom autograd function that wraps the compiled forward and backward functions.\n * steps: defines the forward and backward static methods to handle the execution of the compiled functions.\n   * manages the saving and restoring of tensors and symbolic integers for gradient computation.\n\ncreate_runtime_wrapper\n\n * purpose: creates a runtime wrapper for the compiled function to handle input mutations and other runtime considerations.\n * steps: wraps the compiledfunction.apply method with additional logic for handling input mutations and amp (automatic mixed precision) settings.\n   * returns the wrapped function.\n\ncode\n\ndef aot_dispatch_autograd(...)\n    fx_g, joint_inputs, maybe_subclass_meta = aot_dispatch_autograd_graph(  # type: ignore[misc]\n        flat_fn, flat_args, aot_config, fw_metadata=fw_metadata\n    )\n    ...\n    fw_module, bw_module = aot_config.partition_fn(\n        fx_g, joint_inputs, num_fwd_outputs=num_inner_fwd_outputs\n    )\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# aot_dispatch_autograd_graph\n\nthe aot_dispatch_autograd_graph function is responsible for preparing and tracing a given function (flat_fn) with its arguments (flat_args) for automatic differentiation using aot (ahead-of-time) autograd.\nit processes the function to handle input mutations, creates a joint forward-backward function, and generates an fx graph for the function.\nthe function ensures that the graph is functional (i.e., free of in-place operations) and can handle tensor subclasses if necessary.\n\n * pytree.tree_map: this function processes the traced_tangents to ensure they are detached and contiguous if they are tensors, preparing them for tracing.\n * fn_prepped_for_autograd: prepares the original function for autograd by incorporating metadata about views and mutations, ensuring correct handling of these aspects during tracing.\n * create_joint: creates a joint forward-backward function that traces both the forward and backward passes together, enabling efficient autograd processing.\n * create_functionalized_fn: converts the joint function into a functional form, handling input mutations and tracing the joint structure, ensuring compatibility with autograd.\n * aot_dispatch_subclass: handles tracing for tensor subclasses, ensuring that the autograd process can correctly handle these specialized tensor types.\n * _create_graph: creates an fx graph from the joint function and its inputs, providing a lower-level representation of the function for optimization and execution.\n * fx_g.graph.eliminate_dead_code: eliminates any dead code from the fx graph to optimize it, improving performance and reducing unnecessary computations.\n * fx_g.recompile: recompiles the fx graph after eliminating dead code, ensuring that the graph is up-to-date and optimized for execution.\n\ncode\n\n    ### dispatch_and_compile_graph.py\n    fn_prepared_for_autograd = fn_prepped_for_autograd(\n        flat_fn,\n        fw_metadata,\n    )\n    joint_fn_to_trace = create_joint(fn_prepared_for_autograd, aot_config=aot_config)\n\n    joint_fn_to_trace, updated_joint_inputs = create_functionalized_fn(\n        joint_fn_to_trace,\n        joint_inputs,\n        meta=fw_metadata,\n        aot_config=aot_config,\n        trace_joint=true,\n    )\n\n    subclass_tracing_info = aot_dispatch_subclass(\n        joint_fn_to_trace,\n        updated_joint_inputs,\n        is_joint_structure=true,\n        meta=fw_metadata,\n        fw_only=flat_fn,\n    )\n    ...\n    fx_g = _create_graph(joint_fn_to_trace, updated_joint_inputs, aot_config=aot_config)\n    ...\n    fx_g.graph.eliminate_dead_code()\n    fx_g.recompile()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n# create_joint\n\nthe create_joint function is designed to create a joint forward-backward function for automatic differentiation.\nit ensures that the function can be traced and differentiated correctly, handling the computation of gradients and preserving the necessary metadata.\n\n * inputs\n\n * fn: a callable function that returns a tuple of (outputs, mask). the mask indicates which outputs require tangents.\n * aot_config: configuration for aot (ahead-of-time) autograd, which includes settings like whether tangents are needed.\n\n * outputs\n\n * return a tuple of (outs, mask), where mask tells us which outputs are meant to have tangents.\n * compute tangents for every output that requires grad.\n\ninner_fn\n\nthis is the core function that computes the forward pass, identifies the outputs that require gradients, and performs the backward pass to compute the gradients.\n\n * calls the original function fn with the primal inputs to get the outputs and a mask indicating which outputs require tangents.\n * identifies the inputs and outputs that need gradients.\n * sets up stack trace preservation hooks for the gradient functions. setup_stacktrace_preservation_hooks\n * calls torch.autograd.grad to compute the gradients of the needed outputs with respect to the inputs that require gradients.\n * returns the original outputs and the computed gradients.\n\n# _create_graph\n\n_create_graph wraps make_fx.\n\nthe make_fx function is a utility in pytorch that traces a given function f and its inputs to produce an fx graph.\nthis graph represents the operations performed by the function in a way that can be further analyzed, transformed, and optimized.\nthe function supports different tracing modes (real, fake, symbolic) and can handle decomposition of complex operations into simpler ones.\n\n * tracing_mode handling: determines the mode of tracing (real, fake, symbolic) and sets up the appropriate context for each mode.\n * shapeenv: manages symbolic shapes during tracing, especially in symbolic mode.\n * faketensormode: creates fake tensors to simulate tensor operations without actual computation, used in fake and symbolic modes.\n * proxytorchdispatchmode: sets up a proxy mode to intercept and record tensor operations during tracing.\n * wrap_fake: wraps input tensors as fake tensors or symbolic integers based on the tracing mode.\n * dispatch_trace: performs the actual tracing of the function, recording the operations into an fx graph.\n\n\n# torchinductor\n\ntorchinductor is the backend of pytorch, converting compute graph into target-specific code.\nit also utilize optimization techiniques, memory optimization, parallelism and low-level codegen.\n\nin previous part, after aot_dispatch_autograd() obtain forward/backward fx graph, it will compile the graph with forward/backward compiler.\n\ninductor calls compile_fx_inner() by default. the kernel function is fx_codegen_and_compile(),\nwhich optimizes fx graph optimization and generate code.\n\nthe fx_codegen_and_compile function is responsible for generating and compiling a torch fx (functional transformations) graph module.\nit performs several steps to optimize and prepare the graph for execution, including handling tensor shapes, converting operations, and compiling the graph into an executable function.\nit supports various modes such as aot (ahead-of-time) compilation and inference.\n\nfile location: ./torch/_inductor/compile_fx.py\n\n * _recursive_post_grad_passes: applies post-gradient passes to the graph, optimizing it for inference or training.\n * split_const_gm: splits the graph module into constant and non-constant parts if runtime constant folding is enabled.\n * graphlowering: lowers the fx graph to a lower-level representation, preparing it for code generation and execution.\n * graph.run:executes the lowered graph with the provided example inputs.\n * graph.compile_to_fn: compiles the lowered graph into an executable function.\n * compiledfxgraph: creates a compiled fx graph object that includes the compiled function, graph metadata, and metrics.\n\n\n# _recursicv_post_grad_passes()\n\ngraph optimization.\n\n# group_batch_fusion_passes()\n\noperator fusion.\n\n# remove_noops_ops()\n\nremove aten.clone/alias operations.\n\n# fuse_ddp_communication\n\n# decompose_auto_functionalized\n\nsplit high-level oprations\n\n\n# graphlowering\n\nlower fx graph into inductor ir(lower-level ir)\n\n\n# graphlowering.compile_to_fn()\n\nsource code: compile_fx.py\n\ngenerate target-specific codes\n\nsource code: graph.py\n\ncompile_to_fn -> compile_to_module() -> codegen_with_cpp_wrapper\n\ncodegen_with_cpp_wrapper\n\n * cpu one pass\n * gpu two pass.\n   * jit-compile the model with python wrapper code and run it to generate autotuned kernel binaries in the first pass;\n   * and then generate cpp wrapper code and compile it to a dynamic library in the second pass.\n\ngpu\n\n 1. first pass: compiled = self.compile_to_module().call; compiled(real_inputs)\n 2. second pass: codegen()\n\ncode\n\n    def codegen(self):\n        from .scheduler import scheduler\n\n        self.init_wrapper_code()\n\n        self.scheduler = scheduler(self.buffers)\n        v.debug.draw_orig_fx_graph(self.orig_gm, self.scheduler.nodes)\n\n        self.scheduler.codegen()\n        return self.wrapper_code.generate(self.is_inference)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nfrom above code, we can see that it instantiate scheduler and use codegen function is scheduler.\n\nscheduler.py\n\ncode\n\nclass scheduler:\n    @dynamo_timed\n    def __init__(self, nodes):\n        self.compute_dependencies()\n        self.topological_sort_schedule()\n        self.dead_node_elimination()\n        if config.reorder_for_compute_comm_overlap:\n            comms.decide_global_ordering_of_comms(self.nodes)\n        self.compute_ancestors()\n        metrics.ir_nodes_pre_fusion += len(self.nodes)\n        v.debug.ir_pre_fusion(self.nodes)\n        self.num_orig_nodes = len(self.nodes)\n        self.name_to_fused_node = {n.get_name(): n for n in self.nodes}\n        self.create_foreach_nodes()\n        self.topological_sort_schedule()\n        self.logged_slow_fusion = set()\n        self.fuse_nodes()\n        if config.reorder_for_compute_comm_overlap:\n            # refresh node_users and inverse_users to reflect fused nodes\n            self.compute_node_users()\n            self.nodes = comms.reorder_compute_and_comm_for_overlap(self.nodes)\n        self.compute_last_usage()\n        v.debug.ir_post_fusion(self.nodes)\n        v.debug.graph_diagram(self.nodes)\n        self.debug_draw_graph()\n\n        # used during codegen:\n        self.current_device: torch.device = none  # type: ignore[assignment]\n        self.buffer_names_to_free = set()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# scheduler.init()\n\nkernel fusion optimization\n\n# compute_dependencies()\n\ncreate dependency edges between nodes, handling aliasing and mutation properly.\n\n# fuse_nodes()\n\nmutates self.nodes to combine nodes into fusedschedulernodes.\n\nthis relies on two key functions to control the logic:\n\n * self.can_fuse(): checks if a fusion is legal\n * self.score_fusion(): assigns priority to a given fusion\n\nfuse_nodes_once\n\n * get_possibble_fusions()\n   * can_fuse and not will_fusion_create_cycle\n   * speedup_by_fusion\n   * fused_nodes.remove(node1/node2)\n   * fused_nodes.add(node3)\n * topological_sort_schedule()\n * self.prune_redundant_deps()\n\n# scheduler.codegen()\n\nget_backend(device).codegen_node(node)\n\nthis will call specific backend, such as gpu to generate code for nodes.\n\nthere is a cuda_cpp_scheduling.py defines class cudacppscheduling with codegen_template method.\n\ni am still confused how pytorch knows the mapping between ir and cuda code.\n\nit might be like this:\n\n * native_functions.yaml\n\ncode\n\n- func: _foreach_sub_.scalarlist(tensor(a!)[] self, scalar[] scalars) -> ()\n  device_check: nocheck   # foreach kernels fall back to slow path when tensor are on different devices\n  variants: function\n  dispatch:\n    cpu: foreach_tensor_sub_scalarlist_kernel_slow_\n    cuda: foreach_tensor_sub_scalarlist_kernel_cuda_\n  autogen: _foreach_sub.scalarlist_out\n\n- func: _foreach_mul.scalar(tensor[] self, scalar scalar) -> tensor[]\n  device_check: nocheck   # foreach kernels fall back to slow path when tensor are on different devices\n  variants: function\n  dispatch:\n    cpu: foreach_tensor_mul_scalar_kernel_slow\n    cuda: foreach_tensor_mul_scalar_kernel_cuda\n\n- func: _foreach_mul_.scalar(tensor(a!)[] self, scalar scalar) -> ()\n  device_check: nocheck   # foreach kernels fall back to slow path when tensor are on different devices\n  variants: function\n  dispatch:\n    cpu: foreach_tensor_mul_scalar_kernel_slow_\n    cuda: foreach_tensor_mul_scalar_kernel_cuda_\n  autogen: _foreach_mul.scalar_out\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\nregistercuda.cpp\n\ncode\n\nnamespace {\nvoid wrapper_cuda_scalar__foreach_mul_(at::tensorlist self, const at::scalar & scalar) {\n    // no device check\n  const optionaldeviceguard device_guard(device_of(self));\n  return at::native::foreach_tensor_mul_scalar_kernel_cuda_(self, scalar);\n}\n} // anonymous namespace\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * ./aten/src/aten/native/cuda/foreachbinaryop*.cu",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"CUDA Softmax",frontmatter:{title:"CUDA Softmax",date:"2024-12-27T15:32:49.000Z",permalink:"/pages/f00006/",tags:[null]},regularPath:"/10.mix/06.cuda_softmax.html",relativePath:"10.mix/06.cuda_softmax.md",key:"v-29268d11",path:"/pages/f00006/",headers:[{level:2,title:"Softmax calculation:",slug:"softmax-calculation",normalizedTitle:"softmax calculation:",charIndex:19},{level:3,title:"Naive softmax",slug:"naive-softmax",normalizedTitle:"naive softmax",charIndex:44},{level:3,title:"Safe softmax",slug:"safe-softmax",normalizedTitle:"safe softmax",charIndex:64},{level:3,title:"Online normalizer calculation",slug:"online-normalizer-calculation",normalizedTitle:"online normalizer calculation",charIndex:229},{level:2,title:"Optimization of Softmax has the following ideas:",slug:"optimization-of-softmax-has-the-following-ideas",normalizedTitle:"optimization of softmax has the following ideas:",charIndex:418},{level:2,title:"warp-level sync",slug:"warp-level-sync",normalizedTitle:"warp-level sync",charIndex:471},{level:2,title:"block-level sync",slug:"block-level-sync",normalizedTitle:"block-level sync",charIndex:1338}],headersStr:"Softmax calculation: Naive softmax Safe softmax Online normalizer calculation Optimization of Softmax has the following ideas: warp-level sync block-level sync",content:"# CUDA Softmax\n\n\n# Softmax calculation:\n\n\n# Naive softmax\n\n\n\n\n# Safe softmax\n\nSafe softmax avoid the overflow or under flow due to the exponent.\n\n\n\nThree passes\n\n * get max\n * calc divident\n * calc element-wise normalization\n\n\n# Online normalizer calculation\n\nTwo passes\n\n * get max and calc divident\n * calc element-wise normalization\n\nOnline normalizer calculation for softmax https://arxiv.org/pdf/1805.02867\n\n\n\n\n# Optimization of Softmax has the following ideas:\n\n\n# warp-level sync\n\neach warp can process one row.\nfirst step, each warp process the first 32 elements in one row.\nnext step, each warp processed the next 32 elements in the row, until the end of the row.\nThis achieves memory coalescing.\n\nIn the last step, each thread collects correspoding subset max and divident:\n\n * thread [0] : max of (A[0],A[32],A[96]...)\n * thread [1] : max of (A[1],A[33],A[97]...)\n * thread [31]: max of (A[31],A[63],A[127]...)\n\nNow the result of 32 threads in the warp to be reduced to sum for divident and max.\n\nOne way is to choose one of the thread, maybe thread 0 to iterate through all 32 threads and gets the max.\n\n * use __shfl_xor_sync\n\nIn this configuration, each warp process one row.\n\ncode\n\n\n\n\n\nThe best performance is achieved by online_softmax_kernel4:\n\n * online softmax\n * wrap shuffle\n * float4, each thread process 128bit\n\n\n# block-level sync\n\nIn this configuration, each block process one row.\n\nAfter the first pass, Thread 0 is to calc the max and div, it update the value into shared memory.\n\nIn the second pass, every thread in the block will read from shared memory to get the shared max and divident.",normalizedContent:"# cuda softmax\n\n\n# softmax calculation:\n\n\n# naive softmax\n\n\n\n\n# safe softmax\n\nsafe softmax avoid the overflow or under flow due to the exponent.\n\n\n\nthree passes\n\n * get max\n * calc divident\n * calc element-wise normalization\n\n\n# online normalizer calculation\n\ntwo passes\n\n * get max and calc divident\n * calc element-wise normalization\n\nonline normalizer calculation for softmax https://arxiv.org/pdf/1805.02867\n\n\n\n\n# optimization of softmax has the following ideas:\n\n\n# warp-level sync\n\neach warp can process one row.\nfirst step, each warp process the first 32 elements in one row.\nnext step, each warp processed the next 32 elements in the row, until the end of the row.\nthis achieves memory coalescing.\n\nin the last step, each thread collects correspoding subset max and divident:\n\n * thread [0] : max of (a[0],a[32],a[96]...)\n * thread [1] : max of (a[1],a[33],a[97]...)\n * thread [31]: max of (a[31],a[63],a[127]...)\n\nnow the result of 32 threads in the warp to be reduced to sum for divident and max.\n\none way is to choose one of the thread, maybe thread 0 to iterate through all 32 threads and gets the max.\n\n * use __shfl_xor_sync\n\nin this configuration, each warp process one row.\n\ncode\n\n\n\n\n\nthe best performance is achieved by online_softmax_kernel4:\n\n * online softmax\n * wrap shuffle\n * float4, each thread process 128bit\n\n\n# block-level sync\n\nin this configuration, each block process one row.\n\nafter the first pass, thread 0 is to calc the max and div, it update the value into shared memory.\n\nin the second pass, every thread in the block will read from shared memory to get the shared max and divident.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"CUDA LayerNorm",frontmatter:{title:"CUDA LayerNorm",date:"2024-12-29T15:32:49.000Z",permalink:"/pages/f00007/",tags:[null]},regularPath:"/10.mix/07.cuda_layernorm.html",relativePath:"10.mix/07.cuda_layernorm.md",key:"v-618db02d",path:"/pages/f00007/",headers:[{level:2,title:"Keynotes:",slug:"keynotes",normalizedTitle:"keynotes:",charIndex:45},{level:2,title:"Basic Flow",slug:"basic-flow",normalizedTitle:"basic flow",charIndex:352},{level:2,title:"Naive algorithm",slug:"naive-algorithm",normalizedTitle:"naive algorithm",charIndex:1e3},{level:2,title:"Welford' online algorthim",slug:"welford-online-algorthim",normalizedTitle:"welford' online algorthim",charIndex:1880}],headersStr:"Keynotes: Basic Flow Naive algorithm Welford' online algorthim",content:"# CUDA LayerNorm\n\nFigure Source\n\nFormula\n\n\n# Keynotes:\n\n * online algorthim to reduce pass\n * warp shuffle for warp-level synchronization\n * shared memory cache input vector in first pass, which could be reused in the future\n * float4 for global memory coalesce\n\nThe performance improvement from last two optimization is obvious in this algorithm.\n\n\n# Basic Flow\n\nnormally it needs two pass:\n\n * collect mean\n * calculate variance\n\nAfter those two passes, we obtain mean and variance. The last extra pass, we calculate elementwise normalization.\n\n        for (int i = 0; i < C; ++i) {\n            mean += x[i];\n        }\n        mean /= C;\n\n        float var = 0.f;\n        for (int i = 0; i < C; ++i) {\n            float xShift = x[i] - mean;\n            var += xShift * xShift;\n        }\n        float inv_std = 1.0f / sqrt(var / C + eps);\n\n        for (int i = 0; i < C; ++i) {\n            y[i] = weight[i] * (x[i] - mean) * inv_std + bias[i];\n        }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# Naive algorithm\n\n\n\nThis naive algorithm reduce first two pass into single pass.\nIn one pass, it calculates mean and variance.\n\n            float *const x = input + row * C;\n            float *const y = output + row * C;\n            float partialSum = 0.0f;\n            float partialSum2 = 0.0f;\n            for (int i = laneId; i < C; i += warpSize) {\n                float xi = x[i];\n                partialSum += xi;\n                partialSum2 += xi * xi;\n            }\n\n            float mean = warpReduceSum(partialSum) / C;\n            float mean2 = warpReduceSum(partialSum2) / C;\n\n            float var = (mean2 - mean * mean);\n            float inv_std = rsqrtf(var + eps);\n\n            for (int i = laneId; i < C; i += warpSize) {\n                y[i] = weight[i] * (x[i] - mean) * inv_std + bias[i];\n            }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# Welford' online algorthim\n\n\n\nThis is also single-pass algorithm and numerically stable.\n\n            for (int i = laneId; i < C; i += warpSize) {\n                float xi = x[i];\n                n++;\n                float delta = xi - mean;\n                mean += delta / n;\n                float delta2 = xi - mean;\n                M2 += delta * delta2;\n            }\n\n            welfordWarpReduce(mean,M2,n,&shared[warpId*2],&shared[warpId*2+1]);\n\n            float global_mean = shared[warpId*2];\n            float global_var = shared[warpId*2+1];\n          \n            float inv_std = rsqrtf(global_var + eps);\n\n            for (int i = laneId; i < C; i += warpSize) {\n                y[i] = weight[i] * (x[i] - global_mean) * inv_std + bias[i];\n            }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# Activation Function\n\nActivation function is also implemented. This is elemenment-wise operation.\n\nThere is no too much room to improve performance. The ratio of compute/memory is too low.",normalizedContent:"# cuda layernorm\n\nfigure source\n\nformula\n\n\n# keynotes:\n\n * online algorthim to reduce pass\n * warp shuffle for warp-level synchronization\n * shared memory cache input vector in first pass, which could be reused in the future\n * float4 for global memory coalesce\n\nthe performance improvement from last two optimization is obvious in this algorithm.\n\n\n# basic flow\n\nnormally it needs two pass:\n\n * collect mean\n * calculate variance\n\nafter those two passes, we obtain mean and variance. the last extra pass, we calculate elementwise normalization.\n\n        for (int i = 0; i < c; ++i) {\n            mean += x[i];\n        }\n        mean /= c;\n\n        float var = 0.f;\n        for (int i = 0; i < c; ++i) {\n            float xshift = x[i] - mean;\n            var += xshift * xshift;\n        }\n        float inv_std = 1.0f / sqrt(var / c + eps);\n\n        for (int i = 0; i < c; ++i) {\n            y[i] = weight[i] * (x[i] - mean) * inv_std + bias[i];\n        }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# naive algorithm\n\n\n\nthis naive algorithm reduce first two pass into single pass.\nin one pass, it calculates mean and variance.\n\n            float *const x = input + row * c;\n            float *const y = output + row * c;\n            float partialsum = 0.0f;\n            float partialsum2 = 0.0f;\n            for (int i = laneid; i < c; i += warpsize) {\n                float xi = x[i];\n                partialsum += xi;\n                partialsum2 += xi * xi;\n            }\n\n            float mean = warpreducesum(partialsum) / c;\n            float mean2 = warpreducesum(partialsum2) / c;\n\n            float var = (mean2 - mean * mean);\n            float inv_std = rsqrtf(var + eps);\n\n            for (int i = laneid; i < c; i += warpsize) {\n                y[i] = weight[i] * (x[i] - mean) * inv_std + bias[i];\n            }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# welford' online algorthim\n\n\n\nthis is also single-pass algorithm and numerically stable.\n\n            for (int i = laneid; i < c; i += warpsize) {\n                float xi = x[i];\n                n++;\n                float delta = xi - mean;\n                mean += delta / n;\n                float delta2 = xi - mean;\n                m2 += delta * delta2;\n            }\n\n            welfordwarpreduce(mean,m2,n,&shared[warpid*2],&shared[warpid*2+1]);\n\n            float global_mean = shared[warpid*2];\n            float global_var = shared[warpid*2+1];\n          \n            float inv_std = rsqrtf(global_var + eps);\n\n            for (int i = laneid; i < c; i += warpsize) {\n                y[i] = weight[i] * (x[i] - global_mean) * inv_std + bias[i];\n            }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# activation function\n\nactivation function is also implemented. this is elemenment-wise operation.\n\nthere is no too much room to improve performance. the ratio of compute/memory is too low.",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Understanding Pytorch Source Torch",frontmatter:{title:"Understanding Pytorch Source Torch",date:"2025-01-01T15:32:49.000Z",permalink:"/pages/f00009/",tags:[null]},regularPath:"/10.mix/09.learn_pytorch_source_torch.html",relativePath:"10.mix/09.learn_pytorch_source_torch.md",key:"v-593a8e26",path:"/pages/f00009/",headers:[{level:2,title:"1. Structure of torch Source Code",slug:"_1-structure-of-torch-source-code",normalizedTitle:"1. structure of torch source code",charIndex:421},{level:2,title:"2. Core Functions of torch Source Code",slug:"_2-core-functions-of-torch-source-code",normalizedTitle:"2. core functions of torch source code",charIndex:560},{level:3,title:"a. Tensor API",slug:"a-tensor-api",normalizedTitle:"a. tensor api",charIndex:603},{level:3,title:"b. Functional API",slug:"b-functional-api",normalizedTitle:"b. functional api",charIndex:1053},{level:3,title:"c. Neural Networks",slug:"c-neural-networks",normalizedTitle:"c. neural networks",charIndex:1331},{level:3,title:"d. Autograd (Automatic Differentiation)",slug:"d-autograd-automatic-differentiation",normalizedTitle:"d. autograd (automatic differentiation)",charIndex:1678},{level:3,title:"e. Optimization",slug:"e-optimization",normalizedTitle:"e. optimization",charIndex:2036},{level:3,title:"f. CUDA Utilities",slug:"f-cuda-utilities",normalizedTitle:"f. cuda utilities",charIndex:2295},{level:3,title:"g. TorchScript and JIT",slug:"g-torchscript-and-jit",normalizedTitle:"g. torchscript and jit",charIndex:2541},{level:2,title:"3. torch/_C/: The C++ Backend Integration",slug:"_3-torch-c-the-c-backend-integration",normalizedTitle:"3. torch/_c/: the c++ backend integration",charIndex:2803},{level:2,title:"4. Interaction Between Python and C++",slug:"_4-interaction-between-python-and-c",normalizedTitle:"4. interaction between python and c++",charIndex:3108}],headersStr:"1. Structure of torch Source Code 2. Core Functions of torch Source Code a. Tensor API b. Functional API c. Neural Networks d. Autograd (Automatic Differentiation) e. Optimization f. CUDA Utilities g. TorchScript and JIT 3. torch/_C/: The C++ Backend Integration 4. Interaction Between Python and C++",content:"# torch\n\nThe torch module in PyTorch is the high-level Python interface that provides user-facing APIs.\nIt serves as the entry point for creating tensors, performing operations, defining neural networks, and managing other PyTorch features. The torch source code primarily resides in the torch/ directory of the PyTorch source tree and acts as a bridge between Python and the backend components (e.g., C++ and CUDA).\n\n\n# 1. Structure of torch Source Code\n\nThe torch directory contains various submodules and files that implement PyTorch's core Python APIs\n\n\n# 2. Core Functions of torch Source Code\n\n\n# a. Tensor API\n\nThe torch.Tensor class is the central data structure in PyTorch.\nIt provides the foundation for numerical computations, supporting operations on both CPUs and GPUs.\n\n * Source Code: torch/tensor.py\n * Key Features:\n   * Creation of tensors using methods like torch.tensor(), torch.zeros(), torch.ones().\n   * Provides methods for tensor operations (e.g., add, mul, matmul).\n   * Tensors are backed by the at::Tensor class in ATen.\n\n\n# b. Functional API\n\nThe torch.functional module provides stateless APIs for mathematical operations.\n\n * Source Code: torch/functional.py\n * Examples:\n   * Operations: torch.matmul, torch.sigmoid, torch.softmax.\n   * Often complements tensor methods for advanced operations.\n\n\n# c. Neural Networks\n\nThe torch.nn module is designed for building and training neural networks. It provides building blocks such as layers, loss functions, and the torch.nn.Module class.\n\n * Source Code: torch/nn/\n * Key Features:\n   * Layers like nn.Linear, nn.Conv2d, and nn.ReLU.\n   * Loss functions like nn.CrossEntropyLoss and nn.MSELoss.\n\n\n# d. Autograd (Automatic Differentiation)\n\nThe torch.autograd module powers PyTorch's ability to compute gradients automatically.\n\n * Source Code: torch/autograd/\n * Key Features:\n   * Tracks operations on tensors with requires_grad=True.\n   * Builds a computation graph dynamically during runtime.\n   * Gradients are computed via torch.autograd.backward.\n\n\n# e. Optimization\n\nThe torch.optim module provides implementations of optimization algorithms.\n\n * Source Code: torch/optim/\n * Examples:\n   * Optimizers: torch.optim.SGD, torch.optim.Adam.\n   * Manages model parameters and updates them based on gradients.\n\n\n# f. CUDA Utilities\n\nThe torch.cuda module provides utilities for GPU support.\n\n * Source Code: torch/cuda/\n * Key Features:\n   * Memory management and synchronization.\n   * Functions like torch.cuda.is_available() and torch.cuda.set_device().\n\n\n# g. TorchScript and JIT\n\nThe torch.jit module enables exporting models for deployment.\n\n * Source Code: torch/jit/\n * Key Features:\n   * Converts PyTorch models to a graph-based intermediate representation.\n   * Optimizes and serializes models for inference.\n\n\n# 3. torch/_C/: The C++ Backend Integration\n\nThe torch/_C/ directory provides Python bindings to the C++ backend using Pybind11.\n\nPurpose:\n\n * Exposes the ATen tensor library, dispatcher, and other C++ components to Python.\n * Many high-performance functions are implemented in C++ and accessed via _C.\n\n\n# 4. Interaction Between Python and C++\n\nThe torch module serves as a bridge between Python and the C++ backend: Python API:\n\n * Users interact with torch APIs in Python.\n * Example: torch.add, torch.nn.Linear.\n   * Dispatcher: Calls are routed to C++ implementations via the PyTorch dispatcher.\n   * Execution: The backend executes the operation (e.g., using ATen or CUDA).\n\n",normalizedContent:"# torch\n\nthe torch module in pytorch is the high-level python interface that provides user-facing apis.\nit serves as the entry point for creating tensors, performing operations, defining neural networks, and managing other pytorch features. the torch source code primarily resides in the torch/ directory of the pytorch source tree and acts as a bridge between python and the backend components (e.g., c++ and cuda).\n\n\n# 1. structure of torch source code\n\nthe torch directory contains various submodules and files that implement pytorch's core python apis\n\n\n# 2. core functions of torch source code\n\n\n# a. tensor api\n\nthe torch.tensor class is the central data structure in pytorch.\nit provides the foundation for numerical computations, supporting operations on both cpus and gpus.\n\n * source code: torch/tensor.py\n * key features:\n   * creation of tensors using methods like torch.tensor(), torch.zeros(), torch.ones().\n   * provides methods for tensor operations (e.g., add, mul, matmul).\n   * tensors are backed by the at::tensor class in aten.\n\n\n# b. functional api\n\nthe torch.functional module provides stateless apis for mathematical operations.\n\n * source code: torch/functional.py\n * examples:\n   * operations: torch.matmul, torch.sigmoid, torch.softmax.\n   * often complements tensor methods for advanced operations.\n\n\n# c. neural networks\n\nthe torch.nn module is designed for building and training neural networks. it provides building blocks such as layers, loss functions, and the torch.nn.module class.\n\n * source code: torch/nn/\n * key features:\n   * layers like nn.linear, nn.conv2d, and nn.relu.\n   * loss functions like nn.crossentropyloss and nn.mseloss.\n\n\n# d. autograd (automatic differentiation)\n\nthe torch.autograd module powers pytorch's ability to compute gradients automatically.\n\n * source code: torch/autograd/\n * key features:\n   * tracks operations on tensors with requires_grad=true.\n   * builds a computation graph dynamically during runtime.\n   * gradients are computed via torch.autograd.backward.\n\n\n# e. optimization\n\nthe torch.optim module provides implementations of optimization algorithms.\n\n * source code: torch/optim/\n * examples:\n   * optimizers: torch.optim.sgd, torch.optim.adam.\n   * manages model parameters and updates them based on gradients.\n\n\n# f. cuda utilities\n\nthe torch.cuda module provides utilities for gpu support.\n\n * source code: torch/cuda/\n * key features:\n   * memory management and synchronization.\n   * functions like torch.cuda.is_available() and torch.cuda.set_device().\n\n\n# g. torchscript and jit\n\nthe torch.jit module enables exporting models for deployment.\n\n * source code: torch/jit/\n * key features:\n   * converts pytorch models to a graph-based intermediate representation.\n   * optimizes and serializes models for inference.\n\n\n# 3. torch/_c/: the c++ backend integration\n\nthe torch/_c/ directory provides python bindings to the c++ backend using pybind11.\n\npurpose:\n\n * exposes the aten tensor library, dispatcher, and other c++ components to python.\n * many high-performance functions are implemented in c++ and accessed via _c.\n\n\n# 4. interaction between python and c++\n\nthe torch module serves as a bridge between python and the c++ backend: python api:\n\n * users interact with torch apis in python.\n * example: torch.add, torch.nn.linear.\n   * dispatcher: calls are routed to c++ implementations via the pytorch dispatcher.\n   * execution: the backend executes the operation (e.g., using aten or cuda).\n\n",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Understanding Pytorch CUDA",frontmatter:{title:"Understanding Pytorch CUDA",date:"2024-12-30T15:32:49.000Z",permalink:"/pages/f00008/",tags:[null]},regularPath:"/10.mix/08.learn_pytorch_cuda.html",relativePath:"10.mix/08.learn_pytorch_cuda.md",key:"v-18aac47d",path:"/pages/f00008/",headers:[{level:2,title:"Pytorch CUDA Memory Allocation",slug:"pytorch-cuda-memory-allocation",normalizedTitle:"pytorch cuda memory allocation",charIndex:1478},{level:3,title:"Lower level",slug:"lower-level",normalizedTitle:"lower level",charIndex:1513},{level:3,title:"intermediate level",slug:"intermediate-level",normalizedTitle:"intermediate level",charIndex:4032},{level:3,title:"python level",slug:"python-level",normalizedTitle:"python level",charIndex:6994},{level:3,title:"code gen for memory allocation",slug:"code-gen-for-memory-allocation",normalizedTitle:"code gen for memory allocation",charIndex:9798},{level:2,title:"Pytorch Adam CUDA Kernel",slug:"pytorch-adam-cuda-kernel",normalizedTitle:"pytorch adam cuda kernel",charIndex:12316},{level:3,title:"native_functions.yaml",slug:"native-functions-yaml",normalizedTitle:"native_functions.yaml",charIndex:349},{level:3,title:"FusedAdamKernel.cu",slug:"fusedadamkernel-cu",normalizedTitle:"fusedadamkernel.cu",charIndex:13055},{level:3,title:"fusedadamimpl.cu",slug:"fused-adam-impl-cu",normalizedTitle:"fusedadamimpl.cu",charIndex:null},{level:3,title:"MultiTensorApply.cuh",slug:"multitensorapply-cuh",normalizedTitle:"multitensorapply.cuh",charIndex:16381},{level:2,title:"Pytorch linear lowering",slug:"pytorch-linear-lowering",normalizedTitle:"pytorch linear lowering",charIndex:17412},{level:3,title:"pythonnnfunctions.cpp",slug:"python-nn-functions-cpp",normalizedTitle:"pythonnnfunctions.cpp",charIndex:null},{level:3,title:"linear.h",slug:"linear-h",normalizedTitle:"linear.h",charIndex:17647},{level:3,title:"Operators_0.cpp",slug:"operators-0-cpp",normalizedTitle:"operators_0.cpp",charIndex:17752},{level:3,title:"dispatch",slug:"dispatch",normalizedTitle:"dispatch",charIndex:315},{level:3,title:"boring part, traversing from dispatcher to actual kernel call",slug:"boring-part-traversing-from-dispatcher-to-actual-kernel-call",normalizedTitle:"boring part, traversing from dispatcher to actual kernel call",charIndex:17977},{level:3,title:"RedispatchFunction",slug:"redispatchfunction",normalizedTitle:"redispatchfunction",charIndex:20209},{level:3,title:"Blas.cpp",slug:"blas-cpp",normalizedTitle:"blas.cpp",charIndex:1261}],headersStr:"Pytorch CUDA Memory Allocation Lower level intermediate level python level code gen for memory allocation Pytorch Adam CUDA Kernel native_functions.yaml FusedAdamKernel.cu fusedadamimpl.cu MultiTensorApply.cuh Pytorch linear lowering pythonnnfunctions.cpp linear.h Operators_0.cpp dispatch boring part, traversing from dispatcher to actual kernel call RedispatchFunction Blas.cpp",content:'# Pytorch CUDA\n\nbefore dive into deeper source code, this is the instinct of pytorch source code:\n\ngradually lower code from python to cuda code\n\n * torch\n   * pytorch interface, python oriented -./csrc.autograd mechanism\n     * engine\n     * function\n     * variable\n * c10 core functions -cuda memory management -dispatcher mechanism\n * aten\n   * native_functions.yaml\n     * register all implementations with backend specification\n\n    - func: _slow_conv2d_forward.output(Tensor self, Tensor weight, SymInt[2] kernel_size,\n      Tensor? bias, SymInt[2] stride, SymInt[2] padding, *, Tensor(a!) output) -> Tensor(a!)\n    python_module: nn\n    dispatch:\n    CPU: slow_conv2d_forward_out_cpu\n    CUDA: slow_conv2d_forward_out_cuda\n\n\n1\n2\n3\n4\n5\n6\n\n * src/ATen/Native/core/boxing/\n   * WrapFunctionIntoFunctor.h support dispatcher, unbox function and call into real implementation\n * ./src/ATen/cuda\n   \n   * general functions\n     \n     \n\n * ./src/ATen/native/cuda\n   \n   * implementation of different layers\n     \n     \n\nPytorch code of torch.randn(), it will calls empty() function to allocate memory and cudacachingallocator malloc will be called.\n\nPytorch code of nn.linear(), it will lowered into addmm_out_cuda_impl cuda code in ./aten/src/ATen/native/cuda/Blas.cpp\n\n\n# Pending to be understood\n\nwaiting task: learn deeper how pytorch autograd works related blog:\n\n * http://blog.ezyang.com/2019/05/pytorch-internals/\n * https://www.52coding.com.cn/2019/05/05/PyTorch4/\n\n\n# Pytorch CUDA Memory Allocation\n\n\n# Lower level\n\n./c10/cuda/CUDACachingAllocator.cpp\n\nBlock* malloc(\n      c10::DeviceIndex device,\n      size_t orig_size,\n      cudaStream_t stream) {\n    ...\n    // First, try to get a block from the existing pool.\n    bool block_found =\n        // Search pool\n        get_free_block(params)\n        // Trigger callbacks and retry search\n        || (trigger_free_memory_callbacks(params) && get_free_block(params));\n    ...\n    if (!block_found) {\n      // Do garbage collection if the flag is set.\n      if (C10_UNLIKELY(\n              set_fraction &&\n              CUDAAllocatorConfig::garbage_collection_threshold() > 0.0)) {\n        garbage_collect_cached_blocks(context);\n      }\n    ...\n      // Attempt allocate\n      // WARNING: alloc_block may release the allocator lock when calling\n      // cudaMalloc. So far this function has not modified allocator state, but\n      // keep in mind that any observed allocator state may change across calls\n      // to alloc_block since it may release the lock.\n      block_found = alloc_block(params, false, context, lock)\n          // Free enough available cached blocks to satisfy alloc and retry\n          // alloc.\n          || (release_available_cached_blocks(params, context) &&\n              alloc_block(params, false, context, lock))\n          // Free all non-split cached blocks and retry alloc.\n          || (C10_LIKELY(captures_underway.size() == 0) &&\n              release_cached_blocks(context) &&\n              alloc_block(params, true, context, lock));\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n./aten/src/ATen/cuda\n\n * CachingHostAllocator.h\n\ninline TORCH_CUDA_CPP_API at::DataPtr HostAlloc(size_t size) {\n  return getCachingHostAllocator()->allocate(size);\n}\n\n\n1\n2\n3\n\n * CachingHostAllocator.cpp\n\nstruct CUDACachingHostAllocatorImpl\n    : public CachingHostAllocatorImpl<CUDAStream, EventPool::Event> {\n private:\n  void allocate_host_memory(size_t size, void** ptr) override {\n    // Pinned memory pointers allocated by any device can be directly used by\n    // any other device, regardless of the current device at the time of\n    // allocation, since we assume unified addressing. So we grab any existing\n    // primary context, if available. See pytorch/pytorch#21081.\n    ...\n    // Use cudaHostAlloc for allocating pinned memory (global lock in driver)\n    C10_CUDA_CHECK(cudaHostAlloc(ptr, size, cudaHostAllocDefault));\n    ...\n    }\n  }\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# intermediate level\n\ncode like cudnn use get_workspace_size to allocate space\n\n./aten/src/ATen/native/cuda/MixedDtypesLinear.cu\n\n  // Allocate workspace for CUTLASS mixed datatypes GEMM kernel.\n  const auto workspace_size = Gemm::get_workspace_size(arguments);\n\n\n1\n2\n\n\ncode like: ./aten/src/ATen/native/cuda/ForeachReduceOp.cu\n\nallocate memory by using at::zeros or at::empty\n\n  auto output_per_tensor = at::zeros(\n      {static_cast<int64_t>(ntensors) * max_chunks_per_tensor}, options);\n\n\n1\n2\n\n\nat::zeros is based on:\n./aten/src/ATen/native/cuda/EmptyTensor.cpp\n\nTensorBase empty_cuda(\n    IntArrayRef size,\n    ScalarType dtype,\n    std::optional<Device> device_opt,\n    std::optional<c10::MemoryFormat> memory_format_opt) {\n  at::globalContext().lazyInitDevice(c10::DeviceType::CUDA);\n  const auto device = device_or_default(device_opt);\n  TORCH_INTERNAL_ASSERT(device.is_cuda());\n  const DeviceGuard device_guard(device);\n  auto* allocator = at::cuda::getCUDADeviceAllocator();\n  constexpr c10::DispatchKeySet cuda_dks(c10::DispatchKey::CUDA);\n  return at::detail::empty_generic(\n      size, allocator, cuda_dks, dtype, memory_format_opt);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n./aten/src/ATen/EmptyTensor.cpp\n\nWe have specify allocator and size, so we will call cuda caching allocator to allocate memory.\n\nTensorBase empty_generic(\n    IntArrayRef size,\n    c10::Allocator* allocator,\n    c10::DispatchKeySet ks,\n    ScalarType scalar_type,\n    std::optional<c10::MemoryFormat> memory_format_opt) {\n  return _empty_generic(size, allocator, ks, scalar_type, memory_format_opt);\n}\n\ntemplate <typename T>\nTensorBase _empty_generic(\n    ArrayRef<T> size,\n    c10::Allocator* allocator,\n    c10::DispatchKeySet ks,\n    ScalarType scalar_type,\n    std::optional<c10::MemoryFormat> memory_format_opt) {\n  at::detail::check_size_nonnegative(size);\n  at::detail::raise_warning_for_complex_half(scalar_type);\n  caffe2::TypeMeta dtype = scalarTypeToTypeMeta(scalar_type);\n  auto size_bytes = computeStorageNbytesContiguous(size, dtype.itemsize());\n  auto storage_impl = c10::make_intrusive<StorageImpl>(\n      c10::StorageImpl::use_byte_size_t(),\n      size_bytes,\n      allocator,\n      /*resizeable=*/true);\n\n  auto tensor = detail::make_tensor_base<TensorImpl>(\n      std::move(storage_impl), ks, dtype);\n  // Default TensorImpl has size [0]\n  // NB: test for meta dispatch key to avoid guarding on zero-ness\n  if (ks.has(c10::DispatchKey::Meta) || size.size() != 1 || size[0] != 0) {\n    tensor.unsafeGetTensorImpl()->generic_set_sizes_contiguous(size);\n  }\n\n  if (memory_format_opt.has_value()) {\n    // Restriding a just-created empty contiguous tensor does nothing.\n    if (*memory_format_opt != MemoryFormat::Contiguous) {\n      tensor.unsafeGetTensorImpl()->empty_tensor_restride(*memory_format_opt);\n    }\n  }\n\n  return tensor;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n\n# python level\n\ninit.pyi.in\n\nclass _cuda_CUDAAllocator: ...\n\ndef _cuda_customAllocator(alloc_fn: _int, free_fn: _int) -> _cuda_CUDAAllocator: ...\ndef _cuda_changeCurrentAllocator(allocator: _cuda_CUDAAllocator) -> None: ...\ndef _cuda_getAllocator() -> _cuda_CUDAAllocator: ...\n\n\n1\n2\n3\n4\n5\n\n\n./torch/cuda/memory.py\n\n__all__ = [\n    "caching_allocator_alloc",\n    "caching_allocator_delete",\n    "caching_allocator_enable",\n    ...\n    "memory_allocated",\n    ...\n]\n\ndef caching_allocator_alloc(size, device: Union[Device, int] = None, stream=None):\n    r"""Perform a memory allocation using the CUDA memory allocator.\n\n    Memory is allocated for a given device and a stream, this\n    function is intended to be used for interoperability with other\n    frameworks. Allocated memory is released through\n    :func:`~torch.cuda.caching_allocator_delete`.\n\n    Args:\n        size (int): number of bytes to be allocated.\n        device (torch.device or int, optional): selected device. If it is\n            ``None`` the default CUDA device is used.\n        stream (torch.cuda.Stream or int, optional): selected stream. If is ``None`` then\n            the default stream for the selected device is used.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    """\n    if device is None:\n        device = torch.cuda.current_device()\n    device = _get_device_index(device)\n    if stream is None:\n        stream = torch.cuda.current_stream(device)\n    if isinstance(stream, torch.cuda.streams.Stream):\n        stream = stream.cuda_stream\n    ...\n    with torch.cuda.device(device):\n        return torch._C._cuda_cudaCachingAllocator_raw_alloc(size, stream)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n./torch/csrc/cuda/Module.cpp\n\n    {"_cuda_cudaCachingAllocator_raw_alloc",\n     THCPModule_cudaCachingAllocator_raw_alloc,\n     METH_VARARGS,\n     nullptr},\n\nPyObject* THCPModule_cudaCachingAllocator_raw_alloc(\n    PyObject* _unused,\n    PyObject* args) {\n  HANDLE_TH_ERRORS\n  PyObject* size_o = nullptr;\n  PyObject* stream_o = nullptr;\n  if (!PyArg_ParseTuple(args, "OO", &size_o, &stream_o)) {\n    THPUtils_invalidArguments(\n        args,\n        nullptr,\n        "caching_allocator_alloc",\n        1,\n        "(ssize_t size, intptr_t stream);");\n    return nullptr;\n  }\n  auto size = PyLong_AsSsize_t(size_o);\n  cudaStream_t stream = static_cast<cudaStream_t>(PyLong_AsVoidPtr(stream_o));\n  void* mem = nullptr;\n  {\n    pybind11::gil_scoped_release no_gil;\n    mem = c10::cuda::CUDACachingAllocator::raw_alloc_with_stream(size, stream);\n  }\n  return PyLong_FromVoidPtr(mem);\n  END_HANDLE_TH_ERRORS\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# code gen for memory allocation\n\n./torch/_inductor/codegen/cuda/cuda_kernel.py\n\n    def call_kernel(\n        self,\n        name: str,\n        node: "CUDATemplateBuffer",  # type: ignore[name-defined]\n    ) -> None:\n        if node.get_workspace_size() > 0:\n            ws = WorkspaceArg(\n                count=node.get_workspace_size(),\n                device=V.graph.get_current_device_or_throw(),\n                zero_mode=WorkspaceZeroMode.UNINITIALIZED,\n                outer_name=WorkspaceArg.unique_name(),\n            )\n            wrapper.generate_workspace_allocation(ws)\n            workspace = str(ws.outer_name)\n            call_args.append(\n                workspace\n                if V.graph.cpp_wrapper\n                else f"c_void_p({workspace}.data_ptr())"\n            )\n      .....\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n./torch/_inductor/codegen/wrapper.py\n\n    def generate_workspace_allocation(self, ws: WorkspaceArg):\n        name = ws.get_name()\n        line = AllocateLine(self, ws)\n        if ws.zero_mode == WorkspaceZeroMode.UNINITIALIZED:\n            self.writeline(line)\n        elif ws.zero_mode == WorkspaceZeroMode.ZERO_ON_CALL:\n            self.writeline(line)\n            self.writeline(self.make_zero_buffer(name))\n        elif ws.zero_mode == WorkspaceZeroMode.ZERO_PER_GRAPH:\n            prior = self.allocated_workspaces.get(name)\n            if prior:\n                assert isinstance(prior, AllocateLine)\n                # expand existing allocation\n                prior.node = WorkspaceArg.maximum(prior.node, ws)\n            else:\n                self.writeline(line)\n                self.writeline(self.make_zero_buffer(name))\n                self.allocated_workspaces[name] = line\n        else:\n            raise AssertionError(ws.zero_mode)\n\n        if config.triton.autotune_at_compile_time:\n            self.kernel_autotune_calls.writeline(\n                PythonWrapperCodegen.make_allocation(\n                    self,\n                    name,\n                    ws.device,\n                    ws.dtype,\n                    shape=(V.graph.sizevars.size_hint(ws.count),),\n                    stride=(1,),\n                )\n            )\n            if ws.zero_mode != WorkspaceZeroMode.UNINITIALIZED:\n                self.kernel_autotune_calls.writeline(\n                    PythonWrapperCodegen.make_zero_buffer(self, name)\n                )\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n# Pytorch Adam CUDA Kernel\n\n\n# native_functions.yaml\n\n./aten/src/ATen/native/native_functions.yaml\n\n- func: _fused_adam_(Tensor(a!)[] self,\nTensor(b!)[] grads,\nTensor(c!)[] exp_avgs,\nTensor(d!)[] exp_avg_sqs,\nTensor(e!)[] max_exp_avg_sqs,\nTensor[] state_steps, *,\nfloat lr, float beta1, float beta2,\nfloat weight_decay, float eps, bool amsgrad,\nbool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()\n  # Unlike "foreach" functions, lists of tensors should be guaranteed to be on the same device (for now).\n  variants: function\n  dispatch:\n    CPU: _fused_adam_kernel_cpu_\n    CUDA: _fused_adam_kernel_cuda_\n    MPS: _fused_adam_kernel_mps_\n  autogen: _fused_adam, _fused_adam.out\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# FusedAdamKernel.cu\n\n./aten/arc/ATen/native/cuda\n\n// The following overload simply has a Tensor lr\nvoid _fused_adam_kernel_cuda_(\n    at::TensorList params,\n    at::TensorList grads,\n    at::TensorList exp_avgs,\n    at::TensorList exp_avg_sqs,\n    at::TensorList max_exp_avg_sqs,\n    at::TensorList state_steps,\n    const at::Tensor& lr,\n    const double beta1,\n    const double beta2,\n    const double weight_decay,\n    const double eps,\n    const bool amsgrad,\n    const bool maximize,\n    const std::optional<at::Tensor>& grad_scale,\n    const std::optional<at::Tensor>& found_inf) {\n  if (lr.is_cpu()) {\n    _fused_adam_kernel_cuda_(\n        params,\n        grads,\n        exp_avgs,\n        exp_avg_sqs,\n        max_exp_avg_sqs,\n        state_steps,\n        lr.item<double>(),\n        beta1,\n        beta2,\n        weight_decay,\n        eps,\n        amsgrad,\n        maximize,\n        grad_scale,\n        found_inf);\n    return;\n  }\n\n  // Manually check devices since we specify no device check in\n  // native_functions.yaml\n  ...\n\n  if (amsgrad) {\n    ...\n  } else {\n    TORCH_CHECK(\n        at::native::check_fast_path_restrictions(\n            {params, grads, exp_avgs, exp_avg_sqs}),\n        "params, grads, exp_avgs, and exp_avg_sqs must have same dtype, device, and layout");\n    _fused_adam_cuda_impl_(\n        params,\n        grads,\n        exp_avgs,\n        exp_avg_sqs,\n        state_steps,\n        lr,\n        beta1,\n        beta2,\n        weight_decay,\n        eps,\n        maximize,\n        grad_scale,\n        found_inf);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n\n# fused_adam_impl.cu\n\n./aten/arc/ATen/native/cuda\n\nvoid _fused_adam_cuda_impl_(\n    at::TensorList params,\n    at::TensorList grads,\n    at::TensorList exp_avgs,\n    at::TensorList exp_avg_sqs,\n    at::TensorList state_steps,\n    const double lr,\n    const double beta1,\n    const double beta2,\n    const double weight_decay,\n    const double eps,\n    const bool maximize,\n    const std::optional<at::Tensor>& grad_scale,\n    const std::optional<at::Tensor>& found_inf) {\n  std::vector<std::vector<at::Tensor>> tensor_lists{\n      params.vec(), grads.vec(), exp_avgs.vec(), exp_avg_sqs.vec()};\n\n  const float* grad_scale_ptr =\n      grad_scale.has_value() ? grad_scale->data_ptr<float>() : nullptr;\n  const float* found_inf_ptr =\n      found_inf.has_value() ? found_inf->data_ptr<float>() : nullptr;\n  const float* lr_ptr = nullptr;\n\n  AT_DISPATCH_FLOATING_TYPES_AND2(\n      kHalf,\n      kBFloat16,\n      params[0].scalar_type(),\n      "fused_adam_kernel_cuda",\n      [&]() {\n        multi_tensor_apply_for_fused_optimizer<4>(\n            tensor_lists,\n            state_steps,\n            FusedAdamMathFunctor<scalar_t, 4, ADAM_MODE::ORIGINAL, false>(),\n            lr_ptr, // unused\n            lr,\n            beta1,\n            beta2,\n            weight_decay,\n            eps,\n            maximize,\n            grad_scale_ptr,\n            found_inf_ptr);\n      });\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\nThe callable function is FusedAdamMathFunctor<scalar_t, 4, ADAM_MODE::ORIGINAL, false>().\n\n\n# MultiTensorApply.cuh\n\n\ntemplate <typename T, typename U, typename... ArgTypes>\nC10_LAUNCH_BOUNDS_1(kBlockSize)\n__global__ void multi_tensor_apply_kernel(\n    T tensorListMeta,\n    U callable,\n    ArgTypes... args) {\n  // Hand the chunk information to the user-supplied functor to process however\n  // it likes.\n  callable(kChunkSize, tensorListMeta, args...);\n}\n\ntemplate <int depth, typename T, typename... ArgTypes>\nvoid multi_tensor_apply_for_fused_optimizer(\n    std::vector<std::vector<at::Tensor>>& tensor_lists,\n    at::TensorList state_steps,\n    T callable,\n    ArgTypes... args) {\n    ...\n    for (const auto& chunk : c10::irange(chunks)) {\n            multi_tensor_apply_kernel<<<\n            loc_block_info,\n            kBlockSize,\n            0,\n            at::cuda::getCurrentCUDAStream()>>>(\n            tensorListMeta, callable, args...);\n        C10_CUDA_KERNEL_LAUNCH_CHECK();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n----------------------------------------\n\n\n# Pytorch linear lowering\n\n\n# python_nn_functions.cpp\n\nDepth 31/29\n\n./torch/csrc/autograd/generated/python_nn_functions.cpp\n\noperator() (__closure=0x7ffc7dcf9e88, input=..., weight=..., bias=...)\n\ntorch::autograd::THPVariable_linear\n\n\n# linear.h\n\nDepth 29\n\n./build/aten/src/ATen/ops/linear.h\n\nat::linear (input=..., weight=..., bias=...)\n\n\n# Operators_0.cpp\n\nat namespace Depth 28\n\nat::_ops::linear::call (input=..., weight=..., bias=...) build/aten/src/ATen/Operators_0.cpp:3601\n\n\n# dispatch\n\nc10 namespace\n\n./aten/src/ATen/Core/dispatch/Dispatcher.h dispatch\n\n\n\n\n# boring part, traversing from dispatcher to actual kernel call\n\n# boxing and unbox\n\nDepth 23\n\nc10 namespace\n\naten/src/ATen/core/boxing/KernelFunction_impl.h 105\n\nc10::callUnboxedKernelFunction at\n\naten/src/ATen/core/boxing/KernelFunction_impl.h:53\n\nc10::impl::wrap_kernel_functor_unboxed_ at\n\naten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:468\n\n# From c10 to at namespace\n\nPlease notice the dispatchkeyset, that is related to dispatch mechanism in pytorch.\n\nc10::impl::detail::WrapFunctionIntoFunctor_ in aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13\\\nto\nat::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__linear in\nbuild/aten/src/ATen/RegisterCompositeImplicitAutograd.cpp:2620\n\n\nDepth 20\n\nat::native::linear (input=..., weight=..., bias_opt=...) at aten/src/ATen/native/Linear.cpp:95\n\nTensor linear(const Tensor& input, const Tensor& weight, const c10::optional<Tensor>& bias_opt) {\n  ...\n  if (input_dim == 2 && bias->defined()) {\n    // Fused op is marginally faster.\n    return at::addmm(*bias, input, weight.t());\n  }\n  ...\n}\n\n\nDepth 19\n\nat::addmm (self=..., mat1=..., mat2=..., beta=..., alpha=...) at build/aten/src/ATen/ops/addmm.h:36\n\nDepth 18\n\nat::_ops::addmm::call (self=..., mat1=..., mat2=..., beta=..., alpha=...) at aten/src/ATen/Operators_0.cpp:7151\n\nDepth 12\n\nc10::impl::detail::WrapFunctionIntoFunctor_<\nc10::CompileTimeFunctionPointer<\nat::Tensor(c10::DispatchKeySet, const at::Tensor&, const at::Tensor&, const at::Tensor&, const c10::Scalar&, const c10::Scalar&),\ntorch::autograd::VariableType::(anonymous namespace)::addmm>,\nat::Tensor,\nc10::guts::typelist::typelist<c10::DispatchKeySet, const at::Tensor&, const at::Tensor&, const at::Tensor&, const c10::Scalar&, const c10::Scalar&>\n>::operator() (args#5=..., args#4=..., args#3=..., args#2=..., args#1=..., args#0=..., this=0x30c7a930)\naten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nto torch::autograd::VariableType::(anonymous namespace)::addmm (ks=..., self=..., mat1=..., mat2=..., beta=..., alpha=...) at aten/src/autograd/generated/VariableType_0.cpp:6898\n\n\n# RedispatchFunction\n\nThis is generated function, but I guess most of redispatch function will be generated here.\n\n👍\n\nat::redispatch::addmm (dispatchKeySet=..., self=..., mat1=..., mat2=..., beta=..., alpha=...) at build/aten/src/ATen/RedispatchFunctions.h:8517\n\nAnd then: at::(anonymous namespace)::wrapper_CUDA_addmm build/aten/src/ATen/RegisterCUDA.cpp\n\nlinear\n\n    // aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor\n    inline at::Tensor linear(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias={}) {\n        return at::_ops::linear::redispatch(dispatchKeySet, input, weight, bias);\n    }\n\n\n1\n2\n3\n4\n\n\nrelu\n\n    inline at::Tensor relu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {\n        return at::_ops::relu::redispatch(dispatchKeySet, self);\n    }\n\n\n1\n2\n3\n\n\nsoftmax\n\n    inline at::Tensor softmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype=c10::nullopt) {\n        return at::_ops::softmax_int::redispatch(dispatchKeySet, self, dim, dtype);\n    }\n\n\n1\n2\n3\n\n\n\n# Blas.cpp\n\nBlas.cpp calls to at::cuda::blas::gemm function.\n\n./aten/src/ATen/native/cuda/Blas.cpp\n\nNotice at::cuda::blas::gemm function. it calls into gemm function.\n\n    // addmm_out_cuda_impl function\n\n    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(\n        at::ScalarType::Half,\n        at::ScalarType::BFloat16,\n        scalar_type,\n        "addmm_cuda",\n        [&] {\n          using opmath_t = at::opmath_type<scalar_t>;\n          opmath_t alpha_val = alpha.to<opmath_t>();\n          opmath_t beta_val = beta.to<opmath_t>();\n          const scalar_t* mat1_ptr = args.mata->const_data_ptr<scalar_t>();\n          const scalar_t* mat2_ptr = args.matb->const_data_ptr<scalar_t>();\n          scalar_t* result_ptr = args.result->mutable_data_ptr<scalar_t>();\n          at::cuda::blas::gemm<scalar_t>(\n              args.transa,\n              args.transb,\n              args.m,\n              args.n,\n              args.k,\n              alpha_val,\n              mat1_ptr,\n              args.lda,\n              mat2_ptr,\n              args.ldb,\n              beta_val,\n              result_ptr,\n              args.result_ld);\n        });\n    switch (activation) {\n      case Activation::RELU:\n        at::relu_(const_cast<Tensor&>(*args.result));\n        break;\n      case Activation::GELU:\n        at::gelu_(const_cast<Tensor&>(*args.result), "tanh");\n        break;\n      default: break;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n',normalizedContent:'# pytorch cuda\n\nbefore dive into deeper source code, this is the instinct of pytorch source code:\n\ngradually lower code from python to cuda code\n\n * torch\n   * pytorch interface, python oriented -./csrc.autograd mechanism\n     * engine\n     * function\n     * variable\n * c10 core functions -cuda memory management -dispatcher mechanism\n * aten\n   * native_functions.yaml\n     * register all implementations with backend specification\n\n    - func: _slow_conv2d_forward.output(tensor self, tensor weight, symint[2] kernel_size,\n      tensor? bias, symint[2] stride, symint[2] padding, *, tensor(a!) output) -> tensor(a!)\n    python_module: nn\n    dispatch:\n    cpu: slow_conv2d_forward_out_cpu\n    cuda: slow_conv2d_forward_out_cuda\n\n\n1\n2\n3\n4\n5\n6\n\n * src/aten/native/core/boxing/\n   * wrapfunctionintofunctor.h support dispatcher, unbox function and call into real implementation\n * ./src/aten/cuda\n   \n   * general functions\n     \n     \n\n * ./src/aten/native/cuda\n   \n   * implementation of different layers\n     \n     \n\npytorch code of torch.randn(), it will calls empty() function to allocate memory and cudacachingallocator malloc will be called.\n\npytorch code of nn.linear(), it will lowered into addmm_out_cuda_impl cuda code in ./aten/src/aten/native/cuda/blas.cpp\n\n\n# pending to be understood\n\nwaiting task: learn deeper how pytorch autograd works related blog:\n\n * http://blog.ezyang.com/2019/05/pytorch-internals/\n * https://www.52coding.com.cn/2019/05/05/pytorch4/\n\n\n# pytorch cuda memory allocation\n\n\n# lower level\n\n./c10/cuda/cudacachingallocator.cpp\n\nblock* malloc(\n      c10::deviceindex device,\n      size_t orig_size,\n      cudastream_t stream) {\n    ...\n    // first, try to get a block from the existing pool.\n    bool block_found =\n        // search pool\n        get_free_block(params)\n        // trigger callbacks and retry search\n        || (trigger_free_memory_callbacks(params) && get_free_block(params));\n    ...\n    if (!block_found) {\n      // do garbage collection if the flag is set.\n      if (c10_unlikely(\n              set_fraction &&\n              cudaallocatorconfig::garbage_collection_threshold() > 0.0)) {\n        garbage_collect_cached_blocks(context);\n      }\n    ...\n      // attempt allocate\n      // warning: alloc_block may release the allocator lock when calling\n      // cudamalloc. so far this function has not modified allocator state, but\n      // keep in mind that any observed allocator state may change across calls\n      // to alloc_block since it may release the lock.\n      block_found = alloc_block(params, false, context, lock)\n          // free enough available cached blocks to satisfy alloc and retry\n          // alloc.\n          || (release_available_cached_blocks(params, context) &&\n              alloc_block(params, false, context, lock))\n          // free all non-split cached blocks and retry alloc.\n          || (c10_likely(captures_underway.size() == 0) &&\n              release_cached_blocks(context) &&\n              alloc_block(params, true, context, lock));\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n./aten/src/aten/cuda\n\n * cachinghostallocator.h\n\ninline torch_cuda_cpp_api at::dataptr hostalloc(size_t size) {\n  return getcachinghostallocator()->allocate(size);\n}\n\n\n1\n2\n3\n\n * cachinghostallocator.cpp\n\nstruct cudacachinghostallocatorimpl\n    : public cachinghostallocatorimpl<cudastream, eventpool::event> {\n private:\n  void allocate_host_memory(size_t size, void** ptr) override {\n    // pinned memory pointers allocated by any device can be directly used by\n    // any other device, regardless of the current device at the time of\n    // allocation, since we assume unified addressing. so we grab any existing\n    // primary context, if available. see pytorch/pytorch#21081.\n    ...\n    // use cudahostalloc for allocating pinned memory (global lock in driver)\n    c10_cuda_check(cudahostalloc(ptr, size, cudahostallocdefault));\n    ...\n    }\n  }\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# intermediate level\n\ncode like cudnn use get_workspace_size to allocate space\n\n./aten/src/aten/native/cuda/mixeddtypeslinear.cu\n\n  // allocate workspace for cutlass mixed datatypes gemm kernel.\n  const auto workspace_size = gemm::get_workspace_size(arguments);\n\n\n1\n2\n\n\ncode like: ./aten/src/aten/native/cuda/foreachreduceop.cu\n\nallocate memory by using at::zeros or at::empty\n\n  auto output_per_tensor = at::zeros(\n      {static_cast<int64_t>(ntensors) * max_chunks_per_tensor}, options);\n\n\n1\n2\n\n\nat::zeros is based on:\n./aten/src/aten/native/cuda/emptytensor.cpp\n\ntensorbase empty_cuda(\n    intarrayref size,\n    scalartype dtype,\n    std::optional<device> device_opt,\n    std::optional<c10::memoryformat> memory_format_opt) {\n  at::globalcontext().lazyinitdevice(c10::devicetype::cuda);\n  const auto device = device_or_default(device_opt);\n  torch_internal_assert(device.is_cuda());\n  const deviceguard device_guard(device);\n  auto* allocator = at::cuda::getcudadeviceallocator();\n  constexpr c10::dispatchkeyset cuda_dks(c10::dispatchkey::cuda);\n  return at::detail::empty_generic(\n      size, allocator, cuda_dks, dtype, memory_format_opt);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n./aten/src/aten/emptytensor.cpp\n\nwe have specify allocator and size, so we will call cuda caching allocator to allocate memory.\n\ntensorbase empty_generic(\n    intarrayref size,\n    c10::allocator* allocator,\n    c10::dispatchkeyset ks,\n    scalartype scalar_type,\n    std::optional<c10::memoryformat> memory_format_opt) {\n  return _empty_generic(size, allocator, ks, scalar_type, memory_format_opt);\n}\n\ntemplate <typename t>\ntensorbase _empty_generic(\n    arrayref<t> size,\n    c10::allocator* allocator,\n    c10::dispatchkeyset ks,\n    scalartype scalar_type,\n    std::optional<c10::memoryformat> memory_format_opt) {\n  at::detail::check_size_nonnegative(size);\n  at::detail::raise_warning_for_complex_half(scalar_type);\n  caffe2::typemeta dtype = scalartypetotypemeta(scalar_type);\n  auto size_bytes = computestoragenbytescontiguous(size, dtype.itemsize());\n  auto storage_impl = c10::make_intrusive<storageimpl>(\n      c10::storageimpl::use_byte_size_t(),\n      size_bytes,\n      allocator,\n      /*resizeable=*/true);\n\n  auto tensor = detail::make_tensor_base<tensorimpl>(\n      std::move(storage_impl), ks, dtype);\n  // default tensorimpl has size [0]\n  // nb: test for meta dispatch key to avoid guarding on zero-ness\n  if (ks.has(c10::dispatchkey::meta) || size.size() != 1 || size[0] != 0) {\n    tensor.unsafegettensorimpl()->generic_set_sizes_contiguous(size);\n  }\n\n  if (memory_format_opt.has_value()) {\n    // restriding a just-created empty contiguous tensor does nothing.\n    if (*memory_format_opt != memoryformat::contiguous) {\n      tensor.unsafegettensorimpl()->empty_tensor_restride(*memory_format_opt);\n    }\n  }\n\n  return tensor;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n\n# python level\n\ninit.pyi.in\n\nclass _cuda_cudaallocator: ...\n\ndef _cuda_customallocator(alloc_fn: _int, free_fn: _int) -> _cuda_cudaallocator: ...\ndef _cuda_changecurrentallocator(allocator: _cuda_cudaallocator) -> none: ...\ndef _cuda_getallocator() -> _cuda_cudaallocator: ...\n\n\n1\n2\n3\n4\n5\n\n\n./torch/cuda/memory.py\n\n__all__ = [\n    "caching_allocator_alloc",\n    "caching_allocator_delete",\n    "caching_allocator_enable",\n    ...\n    "memory_allocated",\n    ...\n]\n\ndef caching_allocator_alloc(size, device: union[device, int] = none, stream=none):\n    r"""perform a memory allocation using the cuda memory allocator.\n\n    memory is allocated for a given device and a stream, this\n    function is intended to be used for interoperability with other\n    frameworks. allocated memory is released through\n    :func:`~torch.cuda.caching_allocator_delete`.\n\n    args:\n        size (int): number of bytes to be allocated.\n        device (torch.device or int, optional): selected device. if it is\n            ``none`` the default cuda device is used.\n        stream (torch.cuda.stream or int, optional): selected stream. if is ``none`` then\n            the default stream for the selected device is used.\n\n    .. note::\n        see :ref:`cuda-memory-management` for more details about gpu memory\n        management.\n    """\n    if device is none:\n        device = torch.cuda.current_device()\n    device = _get_device_index(device)\n    if stream is none:\n        stream = torch.cuda.current_stream(device)\n    if isinstance(stream, torch.cuda.streams.stream):\n        stream = stream.cuda_stream\n    ...\n    with torch.cuda.device(device):\n        return torch._c._cuda_cudacachingallocator_raw_alloc(size, stream)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n./torch/csrc/cuda/module.cpp\n\n    {"_cuda_cudacachingallocator_raw_alloc",\n     thcpmodule_cudacachingallocator_raw_alloc,\n     meth_varargs,\n     nullptr},\n\npyobject* thcpmodule_cudacachingallocator_raw_alloc(\n    pyobject* _unused,\n    pyobject* args) {\n  handle_th_errors\n  pyobject* size_o = nullptr;\n  pyobject* stream_o = nullptr;\n  if (!pyarg_parsetuple(args, "oo", &size_o, &stream_o)) {\n    thputils_invalidarguments(\n        args,\n        nullptr,\n        "caching_allocator_alloc",\n        1,\n        "(ssize_t size, intptr_t stream);");\n    return nullptr;\n  }\n  auto size = pylong_asssize_t(size_o);\n  cudastream_t stream = static_cast<cudastream_t>(pylong_asvoidptr(stream_o));\n  void* mem = nullptr;\n  {\n    pybind11::gil_scoped_release no_gil;\n    mem = c10::cuda::cudacachingallocator::raw_alloc_with_stream(size, stream);\n  }\n  return pylong_fromvoidptr(mem);\n  end_handle_th_errors\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n\n# code gen for memory allocation\n\n./torch/_inductor/codegen/cuda/cuda_kernel.py\n\n    def call_kernel(\n        self,\n        name: str,\n        node: "cudatemplatebuffer",  # type: ignore[name-defined]\n    ) -> none:\n        if node.get_workspace_size() > 0:\n            ws = workspacearg(\n                count=node.get_workspace_size(),\n                device=v.graph.get_current_device_or_throw(),\n                zero_mode=workspacezeromode.uninitialized,\n                outer_name=workspacearg.unique_name(),\n            )\n            wrapper.generate_workspace_allocation(ws)\n            workspace = str(ws.outer_name)\n            call_args.append(\n                workspace\n                if v.graph.cpp_wrapper\n                else f"c_void_p({workspace}.data_ptr())"\n            )\n      .....\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n./torch/_inductor/codegen/wrapper.py\n\n    def generate_workspace_allocation(self, ws: workspacearg):\n        name = ws.get_name()\n        line = allocateline(self, ws)\n        if ws.zero_mode == workspacezeromode.uninitialized:\n            self.writeline(line)\n        elif ws.zero_mode == workspacezeromode.zero_on_call:\n            self.writeline(line)\n            self.writeline(self.make_zero_buffer(name))\n        elif ws.zero_mode == workspacezeromode.zero_per_graph:\n            prior = self.allocated_workspaces.get(name)\n            if prior:\n                assert isinstance(prior, allocateline)\n                # expand existing allocation\n                prior.node = workspacearg.maximum(prior.node, ws)\n            else:\n                self.writeline(line)\n                self.writeline(self.make_zero_buffer(name))\n                self.allocated_workspaces[name] = line\n        else:\n            raise assertionerror(ws.zero_mode)\n\n        if config.triton.autotune_at_compile_time:\n            self.kernel_autotune_calls.writeline(\n                pythonwrappercodegen.make_allocation(\n                    self,\n                    name,\n                    ws.device,\n                    ws.dtype,\n                    shape=(v.graph.sizevars.size_hint(ws.count),),\n                    stride=(1,),\n                )\n            )\n            if ws.zero_mode != workspacezeromode.uninitialized:\n                self.kernel_autotune_calls.writeline(\n                    pythonwrappercodegen.make_zero_buffer(self, name)\n                )\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n# pytorch adam cuda kernel\n\n\n# native_functions.yaml\n\n./aten/src/aten/native/native_functions.yaml\n\n- func: _fused_adam_(tensor(a!)[] self,\ntensor(b!)[] grads,\ntensor(c!)[] exp_avgs,\ntensor(d!)[] exp_avg_sqs,\ntensor(e!)[] max_exp_avg_sqs,\ntensor[] state_steps, *,\nfloat lr, float beta1, float beta2,\nfloat weight_decay, float eps, bool amsgrad,\nbool maximize, tensor? grad_scale=none, tensor? found_inf=none) -> ()\n  # unlike "foreach" functions, lists of tensors should be guaranteed to be on the same device (for now).\n  variants: function\n  dispatch:\n    cpu: _fused_adam_kernel_cpu_\n    cuda: _fused_adam_kernel_cuda_\n    mps: _fused_adam_kernel_mps_\n  autogen: _fused_adam, _fused_adam.out\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# fusedadamkernel.cu\n\n./aten/arc/aten/native/cuda\n\n// the following overload simply has a tensor lr\nvoid _fused_adam_kernel_cuda_(\n    at::tensorlist params,\n    at::tensorlist grads,\n    at::tensorlist exp_avgs,\n    at::tensorlist exp_avg_sqs,\n    at::tensorlist max_exp_avg_sqs,\n    at::tensorlist state_steps,\n    const at::tensor& lr,\n    const double beta1,\n    const double beta2,\n    const double weight_decay,\n    const double eps,\n    const bool amsgrad,\n    const bool maximize,\n    const std::optional<at::tensor>& grad_scale,\n    const std::optional<at::tensor>& found_inf) {\n  if (lr.is_cpu()) {\n    _fused_adam_kernel_cuda_(\n        params,\n        grads,\n        exp_avgs,\n        exp_avg_sqs,\n        max_exp_avg_sqs,\n        state_steps,\n        lr.item<double>(),\n        beta1,\n        beta2,\n        weight_decay,\n        eps,\n        amsgrad,\n        maximize,\n        grad_scale,\n        found_inf);\n    return;\n  }\n\n  // manually check devices since we specify no device check in\n  // native_functions.yaml\n  ...\n\n  if (amsgrad) {\n    ...\n  } else {\n    torch_check(\n        at::native::check_fast_path_restrictions(\n            {params, grads, exp_avgs, exp_avg_sqs}),\n        "params, grads, exp_avgs, and exp_avg_sqs must have same dtype, device, and layout");\n    _fused_adam_cuda_impl_(\n        params,\n        grads,\n        exp_avgs,\n        exp_avg_sqs,\n        state_steps,\n        lr,\n        beta1,\n        beta2,\n        weight_decay,\n        eps,\n        maximize,\n        grad_scale,\n        found_inf);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n\n# fused_adam_impl.cu\n\n./aten/arc/aten/native/cuda\n\nvoid _fused_adam_cuda_impl_(\n    at::tensorlist params,\n    at::tensorlist grads,\n    at::tensorlist exp_avgs,\n    at::tensorlist exp_avg_sqs,\n    at::tensorlist state_steps,\n    const double lr,\n    const double beta1,\n    const double beta2,\n    const double weight_decay,\n    const double eps,\n    const bool maximize,\n    const std::optional<at::tensor>& grad_scale,\n    const std::optional<at::tensor>& found_inf) {\n  std::vector<std::vector<at::tensor>> tensor_lists{\n      params.vec(), grads.vec(), exp_avgs.vec(), exp_avg_sqs.vec()};\n\n  const float* grad_scale_ptr =\n      grad_scale.has_value() ? grad_scale->data_ptr<float>() : nullptr;\n  const float* found_inf_ptr =\n      found_inf.has_value() ? found_inf->data_ptr<float>() : nullptr;\n  const float* lr_ptr = nullptr;\n\n  at_dispatch_floating_types_and2(\n      khalf,\n      kbfloat16,\n      params[0].scalar_type(),\n      "fused_adam_kernel_cuda",\n      [&]() {\n        multi_tensor_apply_for_fused_optimizer<4>(\n            tensor_lists,\n            state_steps,\n            fusedadammathfunctor<scalar_t, 4, adam_mode::original, false>(),\n            lr_ptr, // unused\n            lr,\n            beta1,\n            beta2,\n            weight_decay,\n            eps,\n            maximize,\n            grad_scale_ptr,\n            found_inf_ptr);\n      });\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\n\nthe callable function is fusedadammathfunctor<scalar_t, 4, adam_mode::original, false>().\n\n\n# multitensorapply.cuh\n\n\ntemplate <typename t, typename u, typename... argtypes>\nc10_launch_bounds_1(kblocksize)\n__global__ void multi_tensor_apply_kernel(\n    t tensorlistmeta,\n    u callable,\n    argtypes... args) {\n  // hand the chunk information to the user-supplied functor to process however\n  // it likes.\n  callable(kchunksize, tensorlistmeta, args...);\n}\n\ntemplate <int depth, typename t, typename... argtypes>\nvoid multi_tensor_apply_for_fused_optimizer(\n    std::vector<std::vector<at::tensor>>& tensor_lists,\n    at::tensorlist state_steps,\n    t callable,\n    argtypes... args) {\n    ...\n    for (const auto& chunk : c10::irange(chunks)) {\n            multi_tensor_apply_kernel<<<\n            loc_block_info,\n            kblocksize,\n            0,\n            at::cuda::getcurrentcudastream()>>>(\n            tensorlistmeta, callable, args...);\n        c10_cuda_kernel_launch_check();\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n----------------------------------------\n\n\n# pytorch linear lowering\n\n\n# python_nn_functions.cpp\n\ndepth 31/29\n\n./torch/csrc/autograd/generated/python_nn_functions.cpp\n\noperator() (__closure=0x7ffc7dcf9e88, input=..., weight=..., bias=...)\n\ntorch::autograd::thpvariable_linear\n\n\n# linear.h\n\ndepth 29\n\n./build/aten/src/aten/ops/linear.h\n\nat::linear (input=..., weight=..., bias=...)\n\n\n# operators_0.cpp\n\nat namespace depth 28\n\nat::_ops::linear::call (input=..., weight=..., bias=...) build/aten/src/aten/operators_0.cpp:3601\n\n\n# dispatch\n\nc10 namespace\n\n./aten/src/aten/core/dispatch/dispatcher.h dispatch\n\n\n\n\n# boring part, traversing from dispatcher to actual kernel call\n\n# boxing and unbox\n\ndepth 23\n\nc10 namespace\n\naten/src/aten/core/boxing/kernelfunction_impl.h 105\n\nc10::callunboxedkernelfunction at\n\naten/src/aten/core/boxing/kernelfunction_impl.h:53\n\nc10::impl::wrap_kernel_functor_unboxed_ at\n\naten/src/aten/core/boxing/impl/make_boxed_from_unboxed_functor.h:468\n\n# from c10 to at namespace\n\nplease notice the dispatchkeyset, that is related to dispatch mechanism in pytorch.\n\nc10::impl::detail::wrapfunctionintofunctor_ in aten/src/aten/core/boxing/impl/wrapfunctionintofunctor.h:13\\\nto\nat::(anonymous namespace)::(anonymous namespace)::wrapper_compositeimplicitautograd__linear in\nbuild/aten/src/aten/registercompositeimplicitautograd.cpp:2620\n\n\ndepth 20\n\nat::native::linear (input=..., weight=..., bias_opt=...) at aten/src/aten/native/linear.cpp:95\n\ntensor linear(const tensor& input, const tensor& weight, const c10::optional<tensor>& bias_opt) {\n  ...\n  if (input_dim == 2 && bias->defined()) {\n    // fused op is marginally faster.\n    return at::addmm(*bias, input, weight.t());\n  }\n  ...\n}\n\n\ndepth 19\n\nat::addmm (self=..., mat1=..., mat2=..., beta=..., alpha=...) at build/aten/src/aten/ops/addmm.h:36\n\ndepth 18\n\nat::_ops::addmm::call (self=..., mat1=..., mat2=..., beta=..., alpha=...) at aten/src/aten/operators_0.cpp:7151\n\ndepth 12\n\nc10::impl::detail::wrapfunctionintofunctor_<\nc10::compiletimefunctionpointer<\nat::tensor(c10::dispatchkeyset, const at::tensor&, const at::tensor&, const at::tensor&, const c10::scalar&, const c10::scalar&),\ntorch::autograd::variabletype::(anonymous namespace)::addmm>,\nat::tensor,\nc10::guts::typelist::typelist<c10::dispatchkeyset, const at::tensor&, const at::tensor&, const at::tensor&, const c10::scalar&, const c10::scalar&>\n>::operator() (args#5=..., args#4=..., args#3=..., args#2=..., args#1=..., args#0=..., this=0x30c7a930)\naten/src/aten/core/boxing/impl/wrapfunctionintofunctor.h:13\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nto torch::autograd::variabletype::(anonymous namespace)::addmm (ks=..., self=..., mat1=..., mat2=..., beta=..., alpha=...) at aten/src/autograd/generated/variabletype_0.cpp:6898\n\n\n# redispatchfunction\n\nthis is generated function, but i guess most of redispatch function will be generated here.\n\n👍\n\nat::redispatch::addmm (dispatchkeyset=..., self=..., mat1=..., mat2=..., beta=..., alpha=...) at build/aten/src/aten/redispatchfunctions.h:8517\n\nand then: at::(anonymous namespace)::wrapper_cuda_addmm build/aten/src/aten/registercuda.cpp\n\nlinear\n\n    // aten::linear(tensor input, tensor weight, tensor? bias=none) -> tensor\n    inline at::tensor linear(c10::dispatchkeyset dispatchkeyset, const at::tensor & input, const at::tensor & weight, const c10::optional<at::tensor> & bias={}) {\n        return at::_ops::linear::redispatch(dispatchkeyset, input, weight, bias);\n    }\n\n\n1\n2\n3\n4\n\n\nrelu\n\n    inline at::tensor relu(c10::dispatchkeyset dispatchkeyset, const at::tensor & self) {\n        return at::_ops::relu::redispatch(dispatchkeyset, self);\n    }\n\n\n1\n2\n3\n\n\nsoftmax\n\n    inline at::tensor softmax(c10::dispatchkeyset dispatchkeyset, const at::tensor & self, int64_t dim, c10::optional<at::scalartype> dtype=c10::nullopt) {\n        return at::_ops::softmax_int::redispatch(dispatchkeyset, self, dim, dtype);\n    }\n\n\n1\n2\n3\n\n\n\n# blas.cpp\n\nblas.cpp calls to at::cuda::blas::gemm function.\n\n./aten/src/aten/native/cuda/blas.cpp\n\nnotice at::cuda::blas::gemm function. it calls into gemm function.\n\n    // addmm_out_cuda_impl function\n\n    at_dispatch_floating_and_complex_types_and2(\n        at::scalartype::half,\n        at::scalartype::bfloat16,\n        scalar_type,\n        "addmm_cuda",\n        [&] {\n          using opmath_t = at::opmath_type<scalar_t>;\n          opmath_t alpha_val = alpha.to<opmath_t>();\n          opmath_t beta_val = beta.to<opmath_t>();\n          const scalar_t* mat1_ptr = args.mata->const_data_ptr<scalar_t>();\n          const scalar_t* mat2_ptr = args.matb->const_data_ptr<scalar_t>();\n          scalar_t* result_ptr = args.result->mutable_data_ptr<scalar_t>();\n          at::cuda::blas::gemm<scalar_t>(\n              args.transa,\n              args.transb,\n              args.m,\n              args.n,\n              args.k,\n              alpha_val,\n              mat1_ptr,\n              args.lda,\n              mat2_ptr,\n              args.ldb,\n              beta_val,\n              result_ptr,\n              args.result_ld);\n        });\n    switch (activation) {\n      case activation::relu:\n        at::relu_(const_cast<tensor&>(*args.result));\n        break;\n      case activation::gelu:\n        at::gelu_(const_cast<tensor&>(*args.result), "tanh");\n        break;\n      default: break;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n',charsets:{cjk:!0},lastUpdated:"2025/04/02, 18:08:55"},{title:"CUDA Piceces",frontmatter:{title:"CUDA Piceces",date:"2025-01-10T15:32:49.000Z",permalink:"/pages/f00010/",tags:[null]},regularPath:"/10.mix/10.cuda_pieces.html",relativePath:"10.mix/10.cuda_pieces.md",key:"v-a395c0f6",path:"/pages/f00010/",headers:[{level:2,title:"bank conflict in shared memory",slug:"bank-conflict-in-shared-memory",normalizedTitle:"bank conflict in shared memory",charIndex:19},{level:3,title:"about matrix multiplication",slug:"about-matrix-multiplication",normalizedTitle:"about matrix multiplication",charIndex:484},{level:2,title:"open-source implementation to achieve 95% cuBLAS performance",slug:"open-source-implementation-to-achieve-95-cublas-performance",normalizedTitle:"open-source implementation to achieve 95% cublas performance",charIndex:878},{level:2,title:"Vectorized Memory Access",slug:"vectorized-memory-access",normalizedTitle:"vectorized memory access",charIndex:1010},{level:2,title:"Interesting Repos",slug:"interesting-repos",normalizedTitle:"interesting repos",charIndex:1324}],headersStr:"bank conflict in shared memory about matrix multiplication open-source implementation to achieve 95% cuBLAS performance Vectorized Memory Access Interesting Repos",content:"# CUDA Piceces\n\n\n# bank conflict in shared memory\n\nnvidia-forum\n\nLargest transaction size is 128 bytes(32 thread, 4 byte for each thread).\n\nIf each thread access 16 bytes, Thread0 -Thread7 will issue one transaction.\nThread 8 - Thread15 will issue another transaction.\n\n\n\nIn this case, even thread 0 and thread 8 shares the same bank.\nAs long as they are not in the same transaction, there would be no bank conflict.\n\nUsually we will use tile[TILE_SIZE+1] to avoid bank conflict.\n\n\n# about matrix multiplication\n\n\n\nSource: Programming Massively Parallel Processors\n\nIf matrix M and N are not transposed, I suppose both of them will not meet issue of bank conflict.\n\nIf Threads(0,0) and Threads(0,1) are to calculate P(0,0) and P(0,1),\nthey will read the same element from M, since they are in the same warp.\nThey will read different column from N, in thread coalescing way.\n\n\n# open-source implementation to achieve 95% cuBLAS performance\n\n95% cuBLAS\n\ngithub\n\nMain Idea:\n\n * double buffer\n * async loading\n\n\n# Vectorized Memory Access\n\nVectorized Memory Access- LeiMao CUDA Pro Tip\n\nThis blog conduct experiments on 4byte/8byte/16byte per thread accessing global memory.\n\n16-byte per thread achieves effective memory bandwidth.\n\nThis might explain why float4 works.\n\n * higher memory bandwidth usage\n * less instruction\n\n\n# Interesting Repos\n\n 1. vector loading of array of structures withs warp primitive\n 2. ",normalizedContent:"# cuda piceces\n\n\n# bank conflict in shared memory\n\nnvidia-forum\n\nlargest transaction size is 128 bytes(32 thread, 4 byte for each thread).\n\nif each thread access 16 bytes, thread0 -thread7 will issue one transaction.\nthread 8 - thread15 will issue another transaction.\n\n\n\nin this case, even thread 0 and thread 8 shares the same bank.\nas long as they are not in the same transaction, there would be no bank conflict.\n\nusually we will use tile[tile_size+1] to avoid bank conflict.\n\n\n# about matrix multiplication\n\n\n\nsource: programming massively parallel processors\n\nif matrix m and n are not transposed, i suppose both of them will not meet issue of bank conflict.\n\nif threads(0,0) and threads(0,1) are to calculate p(0,0) and p(0,1),\nthey will read the same element from m, since they are in the same warp.\nthey will read different column from n, in thread coalescing way.\n\n\n# open-source implementation to achieve 95% cublas performance\n\n95% cublas\n\ngithub\n\nmain idea:\n\n * double buffer\n * async loading\n\n\n# vectorized memory access\n\nvectorized memory access- leimao cuda pro tip\n\nthis blog conduct experiments on 4byte/8byte/16byte per thread accessing global memory.\n\n16-byte per thread achieves effective memory bandwidth.\n\nthis might explain why float4 works.\n\n * higher memory bandwidth usage\n * less instruction\n\n\n# interesting repos\n\n 1. vector loading of array of structures withs warp primitive\n 2. ",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"archives",frontmatter:{archivesPage:!0,title:"archives",permalink:"/archives/",article:!1},regularPath:"/@pages/archivesPage.html",relativePath:"@pages/archivesPage.md",key:"v-f8e02736",path:"/archives/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Home",frontmatter:{home:!0,heroText:"Notes in Computer System.",actionText:"Start →",actionLink:"/pages/f27694/",bannerBg:"none",postList:"simple"},regularPath:"/",relativePath:"index.md",key:"v-2f8a0a7e",path:"/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"Add pictures",frontmatter:{title:"Add pictures",date:"2023-05-08T00:00:00.000Z",permalink:"/pages/dfd8e2/",tags:[null]},regularPath:"/pictures/addPictures.html",relativePath:"pictures/addPictures.md",key:"v-01216c79",path:"/pages/dfd8e2/",headersStr:null,content:"Add pictures into this foler",normalizedContent:"add pictures into this foler",charsets:{},lastUpdated:"2025/04/02, 18:08:55"},{title:"标签",frontmatter:{tagsPage:!0,title:"标签",permalink:"/tags/",article:!1},regularPath:"/@pages/tagsPage.html",relativePath:"@pages/tagsPage.md",key:"v-4b579c36",path:"/tags/",headersStr:null,content:"",normalizedContent:"",charsets:{}}],themeConfig:{nav:[{text:"Home",link:"/"},{text:"gpu",link:"/gpu/"},{text:"cpu",link:"/cpu/"},{text:"ml&llm",link:"/llm/"},{text:"compiler",link:"/compiler/"},{text:"hbm",link:"/hbm/"},{text:"program",link:"/mix/"},{text:"unix",link:"/unix/"},{text:"CSDN",link:"https://blog.csdn.net/hit_shaoqi"}],sidebarDepth:3,repo:"hitqshao/qishao-notes",searchMaxSuggestions:10,lastUpdated:"上次更新",editLinks:!0,docsDir:"docs",docsBranch:"main",editLinkText:"帮助我们改善此页面",searchPlaceholder:"按下 𝑺 搜索",category:!1,tag:!0,contentBgStyle:6,sidebar:{"/00.目录页/":[["00.Content.md","Content","/pages/f27694/"],["01.hbm.md","HBM","/hbm/"],["02.compiler.md","llvm & mlir","/compiler/"],["03.gpu.md","gpu","/gpu/"],["04.cpu.md","cpu","/cpu/"],["05.llm.md","llm","/llm/"],["06.unix.md","unix","/unix/"],["07.mix.md","programing & mix","/mix/"]],catalogue:{hbm:"/hbm/",compiler:"/compiler/",gpu:"/gpu/",cpu:"/cpu/",llm:"/llm/",unix:"/unix/",mix:"/mix/"},"/01.hbm/":[["01.HBM_Paper_List.md","HBM Paper List","/pages/24769e/"],["02.hbm_dead_block_predictor.md","HBM Dead Block Predictor","/pages/2476af/"],["03.Dynamically_Adapting _Page_Migration_Policies_Based_on_Applications_Memory_Access_Behaviors.md","Dynamically Adapting  Page Migration Policies Based on Applications Memory Access Behaviors","/pages/24769f/"],["04.DRAM_PCM_NVM_Cache.md","DRAM PCM NVM Cache","/pages/24760e/"],["05.cache_mem_compression.md","Cache Memory Compression","/pages/2476bf/"],["06.memory ecc.md","memory-ecc","/pages/f07695/"],["07.hbm-latency.md","hbm-latency","/pages/f07696/"],["08.compression.md","compression","/pages/f07698/"],["09.compressibility_prediction.md","compressibility prediction","/pages/f07699/"],["10.software_memory_paper.md","memory management","/pages/f07692/"]],"/02.compiler/":[["01.llvm_flow.md","llvm flow","/pages/000001/"],["02.GetStartedLLVMChap5Notes.md","Getting Started with LLVM Core Libraries Chap5 IR","/pages/000002/"],["03.GetStartedLLVMChap6Notes.md","Getting Started with LLVM Core Libraries Chap6 Backend","/pages/000003/"],["04. LearningLLVMDiary0.md","Learning LLVM Notes","/pages/000004/"],["05. addInstACE.md","Add New DIY Instruction ACE to LLVM","/pages/000005/"],["06.Value&Use.md","How does LLVM perform instruction combine","/pages/000006/"],["07. UnderstaningLLVMwithSourceCode.md","Understand llvm with its source code","/pages/000007/"],["08.write_tinyriscv_backend.md","Writing TinyRISCV Backend","/pages/000008/"],["09.learn_tvm_1.md","Learn TVM","/pages/000009/"],["10.learn_tpu_mlir.md","Learn TPU_MLIR","/pages/000010/"],["11.learn_mlir_toy.md","MLIR TOY Tutorial","/pages/000011/"],["12.auto_diff.md","Auto Differentiation in Compiler","/pages/000012/"],["13.mlir_notes_01.md","MLIR Open Meeting Notes The Torch MLIR Project","/pages/000013/"],["14.mlir_notes_02.md","MLIR Compiling Flow of Conv2D","/pages/000014/"],["15.mlir_notes_03.md","MLIR Compiling Flow of Conv2D","/pages/000015/"],["16.mlir_notes_04.md","MLIR Compiling Flow of Transformer-Decoder","/pages/000016/"],["17.mlir_notes_05.md","MLIR Essential Concepts","/pages/000017/"],["18.mlir_notes_06.md","MLIR NVGPU Dialect","/pages/000018/"],["19.mlir_notes_07.md","MLIR Linalg Dialect","/pages/000019/"],["20.mlir_notes_08.md","MLIR Bufferization","/pages/000020/"],["21.mlir_notes_09.md","MLIR Bufferization Passes","/pages/000021/"]],"/03.gpu/":[["01.operand_collector.md","Operand Collector","/pages/cc7034/"],["02.warp_execution.md","GPU WARP Scheduler","/pages/2476ae/"],["03.Precise Exception.md","Precision Exception","/pages/14769f/"],["04.Unified_Memory.md","Unified Memory Paper List","/pages/44771e/"],["05.TensorCore.md","TensorCore Paper List","/pages/44871e/"],["06.MemoryBehaviour.md","Memory Behaviour Paper List","/pages/45871e/"],["07.GPUVirtualization.md","GPU Virtualization Paper List","/pages/45871f/"],["08.LLM.md","Large Language Model Paper List","/pages/458720/"],["09.Simulator.md","GPU Simulator","/pages/458721/"],["10. Architectural Survey.md","Architectural Survey","/pages/458722/"],["11.IntegratedCPUGPUMemory.md","Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper","/pages/458724/"],["12.gpgpusim.md","Understanding GPGPU-SIM 1 How to get Instruction","/pages/458725/"],["13.gpgpusim.md","Understanding GPGPU-SIM 2 Instruction Execution","/pages/458726/"],["14.gpgpusim.md","Understanding GPGPU-SIM 3 How is the simulation started","/pages/458727/"],["15.gpgpusim.md","Understanding GPGPU-SIM 4 Microarchitecture","/pages/45872/"],["16.gpgpusim.md","Understanding GPGPU-SIM 5  Memory Interface","/pages/45874/"],["17.warp_mem.md","Warp Related Memory Optimization","/pages/45873/"],["18.gpucoherency.md","GPU Cache Coherency","/pages/45875/"],["19.gpu_cache_mem.md","GPU Cache & Memory Hirerarchy","/pages/45876/"],["20.gpu_tlb.md","GPU TLB","/pages/45877/"],["21.gpu_ptw.md","GPU Page Table Walk","/pages/45878/"],["22.gpu_cache_paper.md","GPU Cache's Papers","/pages/45879/"],["23.gpu_warp_paper.md","GPU WARP Mangement Papers","/pages/45880/"],["24.gpu_novel_um.md","GPU Unified Memory Innovations","/pages/45882/"],["25.gpu_multitask.md","GPU MultiTask","/pages/45883/"],["26.gpu_cuda_training.md","GPU Training Notes","/pages/45884/"],["27.gpu_paper_code.md","GPU Paper with Code","/pages/45885/"],["28.gpu_runtime.md","GPU Driver & Runtime & Compliation","/pages/45886/"],["29.accel_sim.md","Accel-Sim Simulator","/pages/45887/"],["30.gpgpusim.md","Understanding GPGPU-SIM 6 Memory Space","/pages/45889/"],["31.gpu_inst.md","GPU Insturctions","/pages/45890/"],["1234.TODO.md","TO READ","/pages/47871e/"]],"/04.cpu/":[["01.checkpoint.md","checkpoint","/pages/cc7035/"],["02.trend.md","Trend","/pages/cc7036/"],["03.loadstore.md","load store unit","/pages/cc7037/"],["05.cache structure.md","cache & bank structure","/pages/cc7038/"],["06.cache timing.md","cache timing","/pages/cc7039/"],["07.register file.md","cache timing","/pages/cc7040/"],["08. riscv.md","RISC-V Design","/pages/cc7041/"],["1234.markdown.md","two-test-1","/pages/f07697/"]],"/05.llm/":[["01.How_LLM_Works.md","how LLM works","/pages/dc7035/"],["02.LLM_HW_Opt.md","LLM Hardware Optimization","/pages/dc7036/"],["03.gem5_LLAMA.md","How to run llama.cpp with gem5","/pages/dc7037/"],["04.mem_usage_llm.md","Memory Usage in Training LLM","/pages/dc7038/"],["05.llm_optimizations.md","LLM optimizations","/pages/dc7039/"],["06.llm_flash.md","LLM flash algorthms","/pages/dc7040/"],["07.llm_bound.md","LLM compute & memory bound","/pages/dc7041/"],["08.llm_internals.md","LLM Paper List","/pages/dc7042/"],["09.eff_llm.md","Efficient LLM","/pages/dc7043/"],["10.llm_production.md","Estimation of LLM","/pages/dc7045/"],["11.inner_working.md","Summery of Inner Workings of LLM","/pages/dc7046/"],["12.llm_opt_list.md","List of LLM Optimization Techniques","/pages/dc7047/"],["13.llm_mem_opt.md","Memory Optimizations in LLM","/pages/dc7048/"],["14.llm_reasoning.md","Reasoning in LLM","/pages/dc7049/"],["15.llm_quant.md","LLM Mixed Precision & Quantization & Outlier","/pages/dc7050/"],["16.llm_sparsity.md","LLM Sparsity","/pages/dc7051/"],["17.llm_scale.md","LLM Scaling Law","/pages/dc7052/"],["18.llm_att.md","LLM Attention","/pages/dc7055/"],["19.llm_kv_manage.md","LLM KV Cache Management","/pages/dc7056/"],["20. llm_distr.md","LLM Distributed Machine Learning","/pages/dc7057/"],["21.llm_internals.md","LLM Internals","/pages/dc7059/"],["22.llm_post.md","LLM Posttraining/Finetuning","/pages/dc7058/"],["23.llm_moe.md","LLM MOE Inference","/pages/dc7060/"],["24.llm_comp.md","LLM Compression","/pages/dc7061/"],["25.llm_optimizer.md","LLM Compression","/pages/dc7062/"]],"/06.unix/":[["01.malloc.md","how malloc works","/pages/ec7035/"],["02.op.md","No More Asking","/pages/ec7036/"]],"/09.nine/":[["01.留言板.md","留言板","/message-board"],["02.template.md","template","/pages/ee5bf2/"]],"/10.mix/":[["01.leakagecurrent.md","leakage current moores law meets static power","/pages/f00000/"],["02.cuda_blogs.md","Blogs to be read","/pages/f00001/"],["03.learn_pytorch_source.md","Understanding Pytorch Source Code","/pages/f00002/"],["04.learn_pytorch_source_aot.md","Understanding Pytorch Source Code AOT - Inductor IR - Codegen","/pages/f00003/"],["05.cuda_merge.md","CUDA Merge","/pages/f00004/"],["06.cuda_softmax.md","CUDA Softmax","/pages/f00006/"],["07.cuda_layernorm.md","CUDA LayerNorm","/pages/f00007/"],["08.learn_pytorch_cuda.md","Understanding Pytorch CUDA","/pages/f00008/"],["09.learn_pytorch_source_torch.md","Understanding Pytorch Source Torch","/pages/f00009/"],["10.cuda_pieces.md","CUDA Piceces","/pages/f00010/"]]},pageStyle:"line",updateBar:{showToArticle:!1},author:{name:"hitqishao",link:"https://github.com/hitqshao"},social:{icons:[{iconClass:"icon-github",title:"GitHub",link:"https://github.com/hitqshao"},{iconClass:"icon-youjian",title:"发邮件",link:"mailto:hitqshao@163.com"},{iconClass:"icon-gitee",title:"Gitee",link:"https://gitee.com/hitqshao"}]},footer:{createYear:2022,copyrightInfo:'Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a>'}}};var ll=t(151),cl=t(152),dl=t(21);var hl={computed:{$filterPosts(){return this.$site.pages.filter(e=>{const{frontmatter:{pageComponent:n,article:t,home:a}}=e;return!(n||!1===t||!0===a)})},$sortPosts(){return(e=this.$filterPosts).sort((e,n)=>{const t=e.frontmatter.sticky,a=n.frontmatter.sticky;return t&&a?t==a?Object(dl.a)(e,n):t-a:t&&!a?-1:!t&&a?1:Object(dl.a)(e,n)}),e;var e},$sortPostsByDate(){return(e=this.$filterPosts).sort((e,n)=>Object(dl.a)(e,n)),e;var e},$groupPosts(){return function(e){const n={},t={};for(let a=0,i=e.length;a<i;a++){const{frontmatter:{categories:i,tags:r}}=e[a];"array"===Object(dl.n)(i)&&i.forEach(t=>{t&&(n[t]||(n[t]=[]),n[t].push(e[a]))}),"array"===Object(dl.n)(r)&&r.forEach(n=>{n&&(t[n]||(t[n]=[]),t[n].push(e[a]))})}return{categories:n,tags:t}}(this.$sortPosts)},$categoriesAndTags(){return function(e){const n=[],t=[];for(let t in e.categories)n.push({key:t,length:e.categories[t].length});for(let n in e.tags)t.push({key:n,length:e.tags[n].length});return{categories:n,tags:t}}(this.$groupPosts)}}};Wt.component(ll.default),Wt.component(cl.default);function ml(e){return e.toString().padStart(2,"0")}t(306);Wt.component("Badge",()=>Promise.all([t.e(0),t.e(3)]).then(t.bind(null,581))),Wt.component("CodeBlock",()=>Promise.resolve().then(t.bind(null,151))),Wt.component("CodeGroup",()=>Promise.resolve().then(t.bind(null,152)));t(307);
/**
  * vue-class-component v7.2.6
  * (c) 2015-present Evan You
  * @license MIT
  */
function ul(e){return(ul="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e})(e)}function pl(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function gl(e){return function(e){if(Array.isArray(e)){for(var n=0,t=new Array(e.length);n<e.length;n++)t[n]=e[n];return t}}(e)||function(e){if(Symbol.iterator in Object(e)||"[object Arguments]"===Object.prototype.toString.call(e))return Array.from(e)}(e)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance")}()}function fl(){return"undefined"!=typeof Reflect&&Reflect.defineMetadata&&Reflect.getOwnMetadataKeys}function yl(e,n){bl(e,n),Object.getOwnPropertyNames(n.prototype).forEach((function(t){bl(e.prototype,n.prototype,t)})),Object.getOwnPropertyNames(n).forEach((function(t){bl(e,n,t)}))}function bl(e,n,t){(t?Reflect.getOwnMetadataKeys(n,t):Reflect.getOwnMetadataKeys(n)).forEach((function(a){var i=t?Reflect.getOwnMetadata(a,n,t):Reflect.getOwnMetadata(a,n);t?Reflect.defineMetadata(a,i,e,t):Reflect.defineMetadata(a,i,e)}))}var vl={__proto__:[]}instanceof Array;function wl(e){return function(n,t,a){var i="function"==typeof n?n:n.constructor;i.__decorators__||(i.__decorators__=[]),"number"!=typeof a&&(a=void 0),i.__decorators__.push((function(n){return e(n,t,a)}))}}function _l(e,n){var t=n.prototype._init;n.prototype._init=function(){var n=this,t=Object.getOwnPropertyNames(e);if(e.$options.props)for(var a in e.$options.props)e.hasOwnProperty(a)||t.push(a);t.forEach((function(t){Object.defineProperty(n,t,{get:function(){return e[t]},set:function(n){e[t]=n},configurable:!0})}))};var a=new n;n.prototype._init=t;var i={};return Object.keys(a).forEach((function(e){void 0!==a[e]&&(i[e]=a[e])})),i}var kl=["data","beforeCreate","created","beforeMount","mounted","beforeDestroy","destroyed","beforeUpdate","updated","activated","deactivated","render","errorCaptured","serverPrefetch"];function xl(e){var n=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};n.name=n.name||e._componentTag||e.name;var t=e.prototype;Object.getOwnPropertyNames(t).forEach((function(e){if("constructor"!==e)if(kl.indexOf(e)>-1)n[e]=t[e];else{var a=Object.getOwnPropertyDescriptor(t,e);void 0!==a.value?"function"==typeof a.value?(n.methods||(n.methods={}))[e]=a.value:(n.mixins||(n.mixins=[])).push({data:function(){return pl({},e,a.value)}}):(a.get||a.set)&&((n.computed||(n.computed={}))[e]={get:a.get,set:a.set})}})),(n.mixins||(n.mixins=[])).push({data:function(){return _l(this,e)}});var a=e.__decorators__;a&&(a.forEach((function(e){return e(n)})),delete e.__decorators__);var i=Object.getPrototypeOf(e.prototype),r=i instanceof Wt?i.constructor:Wt,o=r.extend(n);return zl(o,e,r),fl()&&yl(o,e),o}var Tl={prototype:!0,arguments:!0,callee:!0,caller:!0};function zl(e,n,t){Object.getOwnPropertyNames(n).forEach((function(a){if(!Tl[a]){var i=Object.getOwnPropertyDescriptor(e,a);if(!i||i.configurable){var r,o,s=Object.getOwnPropertyDescriptor(n,a);if(!vl){if("cid"===a)return;var l=Object.getOwnPropertyDescriptor(t,a);if(r=s.value,o=ul(r),null!=r&&("object"===o||"function"===o)&&l&&l.value===s.value)return}0,Object.defineProperty(e,a,s)}}}))}function Ml(e){return"function"==typeof e?xl(e):function(n){return xl(n,e)}}Ml.registerHooks=function(e){kl.push.apply(kl,gl(e))};var Pl=Ml;function Cl(e){return wl((function(n,t){void 0===n.inject&&(n.inject={}),Array.isArray(n.inject)||(n.inject[t]=e||t)}))}function Al(e){var n=function(){var t=this,a="function"==typeof e?e.call(this):e;for(var i in(a=Object.create(a||null)).__reactiveInject__=this.__reactiveInject__||{},n.managed)a[n.managed[i]]=this[i];var r=function(e){a[n.managedReactive[e]]=o[e],Object.defineProperty(a.__reactiveInject__,n.managedReactive[e],{enumerable:!0,get:function(){return t[e]}})},o=this;for(var i in n.managedReactive)r(i);return a};return n.managed={},n.managedReactive={},n}function Il(e){return"function"!=typeof e||!e.managed&&!e.managedReactive}var Sl="undefined"!=typeof Reflect&&void 0!==Reflect.getMetadata;function Ll(e,n,t){if(Sl&&!Array.isArray(e)&&"function"!=typeof e&&void 0===e.type){var a=Reflect.getMetadata("design:type",n,t);a!==Object&&(e.type=a)}}function Ul(e){return void 0===e&&(e={}),function(n,t){Ll(e,n,t),wl((function(n,t){(n.props||(n.props={}))[t]=e}))(n,t)}}function ql(e,n){void 0===n&&(n={});var t=n.deep,a=void 0!==t&&t,i=n.immediate,r=void 0!==i&&i;return wl((function(n,t){"object"!=typeof n.watch&&(n.watch=Object.create(null));var i=n.watch;"object"!=typeof i[e]||Array.isArray(i[e])?void 0===i[e]&&(i[e]=[]):i[e]=[i[e]],i[e].push({handler:t,deep:a,immediate:r})}))}var El=t(28);const Rl=(e,n)=>`${e}${Object(El.stringify)(n,{addQueryPrefix:!0})}`,Dl=(e,n)=>`${e.replace(/\/$/,"")}/${n.replace(/^\//,"")}`;var Ol=t(148),Gl=t.n(Ol);const Fl=e=>Gl()(e,"YYYY-MM-DD HH:mm:ss"),Bl=e=>(e.split("#")[0]||"").split("?")[0]||"";
/*!
 * vue-i18n v8.28.2 
 * (c) 2022 kazuya kawaguchi
 * Released under the MIT License.
 */
var Nl=["compactDisplay","currency","currencyDisplay","currencySign","localeMatcher","notation","numberingSystem","signDisplay","style","unit","unitDisplay","useGrouping","minimumIntegerDigits","minimumFractionDigits","maximumFractionDigits","minimumSignificantDigits","maximumSignificantDigits"],jl=["dateStyle","timeStyle","calendar","localeMatcher","hour12","hourCycle","timeZone","formatMatcher","weekday","era","year","month","day","hour","minute","second","timeZoneName"];function Vl(e,n){"undefined"!=typeof console&&(console.warn("[vue-i18n] "+e),n&&console.warn(n.stack))}var Hl=Array.isArray;function $l(e){return null!==e&&"object"==typeof e}function Wl(e){return"string"==typeof e}var Kl=Object.prototype.toString;function Ql(e){return"[object Object]"===Kl.call(e)}function Xl(e){return null==e}function Yl(e){return"function"==typeof e}function Zl(){for(var e=[],n=arguments.length;n--;)e[n]=arguments[n];var t=null,a=null;return 1===e.length?$l(e[0])||Hl(e[0])?a=e[0]:"string"==typeof e[0]&&(t=e[0]):2===e.length&&("string"==typeof e[0]&&(t=e[0]),($l(e[1])||Hl(e[1]))&&(a=e[1])),{locale:t,params:a}}function Jl(e){return JSON.parse(JSON.stringify(e))}function ec(e,n){return!!~e.indexOf(n)}var nc=Object.prototype.hasOwnProperty;function tc(e,n){return nc.call(e,n)}function ac(e){for(var n=arguments,t=Object(e),a=1;a<arguments.length;a++){var i=n[a];if(null!=i){var r=void 0;for(r in i)tc(i,r)&&($l(i[r])?t[r]=ac(t[r],i[r]):t[r]=i[r])}}return t}function ic(e,n){if(e===n)return!0;var t=$l(e),a=$l(n);if(!t||!a)return!t&&!a&&String(e)===String(n);try{var i=Hl(e),r=Hl(n);if(i&&r)return e.length===n.length&&e.every((function(e,t){return ic(e,n[t])}));if(i||r)return!1;var o=Object.keys(e),s=Object.keys(n);return o.length===s.length&&o.every((function(t){return ic(e[t],n[t])}))}catch(e){return!1}}function rc(e){return null!=e&&Object.keys(e).forEach((function(n){"string"==typeof e[n]&&(e[n]=e[n].replace(/</g,"&lt;").replace(/>/g,"&gt;").replace(/"/g,"&quot;").replace(/'/g,"&apos;"))})),e}var oc={name:"i18n",functional:!0,props:{tag:{type:[String,Boolean,Object],default:"span"},path:{type:String,required:!0},locale:{type:String},places:{type:[Array,Object]}},render:function(e,n){var t=n.data,a=n.parent,i=n.props,r=n.slots,o=a.$i18n;if(o){var s=i.path,l=i.locale,c=i.places,d=r(),h=o.i(s,l,function(e){var n;for(n in e)if("default"!==n)return!1;return Boolean(n)}(d)||c?function(e,n){var t=n?function(e){0;return Array.isArray(e)?e.reduce(lc,{}):Object.assign({},e)}(n):{};if(!e)return t;var a=(e=e.filter((function(e){return e.tag||""!==e.text.trim()}))).every(cc);0;return e.reduce(a?sc:lc,t)}(d.default,c):d),m=i.tag&&!0!==i.tag||!1===i.tag?i.tag:"span";return m?e(m,t,h):h}}};function sc(e,n){return n.data&&n.data.attrs&&n.data.attrs.place&&(e[n.data.attrs.place]=n),e}function lc(e,n,t){return e[t]=n,e}function cc(e){return Boolean(e.data&&e.data.attrs&&e.data.attrs.place)}var dc,hc={name:"i18n-n",functional:!0,props:{tag:{type:[String,Boolean,Object],default:"span"},value:{type:Number,required:!0},format:{type:[String,Object]},locale:{type:String}},render:function(e,n){var t=n.props,a=n.parent,i=n.data,r=a.$i18n;if(!r)return null;var o=null,s=null;Wl(t.format)?o=t.format:$l(t.format)&&(t.format.key&&(o=t.format.key),s=Object.keys(t.format).reduce((function(e,n){var a;return ec(Nl,n)?Object.assign({},e,((a={})[n]=t.format[n],a)):e}),null));var l=t.locale||r.locale,c=r._ntp(t.value,l,o,s),d=c.map((function(e,n){var t,a=i.scopedSlots&&i.scopedSlots[e.type];return a?a(((t={})[e.type]=e.value,t.index=n,t.parts=c,t)):e.value})),h=t.tag&&!0!==t.tag||!1===t.tag?t.tag:"span";return h?e(h,{attrs:i.attrs,class:i.class,staticClass:i.staticClass},d):d}};function mc(e,n,t){gc(e,t)&&fc(e,n,t)}function uc(e,n,t,a){if(gc(e,t)){var i=t.context.$i18n;(function(e,n){var t=n.context;return e._locale===t.$i18n.locale})(e,t)&&ic(n.value,n.oldValue)&&ic(e._localeMessage,i.getLocaleMessage(i.locale))||fc(e,n,t)}}function pc(e,n,t,a){if(t.context){var i=t.context.$i18n||{};n.modifiers.preserve||i.preserveDirectiveContent||(e.textContent=""),e._vt=void 0,delete e._vt,e._locale=void 0,delete e._locale,e._localeMessage=void 0,delete e._localeMessage}else Vl("Vue instance does not exists in VNode context")}function gc(e,n){var t=n.context;return t?!!t.$i18n||(Vl("VueI18n instance does not exists in Vue instance"),!1):(Vl("Vue instance does not exists in VNode context"),!1)}function fc(e,n,t){var a,i,r=function(e){var n,t,a,i;Wl(e)?n=e:Ql(e)&&(n=e.path,t=e.locale,a=e.args,i=e.choice);return{path:n,locale:t,args:a,choice:i}}(n.value),o=r.path,s=r.locale,l=r.args,c=r.choice;if(o||s||l)if(o){var d=t.context;e._vt=e.textContent=null!=c?(a=d.$i18n).tc.apply(a,[o,c].concat(yc(s,l))):(i=d.$i18n).t.apply(i,[o].concat(yc(s,l))),e._locale=d.$i18n.locale,e._localeMessage=d.$i18n.getLocaleMessage(d.$i18n.locale)}else Vl("`path` is required in v-t directive");else Vl("value type not supported")}function yc(e,n){var t=[];return e&&t.push(e),n&&(Array.isArray(n)||Ql(n))&&t.push(n),t}function bc(e,n){void 0===n&&(n={bridge:!1}),bc.installed=!0;(dc=e).version&&Number(dc.version.split(".")[0]);(function(e){e.prototype.hasOwnProperty("$i18n")||Object.defineProperty(e.prototype,"$i18n",{get:function(){return this._i18n}}),e.prototype.$t=function(e){for(var n=[],t=arguments.length-1;t-- >0;)n[t]=arguments[t+1];var a=this.$i18n;return a._t.apply(a,[e,a.locale,a._getMessages(),this].concat(n))},e.prototype.$tc=function(e,n){for(var t=[],a=arguments.length-2;a-- >0;)t[a]=arguments[a+2];var i=this.$i18n;return i._tc.apply(i,[e,i.locale,i._getMessages(),this,n].concat(t))},e.prototype.$te=function(e,n){var t=this.$i18n;return t._te(e,t.locale,t._getMessages(),n)},e.prototype.$d=function(e){for(var n,t=[],a=arguments.length-1;a-- >0;)t[a]=arguments[a+1];return(n=this.$i18n).d.apply(n,[e].concat(t))},e.prototype.$n=function(e){for(var n,t=[],a=arguments.length-1;a-- >0;)t[a]=arguments[a+1];return(n=this.$i18n).n.apply(n,[e].concat(t))}})(dc),dc.mixin(function(e){function n(){this!==this.$root&&this.$options.__INTLIFY_META__&&this.$el&&this.$el.setAttribute("data-intlify",this.$options.__INTLIFY_META__)}return void 0===e&&(e=!1),e?{mounted:n}:{beforeCreate:function(){var e=this.$options;if(e.i18n=e.i18n||(e.__i18nBridge||e.__i18n?{}:null),e.i18n)if(e.i18n instanceof qc){if(e.__i18nBridge||e.__i18n)try{var n=e.i18n&&e.i18n.messages?e.i18n.messages:{};(e.__i18nBridge||e.__i18n).forEach((function(e){n=ac(n,JSON.parse(e))})),Object.keys(n).forEach((function(t){e.i18n.mergeLocaleMessage(t,n[t])}))}catch(e){0}this._i18n=e.i18n,this._i18nWatcher=this._i18n.watchI18nData()}else if(Ql(e.i18n)){var t=this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof qc?this.$root.$i18n:null;if(t&&(e.i18n.root=this.$root,e.i18n.formatter=t.formatter,e.i18n.fallbackLocale=t.fallbackLocale,e.i18n.formatFallbackMessages=t.formatFallbackMessages,e.i18n.silentTranslationWarn=t.silentTranslationWarn,e.i18n.silentFallbackWarn=t.silentFallbackWarn,e.i18n.pluralizationRules=t.pluralizationRules,e.i18n.preserveDirectiveContent=t.preserveDirectiveContent),e.__i18nBridge||e.__i18n)try{var a=e.i18n&&e.i18n.messages?e.i18n.messages:{};(e.__i18nBridge||e.__i18n).forEach((function(e){a=ac(a,JSON.parse(e))})),e.i18n.messages=a}catch(e){0}var i=e.i18n.sharedMessages;i&&Ql(i)&&(e.i18n.messages=ac(e.i18n.messages,i)),this._i18n=new qc(e.i18n),this._i18nWatcher=this._i18n.watchI18nData(),(void 0===e.i18n.sync||e.i18n.sync)&&(this._localeWatcher=this.$i18n.watchLocale()),t&&t.onComponentInstanceCreated(this._i18n)}else 0;else this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof qc?this._i18n=this.$root.$i18n:e.parent&&e.parent.$i18n&&e.parent.$i18n instanceof qc&&(this._i18n=e.parent.$i18n)},beforeMount:function(){var e=this.$options;e.i18n=e.i18n||(e.__i18nBridge||e.__i18n?{}:null),e.i18n?(e.i18n instanceof qc||Ql(e.i18n))&&(this._i18n.subscribeDataChanging(this),this._subscribing=!0):(this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof qc||e.parent&&e.parent.$i18n&&e.parent.$i18n instanceof qc)&&(this._i18n.subscribeDataChanging(this),this._subscribing=!0)},mounted:n,beforeDestroy:function(){if(this._i18n){var e=this;this.$nextTick((function(){e._subscribing&&(e._i18n.unsubscribeDataChanging(e),delete e._subscribing),e._i18nWatcher&&(e._i18nWatcher(),e._i18n.destroyVM(),delete e._i18nWatcher),e._localeWatcher&&(e._localeWatcher(),delete e._localeWatcher)}))}}}}(n.bridge)),dc.directive("t",{bind:mc,update:uc,unbind:pc}),dc.component(oc.name,oc),dc.component(hc.name,hc),dc.config.optionMergeStrategies.i18n=function(e,n){return void 0===n?e:n}}var vc=function(){this._caches=Object.create(null)};vc.prototype.interpolate=function(e,n){if(!n)return[e];var t=this._caches[e];return t||(t=function(e){var n=[],t=0,a="";for(;t<e.length;){var i=e[t++];if("{"===i){a&&n.push({type:"text",value:a}),a="";var r="";for(i=e[t++];void 0!==i&&"}"!==i;)r+=i,i=e[t++];var o="}"===i,s=wc.test(r)?"list":o&&_c.test(r)?"named":"unknown";n.push({value:r,type:s})}else"%"===i?"{"!==e[t]&&(a+=i):a+=i}return a&&n.push({type:"text",value:a}),n}(e),this._caches[e]=t),function(e,n){var t=[],a=0,i=Array.isArray(n)?"list":$l(n)?"named":"unknown";if("unknown"===i)return t;for(;a<e.length;){var r=e[a];switch(r.type){case"text":t.push(r.value);break;case"list":t.push(n[parseInt(r.value,10)]);break;case"named":"named"===i&&t.push(n[r.value]);break;case"unknown":0}a++}return t}(t,n)};var wc=/^(?:\d)+/,_c=/^(?:\w)+/;var kc=[];kc[0]={ws:[0],ident:[3,0],"[":[4],eof:[7]},kc[1]={ws:[1],".":[2],"[":[4],eof:[7]},kc[2]={ws:[2],ident:[3,0],0:[3,0],number:[3,0]},kc[3]={ident:[3,0],0:[3,0],number:[3,0],ws:[1,1],".":[2,1],"[":[4,1],eof:[7,1]},kc[4]={"'":[5,0],'"':[6,0],"[":[4,2],"]":[1,3],eof:8,else:[4,0]},kc[5]={"'":[4,0],eof:8,else:[5,0]},kc[6]={'"':[4,0],eof:8,else:[6,0]};var xc=/^\s?(?:true|false|-?[\d.]+|'[^']*'|"[^"]*")\s?$/;function Tc(e){if(null==e)return"eof";switch(e.charCodeAt(0)){case 91:case 93:case 46:case 34:case 39:return e;case 95:case 36:case 45:return"ident";case 9:case 10:case 13:case 160:case 65279:case 8232:case 8233:return"ws"}return"ident"}function zc(e){var n,t,a,i=e.trim();return("0"!==e.charAt(0)||!isNaN(e))&&(a=i,xc.test(a)?(t=(n=i).charCodeAt(0))!==n.charCodeAt(n.length-1)||34!==t&&39!==t?n:n.slice(1,-1):"*"+i)}var Mc=function(){this._cache=Object.create(null)};Mc.prototype.parsePath=function(e){var n=this._cache[e];return n||(n=function(e){var n,t,a,i,r,o,s,l=[],c=-1,d=0,h=0,m=[];function u(){var n=e[c+1];if(5===d&&"'"===n||6===d&&'"'===n)return c++,a="\\"+n,m[0](),!0}for(m[1]=function(){void 0!==t&&(l.push(t),t=void 0)},m[0]=function(){void 0===t?t=a:t+=a},m[2]=function(){m[0](),h++},m[3]=function(){if(h>0)h--,d=4,m[0]();else{if(h=0,void 0===t)return!1;if(!1===(t=zc(t)))return!1;m[1]()}};null!==d;)if(c++,"\\"!==(n=e[c])||!u()){if(i=Tc(n),8===(r=(s=kc[d])[i]||s.else||8))return;if(d=r[0],(o=m[r[1]])&&(a=void 0===(a=r[2])?n:a,!1===o()))return;if(7===d)return l}}(e))&&(this._cache[e]=n),n||[]},Mc.prototype.getPathValue=function(e,n){if(!$l(e))return null;var t=this.parsePath(n);if(0===t.length)return null;for(var a=t.length,i=e,r=0;r<a;){var o=i[t[r]];if(null==o)return null;i=o,r++}return i};var Pc,Cc=/<\/?[\w\s="/.':;#-\/]+>/,Ac=/(?:@(?:\.[a-zA-Z]+)?:(?:[\w\-_|./]+|\([\w\-_:|./]+\)))/g,Ic=/^@(?:\.([a-zA-Z]+))?:/,Sc=/[()]/g,Lc={upper:function(e){return e.toLocaleUpperCase()},lower:function(e){return e.toLocaleLowerCase()},capitalize:function(e){return""+e.charAt(0).toLocaleUpperCase()+e.substr(1)}},Uc=new vc,qc=function(e){var n=this;void 0===e&&(e={}),!dc&&"undefined"!=typeof window&&window.Vue&&bc(window.Vue);var t=e.locale||"en-US",a=!1!==e.fallbackLocale&&(e.fallbackLocale||"en-US"),i=e.messages||{},r=e.dateTimeFormats||e.datetimeFormats||{},o=e.numberFormats||{};this._vm=null,this._formatter=e.formatter||Uc,this._modifiers=e.modifiers||{},this._missing=e.missing||null,this._root=e.root||null,this._sync=void 0===e.sync||!!e.sync,this._fallbackRoot=void 0===e.fallbackRoot||!!e.fallbackRoot,this._fallbackRootWithEmptyString=void 0===e.fallbackRootWithEmptyString||!!e.fallbackRootWithEmptyString,this._formatFallbackMessages=void 0!==e.formatFallbackMessages&&!!e.formatFallbackMessages,this._silentTranslationWarn=void 0!==e.silentTranslationWarn&&e.silentTranslationWarn,this._silentFallbackWarn=void 0!==e.silentFallbackWarn&&!!e.silentFallbackWarn,this._dateTimeFormatters={},this._numberFormatters={},this._path=new Mc,this._dataListeners=new Set,this._componentInstanceCreatedListener=e.componentInstanceCreatedListener||null,this._preserveDirectiveContent=void 0!==e.preserveDirectiveContent&&!!e.preserveDirectiveContent,this.pluralizationRules=e.pluralizationRules||{},this._warnHtmlInMessage=e.warnHtmlInMessage||"off",this._postTranslation=e.postTranslation||null,this._escapeParameterHtml=e.escapeParameterHtml||!1,"__VUE_I18N_BRIDGE__"in e&&(this.__VUE_I18N_BRIDGE__=e.__VUE_I18N_BRIDGE__),this.getChoiceIndex=function(e,t){var a=Object.getPrototypeOf(n);if(a&&a.getChoiceIndex)return a.getChoiceIndex.call(n,e,t);var i,r;return n.locale in n.pluralizationRules?n.pluralizationRules[n.locale].apply(n,[e,t]):(i=e,r=t,i=Math.abs(i),2===r?i?i>1?1:0:1:i?Math.min(i,2):0)},this._exist=function(e,t){return!(!e||!t)&&(!Xl(n._path.getPathValue(e,t))||!!e[t])},"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||Object.keys(i).forEach((function(e){n._checkLocaleMessage(e,n._warnHtmlInMessage,i[e])})),this._initVM({locale:t,fallbackLocale:a,messages:i,dateTimeFormats:r,numberFormats:o})},Ec={vm:{configurable:!0},messages:{configurable:!0},dateTimeFormats:{configurable:!0},numberFormats:{configurable:!0},availableLocales:{configurable:!0},locale:{configurable:!0},fallbackLocale:{configurable:!0},formatFallbackMessages:{configurable:!0},missing:{configurable:!0},formatter:{configurable:!0},silentTranslationWarn:{configurable:!0},silentFallbackWarn:{configurable:!0},preserveDirectiveContent:{configurable:!0},warnHtmlInMessage:{configurable:!0},postTranslation:{configurable:!0},sync:{configurable:!0}};qc.prototype._checkLocaleMessage=function(e,n,t){var a=function(e,n,t,i){if(Ql(t))Object.keys(t).forEach((function(r){var o=t[r];Ql(o)?(i.push(r),i.push("."),a(e,n,o,i),i.pop(),i.pop()):(i.push(r),a(e,n,o,i),i.pop())}));else if(Hl(t))t.forEach((function(t,r){Ql(t)?(i.push("["+r+"]"),i.push("."),a(e,n,t,i),i.pop(),i.pop()):(i.push("["+r+"]"),a(e,n,t,i),i.pop())}));else if(Wl(t)){if(Cc.test(t)){var r="Detected HTML in message '"+t+"' of keypath '"+i.join("")+"' at '"+n+"'. Consider component interpolation with '<i18n>' to avoid XSS. See https://bit.ly/2ZqJzkp";"warn"===e?Vl(r):"error"===e&&function(e,n){"undefined"!=typeof console&&(console.error("[vue-i18n] "+e),n&&console.error(n.stack))}(r)}}};a(n,e,t,[])},qc.prototype._initVM=function(e){var n=dc.config.silent;dc.config.silent=!0,this._vm=new dc({data:e,__VUE18N__INSTANCE__:!0}),dc.config.silent=n},qc.prototype.destroyVM=function(){this._vm.$destroy()},qc.prototype.subscribeDataChanging=function(e){this._dataListeners.add(e)},qc.prototype.unsubscribeDataChanging=function(e){!function(e,n){if(e.delete(n));}(this._dataListeners,e)},qc.prototype.watchI18nData=function(){var e=this;return this._vm.$watch("$data",(function(){for(var n,t,a=(n=e._dataListeners,t=[],n.forEach((function(e){return t.push(e)})),t),i=a.length;i--;)dc.nextTick((function(){a[i]&&a[i].$forceUpdate()}))}),{deep:!0})},qc.prototype.watchLocale=function(e){if(e){if(!this.__VUE_I18N_BRIDGE__)return null;var n=this,t=this._vm;return this.vm.$watch("locale",(function(a){t.$set(t,"locale",a),n.__VUE_I18N_BRIDGE__&&e&&(e.locale.value=a),t.$forceUpdate()}),{immediate:!0})}if(!this._sync||!this._root)return null;var a=this._vm;return this._root.$i18n.vm.$watch("locale",(function(e){a.$set(a,"locale",e),a.$forceUpdate()}),{immediate:!0})},qc.prototype.onComponentInstanceCreated=function(e){this._componentInstanceCreatedListener&&this._componentInstanceCreatedListener(e,this)},Ec.vm.get=function(){return this._vm},Ec.messages.get=function(){return Jl(this._getMessages())},Ec.dateTimeFormats.get=function(){return Jl(this._getDateTimeFormats())},Ec.numberFormats.get=function(){return Jl(this._getNumberFormats())},Ec.availableLocales.get=function(){return Object.keys(this.messages).sort()},Ec.locale.get=function(){return this._vm.locale},Ec.locale.set=function(e){this._vm.$set(this._vm,"locale",e)},Ec.fallbackLocale.get=function(){return this._vm.fallbackLocale},Ec.fallbackLocale.set=function(e){this._localeChainCache={},this._vm.$set(this._vm,"fallbackLocale",e)},Ec.formatFallbackMessages.get=function(){return this._formatFallbackMessages},Ec.formatFallbackMessages.set=function(e){this._formatFallbackMessages=e},Ec.missing.get=function(){return this._missing},Ec.missing.set=function(e){this._missing=e},Ec.formatter.get=function(){return this._formatter},Ec.formatter.set=function(e){this._formatter=e},Ec.silentTranslationWarn.get=function(){return this._silentTranslationWarn},Ec.silentTranslationWarn.set=function(e){this._silentTranslationWarn=e},Ec.silentFallbackWarn.get=function(){return this._silentFallbackWarn},Ec.silentFallbackWarn.set=function(e){this._silentFallbackWarn=e},Ec.preserveDirectiveContent.get=function(){return this._preserveDirectiveContent},Ec.preserveDirectiveContent.set=function(e){this._preserveDirectiveContent=e},Ec.warnHtmlInMessage.get=function(){return this._warnHtmlInMessage},Ec.warnHtmlInMessage.set=function(e){var n=this,t=this._warnHtmlInMessage;if(this._warnHtmlInMessage=e,t!==e&&("warn"===e||"error"===e)){var a=this._getMessages();Object.keys(a).forEach((function(e){n._checkLocaleMessage(e,n._warnHtmlInMessage,a[e])}))}},Ec.postTranslation.get=function(){return this._postTranslation},Ec.postTranslation.set=function(e){this._postTranslation=e},Ec.sync.get=function(){return this._sync},Ec.sync.set=function(e){this._sync=e},qc.prototype._getMessages=function(){return this._vm.messages},qc.prototype._getDateTimeFormats=function(){return this._vm.dateTimeFormats},qc.prototype._getNumberFormats=function(){return this._vm.numberFormats},qc.prototype._warnDefault=function(e,n,t,a,i,r){if(!Xl(t))return t;if(this._missing){var o=this._missing.apply(null,[e,n,a,i]);if(Wl(o))return o}else 0;if(this._formatFallbackMessages){var s=Zl.apply(void 0,i);return this._render(n,r,s.params,n)}return n},qc.prototype._isFallbackRoot=function(e){return(this._fallbackRootWithEmptyString?!e:Xl(e))&&!Xl(this._root)&&this._fallbackRoot},qc.prototype._isSilentFallbackWarn=function(e){return this._silentFallbackWarn instanceof RegExp?this._silentFallbackWarn.test(e):this._silentFallbackWarn},qc.prototype._isSilentFallback=function(e,n){return this._isSilentFallbackWarn(n)&&(this._isFallbackRoot()||e!==this.fallbackLocale)},qc.prototype._isSilentTranslationWarn=function(e){return this._silentTranslationWarn instanceof RegExp?this._silentTranslationWarn.test(e):this._silentTranslationWarn},qc.prototype._interpolate=function(e,n,t,a,i,r,o){if(!n)return null;var s,l=this._path.getPathValue(n,t);if(Hl(l)||Ql(l))return l;if(Xl(l)){if(!Ql(n))return null;if(!Wl(s=n[t])&&!Yl(s))return null}else{if(!Wl(l)&&!Yl(l))return null;s=l}return Wl(s)&&(s.indexOf("@:")>=0||s.indexOf("@.")>=0)&&(s=this._link(e,n,s,a,"raw",r,o)),this._render(s,i,r,t)},qc.prototype._link=function(e,n,t,a,i,r,o){var s=t,l=s.match(Ac);for(var c in l)if(l.hasOwnProperty(c)){var d=l[c],h=d.match(Ic),m=h[0],u=h[1],p=d.replace(m,"").replace(Sc,"");if(ec(o,p))return s;o.push(p);var g=this._interpolate(e,n,p,a,"raw"===i?"string":i,"raw"===i?void 0:r,o);if(this._isFallbackRoot(g)){if(!this._root)throw Error("unexpected error");var f=this._root.$i18n;g=f._translate(f._getMessages(),f.locale,f.fallbackLocale,p,a,i,r)}g=this._warnDefault(e,p,g,a,Hl(r)?r:[r],i),this._modifiers.hasOwnProperty(u)?g=this._modifiers[u](g):Lc.hasOwnProperty(u)&&(g=Lc[u](g)),o.pop(),s=g?s.replace(d,g):s}return s},qc.prototype._createMessageContext=function(e,n,t,a){var i=this,r=Hl(e)?e:[],o=$l(e)?e:{},s=this._getMessages(),l=this.locale;return{list:function(e){return r[e]},named:function(e){return o[e]},values:e,formatter:n,path:t,messages:s,locale:l,linked:function(e){return i._interpolate(l,s[l]||{},e,null,a,void 0,[e])}}},qc.prototype._render=function(e,n,t,a){if(Yl(e))return e(this._createMessageContext(t,this._formatter||Uc,a,n));var i=this._formatter.interpolate(e,t,a);return i||(i=Uc.interpolate(e,t,a)),"string"!==n||Wl(i)?i:i.join("")},qc.prototype._appendItemToChain=function(e,n,t){var a=!1;return ec(e,n)||(a=!0,n&&(a="!"!==n[n.length-1],n=n.replace(/!/g,""),e.push(n),t&&t[n]&&(a=t[n]))),a},qc.prototype._appendLocaleToChain=function(e,n,t){var a,i=n.split("-");do{var r=i.join("-");a=this._appendItemToChain(e,r,t),i.splice(-1,1)}while(i.length&&!0===a);return a},qc.prototype._appendBlockToChain=function(e,n,t){for(var a=!0,i=0;i<n.length&&"boolean"==typeof a;i++){var r=n[i];Wl(r)&&(a=this._appendLocaleToChain(e,r,t))}return a},qc.prototype._getLocaleChain=function(e,n){if(""===e)return[];this._localeChainCache||(this._localeChainCache={});var t=this._localeChainCache[e];if(!t){n||(n=this.fallbackLocale),t=[];for(var a,i=[e];Hl(i);)i=this._appendBlockToChain(t,i,n);(i=Wl(a=Hl(n)?n:$l(n)?n.default?n.default:null:n)?[a]:a)&&this._appendBlockToChain(t,i,null),this._localeChainCache[e]=t}return t},qc.prototype._translate=function(e,n,t,a,i,r,o){for(var s,l=this._getLocaleChain(n,t),c=0;c<l.length;c++){var d=l[c];if(!Xl(s=this._interpolate(d,e[d],a,i,r,o,[a])))return s}return null},qc.prototype._t=function(e,n,t,a){for(var i,r=[],o=arguments.length-4;o-- >0;)r[o]=arguments[o+4];if(!e)return"";var s=Zl.apply(void 0,r);this._escapeParameterHtml&&(s.params=rc(s.params));var l=s.locale||n,c=this._translate(t,l,this.fallbackLocale,e,a,"string",s.params);if(this._isFallbackRoot(c)){if(!this._root)throw Error("unexpected error");return(i=this._root).$t.apply(i,[e].concat(r))}return c=this._warnDefault(l,e,c,a,r,"string"),this._postTranslation&&null!=c&&(c=this._postTranslation(c,e)),c},qc.prototype.t=function(e){for(var n,t=[],a=arguments.length-1;a-- >0;)t[a]=arguments[a+1];return(n=this)._t.apply(n,[e,this.locale,this._getMessages(),null].concat(t))},qc.prototype._i=function(e,n,t,a,i){var r=this._translate(t,n,this.fallbackLocale,e,a,"raw",i);if(this._isFallbackRoot(r)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.i(e,n,i)}return this._warnDefault(n,e,r,a,[i],"raw")},qc.prototype.i=function(e,n,t){return e?(Wl(n)||(n=this.locale),this._i(e,n,this._getMessages(),null,t)):""},qc.prototype._tc=function(e,n,t,a,i){for(var r,o=[],s=arguments.length-5;s-- >0;)o[s]=arguments[s+5];if(!e)return"";void 0===i&&(i=1);var l={count:i,n:i},c=Zl.apply(void 0,o);return c.params=Object.assign(l,c.params),o=null===c.locale?[c.params]:[c.locale,c.params],this.fetchChoice((r=this)._t.apply(r,[e,n,t,a].concat(o)),i)},qc.prototype.fetchChoice=function(e,n){if(!e||!Wl(e))return null;var t=e.split("|");return t[n=this.getChoiceIndex(n,t.length)]?t[n].trim():e},qc.prototype.tc=function(e,n){for(var t,a=[],i=arguments.length-2;i-- >0;)a[i]=arguments[i+2];return(t=this)._tc.apply(t,[e,this.locale,this._getMessages(),null,n].concat(a))},qc.prototype._te=function(e,n,t){for(var a=[],i=arguments.length-3;i-- >0;)a[i]=arguments[i+3];var r=Zl.apply(void 0,a).locale||n;return this._exist(t[r],e)},qc.prototype.te=function(e,n){return this._te(e,this.locale,this._getMessages(),n)},qc.prototype.getLocaleMessage=function(e){return Jl(this._vm.messages[e]||{})},qc.prototype.setLocaleMessage=function(e,n){"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||this._checkLocaleMessage(e,this._warnHtmlInMessage,n),this._vm.$set(this._vm.messages,e,n)},qc.prototype.mergeLocaleMessage=function(e,n){"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||this._checkLocaleMessage(e,this._warnHtmlInMessage,n),this._vm.$set(this._vm.messages,e,ac(void 0!==this._vm.messages[e]&&Object.keys(this._vm.messages[e]).length?Object.assign({},this._vm.messages[e]):{},n))},qc.prototype.getDateTimeFormat=function(e){return Jl(this._vm.dateTimeFormats[e]||{})},qc.prototype.setDateTimeFormat=function(e,n){this._vm.$set(this._vm.dateTimeFormats,e,n),this._clearDateTimeFormat(e,n)},qc.prototype.mergeDateTimeFormat=function(e,n){this._vm.$set(this._vm.dateTimeFormats,e,ac(this._vm.dateTimeFormats[e]||{},n)),this._clearDateTimeFormat(e,n)},qc.prototype._clearDateTimeFormat=function(e,n){for(var t in n){var a=e+"__"+t;this._dateTimeFormatters.hasOwnProperty(a)&&delete this._dateTimeFormatters[a]}},qc.prototype._localizeDateTime=function(e,n,t,a,i,r){for(var o=n,s=a[o],l=this._getLocaleChain(n,t),c=0;c<l.length;c++){var d=l[c];if(o=d,!Xl(s=a[d])&&!Xl(s[i]))break}if(Xl(s)||Xl(s[i]))return null;var h,m=s[i];if(r)h=new Intl.DateTimeFormat(o,Object.assign({},m,r));else{var u=o+"__"+i;(h=this._dateTimeFormatters[u])||(h=this._dateTimeFormatters[u]=new Intl.DateTimeFormat(o,m))}return h.format(e)},qc.prototype._d=function(e,n,t,a){if(!t)return(a?new Intl.DateTimeFormat(n,a):new Intl.DateTimeFormat(n)).format(e);var i=this._localizeDateTime(e,n,this.fallbackLocale,this._getDateTimeFormats(),t,a);if(this._isFallbackRoot(i)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.d(e,t,n)}return i||""},qc.prototype.d=function(e){for(var n=[],t=arguments.length-1;t-- >0;)n[t]=arguments[t+1];var a=this.locale,i=null,r=null;return 1===n.length?(Wl(n[0])?i=n[0]:$l(n[0])&&(n[0].locale&&(a=n[0].locale),n[0].key&&(i=n[0].key)),r=Object.keys(n[0]).reduce((function(e,t){var a;return ec(jl,t)?Object.assign({},e,((a={})[t]=n[0][t],a)):e}),null)):2===n.length&&(Wl(n[0])&&(i=n[0]),Wl(n[1])&&(a=n[1])),this._d(e,a,i,r)},qc.prototype.getNumberFormat=function(e){return Jl(this._vm.numberFormats[e]||{})},qc.prototype.setNumberFormat=function(e,n){this._vm.$set(this._vm.numberFormats,e,n),this._clearNumberFormat(e,n)},qc.prototype.mergeNumberFormat=function(e,n){this._vm.$set(this._vm.numberFormats,e,ac(this._vm.numberFormats[e]||{},n)),this._clearNumberFormat(e,n)},qc.prototype._clearNumberFormat=function(e,n){for(var t in n){var a=e+"__"+t;this._numberFormatters.hasOwnProperty(a)&&delete this._numberFormatters[a]}},qc.prototype._getNumberFormatter=function(e,n,t,a,i,r){for(var o=n,s=a[o],l=this._getLocaleChain(n,t),c=0;c<l.length;c++){var d=l[c];if(o=d,!Xl(s=a[d])&&!Xl(s[i]))break}if(Xl(s)||Xl(s[i]))return null;var h,m=s[i];if(r)h=new Intl.NumberFormat(o,Object.assign({},m,r));else{var u=o+"__"+i;(h=this._numberFormatters[u])||(h=this._numberFormatters[u]=new Intl.NumberFormat(o,m))}return h},qc.prototype._n=function(e,n,t,a){if(!qc.availabilities.numberFormat)return"";if(!t)return(a?new Intl.NumberFormat(n,a):new Intl.NumberFormat(n)).format(e);var i=this._getNumberFormatter(e,n,this.fallbackLocale,this._getNumberFormats(),t,a),r=i&&i.format(e);if(this._isFallbackRoot(r)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.n(e,Object.assign({},{key:t,locale:n},a))}return r||""},qc.prototype.n=function(e){for(var n=[],t=arguments.length-1;t-- >0;)n[t]=arguments[t+1];var a=this.locale,i=null,r=null;return 1===n.length?Wl(n[0])?i=n[0]:$l(n[0])&&(n[0].locale&&(a=n[0].locale),n[0].key&&(i=n[0].key),r=Object.keys(n[0]).reduce((function(e,t){var a;return ec(Nl,t)?Object.assign({},e,((a={})[t]=n[0][t],a)):e}),null)):2===n.length&&(Wl(n[0])&&(i=n[0]),Wl(n[1])&&(a=n[1])),this._n(e,a,i,r)},qc.prototype._ntp=function(e,n,t,a){if(!qc.availabilities.numberFormat)return[];if(!t)return(a?new Intl.NumberFormat(n,a):new Intl.NumberFormat(n)).formatToParts(e);var i=this._getNumberFormatter(e,n,this.fallbackLocale,this._getNumberFormats(),t,a),r=i&&i.formatToParts(e);if(this._isFallbackRoot(r)){if(!this._root)throw Error("unexpected error");return this._root.$i18n._ntp(e,n,t,a)}return r||[]},Object.defineProperties(qc.prototype,Ec),Object.defineProperty(qc,"availabilities",{get:function(){if(!Pc){var e="undefined"!=typeof Intl;Pc={dateTimeFormat:e&&void 0!==Intl.DateTimeFormat,numberFormat:e&&void 0!==Intl.NumberFormat}}return Pc}}),qc.install=bc,qc.version="8.28.2";var Rc=qc;
/*!
 * vssue - A vue-powered issue-based comment plugin
 *
 * @version v1.4.8
 * @link https://vssue.js.org
 * @license MIT
 * @copyright 2018-2021 meteorlxy
 */
/*! *****************************************************************************
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use
this file except in compliance with the License. You may obtain a copy of the
License at http://www.apache.org/licenses/LICENSE-2.0

THIS CODE IS PROVIDED ON AN *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED
WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,
MERCHANTABLITY OR NON-INFRINGEMENT.

See the Apache Version 2.0 License for specific language governing permissions
and limitations under the License.
***************************************************************************** */function Dc(e,n,t,a){var i,r=arguments.length,o=r<3?n:null===a?a=Object.getOwnPropertyDescriptor(n,t):a;if("object"==typeof Reflect&&"function"==typeof Reflect.decorate)o=Reflect.decorate(e,n,t,a);else for(var s=e.length-1;s>=0;s--)(i=e[s])&&(o=(r<3?i(o):r>3?i(n,t,o):i(n,t))||o);return r>3&&o&&Object.defineProperty(n,t,o),o}var Oc=Wt.extend({name:"Iconfont"});function Gc(e,n,t,a,i,r,o,s,l,c){"boolean"!=typeof o&&(l=s,s=o,o=!1);const d="function"==typeof t?t.options:t;let h;if(e&&e.render&&(d.render=e.render,d.staticRenderFns=e.staticRenderFns,d._compiled=!0,i&&(d.functional=!0)),a&&(d._scopeId=a),r?(h=function(e){(e=e||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(e=__VUE_SSR_CONTEXT__),n&&n.call(this,l(e)),e&&e._registeredComponents&&e._registeredComponents.add(r)},d._ssrRegister=h):n&&(h=o?function(e){n.call(this,c(e,this.$root.$options.shadowRoot))}:function(e){n.call(this,s(e))}),h)if(d.functional){const e=d.render;d.render=function(n,t){return h.call(t),e(n,t)}}else{const e=d.beforeCreate;d.beforeCreate=e?[].concat(e,h):[h]}return t}"undefined"!=typeof navigator&&/msie [6-9]\\b/.test(navigator.userAgent.toLowerCase());const Fc=Gc({render:function(e,n){var t=n._c;return t("svg",{directives:[{name:"show",rawName:"v-show",value:!1,expression:"false"}]},[t("symbol",{attrs:{id:"vssue-icon-bitbucket",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M579.5522464 489.45249493q4.8371808 38.38537173-30.81752427 61.55702827t-67.95459093 3.66689493q-23.79580907-10.37653333-32.6119616-35.34262826t-0.31207573-50.01020907 31.67573333-35.34262827q21.92335253-11.00068587 44.1587808-7.33379093t39.00952427 21.61127573 16.77409493 41.1160384zM647.19476053 476.65737173q-8.50407573-65.22392427-68.8908192-99.9424t-120.07131413-7.9579424q-38.38537173 17.08617173-61.24495253 53.9111616t-21.0651424 78.95527574q2.41859093 55.4715424 47.20152426 94.48106666t100.87862827 34.1723424q55.4715424-4.8371808 92.60860907-51.18049493t30.50544746-102.43900907zM792.93434133 146.32472427q-12.17097173-16.4620192-34.1723424-27.15062827t-35.34262826-13.41927573-43.30057174-7.64586667q-177.33729493-28.63299093-345.00022826 1.24830507-26.2144 4.29104747-40.25782827 7.33379093t-33.54819093 13.41927573-30.50544747 26.2144q18.2564576 17.08617173 46.34331413 27.6967616t44.78293334 13.41927574 53.36502826 7.02171413q138.95192427 17.71032427 273.06666667 0.62415253 38.38537173-4.8371808 54.53531413-7.33379093t44.1587808-13.1072 45.7191616-28.32091413zM827.65281813 777.10872427q-4.8371808 15.83786667-9.44030506 46.65539093t-8.50407574 51.18049493-17.39824746 42.6764192-35.34262827 34.4064q-52.4288 29.2571424-115.46819093 43.61264747t-123.1140576 13.41927573-122.8019808-11.3127616q-28.0088384-4.8371808-49.69813334-11.00068586t-46.65539093-16.4620192-44.4708576-26.52647574-31.67573333-37.4491424q-15.21371413-58.51428587-34.71847574-177.96144746l3.66689494-9.7523808 11.00068586-5.46133334q135.9091808 90.1900192 308.72137174 90.1900192t309.34552426-90.1900192q12.79512427 3.66689493 14.5895616 14.04342827t-3.0427424 27.46270507-4.8371808 22.54750506zM937.97175147 191.41973333q-15.83786667 101.8148576-67.64251414 399.22346667-3.0427424 18.2564576-16.4620192 34.1723424t-26.52647573 24.3419424-33.23611413 18.88060907q-153.61950507 76.7707424-371.8387808 53.67710506-151.12289493-16.4620192-240.14262827-84.72868586-9.12822827-7.33379093-15.52579093-16.1499424t-10.37653334-21.2992-5.46133333-20.75306667-3.66689493-24.10788587-3.3548192-21.2992q-5.46133333-30.50544747-16.1499424-91.43832426t-17.08617174-98.4600384-14.35550506-89.8779424-13.41927574-96.27550507q1.7944384-15.83786667 10.68860907-29.5692192t19.19268587-22.8595808 27.46270506-18.2564576 28.0088384-13.73135253 29.2571424-11.3127616q76.22460907-28.0088384 190.75657174-39.00952427 231.0144-22.54750507 412.01859093 30.50544747 94.48106667 28.0088384 131.072 74.35215253 9.7523808 12.17097173 10.0644576 31.0515808t-3.3548192 32.9240384z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitea",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M184.31868985 236.10860742C106.94832667 235.94086648 3.32655508 285.13080468 9.02973665 408.46209936c8.93218827 192.65010787 206.32096845 210.5144844 285.20099725 212.06608453 8.63864186 36.14810496 101.48307766 160.77938883 170.21479898 167.32127321h301.09442177c180.57278288-11.99345499 315.77172611-546.07960359 215.54670217-548.09249109-165.7696721 7.79993906-264.02374305 11.74184405-348.27147151 12.41280591v166.69224585l-26.25140843-11.61603761-0.16773997-154.99233728c-96.70246985-0.04193548-181.83083757-4.52899687-343.4069947-12.49667687-20.21274496-0.12580547-48.39316992-3.5644886-78.67035236-3.64835859z m10.94507577 68.14462849h9.22573371c10.98701124 98.75729283 28.85138778 156.50200291 64.99949274 244.73357185-92.25734394-10.90314029-170.75995634-37.69970509-185.18564974-137.75698809-7.46445813-51.78991757 17.69663558-105.84433456 110.96042329-107.01851827z m358.83913087 97.07988723c6.29027343 0.08386999 12.70635233 1.25805468 18.74501482 4.02577499l31.40943263 13.54505513-22.51917887 41.05451824a28.18042496 25.03528825 0 0 0-10.10637297 1.59353561 28.18042496 25.03528825 0 0 0-16.98373825 32.038459 28.18042496 25.03528825 0 0 0 4.69673781 7.29671718l-38.83195528 70.70267333a28.18042496 25.03528825 0 0 0-9.30960467 1.59353659 28.18042496 25.03528825 0 0 0-16.98373825 32.038459 28.18042496 25.03528825 0 0 0 36.06423497 15.09665623 28.18042496 25.03528825 0 0 0 16.94180276-32.08039449 28.18042496 25.03528825 0 0 0-6.62575434-9.22573468l37.82551056-68.85752581a28.18042496 25.03528825 0 0 0 12.28700044-1.25805469 28.18042496 25.03528825 0 0 0 8.93218826-4.69673783c14.59343435 6.12253248 26.54495386 11.11281671 35.14166122 15.34826717 12.91602778 6.37414341 17.48696012 10.60959485 18.87082027 15.30633169 1.38386015 4.61286685-0.12580547 13.50312062-7.42252263 29.10299872-5.45157063 11.61603859-14.46762889 28.09655497-25.11915823 47.51253164a28.18042496 25.03528825 0 0 0-10.52572486 1.59353659 28.18042496 25.03528825 0 0 0-16.98373826 32.038459 28.18042496 25.03528825 0 0 0 36.06423498 15.09665623 28.18042496 25.03528825 0 0 0 16.94180278-32.03845901 28.18042496 25.03528825 0 0 0-5.74511608-8.47090188c10.52572388-19.20630122 19.58371762-35.72875308 25.41270465-48.14155897 7.88380904-16.85793279 11.99345499-29.39654416 8.38703091-41.51580463-3.60642311-12.11926046-14.67730434-20.0030695-29.35460966-27.25785217-9.6450856-4.73867233-21.68047607-9.77089106-36.06423399-15.80955357a28.18042496 25.03528825 0 0 0-1.59353562-10.022502 28.18042496 25.03528825 0 0 0-6.08059796-8.7644483l22.14176246-40.38355541 122.61839638 52.96410227c22.14176247 9.6031511 31.2836262 33.12877372 20.54822685 52.8382968l-84.28966393 154.32137544c-10.77733482 19.66758857-37.23841869 27.80300855-59.38018118 18.24179293l-173.48574115-74.98005927c-22.14176247-9.5612156-31.32556167-33.12877372-20.54822687-52.83829679l84.28966395-154.27943995c7.38058716-13.54505513 22.22563246-21.59660511 37.951317-22.22563246h2.68384935z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitee",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M978.404275 409.561604H455.061338c-25.117645 0-45.499734 20.382089-45.499734 45.499734l-0.031997 113.781333c0 25.117645 20.350092 45.499734 45.499734 45.531731h318.594132c25.117645 0 45.499734 20.382089 45.499734 45.499735v22.749867a136.5312 136.5312 0 0 1-136.5312 136.5312H250.248539a45.499734 45.499734 0 0 1-45.499734-45.499734V341.343999a136.5312 136.5312 0 0 1 136.5312-136.5312L978.308284 204.780802c25.117645 0 45.499734-20.350092 45.499734-45.467738L1023.904009 45.531731h0.031997A45.499734 45.499734 0 0 0 978.468269 0h-0.031997L341.343999 0.031997C152.84967 0.031997 0.031997 152.84967 0.031997 341.343999v637.092273c0 25.117645 20.382089 45.499734 45.499734 45.499734h671.233072a307.171203 307.171203 0 0 0 307.171203-307.171203v-261.671468c0-25.117645-20.382089-45.499734-45.499734-45.499734z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-github",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M512 20.4425c-278.334 0-504 225.6345-504 504 0 222.6735 144.4275 411.6105 344.673 478.233 25.2 4.662 34.461-10.9305 34.461-24.255 0-12.0015-0.4725-51.723-0.693-93.8385-140.238 30.492-169.8165-59.472-169.8165-59.472-22.932-58.2435-55.944-73.7415-55.944-73.7415-45.738-31.2795 3.465-30.6495 3.465-30.6495 50.589 3.5595 77.238 51.9435 77.238 51.9435 44.9505 77.049 117.9045 54.7785 146.664 41.895 4.5045-32.571 17.577-54.81 32.004-67.41-111.951-12.726-229.635-55.9755-229.635-249.0705 0-55.0305 19.6875-99.981 51.9435-135.2925-5.229-12.6945-22.491-63.945 4.8825-133.371 0 0 42.336-13.545 138.6315 51.66 40.194-11.1825 83.3175-16.758 126.1575-16.9785 42.8085 0.189 85.9635 5.796 126.252 16.9785 96.201-65.205 138.4425-51.66 138.4425-51.66 27.4365 69.426 10.1745 120.6765 4.9455 133.371 32.319 35.28 51.8805 80.262 51.8805 135.2925 0 193.5675-117.9045 236.187-230.139 248.6925 18.081 15.6555 34.1775 46.305 34.1775 93.3345 0 67.4415-0.5985 121.716-0.5985 138.3165 0 13.419 9.072 29.1375 34.6185 24.192 200.151-66.717 344.3895-255.5595 344.3895-478.17 0-278.3655-225.666-504-504-504z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitlab",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M66.61375986 405.11600042L512.11376028 976.03999972 23.84576 621.65599958a39.312 39.312 0 0 1-14.07600042-43.30799944l56.8080007-173.26800028z m259.88400014 0h371.26800014L512.14975986 976.03999972zM215.11376 60.88400042l111.384 344.232H66.61375986l111.384-344.232a19.72800014 19.72800014 0 0 1 37.11600014 0z m742.49999972 344.232l56.8080007 173.2679993a39.23999986 39.23999986 0 0 1-14.07600042 43.30800042l-488.26800028 354.38400014 445.50000042-570.92400028z m0 0h-259.88400014l111.384-344.232a19.72800014 19.72800014 0 0 1 37.11600014 0z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-loading",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M843.307 742.24c0 3.217 2.607 5.824 5.824 5.824s5.824-2.607 5.824-5.824a5.823 5.823 0 0 0-5.824-5.824 5.823 5.823 0 0 0-5.824 5.824zM714.731 874.912c0 6.398 5.186 11.584 11.584 11.584s11.584-5.186 11.584-11.584-5.186-11.584-11.584-11.584-11.584 5.186-11.584 11.584zM541.419 943.2c0 9.614 7.794 17.408 17.408 17.408s17.408-7.794 17.408-17.408-7.794-17.408-17.408-17.408-17.408 7.794-17.408 17.408z m-186.56-9.152c0 12.795 10.373 23.168 23.168 23.168s23.168-10.373 23.168-23.168-10.373-23.168-23.168-23.168-23.168 10.373-23.168 23.168zM189.355 849.12c0 16.012 12.98 28.992 28.992 28.992s28.992-12.98 28.992-28.992-12.98-28.992-28.992-28.992-28.992 12.98-28.992 28.992zM74.731 704.736c0 19.228 15.588 34.816 34.816 34.816s34.816-15.588 34.816-34.816-15.588-34.816-34.816-34.816-34.816 15.588-34.816 34.816z m-43.008-177.28c0 22.41 18.166 40.576 40.576 40.576s40.576-18.166 40.576-40.576-18.166-40.576-40.576-40.576-40.576 18.166-40.576 40.576z m35.392-176.128c0 25.626 20.774 46.4 46.4 46.4s46.4-20.774 46.4-46.4c0-25.626-20.774-46.4-46.4-46.4-25.626 0-46.4 20.774-46.4 46.4z m106.176-142.016c0 28.843 23.381 52.224 52.224 52.224s52.224-23.381 52.224-52.224c0-28.843-23.381-52.224-52.224-52.224-28.843 0-52.224 23.381-52.224 52.224z m155.904-81.344c0 32.024 25.96 57.984 57.984 57.984s57.984-25.96 57.984-57.984-25.96-57.984-57.984-57.984-57.984 25.96-57.984 57.984z m175.104-5.056c0 35.24 28.568 63.808 63.808 63.808s63.808-28.568 63.808-63.808c0-35.24-28.568-63.808-63.808-63.808-35.24 0-63.808 28.568-63.808 63.808z m160.32 72.128c0 38.421 31.147 69.568 69.568 69.568s69.568-31.147 69.568-69.568-31.147-69.568-69.568-69.568-69.568 31.147-69.568 69.568z m113.92 135.488c0 41.638 33.754 75.392 75.392 75.392s75.392-33.754 75.392-75.392-33.754-75.392-75.392-75.392-75.392 33.754-75.392 75.392z m45.312 175.488c0 44.854 36.362 81.216 81.216 81.216s81.216-36.362 81.216-81.216c0-44.854-36.362-81.216-81.216-81.216-44.854 0-81.216 36.362-81.216 81.216z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-like",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M885.9 533.7c16.8-22.2 26.1-49.4 26.1-77.7 0-44.9-25.1-87.4-65.5-111.1a67.67 67.67 0 0 0-34.3-9.3H572.4l6-122.9c1.4-29.7-9.1-57.9-29.5-79.4-20.5-21.5-48.1-33.4-77.9-33.4-52 0-98 35-111.8 85.1l-85.9 311H144c-17.7 0-32 14.3-32 32v364c0 17.7 14.3 32 32 32h601.3c9.2 0 18.2-1.8 26.5-5.4 47.6-20.3 78.3-66.8 78.3-118.4 0-12.6-1.8-25-5.4-37 16.8-22.2 26.1-49.4 26.1-77.7 0-12.6-1.8-25-5.4-37 16.8-22.2 26.1-49.4 26.1-77.7-0.2-12.6-2-25.1-5.6-37.1zM184 852V568h81v284h-81z m636.4-353l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 16.5-7.2 32.2-19.6 43l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 16.5-7.2 32.2-19.6 43l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 22.4-13.2 42.6-33.6 51.8H329V564.8l99.5-360.5c5.2-18.9 22.5-32.2 42.2-32.3 7.6 0 15.1 2.2 21.1 6.7 9.9 7.4 15.2 18.6 14.6 30.5l-9.6 198.4h314.4C829 418.5 840 436.9 840 456c0 16.5-7.2 32.1-19.6 43z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-unlike",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M885.9 490.3c3.6-12 5.4-24.4 5.4-37 0-28.3-9.3-55.5-26.1-77.7 3.6-12 5.4-24.4 5.4-37 0-28.3-9.3-55.5-26.1-77.7 3.6-12 5.4-24.4 5.4-37 0-51.6-30.7-98.1-78.3-118.4-8.3-3.6-17.2-5.4-26.5-5.4H144c-17.7 0-32 14.3-32 32v364c0 17.7 14.3 32 32 32h129.3l85.8 310.8C372.9 889 418.9 924 470.9 924c29.7 0 57.4-11.8 77.9-33.4 20.5-21.5 31-49.7 29.5-79.4l-6-122.9h239.9c12.1 0 23.9-3.2 34.3-9.3 40.4-23.5 65.5-66.1 65.5-111 0-28.3-9.3-55.5-26.1-77.7zM184 456V172h81v284h-81z m627.2 160.4H496.8l9.6 198.4c0.6 11.9-4.7 23.1-14.6 30.5-6.1 4.5-13.6 6.8-21.1 6.7-19.6-0.1-36.9-13.4-42.2-32.3L329 459.2V172h415.4c20.4 9.2 33.6 29.4 33.6 51.8 0 9.7-2.3 18.9-6.9 27.3l-13.9 25.4 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 9.7-2.3 18.9-6.9 27.3l-13.9 25.4 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 9.7-2.3 18.9-6.9 27.3l-14 25.5 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 19.1-11 37.5-28.8 48.4z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-heart",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M923 283.6c-13.4-31.1-32.6-58.9-56.9-82.8-24.3-23.8-52.5-42.4-84-55.5-32.5-13.5-66.9-20.3-102.4-20.3-49.3 0-97.4 13.5-139.2 39-10 6.1-19.5 12.8-28.5 20.1-9-7.3-18.5-14-28.5-20.1-41.8-25.5-89.9-39-139.2-39-35.5 0-69.9 6.8-102.4 20.3-31.4 13-59.7 31.7-84 55.5-24.4 23.9-43.5 51.7-56.9 82.8-13.9 32.3-21 66.6-21 101.9 0 33.3 6.8 68 20.3 103.3 11.3 29.5 27.5 60.1 48.2 91 32.8 48.9 77.9 99.9 133.9 151.6 92.8 85.7 184.7 144.9 188.6 147.3l23.7 15.2c10.5 6.7 24 6.7 34.5 0l23.7-15.2c3.9-2.5 95.7-61.6 188.6-147.3 56-51.7 101.1-102.7 133.9-151.6 20.7-30.9 37-61.5 48.2-91 13.5-35.3 20.3-70 20.3-103.3 0.1-35.3-7-69.6-20.9-101.9zM512 814.8S156 586.7 156 385.5C156 283.6 240.3 201 344.3 201c73.1 0 136.5 40.8 167.7 100.4C543.2 241.8 606.6 201 679.7 201c104 0 188.3 82.6 188.3 184.5 0 201.2-356 429.3-356 429.3z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-edit",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M723.2 917.76H286.72c-65.28 0-118.4-51.2-118.4-113.92V261.76C168.32 198.4 221.44 147.2 286.72 147.2h375.04c17.92 0 32 14.08 32 32s-14.08 32-32 32H286.72c-30.08 0-54.4 22.4-54.4 49.92v542.08c0 27.52 24.32 49.92 54.4 49.92H723.2c30.08 0 54.4-22.4 54.4-49.92V440.32c0-17.92 14.08-32 32-32s32 14.08 32 32v363.52c0 62.72-53.12 113.92-118.4 113.92z"}}),n._v(" "),t("path",{attrs:{d:"M499.84 602.24c-7.68 0-14.72-2.56-21.12-7.68-13.44-11.52-14.72-32-3.2-45.44L780.16 198.4c11.52-13.44 32-14.72 45.44-3.2s14.72 32 3.2 45.44L524.16 591.36c-6.4 7.04-15.36 10.88-24.32 10.88z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-delete",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M677.647059 256l0-90.352941c0-37.436235-23.461647-60.235294-61.771294-60.235294L408.094118 105.411765c-38.249412 0-61.741176 22.799059-61.741176 60.235294l0 90.352941-180.705882 0 0 60.235294 60.235294 0 0 512c0 54.272 33.972706 90.352941 90.352941 90.352941l391.529412 0c55.085176 0 90.352941-33.490824 90.352941-90.352941l0-512 60.235294 0 0-60.235294L677.647059 256zM406.588235 165.647059l210.823529 0-1.264941 90.352941L406.588235 256 406.588235 165.647059zM737.882353 858.352941l-451.764706 0 0-542.117647 451.764706 0L737.882353 858.352941zM466.823529 376.470588l-58.729412 0-1.505882 391.529412 60.235294 0L466.823529 376.470588zM617.411765 376.470588l-60.235294 0 0 391.529412 60.235294 0L617.411765 376.470588z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-reply",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M426.666667 384 426.666667 213.333333 128 512 426.666667 810.666667 426.666667 635.733333C640 635.733333 789.333333 704 896 853.333333 853.333333 640 725.333333 426.666667 426.666667 384Z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-error",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M512 720m-48 0a48 48 0 1 0 96 0 48 48 0 1 0-96 0Z"}}),n._v(" "),t("path",{attrs:{d:"M480 416v184c0 4.4 3.6 8 8 8h48c4.4 0 8-3.6 8-8V416c0-4.4-3.6-8-8-8h-48c-4.4 0-8 3.6-8 8z"}}),n._v(" "),t("path",{attrs:{d:"M955.7 856l-416-720c-6.2-10.7-16.9-16-27.7-16s-21.6 5.3-27.7 16l-416 720C56 877.4 71.4 904 96 904h832c24.6 0 40-26.6 27.7-48z m-783.5-27.9L512 239.9l339.8 588.2H172.2z"}})])])},staticRenderFns:[]},void 0,Oc,void 0,!0,void 0,!1,void 0,void 0,void 0);const Bc=Gc({},void 0,Wt.extend({name:"TransitionFade",functional:!0,props:{group:{type:Boolean,required:!1,default:!1},tag:{type:String,required:!1,default:"div"}},render:(e,{props:n,children:t})=>e(n.group?"TransitionGroup":"Transition",{props:{name:"fade",mode:"out-in",appear:!0,tag:n.tag}},t)}),void 0,void 0,void 0,!1,void 0,void 0,void 0);const Nc=Gc({},void 0,Wt.extend({name:"VssueIcon",functional:!0,props:{name:{type:String,required:!0},title:{type:String,required:!1,default:null}},render:(e,{props:n,data:t})=>e("svg",Object.assign(Object.assign({},t),{class:["vssue-icon","vssue-icon-"+n.name],attrs:{"aria-hidden":"true"}}),[e("title",n.title),e("use",{attrs:{"xlink:href":"#vssue-icon-"+n.name}})])}),void 0,void 0,void 0,!1,void 0,void 0,void 0);let jc=class extends Wt{constructor(){super(...arguments),this.editMode=!1,this.editContent=this.comment.contentRaw,this.creatingReactions=[],this.isPutingComment=!1,this.isDeletingComment=!1}get currentUser(){return this.vssue.user?this.vssue.user.username:null}get content(){return this.comment.content}get author(){return this.comment.author}get createdAt(){return Fl(this.comment.createdAt)}get updatedAt(){return Fl(this.comment.updatedAt)}get showReactions(){return Boolean(this.vssue.API&&this.vssue.API.platform.meta.reactable&&this.comment.reactions&&!this.editMode)}get reactionKeys(){return["heart","like","unlike"]}get editContentRows(){return this.editContent.split("\n").length-1}get editInputRows(){return this.editContentRows<3?5:this.editContentRows+2}async postReaction({reaction:e}){try{if(this.creatingReactions.includes(e))return;this.creatingReactions.push(e);await this.vssue.postCommentReaction({commentId:this.comment.id,reaction:e})||this.vssue.$emit("error",new Error(this.vssue.$t("reactionGiven",{reaction:this.vssue.$t(e)})));const n=await this.vssue.getCommentReactions({commentId:this.comment.id});n&&(this.comment.reactions=n)}finally{this.creatingReactions.splice(this.creatingReactions.findIndex(n=>n===e),1)}}enterEdit(){this.editMode=!0,this.$nextTick(()=>{this.$refs.input.focus()})}resetEdit(){this.editMode=!1,this.editContent=this.comment.contentRaw}async putComment(){try{if(this.vssue.isPending)return;if(this.editContent!==this.comment.contentRaw){this.isPutingComment=!0,this.vssue.isUpdatingComment=!0;const e=await this.vssue.putComment({commentId:this.comment.id,content:this.editContent});e&&this.vssue.comments.data.splice(this.vssue.comments.data.findIndex(e=>e.id===this.comment.id),1,e)}this.editMode=!1}finally{this.isPutingComment=!1,this.vssue.isUpdatingComment=!1}}async deleteComment(){try{if(this.vssue.isPending)return;if(!window.confirm(this.vssue.$t("deleteConfirm")))return;this.isDeletingComment=!0,this.vssue.isUpdatingComment=!0;await this.vssue.deleteComment({commentId:this.comment.id})?(this.vssue.comments.count-=1,this.vssue.comments.data.length>1&&this.vssue.comments.data.splice(this.vssue.comments.data.findIndex(e=>e.id===this.comment.id),1),this.vssue.query.page>1&&this.vssue.query.page>Math.ceil(this.vssue.comments.count/this.vssue.query.perPage)?this.vssue.query.page-=1:await this.vssue.getComments()):this.vssue.$emit("error",new Error(this.vssue.$t("deleteFailed")))}finally{this.isDeletingComment=!1,this.vssue.isUpdatingComment=!1}}};Dc([Ul({type:Object,required:!0})],jc.prototype,"comment",void 0),Dc([Cl()],jc.prototype,"vssue",void 0),jc=Dc([Pl({components:{VssueIcon:Nc}})],jc);const Vc=Gc({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-comment",class:{"vssue-comment-edit-mode":e.editMode,"vssue-comment-disabled":e.isDeletingComment||e.isPutingComment}},[t("div",{staticClass:"vssue-comment-avatar"},[t("a",{attrs:{href:e.author.homepage,title:e.author.username,target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:e.author.avatar,alt:e.author.username}})])]),e._v(" "),t("div",{staticClass:"vssue-comment-body"},[e._t("body",[t("div",{staticClass:"vssue-comment-header"},[t("span",{staticClass:"vssue-comment-author"},[t("a",{attrs:{href:e.author.homepage,title:e.author.username,target:"_blank",rel:"noopener noreferrer"}},[e._v("\n            "+e._s(e.author.username)+"\n          ")])]),e._v(" "),t("span",{staticClass:"vssue-comment-created-at"},[e._v("\n          "+e._s(e.createdAt)+"\n        ")])]),e._v(" "),t("div",{staticClass:"vssue-comment-main"},[e.editMode?t("textarea",{directives:[{name:"model",rawName:"v-model",value:e.editContent,expression:"editContent"}],ref:"input",staticClass:"vssue-edit-comment-input",attrs:{rows:e.editInputRows},domProps:{value:e.editContent},on:{keyup:function(n){return!n.type.indexOf("key")&&e._k(n.keyCode,"enter",13,n.key,"Enter")?null:n.ctrlKey?e.putComment():null},input:function(n){n.target.composing||(e.editContent=n.target.value)}}}):t("article",{staticClass:"markdown-body",domProps:{innerHTML:e._s(e.content)}})]),e._v(" "),t("div",{staticClass:"vssue-comment-footer"},[e.editMode?t("span",{staticClass:"vssue-comment-hint"},[e._v("\n          "+e._s(e.vssue.$t("editMode"))+"\n        ")]):e._e(),e._v(" "),e.showReactions?t("span",{staticClass:"vssue-comment-reactions"},e._l(e.reactionKeys,(function(n){return t("span",{key:n,staticClass:"vssue-comment-reaction",attrs:{title:e.vssue.$t(e.creatingReactions.includes(n)?"loading":n)},on:{click:function(t){return e.postReaction({reaction:n})}}},[t("VssueIcon",{attrs:{name:e.creatingReactions.includes(n)?"loading":n,title:e.vssue.$t(e.creatingReactions.includes(n)?"loading":n)}}),e._v(" "),t("span",{staticClass:"vssue-comment-reaction-number"},[e._v("\n              "+e._s(e.comment.reactions[n])+"\n            ")])],1)})),0):e._e(),e._v(" "),t("span",{staticClass:"vssue-comment-operations"},[e.comment.author.username===e.currentUser&&e.editMode?t("span",{staticClass:"vssue-comment-operation",class:{"vssue-comment-operation-muted":e.isPutingComment},attrs:{title:e.vssue.$t(e.isPutingComment?"loading":"submit")},on:{click:function(n){return e.putComment()}}},[t("VssueIcon",{directives:[{name:"show",rawName:"v-show",value:e.isPutingComment,expression:"isPutingComment"}],attrs:{name:"loading",title:e.vssue.$t("loading")}}),e._v("\n\n            "+e._s(e.vssue.$t("submit"))+"\n          ")],1):e._e(),e._v(" "),e.comment.author.username===e.currentUser&&e.editMode?t("span",{staticClass:"vssue-comment-operation vssue-comment-operation-muted",attrs:{title:e.vssue.$t("cancel")},on:{click:function(n){return e.resetEdit()}}},[e._v("\n            "+e._s(e.vssue.$t("cancel"))+"\n          ")]):e._e(),e._v(" "),e.comment.author.username===e.currentUser?t("span",{directives:[{name:"show",rawName:"v-show",value:!e.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(n){return e.enterEdit()}}},[t("VssueIcon",{attrs:{name:"edit",title:e.vssue.$t("edit")}})],1):e._e(),e._v(" "),e.comment.author.username===e.currentUser||e.vssue.isAdmin?t("span",{directives:[{name:"show",rawName:"v-show",value:!e.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(n){return e.deleteComment()}}},[t("VssueIcon",{attrs:{name:e.isDeletingComment?"loading":"delete",title:e.vssue.$t(e.isDeletingComment?"loading":"delete")}})],1):e._e(),e._v(" "),t("span",{directives:[{name:"show",rawName:"v-show",value:!e.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(n){return e.vssue.$emit("reply-comment",e.comment)}}},[t("VssueIcon",{attrs:{name:"reply",title:e.vssue.$t("reply")}})],1)])])])],2)])},staticRenderFns:[]},void 0,jc,void 0,!1,void 0,!1,void 0,void 0,void 0);let Hc=class extends Wt{get disabled(){return this.vssue.isPending}get pageCount(){const e=Math.ceil(this.vssue.comments.count/this.vssue.comments.perPage);return e>1?e:1}get perPageOptions(){const e=[5,10,20,50];return!e.includes(this.vssue.options.perPage)&&this.vssue.options.perPage<100&&e.push(this.vssue.options.perPage),e.sort((e,n)=>e-n)}get page(){return this.vssue.query.page>this.pageCount?this.pageCount:this.vssue.query.page}set page(e){e>0&&e<=this.pageCount&&(this.vssue.query.page=e)}get perPage(){return this.vssue.query.perPage}set perPage(e){this.perPageOptions.includes(e)&&(this.vssue.query.perPage=e)}};Dc([Cl()],Hc.prototype,"vssue",void 0),Hc=Dc([Pl({components:{VssueIcon:Nc}})],Hc);const $c=Gc({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-pagination"},[t("div",{staticClass:"vssue-pagination-per-page"},[t("label",[t("select",{directives:[{name:"model",rawName:"v-model",value:e.perPage,expression:"perPage"}],staticClass:"vssue-pagination-select",attrs:{disabled:e.disabled},on:{change:function(n){var t=Array.prototype.filter.call(n.target.options,(function(e){return e.selected})).map((function(e){return"_value"in e?e._value:e.value}));e.perPage=n.target.multiple?t:t[0]}}},e._l(e.perPageOptions,(function(n){return t("option",{key:n,domProps:{value:n}},[e._v("\n          "+e._s(n)+"\n        ")])})),0),e._v(" "),t("span",[e._v("\n        "+e._s(e.vssue.$t("perPage"))+"\n      ")])]),e._v(" "),e.vssue.API.platform.meta.sortable?t("span",{class:{"vssue-pagination-link":!0,disabled:e.disabled},attrs:{title:e.vssue.$t("sort")},on:{click:function(n){e.vssue.query.sort="asc"===e.vssue.query.sort?"desc":"asc"}}},[e._v("\n      "+e._s("asc"===e.vssue.query.sort?"↑":"↓")+"\n    ")]):e._e()]),e._v(" "),t("div",{staticClass:"vssue-pagination-page"},[t("span",{class:{"vssue-pagination-link":!0,disabled:1===e.page||e.disabled},attrs:{title:e.vssue.$t("prev")},domProps:{textContent:e._s("<")},on:{click:function(n){e.page-=1}}}),e._v(" "),t("label",[t("span",[e._v("\n        "+e._s(e.vssue.$t("page"))+"\n      ")]),e._v(" "),t("select",{directives:[{name:"show",rawName:"v-show",value:e.pageCount>1,expression:"pageCount > 1"},{name:"model",rawName:"v-model",value:e.page,expression:"page"}],staticClass:"vssue-pagination-select",attrs:{disabled:e.disabled},on:{change:function(n){var t=Array.prototype.filter.call(n.target.options,(function(e){return e.selected})).map((function(e){return"_value"in e?e._value:e.value}));e.page=n.target.multiple?t:t[0]}}},e._l(e.pageCount,(function(n){return t("option",{key:n,domProps:{value:n}},[e._v("\n          "+e._s(n)+"\n        ")])})),0),e._v(" "),t("span",{directives:[{name:"show",rawName:"v-show",value:e.pageCount<2,expression:"pageCount < 2"}],domProps:{textContent:e._s(e.page)}}),e._v(" "),t("span",{domProps:{textContent:e._s(" / "+e.pageCount+" ")}})]),e._v(" "),t("span",{class:{"vssue-pagination-link":!0,disabled:e.page===e.pageCount||e.disabled},attrs:{title:e.vssue.$t("next")},domProps:{textContent:e._s(">")},on:{click:function(n){e.page+=1}}})])])},staticRenderFns:[]},void 0,Hc,void 0,!1,void 0,!1,void 0,void 0,void 0);let Wc=class extends Wt{};Dc([Cl()],Wc.prototype,"vssue",void 0),Wc=Dc([Pl({components:{TransitionFade:Bc,VssueComment:Vc,VssuePagination:$c}})],Wc);const Kc=Gc({render:function(){var e=this.$createElement,n=this._self._c||e;return n("div",{staticClass:"vssue-comments"},[n("VssuePagination"),this._v(" "),n("TransitionFade",{attrs:{group:""}},this._l(this.vssue.comments.data,(function(e){return n("VssueComment",{key:e.id,attrs:{comment:e}})})),1),this._v(" "),n("VssuePagination",{directives:[{name:"show",rawName:"v-show",value:this.vssue.comments.data.length>5,expression:"vssue.comments.data.length > 5"}]})],1)},staticRenderFns:[]},void 0,Wc,void 0,!1,void 0,!1,void 0,void 0,void 0);const Qc=Gc({},void 0,Wt.extend({name:"VssueIcon",functional:!0,props:{type:{type:String,required:!1,default:"default"}},render:(e,{props:n,data:t,children:a})=>e("button",Object.assign(Object.assign({},t),{class:["vssue-button","vssue-button-"+n.type]}),a)}),void 0,void 0,void 0,!1,void 0,void 0,void 0);let Xc=class extends Wt{constructor(){super(...arguments),this.content=""}get user(){return this.vssue.user}get platform(){return this.vssue.API&&this.vssue.API.platform.name}get isInputDisabled(){return this.loading||null===this.user||null===this.vssue.issue}get isSubmitDisabled(){return""===this.content||this.vssue.isPending||null===this.vssue.issue}get loading(){return this.vssue.isCreatingComment}get contentRows(){return this.content.split("\n").length-1}get inputRows(){return this.contentRows<3?5:this.contentRows+2}created(){this.vssue.$on("reply-comment",e=>{const n=e.contentRaw.replace(/\n/g,"\n> "),t=`@${e.author.username}\n\n> ${n}\n\n`;this.content=this.content.concat(t),this.focus()})}beforeDestroy(){this.vssue.$off("reply-comment")}focus(){this.$refs.input.focus()}async submit(){this.isSubmitDisabled||(await this.vssue.postComment({content:this.content}),this.content="",await this.vssue.getComments())}};Dc([Cl()],Xc.prototype,"vssue",void 0),Xc=Dc([Pl({components:{VssueButton:Qc,VssueIcon:Nc}})],Xc);const Yc=Gc({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-new-comment"},[t("div",{staticClass:"vssue-comment-avatar"},[e.user?t("a",{attrs:{href:e.user.homepage,title:e.user.username,target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:e.user.avatar,alt:e.user.username}})]):t("VssueIcon",{attrs:{name:e.platform.toLowerCase(),title:e.vssue.$t("loginToComment",{platform:e.platform})},on:{click:function(n){return e.vssue.login()}}})],1),e._v(" "),t("div",{staticClass:"vssue-new-comment-body"},[t("textarea",{directives:[{name:"model",rawName:"v-model",value:e.content,expression:"content"}],ref:"input",staticClass:"vssue-new-comment-input",attrs:{rows:e.inputRows,disabled:e.isInputDisabled,placeholder:e.vssue.$t(e.user?"placeholder":"noLoginPlaceHolder"),spellcheck:!1,"aria-label":"leave a comment"},domProps:{value:e.content},on:{keyup:function(n){return!n.type.indexOf("key")&&e._k(n.keyCode,"enter",13,n.key,"Enter")?null:n.ctrlKey?e.submit():null},input:function(n){n.target.composing||(e.content=n.target.value)}}})]),e._v(" "),t("div",{staticClass:"vssue-new-comment-footer"},[e.user?t("span",{staticClass:"vssue-current-user"},[t("span",[e._v(e._s(e.vssue.$t("currentUser"))+" - "+e._s(e.user.username)+" - ")]),e._v(" "),t("a",{staticClass:"vssue-logout",on:{click:function(n){return e.vssue.logout()}}},[e._v("\n        "+e._s(e.vssue.$t("logout"))+"\n      ")])]):t("span",{staticClass:"vssue-current-user"},[e._v("\n      "+e._s(e.vssue.$t("loginToComment",{platform:e.platform}))+"\n    ")]),e._v(" "),t("div",{staticClass:"vssue-new-comment-operations"},[e.user?t("VssueButton",{staticClass:"vssue-button-submit-comment",attrs:{type:"primary",disabled:e.isSubmitDisabled},on:{click:function(n){return e.submit()}}},[t("VssueIcon",{directives:[{name:"show",rawName:"v-show",value:e.loading,expression:"loading"}],attrs:{name:"loading"}}),e._v("\n\n        "+e._s(e.vssue.$t(e.loading?"submitting":"submitComment"))+"\n      ")],1):t("VssueButton",{staticClass:"vssue-button-login",attrs:{type:"primary",title:e.vssue.$t("loginToComment",{platform:e.platform})},on:{click:function(n){return e.vssue.login()}}},[e._v("\n        "+e._s(e.vssue.$t("login",{platform:e.platform}))+"\n      ")])],1)])])},staticRenderFns:[]},void 0,Xc,void 0,!1,void 0,!1,void 0,void 0,void 0);let Zc=class extends Wt{constructor(){super(...arguments),this.progress={show:!1,percent:0,timer:null,speed:200},this.alert={show:!1,message:null,timer:null}}onLoadingCommentsChange(e){this.vssue.comments&&(e?this.progressStart():this.progressDone())}created(){this.vssue.$on("error",e=>this.alertShow(e.message))}beforeDestroy(){this.vssue.$off("error"),null!==this.progress.timer&&window.clearTimeout(this.progress.timer),null!==this.alert.timer&&window.clearTimeout(this.alert.timer)}progressStart(){this.progress.show=!0,this.progress.percent=0,this.progress.timer=window.setInterval(()=>{this.progress.percent+=5,this.progress.percent>94&&null!==this.progress.timer&&window.clearInterval(this.progress.timer)},this.progress.speed)}progressDone(){this.progress.percent=100,null!==this.progress.timer&&window.clearTimeout(this.progress.timer),this.progress.timer=null,window.setTimeout(()=>{this.progress.show=!1},this.progress.speed)}alertShow(e){this.alert.show=!0,this.alert.message=e,null!==this.alert.timer&&window.clearTimeout(this.alert.timer),this.alert.timer=window.setTimeout(()=>{this.alertHide()},3e3)}alertHide(){this.alert.show=!1,null!==this.alert.timer&&window.clearTimeout(this.alert.timer),this.alert.timer=null}};Dc([Cl()],Zc.prototype,"vssue",void 0),Dc([ql("vssue.isLoadingComments")],Zc.prototype,"onLoadingCommentsChange",null),Zc=Dc([Pl({components:{TransitionFade:Bc}})],Zc);const Jc=Gc({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-notice"},[t("div",{directives:[{name:"show",rawName:"v-show",value:e.progress.show,expression:"progress.show"}],staticClass:"vssue-progress",style:{width:e.progress.percent+"%",transition:"all "+e.progress.speed+"ms linear"}}),e._v(" "),t("TransitionFade",[t("div",{directives:[{name:"show",rawName:"v-show",value:e.alert.show,expression:"alert.show"}],staticClass:"vssue-alert",domProps:{textContent:e._s(e.alert.message)},on:{click:function(n){return e.alertHide()}}})])],1)},staticRenderFns:[]},void 0,Zc,void 0,!1,void 0,!1,void 0,void 0,void 0);let ed=class extends Wt{get status(){return this.vssue.isFailed?"failed":this.vssue.isInitializing?"initializing":this.vssue.isIssueNotCreated&&!this.vssue.isCreatingIssue?this.vssue.isAdmin||!this.vssue.isLogined?"issueNotCreated":"failed":this.vssue.isLoginRequired?"loginRequired":!this.vssue.comments||this.vssue.isCreatingIssue?"loadingComments":0===this.vssue.comments.data.length?"noComments":null}handleClick(){"issueNotCreated"===this.status?this.vssue.postIssue():"loginRequired"===this.status&&this.vssue.login()}};Dc([Cl()],ed.prototype,"vssue",void 0),ed=Dc([Pl({components:{TransitionFade:Bc,VssueIcon:Nc}})],ed);const nd=Gc({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("TransitionFade",[e.status?t("div",{key:e.status,staticClass:"vssue-status"},[["failed","loadingComments","initializing"].includes(e.status)?t("VssueIcon",{attrs:{name:"failed"===e.status?"error":"loading"}}):e._e(),e._v(" "),t("p",{staticClass:"vssue-status-info"},[t(["issueNotCreated","loginRequired"].includes(e.status)?"a":"span",{tag:"Component",on:{click:e.handleClick}},[e._v("\n        "+e._s(e.vssue.$t(e.status))+"\n      ")])],1)],1):e._e()])},staticRenderFns:[]},void 0,ed,void 0,!1,void 0,!1,void 0,void 0,void 0);let td=class extends Wt{};Dc([Cl()],td.prototype,"vssue",void 0),td=Dc([Pl({components:{TransitionFade:Bc,VssueIcon:Nc,VssueComments:Kc,VssueNewComment:Yc,VssueNotice:Jc,VssueStatus:nd}})],td);const ad=Gc({render:function(){var e=this.$createElement,n=this._self._c||e;return n("TransitionFade",[this.vssue.isInitializing?n("VssueStatus"):n("div",{staticClass:"vssue-body"},[this.vssue.API?n("VssueNewComment"):this._e(),this._v(" "),n("VssueNotice"),this._v(" "),n("TransitionFade",[this.vssue.comments&&this.vssue.comments.data.length>0?n("VssueComments"):n("VssueStatus")],1)],1)],1)},staticRenderFns:[]},void 0,td,void 0,!1,void 0,!1,void 0,void 0,void 0);let id=class extends Wt{};Dc([Cl()],id.prototype,"vssue",void 0),id=Dc([Pl],id);const rd=Gc({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-header"},[t("a",{staticClass:"vssue-header-comments-count",attrs:{href:e.vssue.issue?e.vssue.issue.link:null,target:"_blank",rel:"noopener noreferrer"}},[t("span",[e._v("\n      "+e._s(e.vssue.comments?e.vssue.$tc("comments",e.vssue.comments.count,{count:e.vssue.comments.count}):e.vssue.$tc("comments",0))+"\n    ")])]),e._v(" "),t("span",{staticClass:"vssue-header-powered-by"},[t("span",[e._v("Powered by")]),e._v(" "),e.vssue.API?t("span",[t("a",{attrs:{href:e.vssue.API.platform.link,title:e.vssue.API.platform.name+" API "+e.vssue.API.platform.version,target:"_blank",rel:"noopener noreferrer"}},[e._v("\n        "+e._s(e.vssue.API.platform.name)+"\n      ")]),e._v(" "),t("span",[e._v("&")])]):e._e(),e._v(" "),t("a",{attrs:{href:"https://github.com/meteorlxy/vssue",title:"Vssue v"+e.vssue.version,target:"_blank",rel:"noopener noreferrer"}},[e._v("\n      Vssue\n    ")])])])},staticRenderFns:[]},void 0,id,void 0,!1,void 0,!1,void 0,void 0,void 0),od={login:"Login with {platform}",logout:"Logout",currentUser:"Current User",loading:"Loading",submit:"Submit",submitting:"Submitting",submitComment:"Submit Comment",cancel:"Cancel",edit:"Edit",editMode:"Edit Mode",delete:"Delete",reply:"Reply",heart:"Heart",like:"Like",unlike:"Unlike",perPage:"Comments per page",sort:"Click to change the sort direction",page:"Page",prev:"Previous Page",next:"Next Page",comments:"Comments | {count} Comment | {count} Comments",loginToComment:"Login with {platform} account to leave a comment",placeholder:"Leave a comment. Styling with Markdown is supported. Ctrl + Enter to submit.",noLoginPlaceHolder:"Login to leave a comment. Styling with Markdown is supported. ",failed:"Failed to load comments",initializing:"Initializing...",issueNotCreated:"Click to create issue",loadingComments:"Loading comments...",loginRequired:"Login to view comments",noComments:"No comments yet. Leave the first comment !",reactionGiven:"Already given '{reaction}' reaction",deleteConfirm:"Confirm to delete this comment ?",deleteFailed:"Failed to delete comment"},sd={login:"使用 {platform} 登录",logout:"退出登录",currentUser:"当前用户",loading:"加载中",submit:"提交",submitting:"发表中",submitComment:"发表评论",cancel:"取消",edit:"编辑",editMode:"编辑模式",delete:"删除",reply:"回复",heart:"喜欢",like:"赞",unlike:"踩",perPage:"每页评论数",sort:"点击改变排序方式",page:"页数",prev:"上一页",next:"下一页",comments:"评论 | {count} 条评论 | {count} 条评论",loginToComment:"使用 {platform} 帐号登录后发表评论",placeholder:"留下你的评论丨支持 Markdown 语法丨Ctrl + Enter 发表评论",noLoginPlaceHolder:"登录后才能发表评论丨支持 Markdown 语法",failed:"评论加载失败",initializing:"正在初始化...",issueNotCreated:"点击创建 Issue",loadingComments:"正在加载评论...",loginRequired:"登录后查看评论",noComments:"还没有评论，来发表第一条评论吧！",reactionGiven:"已经添加过 '{reaction}' 了",deleteConfirm:"确认要删除该评论吗？",deleteFailed:"评论删除失败"},ld={login:"Entrar com {platform}",logout:"Sair",currentUser:"Usuário Atual",loading:"Carregando",submit:"Enviar",submitting:"Enviando",submitComment:"Enviar Comentário",cancel:"Cancelar",edit:"Editar",editMode:"Modo de Edição",delete:"Apagar",reply:"Responder",heart:"Heart",like:"Like",unlike:"Unlike",perPage:"Comentários por página",sort:"Clique para alterar a ordenação",page:"Página",prev:"Página Anterior",next:"Próxima Página",comments:"Comentários | {count} Comentário | {count} Comentários",loginToComment:"Entre com uma conta {platform} para deixar um comentário",placeholder:"Deixe um comentário. Estilos com Markdown suportados. Ctrl + Enter para enviar.",noLoginPlaceHolder:"Entre para deixar um comentário. Estilos com Markdown suportados. ",failed:"Falha ao carregar comentários",initializing:"Inicializando...",issueNotCreated:"Click to create issue",loadingComments:"Carregando comentários...",loginRequired:"Entrar para visualizar comentários",noComments:"Nenhum comentário. Deixe o primeiro comentário!",reactionGiven:"Já reagiu com '{reaction}'",deleteConfirm:"Apagar este comentário?",deleteFailed:"Falha ao apagar comentário"},cd={login:"{platform} でログイン",logout:"ログアウト",currentUser:"現在のユーザー",loading:"読み込み中",submit:"送信",submitting:"送信中",submitComment:"コメントを送信",cancel:"キャンセル",edit:"編集",editMode:"編集モード",delete:"削除",reply:"返信",heart:"ハート",like:"高評価",unlike:"低評価",perPage:"コメント/ページ",sort:"並び順を変更するにはクリックしてください",page:"ページ",prev:"前のページ",next:"次のページ",comments:"コメント | {count} コメント | {count} コメント",loginToComment:"コメントを残すには {platform} アカウントでログインしてください。",placeholder:"コメントを残してください。Markdown 記法をサポートしています。 Ctrl + Enter で送信できます。",noLoginPlaceHolder:"コメントを残すにはログインしてください。マークダウン記法をサポートしています。",failed:"コメントの読み込みに失敗しました",initializing:"初期化中...",issueNotCreated:"Click to create issue",loadingComments:"コメントの読み込み中...",loginRequired:"コメントを見るにはログインしてください",noComments:"まだコメントがありません。最初のコメントを残しましょう！",reactionGiven:"既に '{reaction}' のリアクションをしています",deleteConfirm:"本当にコメントを削除してもいいですか？",deleteFailed:"コメントの削除に失敗しました"},dd={login:"התחברו עם {platform}",logout:"התנתקו",currentUser:"משתמש/ת נוכחי/ת",loading:"טוען",submit:"שליחה",submitting:"שולח",submitComment:"שליחת תגובה",cancel:"ביטל",edit:"עריכה",editMode:"מצב עריכה",delete:"מחיקה",reply:"תשובה",heart:"לב",like:"לייק",unlike:"אנלייק",perPage:"תגובות לדף",sort:"לחצו כדי לשנות את כיוון המיון",page:"דף",prev:"הדף הקודם",next:"הדף הבא",comments:"תגובות | {count} תגובה | {count} תגובות",loginToComment:"התחברו עם חשבון {platform} כדי להשאיר תגובה",placeholder:"השאירו תגובה. יש תמיכה בעיצוב בעזרת Markdown. Ctrl + Enter כדי לשלוח.",noLoginPlaceHolder:"התחברו כדי להשאיר תגובה. יש תמיכה בעיצוב בעזרת Markdown. ",failed:"כשלון בטעינת התגובות",initializing:"מאתחל...",issueNotCreated:"לחצו ליצירת issue",loadingComments:"טוען תגובות...",loginRequired:"התחברו כדי לצפות בתגובות",noComments:"עדיין אין תגובות. השאירו תגובה ראשונה !",reactionGiven:"כבר ניתן חיווי '{reaction}'",deleteConfirm:"בטוחים במחיקת התגובה ?",deleteFailed:"כשלון במחיקת התגובה"};Object.prototype.hasOwnProperty.call(Wt,"$i18n")||Wt.use(Rc);const hd=new Rc({locale:"en",fallbackLocale:"en",messages:{en:od,"en-US":od,zh:sd,"zh-CN":sd,pt:ld,"pt-BR":ld,ja:cd,"ja-JP":cd,he:dd,"he-IL":dd}});let md=class extends Wt{constructor(){super(...arguments),this.title=e=>`${e.prefix}${document.title}`,this.issueId=null,this.options=null,this.API=null,this.accessToken=null,this.user=null,this.issue=null,this.comments=null,this.query={page:1,perPage:10,sort:"desc"},this.isInitializing=!0,this.isIssueNotCreated=!1,this.isLoginRequired=!1,this.isFailed=!1,this.isCreatingIssue=!1,this.isLoadingComments=!1,this.isCreatingComment=!1,this.isUpdatingComment=!1}get version(){return"1.4.8"}get issueTitle(){return null===this.options?"":"function"==typeof this.title?this.title(this.options):`${this.options.prefix}${this.title}`}get isPending(){return this.isLoadingComments||this.isCreatingComment||this.isUpdatingComment}get isLogined(){return null!==this.accessToken&&null!==this.user}get isAdmin(){return null!==this.options&&null!==this.accessToken&&null!==this.user&&(this.user.username===this.options.owner||this.options.admins.includes(this.user.username))}get accessTokenKey(){return this.API?`Vssue.${this.API.platform.name.toLowerCase()}.access_token`:""}onQueryPerPageChange(){this.query.page=1,this.getComments()}onQueryChange(){this.getComments()}setOptions(e){this.options=Object.assign({labels:["Vssue"],state:"Vssue",prefix:"[Vssue]",admins:[],perPage:10,proxy:e=>"https://cors-anywhere.azm.workers.dev/"+e,issueContent:({url:e})=>e,autoCreateIssue:!1},e);const n=["api","owner","repo","clientId"];for(const e of n)this.options[e]||console.warn(`[Vssue] the option '${e}' is required`);if(this.options.locale)this.$i18n.locale=this.options.locale;else{const e=Object.keys(this.$i18n.messages),n=window.navigator.languages;this.$i18n.locale=n.filter(n=>e.includes(n)).shift()||"en"}}async init(){try{await this.initStore(),await this.initComments()}catch(e){e.response&&[401,403].includes(e.response.status)?this.isLoginRequired=!0:this.isFailed=!0,console.error(e)}}async initStore(){try{if(!this.options)throw new Error("Options are required to initialize Vssue");this.API=null,this.accessToken=null,this.user=null,this.issue=null,this.comments=null,this.query={page:1,perPage:this.options.perPage,sort:"desc"},this.isInitializing=!0,this.isIssueNotCreated=!1,this.isLoginRequired=!1,this.isFailed=!1,this.isCreatingIssue=!1,this.isLoadingComments=!1,this.isCreatingComment=!1,this.isUpdatingComment=!1;const e=this.options.api;this.API=new e({baseURL:this.options.baseURL,labels:this.options.labels,state:this.options.state,owner:this.options.owner,repo:this.options.repo,clientId:this.options.clientId,clientSecret:this.options.clientSecret,proxy:this.options.proxy}),await this.handleAuth()}finally{this.isInitializing=!1}}async initComments(){if(this.API&&this.options)if(this.issueId){const[e,n]=await Promise.all([this.API.getIssue({accessToken:this.accessToken,issueId:this.issueId}),this.API.getComments({accessToken:this.accessToken,issueId:this.issueId,query:this.query})]);this.issue=e,this.comments=n}else this.issue=await this.API.getIssue({accessToken:this.accessToken,issueTitle:this.issueTitle}),null===this.issue?(this.isIssueNotCreated=!0,this.options.autoCreateIssue&&await this.postIssue()):await this.getComments()}async postIssue(){if(this.API&&this.options&&!this.issue&&!this.issueId&&(this.isLogined||this.login(),this.isAdmin))try{this.isCreatingIssue=!0;const e=await this.API.postIssue({title:this.issueTitle,content:await this.options.issueContent({options:this.options,url:Bl(window.location.href)}),accessToken:this.accessToken});this.issue=e,this.isIssueNotCreated=!1,await this.getComments()}catch(e){this.isFailed=!0}finally{this.isCreatingIssue=!1}}async getComments(){try{if(!this.API||!this.issue||this.isLoadingComments)return;this.isLoadingComments=!0;const e=await this.API.getComments({accessToken:this.accessToken,issueId:this.issue.id,query:this.query});return this.comments=e,this.query.page!==e.page&&(this.query.page=e.page),this.query.perPage!==e.perPage&&(this.query.perPage=e.perPage),e}catch(e){if(!e.response||![401,403].includes(e.response.status)||this.isLogined)throw this.$emit("error",e),e;this.isLoginRequired=!0}finally{this.isLoadingComments=!1}}async postComment({content:e}){try{if(!this.API||!this.issue||this.isCreatingComment)return;this.isCreatingComment=!0;return await this.API.postComment({accessToken:this.accessToken,content:e,issueId:this.issue.id})}catch(e){throw this.$emit("error",e),e}finally{this.isCreatingComment=!1}}async putComment({commentId:e,content:n}){try{if(!this.API||!this.issue)return;return await this.API.putComment({accessToken:this.accessToken,issueId:this.issue.id,commentId:e,content:n})}catch(e){throw this.$emit("error",e),e}}async deleteComment({commentId:e}){try{if(!this.API||!this.issue)return;return await this.API.deleteComment({accessToken:this.accessToken,issueId:this.issue.id,commentId:e})}catch(e){throw this.$emit("error",e),e}}async getCommentReactions({commentId:e}){try{if(!this.API||!this.issue)return;return await this.API.getCommentReactions({accessToken:this.accessToken,issueId:this.issue.id,commentId:e})}catch(e){throw this.$emit("error",e),e}}async postCommentReaction({commentId:e,reaction:n}){try{if(!this.API||!this.issue)return!1;return await this.API.postCommentReaction({accessToken:this.accessToken,issueId:this.issue.id,commentId:e,reaction:n})}catch(e){throw this.$emit("error",e),e}}login(){this.API&&this.API.redirectAuth()}logout(){this.setAccessToken(null),this.user=null}async handleAuth(){if(!this.API)return;const e=await this.API.handleAuth();e?(this.setAccessToken(e),this.user=await this.API.getUser({accessToken:e})):this.getAccessToken()?this.user=await this.API.getUser({accessToken:this.accessToken}):(this.setAccessToken(null),this.user=null)}getAccessToken(){return this.accessToken=window.localStorage.getItem(this.accessTokenKey),this.accessToken}setAccessToken(e){null===e?window.localStorage.removeItem(this.accessTokenKey):window.localStorage.setItem(this.accessTokenKey,e),this.accessToken=e}};Dc([ql("query.perPage")],md.prototype,"onQueryPerPageChange",null),Dc([ql("query.page"),ql("query.sort")],md.prototype,"onQueryChange",null),md=Dc([Pl({i18n:hd})],md);var ud=md;let pd=class extends Wt{constructor(){super(...arguments),this.vssue=new ud}onOptionsChange(e){this.vssue.setOptions(e)}mounted(){null!==this.title&&(this.vssue.title=this.title),null!==this.issueId&&(this.vssue.issueId=this.issueId),this.vssue.setOptions(this.options),this.vssue.init()}};var gd;Dc([Ul({type:[String,Function],required:!1,default:null})],pd.prototype,"title",void 0),Dc([Ul({type:[String,Number],required:!1,default:null})],pd.prototype,"issueId",void 0),Dc([Ul({type:Object,required:!1,default:()=>({})})],pd.prototype,"options",void 0),Dc([(gd="vssue",wl((function(e,n){var t=e.provide;Il(t)&&(t=e.provide=Al(t)),t.managed[n]=gd||n})))],pd.prototype,"vssue",void 0),Dc([ql("options",{deep:!0})],pd.prototype,"onOptionsChange",null),pd=Dc([Pl({components:{Iconfont:Fc,VssueBody:ad,VssueHeader:rd}})],pd);const fd=Gc({render:function(){var e=this.$createElement,n=this._self._c||e;return n("div",{staticClass:"vssue"},[n("Iconfont"),this._v(" "),n("VssueHeader"),this._v(" "),n("VssueBody")],1)},staticRenderFns:[]},void 0,pd,void 0,!1,void 0,!1,void 0,void 0,void 0);var yd=t(149),bd=t.n(yd);function vd(e){return{username:e.login,avatar:e.avatar_url,homepage:e.html_url}}function wd(e){return{id:e.number,title:e.title,content:e.body,link:e.html_url}}function _d(e){return{like:e["+1"],unlike:e[-1],heart:e.heart}}function kd(e){return{id:e.id,content:e.body_html,contentRaw:e.body,author:vd(e.user),createdAt:e.created_at,updatedAt:e.updated_at,reactions:_d(e.reactions)}}function xd(e){return"like"===e?"+1":"unlike"===e?"-1":e}class Td{constructor({baseURL:e="https://github.com",owner:n,repo:t,labels:a,clientId:i,clientSecret:r,state:o,proxy:s}){if(void 0===r||void 0===s)throw new Error("clientSecret and proxy is required for GitHub V3");this.baseURL=e,this.owner=n,this.repo=t,this.labels=a,this.clientId=i,this.clientSecret=r,this.state=o,this.proxy=s,this.$http=bd.a.create({baseURL:"https://github.com"===e?"https://api.github.com":Dl(e,"api/v3"),headers:{Accept:"application/vnd.github.v3+json"}}),this.$http.interceptors.response.use(e=>e.data&&e.data.error?Promise.reject(new Error(e.data.error_description)):e,e=>(void 0===e.response&&"Network Error"===e.message&&(e.response={status:403}),Promise.reject(e)))}get platform(){return{name:"GitHub",link:this.baseURL,version:"v3",meta:{reactable:!0,sortable:!1}}}redirectAuth(){window.location.href=Rl(Dl(this.baseURL,"login/oauth/authorize"),{client_id:this.clientId,redirect_uri:window.location.href,scope:"public_repo",state:this.state})}async handleAuth(){const e=(n=window.location.search,Object(El.parse)(n,{ignoreQueryPrefix:!0}));var n;if(e.code){if(e.state!==this.state)return null;const n=e.code;delete e.code,delete e.state;const t=Rl(Bl(window.location.href),e)+window.location.hash;window.history.replaceState(null,"",t);return await this.getAccessToken({code:n})}return null}async getAccessToken({code:e}){const n=Dl(this.baseURL,"login/oauth/access_token"),t="function"==typeof this.proxy?this.proxy(n):this.proxy,{data:a}=await this.$http.post(t,{client_id:this.clientId,client_secret:this.clientSecret,code:e},{headers:{Accept:"application/json"}});return a.access_token}async getUser({accessToken:e}){const{data:n}=await this.$http.get("user",{headers:{Authorization:"token "+e}});return vd(n)}async getIssue({accessToken:e,issueId:n,issueTitle:t}){const a={};if(e&&(a.headers={Authorization:"token "+e}),!n){a.params={q:[`"${t}"`,"is:issue","in:title",`repo:${this.owner}/${this.repo}`,"is:public",...this.labels.map(e=>"label:"+e)].join(" "),timestamp:Date.now()};const{data:e}=await this.$http.get("search/issues",a);return e.items.map(wd).find(e=>e.title===t)||null}try{a.params={timestamp:Date.now()};const{data:e}=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/${n}`,a);return wd(e)}catch(e){if(e.response&&404===e.response.status)return null;throw e}}async postIssue({accessToken:e,title:n,content:t}){const{data:a}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues`,{title:n,body:t,labels:this.labels},{headers:{Authorization:"token "+e}});return wd(a)}async getComments({accessToken:e,issueId:n,query:{page:t=1,perPage:a=10}={}}){const i={params:{timestamp:Date.now()}},r={params:{page:t,per_page:a,timestamp:Date.now()},headers:{Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}};e&&(i.headers={Authorization:"token "+e},r.headers.Authorization="token "+e);const[o,s]=await Promise.all([this.$http.get(`repos/${this.owner}/${this.repo}/issues/${n}`,i),this.$http.get(`repos/${this.owner}/${this.repo}/issues/${n}/comments`,r)]),l=s.headers.link||null,c=/rel="next"/.test(l)?Number(l.replace(/^.*[^_]page=(\d*).*rel="next".*$/,"$1"))-1:/rel="prev"/.test(l)?Number(l.replace(/^.*[^_]page=(\d*).*rel="prev".*$/,"$1"))+1:1,d=l?Number(l.replace(/^.*per_page=(\d*).*$/,"$1")):a;return{count:Number(o.data.comments),page:c,perPage:d,data:s.data.map(kd)}}async postComment({accessToken:e,issueId:n,content:t}){const{data:a}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues/${n}/comments`,{body:t},{headers:{Authorization:"token "+e,Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}});return kd(a)}async putComment({accessToken:e,commentId:n,content:t}){const{data:a}=await this.$http.patch(`repos/${this.owner}/${this.repo}/issues/comments/${n}`,{body:t},{headers:{Authorization:"token "+e,Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}});return kd(a)}async deleteComment({accessToken:e,commentId:n}){const{status:t}=await this.$http.delete(`repos/${this.owner}/${this.repo}/issues/comments/${n}`,{headers:{Authorization:"token "+e}});return 204===t}async getCommentReactions({accessToken:e,commentId:n}){const{data:t}=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/comments/${n}`,{params:{timestamp:Date.now()},headers:{Authorization:"token "+e,Accept:"application/vnd.github.squirrel-girl-preview"}});return _d(t.reactions)}async postCommentReaction({accessToken:e,commentId:n,reaction:t}){const a=await this.$http.post(`repos/${this.owner}/${this.repo}/issues/comments/${n}/reactions`,{content:xd(t)},{headers:{Authorization:"token "+e,Accept:"application/vnd.github.squirrel-girl-preview"}});return 200===a.status?this.deleteCommentReaction({accessToken:e,commentId:n,reactionId:a.data.id}):201===a.status}async deleteCommentReaction({accessToken:e,commentId:n,reactionId:t}){return 204===(await this.$http.delete(`repos/${this.owner}/${this.repo}/issues/comments/${n}/reactions/${t}`,{headers:{Authorization:"token "+e,Accept:"application/vnd.github.squirrel-girl-preview"}})).status}}var zd=t(150),Md=t.n(zd),Pd=(t(373),{components:{VssueComponent:fd},data:()=>({isLoad:!1}),watch:{"$page.key"(e,n){e&&n!==e&&this.initialize()}},mounted(){this.$nextTick(()=>{this.initialize()})},methods:{initialize(){this.$el.remove(),this.isLoad=!1,setTimeout(()=>{if(this.needComment(this.$frontmatter)){document.querySelector("main").appendChild(this.$el),this.title=Md.a.render("[Comment]<%- frontmatter.title %>",{frontmatter:this.$frontmatter}),this.options={api:Td,autoCreateIssue:!0,clientId:"adb9fb0ac1159e00ce7f",clientSecret:"27da8dc85f808c2bd1b6e44da5ae69c4ddf17d8d",owner:"eryajf",repo:"qishao-notes"},this.isLoad=!0}},1e3)},needComment:e=>!1!==e.comment&&!1!==e.comments}}),Cd=Object(il.a)(Pd,(function(){var e=this._self._c;return this.isLoad?e("div",[e("VssueComponent",{attrs:{title:this.title,options:this.options}})],1):this._e()}),[],!1,null,null,null).exports,Ad=[({Vue:e,options:n,router:t,siteData:a})=>{},({Vue:e,options:n,router:t,siteData:a})=>{a.pages.map(e=>{const{frontmatter:{date:n,author:t}}=e;"string"==typeof n&&"Z"===n.charAt(n.length-1)&&(e.frontmatter.date=function(e){e instanceof Date||(e=new Date(e));return`${e.getUTCFullYear()}-${ml(e.getUTCMonth()+1)}-${ml(e.getUTCDate())} ${ml(e.getUTCHours())}:${ml(e.getUTCMinutes())}:${ml(e.getUTCSeconds())}`}(n)),t?e.author=t:a.themeConfig.author&&(e.author=a.themeConfig.author)}),e.mixin(hl)},{},({Vue:e})=>{e.mixin({computed:{$dataBlock(){return this.$options.__data__block__}}})},{},{},({Vue:e})=>{e.component("Vssue",Cd)}],Id=["Vssue"];class Sd extends class{constructor(){this.store=new Wt({data:{state:{}}})}$get(e){return this.store.state[e]}$set(e,n){Wt.set(this.store.state,e,n)}$emit(...e){this.store.$emit(...e)}$on(...e){this.store.$on(...e)}}{}Object.assign(Sd.prototype,{getPageAsyncComponent:ss,getLayoutAsyncComponent:ls,getAsyncComponent:cs,getVueComponent:ds});var Ld={install(e){const n=new Sd;e.$vuepress=n,e.prototype.$vuepress=n}};function Ud(e,n){return e.options.routes.filter(e=>e.path.toLowerCase()===n.toLowerCase()).length>0}var qd={props:{pageKey:String,slotKey:{type:String,default:"default"}},render(e){const n=this.pageKey||this.$parent.$page.key;return ms("pageKey",n),Wt.component(n)||Wt.component(n,ss(n)),Wt.component(n)?e(n):e("")}},Ed={functional:!0,props:{slotKey:String,required:!0},render:(e,{props:n,slots:t})=>e("div",{class:["content__"+n.slotKey]},t()[n.slotKey])},Rd={computed:{openInNewWindowTitle(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},Dd=(t(374),t(375),Object(il.a)(Rd,(function(){var e=this._self._c;return e("span",[e("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[e("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),e("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),e("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports),Od={functional:!0,render(e,{parent:n,children:t}){if(n._isMounted)return t;n.$once("hook:mounted",()=>{n.$forceUpdate()})}};Wt.config.productionTip=!1,Wt.use(Ho),Wt.use(Ld),Wt.mixin(function(e,n,t=Wt){!function(e){e.locales&&Object.keys(e.locales).forEach(n=>{e.locales[n].path=n});Object.freeze(e)}(n),t.$vuepress.$set("siteData",n);const a=new(e(t.$vuepress.$get("siteData"))),i=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(a)),r={};return Object.keys(i).reduce((e,n)=>(n.startsWith("$")&&(e[n]=i[n].get),e),r),{computed:r}}(e=>class{setPage(e){this.__page=e}get $site(){return e}get $themeConfig(){return this.$site.themeConfig}get $frontmatter(){return this.$page.frontmatter}get $localeConfig(){const{locales:e={}}=this.$site;let n,t;for(const a in e)"/"===a?t=e[a]:0===this.$page.path.indexOf(a)&&(n=e[a]);return n||t||{}}get $siteTitle(){return this.$localeConfig.title||this.$site.title||""}get $canonicalUrl(){const{canonicalUrl:e}=this.$page.frontmatter;return"string"==typeof e&&e}get $title(){const e=this.$page,{metaTitle:n}=this.$page.frontmatter;if("string"==typeof n)return n;const t=this.$siteTitle,a=e.frontmatter.home?null:e.frontmatter.title||e.title;return t?a?a+" | "+t:t:a||"VuePress"}get $description(){const e=function(e){if(e){const n=e.filter(e=>"description"===e.name)[0];if(n)return n.content}}(this.$page.frontmatter.meta);return e||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}get $lang(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}get $localePath(){return this.$localeConfig.path||"/"}get $themeLocaleConfig(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}get $page(){return this.__page?this.__page:function(e,n){for(let t=0;t<e.length;t++){const a=e[t];if(a.path.toLowerCase()===n.toLowerCase())return a}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}},sl)),Wt.component("Content",qd),Wt.component("ContentSlotsDistributor",Ed),Wt.component("OutboundLink",Dd),Wt.component("ClientOnly",Od),Wt.component("Layout",ls("Layout")),Wt.component("NotFound",ls("NotFound")),Wt.prototype.$withBase=function(e){const n=this.$site.base;return"/"===e.charAt(0)?n+e.slice(1):e},window.__VUEPRESS__={version:"1.8.0",hash:"f77ae43"},async function(e){const n="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:sl.routerBase||sl.base,t=new Ho({base:n,mode:"history",fallback:!1,routes:ol,scrollBehavior:(e,n,t)=>t||(e.hash?!Wt.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(e.hash)}:{x:0,y:0})});!function(e){e.beforeEach((n,t,a)=>{if(Ud(e,n.path))a();else if(/(\/|\.html)$/.test(n.path))if(/\/$/.test(n.path)){const t=n.path.replace(/\/$/,"")+".html";Ud(e,t)?a(t):a()}else a();else{const t=n.path+"/",i=n.path+".html";Ud(e,i)?a(i):Ud(e,t)?a(t):a()}})}(t);const a={};try{await Promise.all(Ad.filter(e=>"function"==typeof e).map(n=>n({Vue:Wt,options:a,router:t,siteData:sl,isServer:e})))}catch(e){console.error(e)}return{app:new Wt(Object.assign(a,{router:t,render:e=>e("div",{attrs:{id:"app"}},[e("RouterView",{ref:"layout"}),e("div",{class:"global-ui"},Id.map(n=>e(n)))])})),router:t}}(!1).then(({app:e,router:n})=>{n.onReady(()=>{e.$mount("#app")})})}]);