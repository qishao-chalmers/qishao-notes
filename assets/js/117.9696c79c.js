(window.webpackJsonp=window.webpackJsonp||[]).push([[117],{570:function(e,t,s){"use strict";s.r(t);var a=s(9),n=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("[Y2024] Understanding Performance Implications of LLM Inference on CPUs")])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_1-y2024-understanding-performance-implications-of-llm-inference-on-cpus"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-y2024-understanding-performance-implications-of-llm-inference-on-cpus"}},[e._v("#")]),e._v(" 1. [Y2024] Understanding Performance Implications of LLM Inference on CPUs")]),e._v(" "),t("p",[t("strong",[e._v("Flops")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/43b30c0a-14ac-4239-9cd1-61c65fb0d21f",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("Intel AMX Architecture")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/fe5d81c7-97bc-4029-ac73-118328ae5e6a",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("Model Size")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/b5dbd3ab-f3e0-4772-a8c8-eb43bc777afa",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("KV Cache Size")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/1d7eabab-388f-4cd8-b7aa-9351dc79420e",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("CPU Servers")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/3d5b683a-86d2-4fff-9093-8322c3448027",alt:"image"}})]),e._v(" "),t("p",[e._v("The memory of CPU is much bigger than GPU.")]),e._v(" "),t("p",[e._v("These normalized results highlight the performance benefits gained from the use of both the matrix multiplication accelerator and high-bandwidth memory on the SPR Max CPU.")]),e._v(" "),t("p",[e._v("The significant reduction in latency and improvement in throughput during the prefill phase is due to AMX support on the SPR Max CPU.")]),e._v(" "),t("p",[e._v("The throughput improvement in the memorybound decode phase is made possible by the higher memory bandwidth provided by HBM.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/c75e9230-f6c0-4087-b349-e563d3f4ebb5",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("With larger batch sizes, both models exhibit a decrease in LLC MPKI and an increase in core utilization, indicating a shift towards a more compute-bound execution.")])]),e._v(" "),t("p",[t("strong",[e._v("Key Finding#1")])]),e._v(" "),t("p",[e._v("With AMX support, larger cores and cache, and HBM integration, the SPR Max CPU significantly reduces latency and increases throughput for BF16 LLM inference compared to the ICL CPU.")]),e._v(" "),t("p",[t("strong",[e._v("Key Finding#2")])]),e._v(" "),t("p",[e._v("Proper memory and clustering configurations are essential for optimizing performance.")]),e._v(" "),t("p",[e._v("The Flat memory mode with Quadrant clustering offers the best latency and throughput for LLM inference.")]),e._v(" "),t("p",[t("strong",[e._v("Key Finding#3")])]),e._v(" "),t("p",[e._v("Using 48 SPR cores with HBM maximizes core utilization and minimizes inter-socket communication,\nresulting in the best performance across models.")]),e._v(" "),t("p",[t("strong",[e._v("GPU")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/98d80cd2-4d22-448a-bda7-48cd7be9cc7d",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("Key Finding#4")])]),e._v(" "),t("p",[e._v("Overall, GPUs outperform CPUs in LLM inference, but AMX-enabled CPUs can achieve lower latency and higher throughput for larger models requiring offloading.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/ac3a7b7d-ebc3-4a9f-8c13-a113f04bd62b",alt:"image"}})]),e._v(" "),t("p",[e._v("As the number of input tokens increases, GPU latency and throughput remain stable, while the SPR Max 9468 CPU shows more variability.")]),e._v(" "),t("p",[e._v("This is due to the CPU’s lower compute throughput and memory bandwidth, resulting in less favorable performance scalability.")]),e._v(" "),t("p",[e._v("Interestingly, for larger models such as LLaMA2-70B, the CPU outperforms the GPU in both latency and throughput across all sequence lengths.")]),e._v(" "),t("p",[e._v("This is primarily due to the significant time spent on data loading via the PCIe bus when the batch size is set to 1, as shown in Figure 18.")]),e._v(" "),t("p",[e._v("As the batch size increases to 16, the performance gap between CPUs and GPUs widens, particularly for smaller models.")]),e._v(" "),t("p",[e._v("For larger models such as LLaMA2-70B, we observed that at sequence lengths of 256 or more, the H100 GPU even when using offloading-based LLM inference—achieves lower latency compared to the CPU.")]),e._v(" "),t("p",[e._v("This is because, at these longer sequence lengths, the CPU’s LLM inference throughput continues to decline, resulting in lower performance than the H100.")]),e._v(" "),t("p",[e._v("However, in the case of the A100 GPU, the CPU outperforms the GPU across all sequence lengths.")]),e._v(" "),t("p",[e._v("This demonstrates that lower PCIe bandwidth significantly degrades the performance of offloading-based LLM serving systems.")]),e._v(" "),t("p",[t("strong",[e._v("Key Finding#5")])]),e._v(" "),t("p",[e._v("For larger batch sizes, GPUs outperform CPUs in small models. Even in larger models that require offloading, CPUs may underperform at longer sequence lengths due to lower compute throughput.")]),e._v(" "),t("p",[t("strong",[e._v("We also note that new Grace-Hopper Superchip would see lower overheads for offloading from DRAM to the integrated H100 due to its higher NVLink bandwidth (900 GB/s versus PCIe 5.0’s 128 GB/s), albeit at a cost of ∼4x of the SPR CPU\nand DDR5 [40].")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/54409207-cc8b-48aa-b0c5-67365acf1b4e",alt:"image"}})])])}),[],!1,null,null,null);t.default=n.exports}}]);