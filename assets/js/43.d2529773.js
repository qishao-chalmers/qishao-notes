(window.webpackJsonp=window.webpackJsonp||[]).push([[43],{497:function(e,a,s){"use strict";s.r(a);var t=s(8),n=Object(t.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"mlir-bufferization-passes"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mlir-bufferization-passes"}},[e._v("#")]),e._v(" MLIR Bufferization Passes")]),e._v(" "),a("blockquote",[a("p",[e._v("This is generated by ChatGPT.")])]),e._v(" "),a("h2",{attrs:{id:"_1Ô∏è‚É£-what-is-bufferization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1Ô∏è‚É£-what-is-bufferization"}},[e._v("#")]),e._v(" 1Ô∏è‚É£ What is Bufferization?")]),e._v(" "),a("p",[e._v("Bufferization is the process of converting tensor-based computations into memory-based (memref) computations in MLIR.\nIt allows transitioning from a high-level functional-style representation (immutable tensors) to explicit memory management (mutable memrefs), which is required for hardware execution (CPU, GPU, etc.).")]),e._v(" "),a("h3",{attrs:{id:"‚úÖ-why-is-bufferization-needed"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#‚úÖ-why-is-bufferization-needed"}},[e._v("#")]),e._v(" ‚úÖ Why is Bufferization Needed?")]),e._v(" "),a("ul",[a("li",[e._v("Tensors are immutable: Every tensor operation creates a new tensor.")]),e._v(" "),a("li",[e._v("MemRefs are mutable: Avoids unnecessary copies, enabling in-place updates.")]),e._v(" "),a("li",[e._v("Hardware requires explicit memory management: Low-level backends (LLVM, CUDA) work with pointers, not abstract tensors.")])]),e._v(" "),a("h2",{attrs:{id:"_2Ô∏è‚É£-bufferization-pipeline"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2Ô∏è‚É£-bufferization-pipeline"}},[e._v("#")]),e._v(" 2Ô∏è‚É£ Bufferization Pipeline")]),e._v(" "),a("p",[e._v("Bufferization transforms tensor-based operations into memref-based operations in multiple stages.")]),e._v(" "),a("h3",{attrs:{id:"üîπ-high-level-tensor-computation-functional-style"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#üîπ-high-level-tensor-computation-functional-style"}},[e._v("#")]),e._v(" üîπ High-Level Tensor Computation (Functional Style)")]),e._v(" "),a("p",[e._v("Operations use immutable tensor"),a("shape",{attrs:{x:"",type:""}},[e._v(" values.\nExample:")])],1),e._v(" "),a("div",{staticClass:"language-mlir line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("%B = linalg.matmul ins(%A, %A : tensor<4x4xf32>, tensor<4x4xf32>)\n                   outs(%C : tensor<4x4xf32>) -> tensor<4x4xf32>\n")])]),e._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[e._v("1")]),a("br"),a("span",{staticClass:"line-number"},[e._v("2")]),a("br")])]),a("p",[e._v("%B is not modified in-place.")]),e._v(" "),a("p",[e._v("A new tensor is allocated implicitly.")]),e._v(" "),a("h3",{attrs:{id:"üîπ-bufferization-converting-tensor-to-memref"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#üîπ-bufferization-converting-tensor-to-memref"}},[e._v("#")]),e._v(" üîπ Bufferization (Converting Tensor to MemRef)")]),e._v(" "),a("p",[e._v("Explicit memory allocation (memref.alloc).\nUses mutability for in-place updates.\nExample:")]),e._v(" "),a("div",{staticClass:"language-mlir line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("%B_mem = memref.alloc() : memref<4x4xf32>\nlinalg.matmul ins(%A_mem, %A_mem : memref<4x4xf32>, memref<4x4xf32>)\n             outs(%B_mem : memref<4x4xf32>)\nmemref.dealloc %B_mem\n")])]),e._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[e._v("1")]),a("br"),a("span",{staticClass:"line-number"},[e._v("2")]),a("br"),a("span",{staticClass:"line-number"},[e._v("3")]),a("br"),a("span",{staticClass:"line-number"},[e._v("4")]),a("br")])]),a("p",[e._v("tensor<4x4xf32> ‚Üí memref<4x4xf32>.\nExplicit memory allocation (memref.alloc).\nManual deallocation (memref.dealloc).")]),e._v(" "),a("h3",{attrs:{id:"üîπ-lowering-to-llvm-final-execution"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#üîπ-lowering-to-llvm-final-execution"}},[e._v("#")]),e._v(" üîπ Lowering to LLVM (Final Execution)")]),e._v(" "),a("p",[e._v("MemRef is converted into LLVM pointers (llvm.ptr"),a("T",[e._v(").\nExample:")])],1),e._v(" "),a("div",{staticClass:"language-mlir line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("%ptr = llvm.getelementptr %B_mem[%i, %j] : (!llvm.ptr<f32>, i32, i32) -> !llvm.ptr<f32>\n%val = llvm.load %ptr : !llvm.ptr<f32>\n")])]),e._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[e._v("1")]),a("br"),a("span",{staticClass:"line-number"},[e._v("2")]),a("br")])]),a("h2",{attrs:{id:"_3Ô∏è‚É£-types-of-bufferization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3Ô∏è‚É£-types-of-bufferization"}},[e._v("#")]),e._v(" 3Ô∏è‚É£ Types of Bufferization")]),e._v(" "),a("p",[e._v("MLIR provides two types of bufferization:")]),e._v(" "),a("h3",{attrs:{id:"üîπ-1-one-shot-bufferization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#üîπ-1-one-shot-bufferization"}},[e._v("#")]),e._v(" üîπ 1. One-Shot Bufferization")]),e._v(" "),a("p",[e._v("Converts all tensors into memrefs in a single pass.\nLess flexible but efficient for static memory allocation.\nExample Pass:")]),e._v(" "),a("div",{staticClass:"language-sh line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sh"}},[a("code",[e._v("mlir-opt "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("--bufferize")]),e._v(" input.mlir\n")])]),e._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[e._v("1")]),a("br")])]),a("h3",{attrs:{id:"üîπ-2-progressive-partial-bufferization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#üîπ-2-progressive-partial-bufferization"}},[e._v("#")]),e._v(" üîπ 2. Progressive (Partial) Bufferization")]),e._v(" "),a("p",[e._v("Converts tensors incrementally, allowing analysis-based optimizations.\nHandles aliasing and inplace updates safely.\nExample:")]),e._v(" "),a("div",{staticClass:"language-sh line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sh"}},[a("code",[e._v("mlir-opt --partial-bufferize input.mlir\n")])]),e._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[e._v("1")]),a("br")])]),a("h2",{attrs:{id:"_4Ô∏è‚É£-bufferization-analysis-handling-aliasing-copies"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4Ô∏è‚É£-bufferization-analysis-handling-aliasing-copies"}},[e._v("#")]),e._v(" 4Ô∏è‚É£ Bufferization Analysis: Handling Aliasing & Copies")]),e._v(" "),a("p",[e._v("Bufferization must analyze if a tensor operation can be safely replaced with a mutable memref.")]),e._v(" "),a("h3",{attrs:{id:"üîπ-case-1-no-copy-needed-in-place-bufferization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#üîπ-case-1-no-copy-needed-in-place-bufferization"}},[e._v("#")]),e._v(" üîπ Case 1: No Copy Needed (In-Place Bufferization)")]),e._v(" "),a("p",[e._v("If only one operation writes to a tensor, it can be directly mapped to a memref.\nExample: In-Place Bufferization (No Copy Needed)")]),e._v(" "),a("div",{staticClass:"language-mlir line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("func.func @inplace_add(%A: tensor<4xf32>) -> tensor<4xf32> {\n  %B = tensor.add %A, %A : tensor<4xf32>\n  return %B\n}\n")])]),e._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[e._v("1")]),a("br"),a("span",{staticClass:"line-number"},[e._v("2")]),a("br"),a("span",{staticClass:"line-number"},[e._v("3")]),a("br"),a("span",{staticClass:"line-number"},[e._v("4")]),a("br")])]),a("p",[e._v("‚û° After Bufferization")]),e._v(" "),a("div",{staticClass:"language-mlir line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("func.func @inplace_add(%A_mem: memref<4xf32>) {\n  %B_mem = %A_mem  // No copy needed\n  scf.for %i = 0 to 4 {\n    %a = memref.load %A_mem[%i] : memref<4xf32>\n    %b = arith.addf %a, %a : f32\n    memref.store %b, %A_mem[%i] : memref<4xf32>\n  }\n}\n")])]),e._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[e._v("1")]),a("br"),a("span",{staticClass:"line-number"},[e._v("2")]),a("br"),a("span",{staticClass:"line-number"},[e._v("3")]),a("br"),a("span",{staticClass:"line-number"},[e._v("4")]),a("br"),a("span",{staticClass:"line-number"},[e._v("5")]),a("br"),a("span",{staticClass:"line-number"},[e._v("6")]),a("br"),a("span",{staticClass:"line-number"},[e._v("7")]),a("br"),a("span",{staticClass:"line-number"},[e._v("8")]),a("br")])]),a("p",[e._v("üìå No additional alloc() needed!")]),e._v(" "),a("h3",{attrs:{id:"üîπ-case-2-copy-needed-aliased-data"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#üîπ-case-2-copy-needed-aliased-data"}},[e._v("#")]),e._v(" üîπ Case 2: Copy Needed (Aliased Data)")]),e._v(" "),a("p",[e._v("If a tensor is used multiple times, a copy is required to prevent unintended modifications.")]),e._v(" "),a("p",[e._v("Example: Copy Required Due to Aliasing")]),e._v(" "),a("div",{staticClass:"language-mlir line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("func.func @aliasing_problem(%A: tensor<4xf32>) -> tensor<4xf32> {\n  %B = tensor.add %A, %A : tensor<4xf32>\n  %C = tensor.add %B, %A : tensor<4xf32>\n  return %C\n}\n")])]),e._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[e._v("1")]),a("br"),a("span",{staticClass:"line-number"},[e._v("2")]),a("br"),a("span",{staticClass:"line-number"},[e._v("3")]),a("br"),a("span",{staticClass:"line-number"},[e._v("4")]),a("br"),a("span",{staticClass:"line-number"},[e._v("5")]),a("br")])]),a("p",[e._v("‚û° After Bufferization")]),e._v(" "),a("div",{staticClass:"language-mlir line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("func.func @aliasing_problem(%A_mem: memref<4xf32>) {\n  %B_mem = memref.alloc() : memref<4xf32>\n  scf.for %i = 0 to 4 {\n    %a = memref.load %A_mem[%i] : memref<4xf32>\n    %b = arith.addf %a, %a : f32\n    memref.store %b, %B_mem[%i] : memref<4xf32>\n  }\n\n  %C_mem = memref.alloc() : memref<4xf32>\n  scf.for %i = 0 to 4 {\n    %b = memref.load %B_mem[%i] : memref<4xf32>\n    %a = memref.load %A_mem[%i] : memref<4xf32>\n    %c = arith.addf %b, %a : f32\n    memref.store %c, %C_mem[%i] : memref<4xf32>\n  }\n\n  memref.dealloc %B_mem\n  return\n}\n")])]),e._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[e._v("1")]),a("br"),a("span",{staticClass:"line-number"},[e._v("2")]),a("br"),a("span",{staticClass:"line-number"},[e._v("3")]),a("br"),a("span",{staticClass:"line-number"},[e._v("4")]),a("br"),a("span",{staticClass:"line-number"},[e._v("5")]),a("br"),a("span",{staticClass:"line-number"},[e._v("6")]),a("br"),a("span",{staticClass:"line-number"},[e._v("7")]),a("br"),a("span",{staticClass:"line-number"},[e._v("8")]),a("br"),a("span",{staticClass:"line-number"},[e._v("9")]),a("br"),a("span",{staticClass:"line-number"},[e._v("10")]),a("br"),a("span",{staticClass:"line-number"},[e._v("11")]),a("br"),a("span",{staticClass:"line-number"},[e._v("12")]),a("br"),a("span",{staticClass:"line-number"},[e._v("13")]),a("br"),a("span",{staticClass:"line-number"},[e._v("14")]),a("br"),a("span",{staticClass:"line-number"},[e._v("15")]),a("br"),a("span",{staticClass:"line-number"},[e._v("16")]),a("br"),a("span",{staticClass:"line-number"},[e._v("17")]),a("br"),a("span",{staticClass:"line-number"},[e._v("18")]),a("br"),a("span",{staticClass:"line-number"},[e._v("19")]),a("br")])]),a("p",[e._v("üìå A copy (memref.alloc()) is required for %B_mem because %A_mem is still used!")]),e._v(" "),a("h2",{attrs:{id:"_5Ô∏è‚É£-bufferization-for-gpu-execution"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5Ô∏è‚É£-bufferization-for-gpu-execution"}},[e._v("#")]),e._v(" 5Ô∏è‚É£ Bufferization for GPU Execution")]),e._v(" "),a("p",[e._v("Bufferization is critical for GPU execution because:")]),e._v(" "),a("ul",[a("li",[e._v("Tensors cannot be used in GPU kernels (they are immutable).")]),e._v(" "),a("li",[e._v("MemRefs explicitly allocate GPU memory (#gpu.memory_space).\nBufferization ensures correct memory aliasing for parallel execution.\nExample: Bufferizing a GPU Kernel\nBefore Bufferization (Tensor Representation)")])]),e._v(" "),a("div",{staticClass:"language-mlir line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("gpu.func @kernel(%A: tensor<1024xf32>) {\n  %B = linalg.matmul ins(%A, %A) -> tensor<1024xf32>\n  return %B\n}\n")])]),e._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[e._v("1")]),a("br"),a("span",{staticClass:"line-number"},[e._v("2")]),a("br"),a("span",{staticClass:"line-number"},[e._v("3")]),a("br"),a("span",{staticClass:"line-number"},[e._v("4")]),a("br")])]),a("p",[e._v("After Bufferization (MemRef for GPU Execution)")]),e._v(" "),a("div",{staticClass:"language-mlir line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("gpu.func @kernel(%A_mem: memref<1024xf32, #gpu.memory_space<global>>) {\n  %B_mem = memref.alloc() : memref<1024xf32, #gpu.memory_space<global>>\n  linalg.matmul ins(%A_mem, %A_mem) outs(%B_mem)\n  return\n} \n")])]),e._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[e._v("1")]),a("br"),a("span",{staticClass:"line-number"},[e._v("2")]),a("br"),a("span",{staticClass:"line-number"},[e._v("3")]),a("br"),a("span",{staticClass:"line-number"},[e._v("4")]),a("br"),a("span",{staticClass:"line-number"},[e._v("5")]),a("br")])]),a("p",[e._v("üìå MemRefs are placed in #gpu.memory_space"),a("global",[e._v(" to allocate GPU memory.")])],1),e._v(" "),a("h2",{attrs:{id:"_6Ô∏è‚É£-bufferization-passes-in-mlir"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6Ô∏è‚É£-bufferization-passes-in-mlir"}},[e._v("#")]),e._v(" 6Ô∏è‚É£ Bufferization Passes in MLIR")]),e._v(" "),a("p",[e._v("To apply bufferization, MLIR provides passes:")]),e._v(" "),a("p",[e._v("Pass Name\tCommand\nOne-Shot Bufferization\tmlir-opt --bufferize\nPartial Bufferization\tmlir-opt --partial-bufferize\nBuffer Deallocation\tmlir-opt --buffer-deallocation")]),e._v(" "),a("h2",{attrs:{id:"_7Ô∏è‚É£-summary"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7Ô∏è‚É£-summary"}},[e._v("#")]),e._v(" 7Ô∏è‚É£ Summary")]),e._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}},[e._v("Feature")]),e._v(" "),a("th",[e._v("Bufferization")])])]),e._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[e._v("Purpose")]),e._v(" "),a("td",[e._v("Converts immutable tensors ‚Üí mutable memrefs")])]),e._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[e._v("Avoids Copies?")]),e._v(" "),a("td",[e._v("‚úÖ Yes (if no aliasing)")])]),e._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[e._v("Handles Aliasing?")]),e._v(" "),a("td",[e._v("‚úÖ Yes (inserts memref.alloc if needed)")])]),e._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[e._v("Needed for GPU?")]),e._v(" "),a("td",[e._v("‚úÖ Yes (memrefs required for GPU execution)")])]),e._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[e._v("Final Target?")]),e._v(" "),a("td",[e._v("‚úÖ LLVM (llvm.ptr"),a("T",[e._v(")")])],1)])])]),e._v(" "),a("h2",{attrs:{id:"_8Ô∏è‚É£-conclusion"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_8Ô∏è‚É£-conclusion"}},[e._v("#")]),e._v(" 8Ô∏è‚É£ Conclusion")]),e._v(" "),a("p",[e._v("‚úÖ Bufferization is essential for transitioning from high-level tensor computations to hardware execution.\n‚úÖ It minimizes memory allocations, reducing overhead and improving efficiency.\n‚úÖ Works with CPU and GPU lowering (via MemRef dialect).")])])}),[],!1,null,null,null);a.default=n.exports}}]);