(window.webpackJsonp=window.webpackJsonp||[]).push([[82],{535:function(e,a,t){"use strict";t.r(a);var i=t(9),n=Object(i.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("ol",[a("li",[e._v("[1 2024] Uncovering Real GPU NoC Chara cteristics:Implications on Interconnect Architecture")])]),e._v(" "),a("hr"),e._v(" "),a("p",[e._v('The paper, "Uncovering Real GPU NoC Characteristics: Implications on Interconnect Architecture", thoroughly analyzes the network-on-chip (NoC) architectures of modern GPUs from NVIDIA (V100, A100, H100), examining critical aspects such as latency and bandwidth, and discusses key architectural implications.')]),e._v(" "),a("p",[e._v("Here is a profound explanation of the key observations and findings from this significant work:")]),e._v(" "),a("h3",{attrs:{id:"key-observations-and-findings"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#key-observations-and-findings"}},[e._v("#")]),e._v(" Key Observations and Findings:")]),e._v(" "),a("h4",{attrs:{id:"_1-non-uniform-latency"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-non-uniform-latency"}},[e._v("#")]),e._v(" 1. Non-uniform Latency:")]),e._v(" "),a("p",[e._v("Core-L2 slice latency varies significantly based on their physical placement within GPUs.")]),e._v(" "),a("p",[e._v("For instance, the V100 GPU shows a latency variation up to 71 cycles (about 33%), depending on the proximity of the SM (Streaming Multiprocessor) to the specific L2 slice accessed.")]),e._v(" "),a("p",[e._v("Physical location heavily influences latency, thus emphasizing the importance of core and memory slice placement strategies in GPU architecture.")]),e._v(" "),a("h4",{attrs:{id:"_2-impact-of-physical-placement"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-impact-of-physical-placement"}},[e._v("#")]),e._v(" 2. Impact of Physical Placement:")]),e._v(" "),a("p",[e._v("Detailed analysis using Pearson correlation identifies that SMs within the same GPU cluster (GPC) tend to have highly correlated latency patterns.")]),e._v(" "),a("p",[e._v("This characteristic allows reverse-engineering of physical SM placement using latency patterns, presenting opportunities for optimization and potential security vulnerabilities (such as side-channel attacks).")]),e._v(" "),a("h4",{attrs:{id:"_3-hierarchy-induced-non-uniformity-a100-h100-gpus"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-hierarchy-induced-non-uniformity-a100-h100-gpus"}},[e._v("#")]),e._v(" 3. Hierarchy-Induced Non-uniformity (A100, H100 GPUs):")]),e._v(" "),a("p",[e._v("Recent GPUs (A100, H100) introduce multi-partition architectures, significantly amplifying latency non-uniformities due to their increased complexity and larger physical size.")]),e._v(" "),a("p",[e._v("H100 introduces an intermediate hierarchy known as Compute Processing Cluster (CPC), causing further latency differentiation within a GPU cluster (GPC). This hierarchical change is notable in how cores within the same GPC can exhibit varied latency characteristics depending on their CPC placement.")]),e._v(" "),a("h4",{attrs:{id:"_4-latency-crossing-gpu-partitions"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-latency-crossing-gpu-partitions"}},[e._v("#")]),e._v(" 4. Latency Crossing GPU Partitions:")]),e._v(" "),a("p",[e._v("Latency significantly increases when accesses cross GPU partitions, especially evident in A100, where latency to a remote partition nearly doubles from approximately 212 cycles to around 400 cycles.")]),e._v(" "),a("p",[e._v("H100 partially mitigates this issue through a localized caching policy, thus providing more uniform latency compared to A100 when accessing different partitions.")]),e._v(" "),a("h4",{attrs:{id:"_5-on-chip-bandwidth-uniformity-and-non-uniformity"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-on-chip-bandwidth-uniformity-and-non-uniformity"}},[e._v("#")]),e._v(" 5. On-chip Bandwidth Uniformity and Non-uniformity:")]),e._v(" "),a("p",[e._v("Interestingly, while latency shows clear non-uniformity, bandwidth remains relatively uniform across different cores and L2 slices.")]),e._v(" "),a("p",[e._v("Bandwidth uniformity ensures reliable performance irrespective of SM or L2 slice placement, crucial for balanced parallel execution.")]),e._v(" "),a("h4",{attrs:{id:"_6-increasing-on-chip-bandwidth-vs-memory-bandwidth"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-increasing-on-chip-bandwidth-vs-memory-bandwidth"}},[e._v("#")]),e._v(" 6. Increasing On-chip Bandwidth vs. Memory Bandwidth:")]),e._v(" "),a("p",[e._v("The total aggregate on-chip L2 bandwidth far exceeds off-chip memory bandwidth (by 2.4× to 3.5×).")]),e._v(" "),a("p",[e._v("Such high internal bandwidth is necessary to prevent performance bottlenecks, emphasizing the critical role of NoC architecture design.")]),e._v(" "),a("h4",{attrs:{id:"_7-hierarchical-bandwidth-speedup"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7-hierarchical-bandwidth-speedup"}},[e._v("#")]),e._v(" 7. Hierarchical Bandwidth Speedup:")]),e._v(" "),a("p",[e._v("The study identifies hierarchical speedup strategies (TPC, GPC levels) implemented by GPUs to ensure sufficient bandwidth.")]),e._v(" "),a("p",[e._v("Recent GPUs provide substantial speedup improvements compared to earlier generations, although non-uniformities emerge, especially evident when crossing partitions.")]),e._v(" "),a("h4",{attrs:{id:"_8-load-balancing-is-crucial"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_8-load-balancing-is-crucial"}},[e._v("#")]),e._v(" 8. Load-balancing is Crucial:")]),e._v(" "),a("p",[e._v("Load balancing across cores (SMs) is crucial due to asymmetric speedups at various NoC hierarchy levels.")]),e._v(" "),a("p",[e._v("Conversely, load balancing across L2 slices is less critical, highlighting the need to focus on SM-level workload distribution for optimal performance.")]),e._v(" "),a("h4",{attrs:{id:"_9-traffic-distribution-and-load-balancing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_9-traffic-distribution-and-load-balancing"}},[e._v("#")]),e._v(" 9. Traffic Distribution and Load-Balancing:")]),e._v(" "),a("p",[e._v("Real workloads, analyzed through Rodinia benchmarks, exhibit balanced on-chip traffic, implying that GPU address hashing techniques effectively distribute memory accesses evenly across memory channels and thus NoC traffic.")]),e._v(" "),a("h3",{attrs:{id:"ideas"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ideas"}},[e._v("#")]),e._v(" Ideas")]),e._v(" "),a("h4",{attrs:{id:"noc-aware-cta-scheduler"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#noc-aware-cta-scheduler"}},[e._v("#")]),e._v(" NoC-Aware CTA Scheduler")]),e._v(" "),a("p",[a("strong",[e._v("Insight used.")]),e._v(" Performance collapses when many CTAs run on SMs inside one GPC and hammer a single memory partition because GPC-level speed-up is limited and asymmetric")]),e._v(" "),a("p",[a("strong",[e._v("Idea.")]),e._v(" Extend Accel-Sim’s thread-block scheduler so that, at launch time, it spreads CTAs across GPCs/partitions to equalise NoC ingress bandwidth.")]),e._v(" "),a("p",[a("strong",[e._v("Evaluation.")]),e._v(" Compare latency-sensitive kernels (e.g., pointer-chasing, Graph workloads) under: (i) stock round-robin scheduling and (ii) your NoC-aware scheme. Measure SM-stall cycles and achieved L2 slice bandwidth.")]),e._v(" "),a("h4",{attrs:{id:"partition-aware-memory-allocator-for-multi-tenant-gpus"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#partition-aware-memory-allocator-for-multi-tenant-gpus"}},[e._v("#")]),e._v(" Partition-Aware Memory Allocator for Multi-Tenant GPUs")]),e._v(" "),a("p",[a("strong",[e._v("Insight used.")]),e._v(" Crossing the partition boundary in Ampere doubles L2 hit latency (~212 → ~400 cy) and throttles single-SM bandwidth (39 → 26 GB s⁻¹) .")]),e._v(" "),a("p",[a("strong",[e._v("Idea.")]),e._v(" In a multi-application setting, modify the simulated CUDA malloc to prefer pages whose L2 slice lives on the same partition as the requesting SM set.")]),e._v(" "),a("p",[a("strong",[e._v("Evaluation.")]),e._v(" Run two co-located DL inference kernels. Track tail latency and fairness with vs. without partition-aware placement.")]),e._v(" "),a("h4",{attrs:{id:"sm-locality-driven-cache-compression"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sm-locality-driven-cache-compression"}},[e._v("#")]),e._v(" SM-Locality-Driven Cache Compression")]),e._v(" "),a("p",[a("strong",[e._v("Insight used.")]),e._v(" SM-to-L2 latency is lowest when the slice is physically near the SM; but bandwidth uniformity means compressed lines mainly help latency, not BW.")]),e._v(" "),a("p",[a("strong",[e._v("Idea.")]),e._v(" Implement a simple line-level compressor in the L1. When compression ratio ≥ R, cache the line in the nearest L2 slice (distance calculated from the paper’s measured latency heat-map).")]),e._v(" "),a("p",[a("strong",[e._v("Evaluation.")]),e._v(" Compare average memory stall cycles of SPEC/GPU workloads with: (i) baseline uncompressed, (ii) random L2 placement of compressed data, (iii) locality-aware placement.")]),e._v(" "),a("h4",{attrs:{id:"dynamic-prefetch-throttling-for-irregular-accesses"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dynamic-prefetch-throttling-for-irregular-accesses"}},[e._v("#")]),e._v(" Dynamic Prefetch Throttling for Irregular Accesses")]),e._v(" "),a("p",[a("strong",[e._v("Insight used.")]),e._v(" Only four SMs can saturate a single L2 slice; beyond that, latency not bandwidth is the bottleneck.")]),e._v(" "),a("p",[a("strong",[e._v("Idea.")]),e._v(" Add a prefetch controller that monitors per-slice queue depth; when > X, it throttles additional irregular prefetches from “far” SMs but allows those from “near” SMs.")]),e._v(" "),a("p",[a("strong",[e._v("Evaluation.")]),e._v(" Use irregular pointer-chase micro-benchmarks; plot IPC vs. prefetch accuracy/coverage.")]),e._v(" "),a("h4",{attrs:{id:"hierarchical-load-balancing-policies"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hierarchical-load-balancing-policies"}},[e._v("#")]),e._v(" Hierarchical Load-Balancing Policies")]),e._v(" "),a("p",[a("strong",[e._v("Insight used.")]),e._v(" Some GPC-speed-up is “in space” (extra links) rather than “in time”; distributing SMs across more MPs yields 2.18× speed-up over packing them into one MP.")]),e._v(" "),a("p",[a("strong",[e._v("Idea.")]),e._v(" Build a run-time policy that (a) groups CTAs by data locality, then (b) assigns these groups to MPs in a way that maximises the number of distinct MP destinations seen by each GPC.")]),e._v(" "),a("p",[a("strong",[e._v("Evaluation.")]),e._v(" Stress-test with graph analytics and stencil mixes; report overall throughput and worst-case SM idle time.")]),e._v(" "),a("h4",{attrs:{id:"model-driven-interconnect-speed-up-exploration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#model-driven-interconnect-speed-up-exploration"}},[e._v("#")]),e._v(" Model-Driven Interconnect Speed-up Exploration")]),e._v(" "),a("p",[a("strong",[e._v("Insight used.")]),e._v(" The paper quantifies TPC, GPC_local, GPC_global and (for H100) CPC speed-ups for reads vs. writes.")]),e._v(" "),a("p",[a("strong",[e._v("Idea.")]),e._v(" Parameterise these speed-ups in Accel-Sim and sweep them to find the knee point where extra NoC ports stop improving perf/area.")]),e._v(" "),a("p",[a("strong",[e._v("Evaluation.")]),e._v(" Produce Pareto curves (perf vs. simulated crossbar area/energy).")]),e._v(" "),a("h4",{attrs:{id:"compression-aware-reply-coalescing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compression-aware-reply-coalescing"}},[e._v("#")]),e._v(" Compression-Aware Reply Coalescing")]),e._v(" "),a("p",[a("strong",[e._v("Insight used.")]),e._v(" Reply traffic (128-byte cache lines) is the dominant consumer of NoC BW; input speed-up exists but is still limited for writes.")]),e._v(" "),a("p",[a("strong",[e._v("Idea.")]),e._v(" Compress replies on-the-fly (e.g., frequent-value) and coalesce multiple compressed lines into a single flit when returning to an SM group.")]),e._v(" "),a("p",[a("strong",[e._v("Evaluation.")]),e._v(" Instrument Accel-Sim’s interconnect queue; measure flit count reduction and end-to-end energy per memory op.")])])}),[],!1,null,null,null);a.default=n.exports}}]);