(window.webpackJsonp=window.webpackJsonp||[]).push([[100],{560:function(t,a,s){"use strict";s.r(a);var n=s(8),e=Object(n.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"aot-dispatch-create-joint-graph"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aot-dispatch-create-joint-graph"}},[t._v("#")]),t._v(" aot dispatch create joint graph")]),t._v(" "),a("p",[t._v("./torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py.")]),t._v(" "),a("p",[t._v("aot_dispatch_autograd")]),t._v(" "),a("p",[t._v("The aot_dispatch_autograd function is responsible for tracing, partitioning, and compiling a given function for automatic differentiation using Ahead-Of-Time (AOT) Autograd.")]),t._v(" "),a("p",[t._v("It handles the creation of "),a("strong",[t._v("forward and backward graphs")]),t._v(", manages metadata, and ensures that the compiled function can be executed efficiently with support for gradient computation.")]),t._v(" "),a("p",[t._v("Inputs")]),t._v(" "),a("ul",[a("li",[t._v("flat_fn: The original function to be traced and compiled.")]),t._v(" "),a("li",[t._v("flat_args: A list of arguments to be passed to the function.")]),t._v(" "),a("li",[t._v("aot_config: Configuration for AOT Autograd, which includes settings for partitioning, logging, and compilation.")]),t._v(" "),a("li",[t._v("fw_metadata: Metadata about the function's inputs and outputs, including information about views and mutations.")])]),t._v(" "),a("p",[t._v("Outputs")]),t._v(" "),a("ul",[a("li",[t._v("compiled_function: A compiled version of the original function that includes both the forward and backward passes, optimized for execution with support for gradient computation.")])]),t._v(" "),a("p",[t._v("Major Functions in aot_dispatch_autograd")]),t._v(" "),a("p",[a("strong",[t._v("aot_dispatch_autograd_graph")])]),t._v(" "),a("ul",[a("li",[t._v("Purpose: Traces the original function and creates a joint forward-backward FX graph.")]),t._v(" "),a("li",[t._v("Steps: Calls aot_dispatch_autograd_graph to trace the function and generate the FX graph.\n"),a("ul",[a("li",[t._v("Returns the FX graph, joint inputs, and subclass metadata (if any).")])])])]),t._v(" "),a("p",[a("strong",[t._v("partition_fn:")])]),t._v(" "),a("ul",[a("li",[t._v("Purpose: Partitions the joint FX graph into separate forward and backward graphs.")]),t._v(" "),a("li",[t._v("Steps: Uses the partition function specified in aot_config to split the FX graph into forward and backward modules.\n"),a("ul",[a("li",[t._v("Returns the forward and backward modules.")]),t._v(" "),a("li",[a("strong",[t._v("min_cut_rematerialization_partition")]),t._v(" ðŸ‘ ðŸ‘")])])])]),t._v(" "),a("p",[a("strong",[t._v("fw_compiler and bw_compiler:")])]),t._v(" "),a("ul",[a("li",[t._v("Purpose: Compiles the forward and backward FX graphs into executable functions.")]),t._v(" "),a("li",[t._v("Steps: Uses the forward and backward compilers specified in aot_config to compile the FX modules.\n"),a("ul",[a("li",[t._v("Returns the compiled forward and backward functions.")])])])]),t._v(" "),a("p",[a("strong",[t._v("CompiledFunction:")])]),t._v(" "),a("ul",[a("li",[t._v("Purpose: A custom autograd function that wraps the compiled forward and backward functions.")]),t._v(" "),a("li",[t._v("Steps: Defines the forward and backward static methods to handle the execution of the compiled functions.\n"),a("ul",[a("li",[t._v("Manages the saving and restoring of tensors and symbolic integers for gradient computation.")])])])]),t._v(" "),a("p",[a("strong",[t._v("create_runtime_wrapper")])]),t._v(" "),a("ul",[a("li",[t._v("Purpose: Creates a runtime wrapper for the compiled function to handle input mutations and other runtime considerations.")]),t._v(" "),a("li",[t._v("Steps: Wraps the CompiledFunction.apply method with additional logic for handling input mutations and AMP (Automatic Mixed Precision) settings.\n"),a("ul",[a("li",[t._v("Returns the wrapped function.")])])])]),t._v(" "),a("details",[a("summary",[t._v("Code")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("aot_dispatch_autograd")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    fx_g"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" joint_inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" maybe_subclass_meta "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aot_dispatch_autograd_graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# type: ignore[misc]")]),t._v("\n        flat_fn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" flat_args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" aot_config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fw_metadata"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("fw_metadata\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    fw_module"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bw_module "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aot_config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("partition_fn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        fx_g"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" joint_inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_fwd_outputs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("num_inner_fwd_outputs\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br")])])]),t._v(" "),a("h3",{attrs:{id:"aot-dispatch-autograd-graph"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aot-dispatch-autograd-graph"}},[t._v("#")]),t._v(" aot_dispatch_autograd_graph")]),t._v(" "),a("p",[t._v("The aot_dispatch_autograd_graph function is responsible for preparing and tracing a given function (flat_fn) with its arguments (flat_args) for automatic differentiation using AOT (Ahead-Of-Time) Autograd."),a("br"),t._v("\nIt processes the function to handle input mutations, creates a joint forward-backward function, and generates an "),a("strong",[t._v("FX graph")]),t._v(" for the function."),a("br"),t._v("\nThe function ensures that the graph is functional (i.e., free of in-place operations) and can handle tensor subclasses if necessary.")]),t._v(" "),a("ul",[a("li",[t._v("pytree.tree_map: This function processes the traced_tangents to ensure they are detached and contiguous if they are tensors, preparing them for tracing.")]),t._v(" "),a("li",[t._v("fn_prepped_for_autograd: Prepares the original function for autograd by incorporating metadata about views and mutations, ensuring correct handling of these aspects during tracing.")]),t._v(" "),a("li",[a("em",[a("strong",[t._v("create_joint")])]),t._v(": Creates a joint forward-backward function that traces both the forward and backward passes together, enabling efficient autograd processing.")]),t._v(" "),a("li",[t._v("create_functionalized_fn: Converts the joint function into a functional form, handling input mutations and tracing the joint structure, ensuring compatibility with autograd.")]),t._v(" "),a("li",[t._v("aot_dispatch_subclass: Handles tracing for tensor subclasses, ensuring that the autograd process can correctly handle these specialized tensor types.")]),t._v(" "),a("li",[a("em",[a("strong",[t._v("_create_graph")])]),t._v(": Creates an FX graph from the joint function and its inputs, providing a lower-level representation of the function for optimization and execution.")]),t._v(" "),a("li",[a("em",[a("strong",[t._v("fx_g.graph.eliminate_dead_code")])]),t._v(": Eliminates any dead code from the FX graph to optimize it, improving performance and reducing unnecessary computations.")]),t._v(" "),a("li",[a("em",[a("strong",[t._v("fx_g.recompile")])]),t._v(": Recompiles the FX graph after eliminating dead code, ensuring that the graph is up-to-date and optimized for execution.")])]),t._v(" "),a("details",[a("summary",[t._v("Code")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("### dispatch_and_compile_graph.py")]),t._v("\n    fn_prepared_for_autograd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fn_prepped_for_autograd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        flat_fn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        fw_metadata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    joint_fn_to_trace "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" create_joint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fn_prepared_for_autograd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" aot_config"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("aot_config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    joint_fn_to_trace"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" updated_joint_inputs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" create_functionalized_fn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        joint_fn_to_trace"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        joint_inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        meta"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("fw_metadata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        aot_config"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("aot_config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        trace_joint"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    subclass_tracing_info "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aot_dispatch_subclass"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        joint_fn_to_trace"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        updated_joint_inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        is_joint_structure"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        meta"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("fw_metadata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        fw_only"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("flat_fn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    fx_g "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _create_graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("joint_fn_to_trace"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" updated_joint_inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" aot_config"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("aot_config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    fx_g"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("eliminate_dead_code"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    fx_g"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("recompile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br"),a("span",{staticClass:"line-number"},[t._v("27")]),a("br")])])]),t._v(" "),a("h4",{attrs:{id:"create-joint"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#create-joint"}},[t._v("#")]),t._v(" create_joint")]),t._v(" "),a("p",[t._v("The create_joint function is designed to create a joint forward-backward function for automatic differentiation."),a("br"),t._v("\nIt ensures that the function can be traced and differentiated correctly, handling the computation of gradients and preserving the necessary metadata.")]),t._v(" "),a("ul",[a("li",[t._v("Inputs")])]),t._v(" "),a("ul",[a("li",[t._v("fn: A callable function that returns a tuple of (outputs, mask). The mask indicates which outputs require tangents.")]),t._v(" "),a("li",[t._v("aot_config: Configuration for AOT (Ahead-Of-Time) Autograd, which includes settings like whether tangents are needed.")])]),t._v(" "),a("ul",[a("li",[t._v("Outputs")])]),t._v(" "),a("ul",[a("li",[t._v("return a tuple of (outs, mask), where "),a("code",[t._v("mask")]),t._v(" tells us which outputs are meant to have tangents.")]),t._v(" "),a("li",[t._v("compute tangents for every output that requires grad.")])]),t._v(" "),a("p",[a("em",[a("strong",[t._v("inner_fn")])])]),t._v(" "),a("p",[t._v("This is the core function that computes the forward pass, identifies the outputs that require gradients, and performs the backward pass to compute the gradients.")]),t._v(" "),a("ul",[a("li",[t._v("Calls the original function fn with the primal inputs to get the outputs and a mask indicating which outputs require tangents.")]),t._v(" "),a("li",[t._v("Identifies the inputs and outputs that need gradients.")]),t._v(" "),a("li",[t._v("Sets up stack trace preservation hooks for the gradient functions. "),a("strong",[t._v("setup_stacktrace_preservation_hooks")])]),t._v(" "),a("li",[t._v("Calls torch.autograd.grad to compute the gradients of the needed outputs with respect to the inputs that require gradients.")]),t._v(" "),a("li",[t._v("Returns the original outputs and the computed gradients.")])]),t._v(" "),a("h4",{attrs:{id:"create-graph"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#create-graph"}},[t._v("#")]),t._v(" _create_graph")]),t._v(" "),a("p",[t._v("_create_graph wraps make_fx.")]),t._v(" "),a("p",[t._v("The make_fx function is a utility in PyTorch that traces a given function f and its inputs to produce an FX graph."),a("br"),t._v("\nThis graph represents the operations performed by the function in a way that can be further analyzed, transformed, and optimized."),a("br"),t._v("\nThe function supports different tracing modes (real, fake, symbolic) and can handle decomposition of complex operations into simpler ones.")]),t._v(" "),a("ul",[a("li",[t._v("tracing_mode Handling: Determines the mode of tracing (real, fake, symbolic) and sets up the appropriate context for each mode.")]),t._v(" "),a("li",[t._v("ShapeEnv: Manages symbolic shapes during tracing, especially in symbolic mode.")]),t._v(" "),a("li",[t._v("FakeTensorMode: Creates fake tensors to simulate tensor operations without actual computation, used in fake and symbolic modes.")]),t._v(" "),a("li",[t._v("ProxyTorchDispatchMode: Sets up a proxy mode to intercept and record tensor operations during tracing.")]),t._v(" "),a("li",[t._v("wrap_fake: Wraps input tensors as fake tensors or symbolic integers based on the tracing mode.")]),t._v(" "),a("li",[t._v("dispatch_trace: Performs the actual tracing of the function, recording the operations into an FX graph.")])]),t._v(" "),a("h2",{attrs:{id:"torchinductor"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#torchinductor"}},[t._v("#")]),t._v(" TorchInductor")]),t._v(" "),a("p",[t._v("TorchInductor is the backend of pytorch, converting compute graph into target-specific code."),a("br"),t._v("\nIt also utilize optimization techiniques, memory optimization, parallelism and low-level codegen.")]),t._v(" "),a("p",[t._v("In previous part, after aot_dispatch_autograd() obtain forward/backward FX graph, it will compile the graph with forward/backward compiler.")]),t._v(" "),a("p",[t._v("inductor calls compile_fx_inner() by default. THe kernel function is fx_codegen_and_compile(),"),a("br"),t._v("\nwhich optimizes FX graph optimization and generate code.")]),t._v(" "),a("p",[t._v("The fx_codegen_and_compile function is responsible for generating and compiling a Torch FX (Functional Transformations) graph module."),a("br"),t._v("\nIt performs several steps to optimize and prepare the graph for execution, including handling tensor shapes, converting operations, and compiling the graph into an executable function."),a("br"),t._v("\nIt supports various modes such as AOT (Ahead-Of-Time) compilation and inference.")]),t._v(" "),a("p",[t._v("File location: ./torch/_inductor/compile_fx.py")]),t._v(" "),a("ul",[a("li",[t._v("_recursive_post_grad_passes: Applies post-gradient passes to the graph, optimizing it for inference or training.")]),t._v(" "),a("li",[t._v("split_const_gm: Splits the graph module into constant and non-constant parts if runtime constant folding is enabled.")]),t._v(" "),a("li",[t._v("GraphLowering: Lowers the FX graph to a lower-level representation, preparing it for code generation and execution.")]),t._v(" "),a("li",[t._v("graph.run:Executes the lowered graph with the provided example inputs.")]),t._v(" "),a("li",[t._v("graph.compile_to_fn: Compiles the lowered graph into an executable function.")]),t._v(" "),a("li",[t._v("CompiledFxGraph: Creates a compiled FX graph object that includes the compiled function, graph metadata, and metrics.")])]),t._v(" "),a("h3",{attrs:{id:"recursicv-post-grad-passes"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#recursicv-post-grad-passes"}},[t._v("#")]),t._v(" _recursicv_post_grad_passes()")]),t._v(" "),a("p",[t._v("Graph optimization.")]),t._v(" "),a("h4",{attrs:{id:"group-batch-fusion-passes"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#group-batch-fusion-passes"}},[t._v("#")]),t._v(" group_batch_fusion_passes()")]),t._v(" "),a("p",[t._v("operator fusion.")]),t._v(" "),a("h4",{attrs:{id:"remove-noops-ops"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#remove-noops-ops"}},[t._v("#")]),t._v(" remove_noops_ops()")]),t._v(" "),a("p",[t._v("remove aten.clone/alias operations.")]),t._v(" "),a("h4",{attrs:{id:"fuse-ddp-communication"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fuse-ddp-communication"}},[t._v("#")]),t._v(" fuse_ddp_communication")]),t._v(" "),a("h4",{attrs:{id:"decompose-auto-functionalized"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#decompose-auto-functionalized"}},[t._v("#")]),t._v(" decompose_auto_functionalized")]),t._v(" "),a("p",[t._v("split high-level oprations")]),t._v(" "),a("h3",{attrs:{id:"graphlowering"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#graphlowering"}},[t._v("#")]),t._v(" GraphLowering")]),t._v(" "),a("p",[t._v("Lower FX graph into Inductor IR(lower-level IR)")]),t._v(" "),a("h3",{attrs:{id:"graphlowering-compile-to-fn"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#graphlowering-compile-to-fn"}},[t._v("#")]),t._v(" GraphLowering.compile_to_fn()")]),t._v(" "),a("p",[t._v("source code: compile_fx.py")]),t._v(" "),a("p",[t._v("generate target-specific codes")]),t._v(" "),a("p",[t._v("source code: graph.py")]),t._v(" "),a("p",[t._v("compile_to_fn -> compile_to_module() -> codegen_with_cpp_wrapper")]),t._v(" "),a("p",[a("strong",[t._v("codegen_with_cpp_wrapper")])]),t._v(" "),a("ul",[a("li",[t._v("CPU one pass")]),t._v(" "),a("li",[t._v("GPU two pass.\n"),a("ul",[a("li",[t._v("JIT-compile the model with python wrapper code and run it to generate autotuned kernel binaries in the first pass;")]),t._v(" "),a("li",[t._v("and then generate cpp wrapper code and compile it to a dynamic library in the second pass.")])])])]),t._v(" "),a("p",[a("strong",[t._v("GPU")])]),t._v(" "),a("ol",[a("li",[t._v("First pass: compiled = self.compile_to_module().call; compiled(real_inputs)")]),t._v(" "),a("li",[t._v("Second pass: codegen()")])]),t._v(" "),a("details",[a("summary",[t._v("Code")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("codegen")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("scheduler "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Scheduler\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("init_wrapper_code"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("scheduler "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Scheduler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("buffers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("debug"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("draw_orig_fx_graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("orig_gm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("scheduler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("scheduler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("codegen"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("wrapper_code"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("generate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_inference"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br")])])]),t._v(" "),a("p",[t._v("From above code, we can see that it instantiate scheduler and use codegen function is scheduler.")]),t._v(" "),a("p",[t._v("scheduler.py")]),t._v(" "),a("details",[a("summary",[t._v("Code")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Scheduler")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@dynamo_timed")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("compute_dependencies"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("topological_sort_schedule"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dead_node_elimination"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reorder_for_compute_comm_overlap"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            comms"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decide_global_ordering_of_comms"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("compute_ancestors"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        metrics"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ir_nodes_pre_fusion "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("debug"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ir_pre_fusion"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num_orig_nodes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name_to_fused_node "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create_foreach_nodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("topological_sort_schedule"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("logged_slow_fusion "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("set")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fuse_nodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reorder_for_compute_comm_overlap"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Refresh node_users and inverse_users to reflect fused nodes")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("compute_node_users"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nodes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" comms"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reorder_compute_and_comm_for_overlap"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("compute_last_usage"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("debug"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ir_post_fusion"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("debug"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("graph_diagram"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("debug_draw_graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# used during codegen:")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("current_device"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# type: ignore[assignment]")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("buffer_names_to_free "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("set")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br"),a("span",{staticClass:"line-number"},[t._v("27")]),a("br"),a("span",{staticClass:"line-number"},[t._v("28")]),a("br"),a("span",{staticClass:"line-number"},[t._v("29")]),a("br")])])]),t._v(" "),a("h4",{attrs:{id:"scheduler-init"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scheduler-init"}},[t._v("#")]),t._v(" scheduler."),a("strong",[t._v("init")]),t._v("()")]),t._v(" "),a("p",[t._v("kernel fusion optimization")]),t._v(" "),a("h5",{attrs:{id:"compute-dependencies"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compute-dependencies"}},[t._v("#")]),t._v(" compute_dependencies()")]),t._v(" "),a("p",[t._v("Create dependency edges between nodes, handling aliasing and mutation properly.")]),t._v(" "),a("h5",{attrs:{id:"fuse-nodes"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fuse-nodes"}},[t._v("#")]),t._v(" fuse_nodes()")]),t._v(" "),a("p",[t._v("Mutates self.nodes to combine nodes into FusedSchedulerNodes.")]),t._v(" "),a("p",[t._v("This relies on two key functions to control the logic:")]),t._v(" "),a("ul",[a("li",[t._v("self.can_fuse(): checks if a fusion is legal")]),t._v(" "),a("li",[t._v("self.score_fusion(): assigns priority to a given fusion")])]),t._v(" "),a("p",[a("strong",[t._v("fuse_nodes_once")])]),t._v(" "),a("ul",[a("li",[t._v("get_possibble_fusions()\n"),a("ul",[a("li",[t._v("can_fuse and not will_fusion_create_cycle")]),t._v(" "),a("li",[t._v("speedup_by_fusion")]),t._v(" "),a("li",[t._v("fused_nodes.remove(node1/node2)")]),t._v(" "),a("li",[t._v("fused_nodes.add(node3)")])])]),t._v(" "),a("li",[t._v("topological_sort_schedule()")]),t._v(" "),a("li",[t._v("self.prune_redundant_deps()")])]),t._v(" "),a("h4",{attrs:{id:"scheduler-codegen"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scheduler-codegen"}},[t._v("#")]),t._v(" Scheduler.codegen()")]),t._v(" "),a("p",[a("strong",[t._v("get_backend(device).codegen_node(node)")])]),t._v(" "),a("p",[t._v("This will call specific backend, such as gpu to generate code for nodes.")]),t._v(" "),a("p",[t._v("There is a "),a("em",[t._v("cuda_cpp_scheduling.py")]),t._v(" defines class CUDACPPScheduling with codegen_template method.")]),t._v(" "),a("p",[t._v("I am still confused how pytorch knows the mapping between IR and CUDA code.")]),t._v(" "),a("p",[t._v("It might be like this:")]),t._v(" "),a("ul",[a("li",[t._v("native_functions.yaml")])]),t._v(" "),a("details",[a("summary",[t._v("Code")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" func"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" _foreach_sub_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ScalarList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("a!"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Scalar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" scalars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  device_check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" NoCheck   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# foreach kernels fall back to slow path when tensor are on different devices")]),t._v("\n  variants"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" function\n  dispatch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    CPU"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" foreach_tensor_sub_scalarlist_kernel_slow_\n    CUDA"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" foreach_tensor_sub_scalarlist_kernel_cuda_\n  autogen"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" _foreach_sub"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ScalarList_out\n\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" func"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" _foreach_mul"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Scalar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Scalar scalar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" Tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n  device_check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" NoCheck   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# foreach kernels fall back to slow path when tensor are on different devices")]),t._v("\n  variants"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" function\n  dispatch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    CPU"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" foreach_tensor_mul_scalar_kernel_slow\n    CUDA"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" foreach_tensor_mul_scalar_kernel_cuda\n\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" func"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" _foreach_mul_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Scalar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("a!"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Scalar scalar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  device_check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" NoCheck   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# foreach kernels fall back to slow path when tensor are on different devices")]),t._v("\n  variants"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" function\n  dispatch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    CPU"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" foreach_tensor_mul_scalar_kernel_slow_\n    CUDA"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" foreach_tensor_mul_scalar_kernel_cuda_\n  autogen"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" _foreach_mul"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Scalar_out\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br")])])]),t._v(" "),a("p",[t._v("RegisterCUDA.cpp")]),t._v(" "),a("details",[a("summary",[t._v("Code")]),t._v(" "),a("div",{staticClass:"language-cpp line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-cpp"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("namespace")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("wrapper_CUDA_Scalar__foreach_mul_")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("at"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[t._v("::")]),t._v("TensorList self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" at"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[t._v("::")]),t._v("Scalar "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&")]),t._v(" scalar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// No device check")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" OptionalDeviceGuard "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("device_guard")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("device_of")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" at"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[t._v("::")]),t._v("native"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[t._v("::")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("foreach_tensor_mul_scalar_kernel_cuda_")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" scalar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// anonymous namespace")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br")])])]),t._v(" "),a("ul",[a("li",[t._v("./aten/src/ATen/Native/cuda/ForeachBinaryOp*.cu")])])])}),[],!1,null,null,null);a.default=e.exports}}]);