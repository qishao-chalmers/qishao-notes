(window.webpackJsonp=window.webpackJsonp||[]).push([[36],{492:function(e,t,a){"use strict";a.r(t);var i=a(8),s=Object(i.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory")]),e._v(" "),t("li",[e._v("In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing")]),e._v(" "),t("li",[e._v("Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions")]),e._v(" "),t("li",[e._v("Performance Evaluation of Advanced Features in CUDA Unified Memory")]),e._v(" "),t("li",[e._v("Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory")]),e._v(" "),t("li",[e._v("Unified Memory: GPGPU-Sim/UVM Smart Integration")]),e._v(" "),t("li",[e._v("Batch-Aware Unified Memory Management in GPUs for Irregular Workloads")]),e._v(" "),t("li",[e._v("An Intelligent Framework for Oversubscription Management in CPU-GPU Unified Memory")]),e._v(" "),t("li",[e._v("Architectural Support for Address Translation on GPUs Designing Memory Management Units for CPU/GPUs with Unified Address Spaces")]),e._v(" "),t("li",[e._v("Machine Learning Guided Optimal Use of GPU Unified Memory")]),e._v(" "),t("li",[e._v("Towards High Performance Paged Memory for GPUs")]),e._v(" "),t("li",[e._v("[Virtualization] Virtual Thread: Maximizing Thread-Level Parallelism beyond GPU Scheduling Limit.")]),e._v(" "),t("li",[e._v("[Virtualization] A Survey of GPU Multitasking Methods Supported by Hardware Architecture")]),e._v(" "),t("li",[e._v("Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory")])]),e._v(" "),t("p",[t("strong",[t("strong",[e._v("Plan to read")])])]),e._v(" "),t("ol",[t("li",[e._v("Early-Adaptor: An Adaptive Framework for Proactive UVM Memory Management")]),e._v(" "),t("li",[e._v("Liberator: A Data Reuse Framework for Out-of-Memory Graph Computing on GPUs")]),e._v(" "),t("li",[e._v("[HPCA] Enabling Large Dynamic Neural Network Training with Learning-based Memory Management")]),e._v(" "),t("li",[e._v("GPUswap: Enabling Oversubscription of GPU Memory through Transparent Swapping")])]),e._v(" "),t("hr"),e._v(" "),t("p",[t("strong",[e._v("Unified Memory History")]),e._v("\ncopied from "),t("em",[t("strong",[e._v("Evolution of Nvidia GPU from microarchitectures Pascal to Ampere")])])]),e._v(" "),t("p",[e._v("CUDA 4 introduced UVA (Unified Virtual Addressing) to provide a single virtual memory address space for both CPU and GPU memory and enable pointers to be accessed from GPU code no matter where in the system they reside. UVA enables Zero-Copy memory, a pinned CPU memory accessible by GPU code directly, over PCIe, without the need for memory copy. This provides some of the convince of Unified Memory, but at the cost of worse performance, because GPU always accesses it with PCIe‚Äôs low bandwidth and high latency.[1]")]),e._v(" "),t("p",[e._v("Later, CUDA 6 introduced Unified Memory, which creates a pool of managed memory that programs running on the CPU and GPU can access without explicit data movement. However, only when CPU and GPU processes are not running together because of the limitation of the Kepler and Maxwell GPU microarchitecture. Also, the Unified Memory address space was limited to the size of the GPU memory.[1, 3]")]),e._v(" "),t("p",[e._v("CUDA 8 and Pascal microarchitectures improve Unified Memory functionality by "),t("strong",[e._v("adding 49-bit virtual addressing and page faulting capability")]),e._v(". The larger 49-bit virtual addresses are sufficient to enable GPUs to access the entire system memory plus the memory of all GPUs in the system. Because of the memory page faulting functionality, the CUDA system software does not need to synchronize all managed memory allocations to the GPU before each kernel lunch. Instead, when a thread running on GPU faults on non-resident memory access("),t("strong",[e._v("demanding page")]),e._v("), it stalls until the page can be migrated and the page table updated. Alternatively, the page may be mapped for remote access over PCIe or NVLink interconnects.[1, 3, 6]")]),e._v(" "),t("p",[e._v("These new features of Unified Memory enable oversubscription of memory, which means that application running on a GPU can use data sets larger than ten their device memory.[1] While the Unified Memory model makes GPU programming more convenient, it comes at a cost; handling page faults and page migrations can be expensive. CUDA 8 addresses this issue with features like prefetch and memory advice.")]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_1-holistic-performance-analysis-and-optimization-of-unified-virtual-holistic-performance-analysis-and-optimization-of-unified-virtual-memory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-holistic-performance-analysis-and-optimization-of-unified-virtual-holistic-performance-analysis-and-optimization-of-unified-virtual-memory"}},[e._v("#")]),e._v(" 1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory")]),e._v(" "),t("p",[e._v("Same author with "),t("strong",[e._v("In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing")])]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_3-oversubscribing-gpu-unified-virtual-memory-implications-and-suggestions"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-oversubscribing-gpu-unified-virtual-memory-implications-and-suggestions"}},[e._v("#")]),e._v(" 3. Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions")]),e._v(" "),t("p",[e._v("UVM supports memory oversubscription, giving GPU programs the ability to use a larger amount of memory than the physical memory, without worrying about the problem of memory shortage.")]),e._v(" "),t("p",[e._v("Advanced optimization techniques, mainly prefetching and memory usage hints [1], can be used to fine-tune the performance of UVM applications, mitigating the overheads caused by UVM.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/fd76fb3e-7747-424b-8235-cdefe81cbf23",alt:"image"}})]),e._v(" "),t("p",[e._v("2ÔºâPrefetching and Hints\nPrefetching and UVM hints are the major approaches provided by CUDA, with the hope that page faults and memory thrashing could be prevented by fine-tuning the behavior of UVM at runtime.")]),e._v(" "),t("p",[e._v("By calling cudaMemPrefetchAsync (PF), a memory block could be prefetched to GPU. UVM hints provide informed decisions on page handling by indicating the access patterns of data.")]),e._v(" "),t("p",[e._v("Changing UVM hints is done by invoking cudaMemAdvise with one of the following policiesÔºö")]),e._v(" "),t("p",[e._v("‚Ä¢ cudaMemAdviseSetAccessedBy (AB) implies that the device keeps a direct mapping in its page table. When the data is migrated, the mapping is re-established."),t("br"),e._v("\n‚Ä¢ cudaMemAdviseSetPreferredLocation (PL) pins the data and prevents the page to be migrated, which is useful when the page is mainly accessed on one side."),t("br"),e._v("\n‚Ä¢ cudaMemAdviseSetReadMostly (RM) indicates the data region is read-intensive. It creates a read-only copy of the page on the faulting side, allowing  on current access on both sides."),t("br")]),e._v(" "),t("p",[e._v("Only one policy (AB, PL, or RM) could be specified for each memory block, but each policy can be used along with prefetching.")]),e._v(" "),t("p",[e._v("Suggestions: To ensure performance under all oversubscription conditions, programmer needs to choose the UVM hints dynamically based on the application‚Äôs memory usage and available GPU memory. As a prerequisite, the size of the FALL pages needs to be estimated or measured by experiment. Before kernel launch, the program should first check the size of available GPU memory (e.g. via the cudaMemGetInfo API). If no oversubscription will happen, or the available memory is larger than the size of FALL pages, the programmer could set hints based on the conclusions provided by related researches [24]. Otherwise, based on our findings, applying the hint AB is a preferable choice.")]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_4-performance-evaluation-of-advanced-features-in-cuda-unified-memory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-performance-evaluation-of-advanced-features-in-cuda-unified-memory"}},[e._v("#")]),e._v(" 4. Performance Evaluation of Advanced Features in CUDA Unified Memory")]),e._v(" "),t("p",[e._v("CUDA has introduced new features for optimizing the data migration on UM, i.e., memory advises and prefetch. Instead of solely relying on page faults, the memory advises feature allows the programmer to provide data access pattern for each memory object so that the runtime can optimize migration decisions. The prefetch proactively triggers asynchronous data migration to GPU before the data is accessed, which reduces page faults and, consequently, the overhead in handling page faults.")]),e._v(" "),t("p",[e._v("-Using memory advises improves application performance in oversubscription execution on the Intel platform and in-memory executions on the IBM platform.")]),e._v(" "),t("p",[e._v("-UM prefetch provides a significant performance improvement on the Intel-Volta/Pascal-PCI-E based systems while it does not show a performance improvement on the Power9-Volta-NVLink based system")]),e._v(" "),t("p",[e._v("UM was first introduced in CUDA 6.0 [21]. Only until the recent Nvidia Pascal microarchitecture that has hardware support for page faults.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/888e3ff6-18dc-4174-9ba3-998f5a30b651",alt:"image"}})]),e._v(" "),t("p",[e._v("‚Ä¢ cudaMemAdviseSetAccessedBy establishes a direct mapping of data to a specified device. Figure 2c illustrates an example of a physical page on GPU being remotely access from the host. When cudaMemAdviseSetPreferredLocation is applied, CUDA runtime tries to build a direct mapping to the page to avoid data migration so that the destination can access data remotely. Differently from cudaMemAdviseSetPreferredLocation, this cudaMemAdviseSetAccessedBy does not try to\npin pages on a specific device; instead, its main effect is to establish mapping on the remote device. This advice takes effect on the creation of the memory pages. The mapping will be re-established after the pages are migrated.")]),e._v(" "),t("p",[e._v("‚Ä¢ cudaMemAdviseSetPreferredLocation sets the preferred physical location of pages. This advice pins a page and prevents it from migrating to other memories. Figure 2b illustrates a page preferred on the host side, and GPU uses remote mapping to access the page. This advice established a direct (remote) mapping to the memory page.  When accessing the page remotely, data is fetched through the remote memory instead of generating a page fault. If the underlying hardware does not support the remote mapping, the page will be migrated as in the standard UM.  cudaMemAdviseSetPreferredLocation is useful for applications with little data sharing between CPU and GPU, i.e., part of the application is executed completely on the GPU, and the rest of the application executes on the host. Data that is being used mostly by the GPU can be pinned to the GPU with the advice, avoiding memory thrashing.")]),e._v(" "),t("p",[e._v("‚Ä¢ cudaMemAdviseSetReadMostly implies a read-intensive data region. In the basic UM, accessing a page on a remote side triggers page migration. However, with cudaMemAdviseSetReadMostly, a read-only duplicate of the page will be created on the faulting side, which prevents page faults and data migration in the future. Figure 2a  illustrates an example, where the second access (step 5) has no page fault and is local access. This mechanism, however, results in a high  "),t("strong",[e._v("overhead if there is any update to this memory region because all copies of the corresponding page will be invalidated to preserve consistency between different copies")]),e._v(". Thus, this advice is often used in read-only data structures, such as lookup tables and application parameters.")]),e._v(" "),t("p",[t("strong",[e._v("In general, we found both memory advises and prefetch to be simple and effective.")])]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_5-interplay-between-hardware-prefetcher-and-page-eviction-policy-in-cpu-gpu-unified-virtual-memory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-interplay-between-hardware-prefetcher-and-page-eviction-policy-in-cpu-gpu-unified-virtual-memory"}},[e._v("#")]),e._v(" 5. Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory")]),e._v(" "),t("p",[e._v("Cons in traditional GPU:\nComplicated asynchronous user-directed constructs to overlap data migration and kernel execution are used to address this issue. The second challenge is memory over-subscription. When the working set of the GPU kernel cannot fit in the device memory, the programmers have to painstakingly redefine the data structures and tile the data to transfer back and  forth in chunks.")]),e._v(" "),t("p",[e._v("This flow is inspired by -> 11. "),t("em",[t("strong",[e._v("Towards High Performance Paged Memory for GPU")])]),e._v(".")]),e._v(" "),t("p",[e._v("1 Scheduled threads generate global memory accesses.")]),e._v(" "),t("p",[e._v("2 Each SM has its own load/store unit. Every load/store unit has its own TLB. Load/store unit performs a TLB look up to find whether the translation for the issued memory access is cached in TLB or not. A TLB miss is relayed to the GMMU.")]),e._v(" "),t("p",[e._v("3 The GMMU walks through the page table looking for a PTE corresponding to the requested page with valid flag set. A far-fault occurs if there is no PTE for the requested page or the valid flag is not set. Then the far-fault is registered in the Far-fault Miss Status Handling Registers (MSHRs).")]),e._v(" "),t("p",[e._v("4 The page is scheduled for transfer over CPU-GPU PCI-e interconnect.")]),e._v(" "),t("p",[e._v("5 A 4KB page is allocated on demand and data is migrated from host to device memory.")]),e._v(" "),t("p",[e._v("6 The MSHRs are consulted to notify the corresponding load/store unit and the memory access is replayed. A new PTE entry is added to the page table with valid")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/140f624a-afd0-4f09-bdf9-49551b0b6fe5",alt:"image"}})]),e._v(" "),t("p",[e._v("This paper introduces "),t("em",[t("strong",[e._v("random, sequential and tree-based Neighborhood prefetcher in detail")])]),e._v(".")]),e._v(" "),t("p",[e._v("And come up with pre-eviction for tree-based Neighborhood, different from LRU eviction used in Nvidia.")]),e._v(" "),t("div",{attrs:{align:"center"}},[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/c59d9159-49ab-4d3f-aa1d-c75bafb322d5",alt:"image"}})]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_7-batch-aware-unified-memory-management-in-gpus-for-irregular-workloads-2020"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-batch-aware-unified-memory-management-in-gpus-for-irregular-workloads-2020"}},[e._v("#")]),e._v(" 7. Batch-Aware Unified Memory Management in GPUs for Irregular Workloads 2020")]),e._v(" "),t("p",[e._v("Propose:")]),e._v(" "),t("p",[e._v("(1) increases the batch size (i.e., the number of page faults handled together), thereby amortizing the GPU runtime fault handling time, and reduces the number of batches by supporting CPU-like thread block context switching")]),e._v(" "),t("p",[e._v("Thread Oversubscription (TO), a CPU-like thread block context switching technique, to effectively amortize the GPU runtime fault handling time by increasing the batch size (i.e., the number of page faults handled together).")]),e._v(" "),t("p",[e._v("(2) takes page eviction off the critical path with no hardware changes by overlapping evictions with CPU-to-GPU page migrations.\nUnobtrusive Eviction (UE) to take GPU page evictions off the critical path with no hardware changes based on the idea of overlapping page evictions with CPU-to-GPU page migrations.")]),e._v(" "),t("p",[e._v("Prior work reports that page fault handling latency ranges from 20¬µs to 50¬µs [53]. We find that these numbers are conservative and can be worse depending on the applications and systems. Unfortunately, this page fault latency, which is in the order of microseconds, cannot be easily hidden even with ample thread-level parallelism (TLP) in GPUs, especially when GPU memory is oversubscribed.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/bd2260eb-2540-4246-be7a-ae14d97afd3c",alt:"image"}})]),e._v(" "),t("p",[e._v("The GPU runtime processes a group of GPU page faults together, rather than processing each individual one, in order to amortize the overhead of multiple round-trip latencies over the PCIe bus and to avoid invoking multiple interrupt service routines (ISRs) in the operating system (OS).\nTo efficiently process an excessive number of page faults, the GPU runtime performs a series of operations such as preprocessing all the page faults and inserting page prefetching requests, which takes a significant amount of time (in the range of tens to hundreds of microseconds). Once all the operations (e.g., CPU page table walks for all the page faults, page allocation and eviction scheduling, etc.) are finished, page migrations between the CPU and the GPU begin.")]),e._v(" "),t("p",[e._v("This page fault handling is expensive because (1) it requires long latency communications between the CPU and GPU over the PCIe bus, and (2) the GPU runtime performs a very expensive fault handling service routine.")]),e._v(" "),t("p",[e._v("To amortize the overhead, the GPU runtime processes a group of page faults together, which we refer to as batch processing.")]),e._v(" "),t("p",[e._v("When a page fault exception is raised by the GPU memory management unit (MMU), the GPU runtime begins to handle the exception, shown in 1.")]),e._v(" "),t("div",{attrs:{align:"center"}},[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/7f1ecd49-1039-44d8-9e84-928388332782",alt:"image"}})]),e._v(" "),t("p",[e._v("From this, we conclude that page evictions and new page allocations are serialized in modern GPUs to prevent the new pages from overwriting the evicted pages. Note that an eviction is required on every page fault once the pages resident in the GPU‚Äôs memory are at capacity.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/b2b52441-6cc9-4a22-8c0f-6ee3511d3ec1",alt:"image"}})]),e._v(" "),t("p",[e._v("This preprocessing includes sorting the page faults in ascending order of page addresses (to accelerate the page table walks) and the analysis of page addresses to insert page prefetching requests.1 We refer to the time taken by the GPU runtime to perform a collection of operations to handle many page faults together as GPU  runtime fault handling time.")]),e._v(" "),t("p",[e._v("https://github.com/acsl-technion/gaia_nvidia/blob/e23e4d926f576c2c4169664b6add89e1368ee849/kernel/nvidia-uvm/uvm8_gpu_replayable_faults.c#L787")]),e._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('// Fault cache preprocessing for fault coalescing\n//\n// This function generates an ordered view of the given fault_cache in which faults are sorted by VA space, fault\n// address (aligned to 4K) and access type "intrusiveness" (atomic - write - read - prefetch). In order to minimize\n// the number of instance_ptr to VA space translations we perform a first sort by instance_ptr.\n//\n// This function returns NV_WARN_MORE_PROCESSING_REQUIRED if a fault buffer flush occurred during instance_ptr\n// translation and executed successfully, or the error code if it failed. NV_OK otherwise.\n//\n// Current scheme:\n// 1) sort by instance_ptr\n// 2) translate all instance_ptrs to VA spaces\n// 3) sort by va_space, fault address (GPU already reports 4K-aligned address) and access type\nstatic NV_STATUS preprocess_fault_batch(uvm_gpu_t *gpu, uvm_fault_service_batch_context_t *batch_context)\n')])]),e._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[e._v("1")]),t("br"),t("span",{staticClass:"line-number"},[e._v("2")]),t("br"),t("span",{staticClass:"line-number"},[e._v("3")]),t("br"),t("span",{staticClass:"line-number"},[e._v("4")]),t("br"),t("span",{staticClass:"line-number"},[e._v("5")]),t("br"),t("span",{staticClass:"line-number"},[e._v("6")]),t("br"),t("span",{staticClass:"line-number"},[e._v("7")]),t("br"),t("span",{staticClass:"line-number"},[e._v("8")]),t("br"),t("span",{staticClass:"line-number"},[e._v("9")]),t("br"),t("span",{staticClass:"line-number"},[e._v("10")]),t("br"),t("span",{staticClass:"line-number"},[e._v("11")]),t("br"),t("span",{staticClass:"line-number"},[e._v("12")]),t("br"),t("span",{staticClass:"line-number"},[e._v("13")]),t("br"),t("span",{staticClass:"line-number"},[e._v("14")]),t("br")])]),t("p",[e._v("The batch processing time is measured to be in the range of 223¬µs to 553¬µs with a median of 313¬µs, of which, GPU runtime fault handling accounts for an average of 46.69% of the time (measured to be in the range of 50¬µs to 430¬µs with a median of 140¬µs).")]),e._v(" "),t("p",[e._v("1ÔºâThread Oversubscription\n"),t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/d0ebc3d7-2c58-431a-97cd-fa019c36488d",alt:"image"}})]),e._v(" "),t("p",[e._v("We enable thread oversubscription from the beginning of the execution by allocating one additional thread block to each SM ( 1 ). The thread block additionally allocated to each SM is inactive at first. It is important to note that the number of active thread blocks does not exceed that of the baseline, which is determined by the physical resource constraints. Once all of the warps in an active thread block are stalled due to page faults, the thread oversubscription mechanism context switches the active (but stalled) thread block with an inactive thread block ( 2 ). The thread oversubscription mechanism can be detrimental if it causes premature evictions. To prevent this, the GPU runtime monitors the premature eviction rates by periodically estimating the running average of the lifetime of pages by tracking when each page is allocated and evicted. We use the running average as\nan indicator of premature evictions. If the running average is decreased by a certain threshold, the thread oversubscription mechanism does not allow any more context switching by decrementing (and limiting) the number of concurrently runnable thread blocks ( 3 ).6 Otherwise, thread oversubscription allocates one additional thread block to each SM in an incremental manner.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/31b20d5e-2b4f-4e1f-aeb2-338712decb97",alt:"image"}})]),e._v(" "),t("ol",{attrs:{start:"2"}},[t("li",[e._v("Unobstrusive Eviction")])]),e._v(" "),t("p",[e._v("When a page fault interrupt is raised by the GPU MMU, the top-half interrupt service routine (ISR) responds. It checks whether the number of GPU resident pages is at capacity via the GPU memory status tracker. If so, it sends a preemptive eviction request to the GPU. The rest of the fault handling (e.g., preprocessing of the page faults, CPU-side page table walks) is performed by the bottom-half ISR.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/5cb0d5d8-ab8e-47bd-92fb-fbcd901a3882",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/eb5a8d80-6ffa-4dae-a0bb-e2d903172760",alt:"image"}})]),e._v(" "),t("p",[e._v("When the GPU runtime begins a batch‚Äôs processing, it checks the GPU memory status. If it is at capacity, it initiates a single page eviction ( 1 ). Once page X is evicted from the GPU‚Äôs memory, both CPU and GPU page tables are updated ( 2 ). Unlike the baseline case (Figure 4), page A can be migrated to the GPU memory without any delay ( 3 ). At the same time, page Y can be evicted using bidirectional transfers. Since the data transfer speed from the GPU to CPU memory is faster than the other way around [29], eviction is completely unobtrusive and migrations to the GPU can occur without any delay.")]),e._v(" "),t("p",[t("strong",[e._v("In short, thread oversubscription increase the batch size by switching in in-active thread block. and unobstrusive eviction avoid the serialization of swap pages between host and device.")])]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_10-machine-learning-guided-optimal-use-of-gpu-unified-memory-2019"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_10-machine-learning-guided-optimal-use-of-gpu-unified-memory-2019"}},[e._v("#")]),e._v(" 10. Machine Learning Guided Optimal Use of GPU Unified Memory 2019")]),e._v(" "),t("p",[e._v("To enable better performance of UM, CUDA allows developers to give the UM driver additional advice on managing a given GPU memory range via an API function named cudaMemAdvise(const void *, size_t, enum cudaMemoryAdvise, int). The first two parameters of this function accept a pointer to a memory range with a specified size. The memory range should be allocated via cudaMallocManaged or declared via __managed__variables. The third parameter sets the advice for the memory range. The last parameter indicates the associated device‚Äôs id, which can indicate either a CPU or GPU device. The details and differences of these four kinds of advice are presented as follows:")]),e._v(" "),t("p",[e._v("‚Ä¢ Default: This represents the default on-demand page migration to accessing processor, using the first-touch policy.")]),e._v(" "),t("p",[e._v("‚Ä¢ cudaMemAdviseSetReadMostly: This advice is used for the data which is mostly going to be read from and only occasionally written to. The UM driver may create read-only copies of the data in a processor‚Äôs memory when that processor accesses it. If this region encounters any write requests, then only the write occurred page will be valid and other copies will be invalid.")]),e._v(" "),t("p",[e._v("‚Ä¢ cudaMemAdviseSetPreferredLocation: Once a target device is specified, this device memory can be set as the preferred location for the allocated data. The host memory can also be specified as the preferred location. Setting the preferred location does not cause data to migrate to that location immediately. The policy only guides what will happen when a fault occurs on the specified memory region: if data is already in the preferred location, the faulting processor will try to directly  establish a mapping to the region without causing page migration. Otherwise, the data will be migrated to the processor accessing it if the data is not in the preferred location or if a direct mapping cannot be established.")]),e._v(" "),t("p",[e._v("‚Ä¢ cudaMemAdviseSetAccessedBy: This advice implies that the data will be accessed by a specified CPU or GPU device. It has no impact on the data location and will not cause data migration. It only causes the data to be always mapped in the specified processor‚Äôs page tables, when applicable. The mapping will be accordingly updated if the data is migrated somehow. This advice is useful to indicate that avoiding faults is important for some data, especially when the data is accessed by a GPU within  a system containing multiple GPUs with peer-to-peer access enabled.")]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_14-fine-grain-quantitative-analysis-of-demand-paging-in-unified-virtual-memory-2024"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_14-fine-grain-quantitative-analysis-of-demand-paging-in-unified-virtual-memory-2024"}},[e._v("#")]),e._v(" 14. Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory [2024] üëçüëçüëçüëç")]),e._v(" "),t("p",[e._v("Same author: In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing[2021]")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/868d05d4-b668-4622-a41c-db6fd7e97496",alt:"image"}})]),e._v(" "),t("p",[e._v("The UVM host driver on the host is open source with dependencies on the proprietary nvidia driver/resource manager and the host OS for memory management. This driver is a runtime fault servicing engine and the memory manager for managed memory allocations."),t("br")]),e._v(" "),t("ol",[t("li",[e._v("the fault is generated and handled by the hardware thread‚Äôs corresponding ¬µTLB. The thread may continue executing instructions not blocked by a memory dependency. The fault propagates to the GPU memory management unit (GMMU), which writes the corresponding fault information into the GPU Fault Buffer and sends a hardware interrupt to the host. The fault buffer acts as a circular array, configured and managed by the UVM driver."),t("br")]),e._v(" "),t("li",[e._v("The nvidia-uvm driver fetches the fault information, caches it on the host, and services the faults through")]),e._v(" "),t("li",[e._v("page processing: page table update and TLB shootdown on the host and "),t("em",[t("strong",[e._v("GPU page table update")])])]),e._v(" "),t("li",[e._v("page migration: involves page migration.")])]),e._v(" "),t("p",[e._v("The GPU exposes two functionalities to the host via the GPU command push-buffer‚Äîhost-to GPU memory copy and fault replay. "),t("br"),e._v("\nAs part of the fault servicing process, the driver instructs the GPU to copy pages into its memory, generally using high-performance hardware ‚Äúcopy engines.‚Äù "),t("br"),e._v("\nOnce the GPU‚Äôs page tables are updated and the data is successfully migrated, the driver issues a fault replay, which clears the waiting status of ¬µTLB, causing them to ‚Äúreplay‚Äù the prior miss.")]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Fault Handling:")])])]),e._v(" "),t("p",[e._v("First, the GPU sends an interrupt over the interconnect to alert the host UVM driver of a page fault. The interrupt wakes up a worker thread to begin fault servicing if none is awake. "),t("br"),e._v("\nSecond, the host retrieves the complete fault information from the GPU Fault Buffer."),t("br"),e._v("\nThe default fault retrieval policy reads faults until the batch size limit (i.e., 256 faults) is reached or no faults remain in the buffer."),t("br")]),e._v(" "),t("p",[e._v("These VABlocks serve as logical boundaries; the driver processes all batch faults within a single VABlock together, and each VABlock within a batch requires a distinct processing step.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/5272ffc7-8eac-44a5-842e-837b8569d8d0",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/96f7b0b1-d205-4da5-908a-593bb0419676",alt:"image"}})]),e._v(" "),t("p",[e._v("Notes:")]),e._v(" "),t("ol",[t("li",[e._v("when prefetching is not enabled, Service Faults is the major part of delay in CPU-GPU system. In this case, even NVlink does not matter. The reason is that unmapping and tlb-shut down in multi-cpu costs a lot."),t("br")]),e._v(" "),t("li",[e._v("Pretching reduce the overhead by reduction of page fault and also increase the efficiency of NVlink."),t("br")]),e._v(" "),t("li",[e._v("Oversubscription worse the case by finding empty space failed first and then evictim block to GPU. This worsen the performance."),t("br")])]),e._v(" "),t("p",[e._v("1 & 2 explained:")]),e._v(" "),t("p",[e._v("(1) unmapping host-side data takes place on the fault path and incurs significant overhead"),t("br"),e._v("\n(2) certain hostside parallelizations of an application using UVM can exaggerate these unmapping costs."),t("br"),e._v("\nThe host OS performs this operation, and the costs likely stem from issues with virtual mappings across CPU cores, flushing dirty pages from caches and TLBs, NUMA, and other memory-adjacent issues."),t("br"),e._v("\nAdditionally, these operations do not take place in bulk due to the logical separation of VABlocks within UVM. "),t("br"),e._v("\nThis is an area that deserves particular scrutiny as HMM also performs host page unmapping on the fault path using host OS mechanisms, implying a similar cost could be applied to\nall devices when using HMM [15, 26].")]),e._v(" "),t("p",[e._v("compared to cpu-gpu case, GPU-GPU on-demand page migration is faster due to the actual page table updates offloaded to the source GPU. "),t("br"),e._v("\nFault servicing includes operations such as page unmapping and TLB shootdown on the source device."),t("br"),e._v("\nGPU page table updates and TLB shootdown are hardware based and relatively much faster."),t("br")]),e._v(" "),t("ol",{attrs:{start:"3"}},[t("li",[e._v("Explained\nProcess:\n(1) fail allocation"),t("br"),e._v("\n(2) evict a VABlock and migrate the data back to the host"),t("br"),e._v("\n(3) restart the block migration process, including host unmapping, data transfer, GPU mapping, page population, a process by which pages are filled with zero values before data is migrated\nto them.")])]),e._v(" "),t("p",[e._v("Interestingly, oversubscription diminishes the benefits of NVLink2. Oversubscription, as it is currently implemented, "),t("em",[t("strong",[e._v("always evicts pages back to the host memory")])]),e._v(". This causes the CPU-GPU PCIe interconnect to become active for data eviction.")]),e._v(" "),t("p",[e._v("üëâ In short, in CPU-GPU system, service faults are major issue due to tlb shutdown and page table update. This even diminish the power of NVLink. Memory Oversubscription worsen the situation by failing to allocate memory in GPU, find eviction and eviction to CPU, adding these operation worsen the performance."),t("br"),e._v("\nüëâ GPU-GPU does not have the service faults problem since page table update and tlb shutdown are handled by faster gpu hardware."),t("br"),e._v("\nüëâ Besides, prefetching helps to improve performance a lot by reducing fault and better bandwitdh efficiency."),t("br")])])}),[],!1,null,null,null);t.default=s.exports}}]);