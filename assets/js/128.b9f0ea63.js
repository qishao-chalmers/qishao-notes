(window.webpackJsonp=window.webpackJsonp||[]).push([[128],{591:function(t,e,i){"use strict";i.r(e);var a=i(12),n=Object(a.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("ol",{attrs:{start:"0"}},[e("li",[t._v("[backgroud] background of RL")]),t._v(" "),e("li",[t._v("[qwen] Stabilizing RL with LLMs: Formulation and Practices")])]),t._v(" "),e("hr"),t._v(" "),e("h1",{attrs:{id:"ðŸ¤–-0-summary-of-reinforcement-learning-concepts"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#ðŸ¤–-0-summary-of-reinforcement-learning-concepts"}},[t._v("#")]),t._v(" ðŸ¤– [0] Summary of Reinforcement Learning Concepts")]),t._v(" "),e("p",[t._v('This document summarizes the core definitions and differences for key concepts in policy gradient methods, as discussed in the context of the paper, "Stabilizing Reinforcement Learning with LLMs".')]),t._v(" "),e("h2",{attrs:{id:"_1-on-policy-vs-off-policy-training"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-on-policy-vs-off-policy-training"}},[t._v("#")]),t._v(" 1. On-Policy vs. Off-Policy Training")]),t._v(" "),e("p",[t._v("These terms define the relationship between the policy generating the data and the policy being optimized.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"left"}},[t._v("Feature")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("On-Policy Training")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("Off-Policy Training")])])]),t._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("Definition")])]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("The policy sampling the responses ($\\mu_{\\theta_{old}}$) is the same as the policy being optimized ($\\pi_{\\theta}$).")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("The policy sampling the responses ($\\mu_{\\theta_{old}}$) is "),e("em",[t._v("different")]),t._v(" from the policy being optimized ($\\pi_{\\theta}$).")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("Policy Staleness")])]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Minimized, as the policies are theoretically identical (omitting training-inference discrepancy).")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Increased, typically to accelerate training by reusing data for multiple gradient updates.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("Sample Efficiency")])]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Lower. Requires generating a new batch of data for every (or very few) update steps.")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Higher. Allows for reusing old data for multiple gradient steps.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("LLM Stabilization")])]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Requires "),e("strong",[t._v("IS Correction")]),t._v(" to address the training-inference discrepancy.")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Requires "),e("strong",[t._v("Clipping")]),t._v(" and "),e("strong",[t._v("Routing Replay (R2/R3)")]),t._v(" to safely mitigate the instability caused by policy staleness.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("Example Algorithms")])]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("REINFORCE, traditional GRPO (when data is used once).")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Modern PPO, GRPO/MiniRL when using large batch sizes split into mini-batches.")])])])]),t._v(" "),e("h2",{attrs:{id:"_2-reinforce-vs-ppo"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-reinforce-vs-ppo"}},[t._v("#")]),t._v(" 2. REINFORCE vs. PPO")]),t._v(" "),e("p",[t._v("These are two of the most significant algorithms in the policy gradient family.")]),t._v(" "),e("h3",{attrs:{id:"a-reinforce-the-foundation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#a-reinforce-the-foundation"}},[t._v("#")]),t._v(" A. REINFORCE (The Foundation)")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Policy Type:")]),t._v(" On-Policy.")]),t._v(" "),e("li",[e("strong",[t._v("Mechanism:")]),t._v(" It is the foundational token-level objective used to optimize the policy directly by moving the probability of actions up based on the final "),e("strong",[t._v("Monte Carlo")]),t._v(" reward (return, $G_t$).")]),t._v(" "),e("li",[e("strong",[t._v("Drawback:")]),t._v(" Suffers from "),e("strong",[t._v("high variance")]),t._v(" in its gradient estimates, leading to unstable and slow training. The paper's "),e("strong",[t._v("MiniRL")]),t._v(" baseline is a modified REINFORCE objective.")])]),t._v(" "),e("h3",{attrs:{id:"b-ppo-proximal-policy-optimization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#b-ppo-proximal-policy-optimization"}},[t._v("#")]),t._v(" B. PPO (Proximal Policy Optimization)")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Policy Type:")]),t._v(" On-Policy (Designed for safe data reuse).")]),t._v(" "),e("li",[e("strong",[t._v("Mechanism:")]),t._v(" PPO is an "),e("strong",[t._v("Actor-Critic")]),t._v(" method. It replaces the noisy Monte Carlo return ($G_t$) with a low-variance "),e("strong",[t._v("Advantage Estimate")]),t._v(" ($A_t$) calculated using a value network (Critic).")]),t._v(" "),e("li",[e("strong",[t._v("Key Stability Feature:")]),t._v(" It uses a "),e("strong",[t._v("clipped surrogate objective")]),t._v(" to explicitly limit how far the new policy can deviate from the old policy in a single step. This ensures stability and allows for safe data reuse.")])]),t._v(" "),e("h3",{attrs:{id:"c-comparison"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#c-comparison"}},[t._v("#")]),t._v(" C. Comparison")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"left"}},[t._v("Feature")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("REINFORCE")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("PPO (Proximal Policy Optimization)")])])]),t._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("Stability")])]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Low stability (High variance in $G_t$).")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("High stability (Low variance in $A_t$ and uses clipping).")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("Value Function (Critic)")])]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("No Critic (Pure Policy Gradient).")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Yes, uses a Critic to calculate the Advantage ($A_t$).")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("Data Usage")])]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Sample Inefficient (Data used once).")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Sample Efficient (Data reused safely due to clipping).")])])])]),t._v(" "),e("h2",{attrs:{id:"_3-grpo-group-policy-optimization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-grpo-group-policy-optimization"}},[t._v("#")]),t._v(" 3. GRPO (Group Policy Optimization)")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Framework:")]),t._v(" GRPO operates within a "),e("strong",[t._v("policy optimization framework")]),t._v(".")]),t._v(" "),e("li",[e("strong",[t._v("Classification:")]),t._v(" It is a policy gradient algorithm often used in an on-policy setup.")]),t._v(" "),e("li",[e("strong",[t._v("Paper's Finding of 1[qwen] paper")]),t._v(" The paper argues that GRPO's common objective is prone to failure because it includes "),e("strong",[t._v("length normalization")]),t._v(" (which introduces bias) and omits the crucial "),e("strong",[t._v("training-inference IS correction")]),t._v(".")])]),t._v(" "),e("hr"),t._v(" "),e("h1",{attrs:{id:"_1-qwen-stabilizing-rl-with-llms-formulation-and-practices"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-qwen-stabilizing-rl-with-llms-formulation-and-practices"}},[t._v("#")]),t._v(" 1 [qwen] Stabilizing RL with LLMs: Formulation and Practices")]),t._v(" "),e("p",[t._v("This paper proposes a theoretical formulation and provides empirical practices for stabilizing Reinforcement Learning (RL) with Large Language Models (LLMs). The central challenge addressed is the mismatch between optimizing a "),e("strong",[t._v("sequence-level reward")]),t._v(" (score for the complete response) and using a "),e("strong",[t._v("token-level objective")]),t._v(" (updates based on individual tokens).")]),t._v(" "),e("h3",{attrs:{id:"_1-core-theoretical-insight"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-core-theoretical-insight"}},[t._v("#")]),t._v(" 1. Core Theoretical Insight")]),t._v(" "),e("p",[t._v("The paper posits that the token-level objective ($\\mathcal{J}^{token}$) can be used as a "),e("strong",[t._v("first-order approximation")]),t._v(" of the true sequence-level objective ($\\mathcal{J}^{seq}$). This approximation is valid only when two specific discrepancies are minimized:")]),t._v(" "),e("ol",[e("li",[e("strong",[t._v("Training-Inference Discrepancy:")]),t._v(" The numerical difference between the training engine ($\\pi_{\\theta_{old}}$) and the inference engine ($\\mu_{\\theta_{old}}$).")]),t._v(" "),e("li",[e("strong",[t._v("Policy Staleness:")]),t._v(" The difference between the data-sampling policy ($\\mu_{\\theta_{old}}$) and the policy being optimized ($\\pi_{\\theta}$).")])]),t._v(" "),e("h3",{attrs:{id:"_2-key-stabilization-techniques-including-your-questions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-key-stabilization-techniques-including-your-questions"}},[t._v("#")]),t._v(" 2. Key Stabilization Techniques (Including Your Questions)")]),t._v(" "),e("p",[t._v("The paper explains how crucial stabilization techniques work by minimizing these discrepancies:")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"left"}},[t._v("Technique")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("Discrepancy Addressed")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("Role in MoE Models")])])]),t._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("Importance Sampling (IS) Correction")])]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Training-Inference Discrepancy.")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("The factor $\\frac{\\pi_{\\theta_{old}}}{\\mu_{\\theta_{old}}}$ corrects for numerical differences between the training and inference engines, despite having the same parameters. "),e("strong",[t._v("Omitting this causes rapid training collapse")]),t._v(".")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("Clipping")])]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Policy Staleness.")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Prevents aggressive policy updates, thereby restraining policy staleness.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("Routing Replay (R2/R3)")])]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Training-Inference Discrepancy (R3) & Policy Staleness (R2/R3).")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("Essential for MoE models")]),t._v(". It stabilizes training by fixing the routed experts during policy optimization.")])])])]),t._v(" "),e("h3",{attrs:{id:"_3-empirical-findings-on-policy-vs-off-policy"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-empirical-findings-on-policy-vs-off-policy"}},[t._v("#")]),t._v(" 3. Empirical Findings (On-Policy vs. Off-Policy)")]),t._v(" "),e("p",[t._v("The experiments use "),e("strong",[t._v("MiniRL")]),t._v(", a baseline based on the REINFORCE objective but with IS correction and clipping.")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("On-Policy Training")]),t._v(" (Low Staleness): The basic policy gradient algorithm with "),e("strong",[t._v("IS correction (MiniRL)")]),t._v(" is the most stable and performs best.")]),t._v(" "),e("li",[e("strong",[t._v("Off-Policy Training")]),t._v(" (High Staleness): Introducing off-policy updates (data reuse for multiple steps) requires "),e("strong",[t._v("both clipping and Routing Replay (R2/R3)")]),t._v(" to achieve stability and prevent collapse. The degree of off-policiness determines whether R2 or R3 performs better.")])]),t._v(" "),e("h3",{attrs:{id:"_4-algorithm-context-grpo"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-algorithm-context-grpo"}},[t._v("#")]),t._v(" 4. Algorithm Context (GRPO)")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("GRPO (Group Policy Optimization)")]),t._v(" is a policy gradient algorithm with a token-level optimization framework.")]),t._v(" "),e("li",[t._v("The paper argues that GRPO's common formulationâ€”which includes "),e("strong",[t._v("length normalization")]),t._v(" and "),e("strong",[t._v("omits the training-inference IS correction")]),t._v("â€”invalidates the first-order approximation, leading to a biased objective and suboptimal performance.")])]),t._v(" "),e("h3",{attrs:{id:"_5-final-conclusion"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-final-conclusion"}},[t._v("#")]),t._v(" 5. Final Conclusion")]),t._v(" "),e("p",[t._v("Stable training plays a decisive role in successfully scaling RL. Once training is stabilized with the proper techniques, the model consistently converges to comparable final performance, regardless of the initial cold-start data.")])])}),[],!1,null,null,null);e.default=n.exports}}]);