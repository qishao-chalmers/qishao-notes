(window.webpackJsonp=window.webpackJsonp||[]).push([[112],{565:function(e,n,i){"use strict";i.r(n);var t=i(8),r=Object(t.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("ol",[n("li",[e._v("[409 2022] ORCA: A Distributed Serving System for Transformer-Based Generative Models")]),e._v(" "),n("li",[e._v("[Y2023] Splitwise: Efficient generative llm inference using phase splitting")]),e._v(" "),n("li",[e._v("[Y2023] SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills")]),e._v(" "),n("li",[e._v("[Y2023] Ring Attention with Blockwise Transformers for Near-Infinite Context")]),e._v(" "),n("li",[e._v("[146 2024] Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve")]),e._v(" "),n("li",[e._v("[Y2024] DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving")]),e._v(" "),n("li",[e._v("[Y2024] Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads")]),e._v(" "),n("li",[e._v("[Y2024] Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving")]),e._v(" "),n("li",[e._v("[Y2025] HexGen-2: Disaggregated Generated Inference of LLM in Heterogeneous Environment")])])])}),[],!1,null,null,null);n.default=r.exports}}]);