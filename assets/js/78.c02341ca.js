(window.webpackJsonp=window.webpackJsonp||[]).push([[78],{540:function(e,a,t){"use strict";t.r(a);var i=t(12),s=Object(i.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("ol",[a("li",[e._v("[Bachelor Thesis] Developing an auto-tunable GEMM kernel that utilizes Tensor Cores")]),e._v(" "),a("li",[e._v("[NVIDIA BLOG] https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html")]),e._v(" "),a("li",[e._v("Understanding GEMM Performance and Energy on NVIDIA Ada Lovelace: A Machine Learning-Based Analytical Approach")])]),e._v(" "),a("hr"),e._v(" "),a("h2",{attrs:{id:"_1-bachelor-thesis-developing-an-auto-tunable-gemm-kernel-that-utilizes-tensor-cores"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-bachelor-thesis-developing-an-auto-tunable-gemm-kernel-that-utilizes-tensor-cores"}},[e._v("#")]),e._v(" 1. [Bachelor Thesis] Developing an auto-tunable GEMM kernel that utilizes Tensor Cores")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/8fc44f80-eb79-4dfd-80a4-60a80343d087",alt:"image"}})]),e._v(" "),a("p",[a("strong",[e._v("Thread Block Tile")])]),e._v(" "),a("p",[e._v("Here it can be observed that the optimum lies somewhere between the blue colours, which represent values of 64 and 128.")]),e._v(" "),a("p",[a("strong",[e._v("Warp Tile")])]),e._v(" "),a("p",[e._v("For both GPUs, it is visible that for the 32 × 8 × 16 shaped WMMA operation, a higher WMMA_COLS value performs better, and vice-versa for 8 × 32 × 16 and WMMA_ROWS.")]),e._v(" "),a("p",[e._v("This experiments are done on NVIDIA A4000 Ada.")]),e._v(" "),a("hr"),e._v(" "),a("h2",{attrs:{id:"_2-nvidia-blog-matrix-multiplication-background-user-s-guide"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-nvidia-blog-matrix-multiplication-background-user-s-guide"}},[e._v("#")]),e._v(" 2. [NVIDIA BLOG] Matrix Multiplication Background User's Guide")]),e._v(" "),a("h3",{attrs:{id:"tile-for-thread-block"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tile-for-thread-block"}},[e._v("#")]),e._v(" Tile for Thread Block")]),e._v(" "),a("p",[e._v("For cuBLAS GEMMs, thread block tile sizes typically but not necessarily use power-of-two dimensions.")]),e._v(" "),a("p",[e._v("Different tile sizes might be used for different use cases, but as a starting point, the following tiles are available:")]),e._v(" "),a("ul",[a("li",[e._v("256x128 and 128x256 (most efficient)")]),e._v(" "),a("li",[e._v("128x128")]),e._v(" "),a("li",[e._v("256x64 and 64x256")]),e._v(" "),a("li",[e._v("128x64 and 64x128")]),e._v(" "),a("li",[e._v("64x64 (least efficient)")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/7fb2ff1b-f665-484e-b44b-9c869b89fd02",alt:"image"}})]),e._v(" "),a("hr"),e._v(" "),a("h2",{attrs:{id:"_3-understanding-gemm-performance-and-energy-on-nvidia-ada-lovelace-a-machine-learning-based-analytical-approach"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-understanding-gemm-performance-and-energy-on-nvidia-ada-lovelace-a-machine-learning-based-analytical-approach"}},[e._v("#")]),e._v(" 3. Understanding GEMM Performance and Energy on NVIDIA Ada Lovelace: A Machine Learning-Based Analytical Approach")]),e._v(" "),a("p",[e._v("This papers seems that they didnt consider double buffer pingpong.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/14c8b856-689f-4c9c-9e6b-804571ab5382",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"matrix-operation-performance-analysis"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#matrix-operation-performance-analysis"}},[e._v("#")]),e._v(" Matrix Operation Performance Analysis")]),e._v(" "),a("p",[e._v("The experimental results demonstrate several significant correlations between matrix operations and performance characteristics. Our analysis introduces key performance metrics:")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/71311a09-619e-49b7-96bf-1c67becc1673",alt:"image"}})]),e._v(" "),a("ol",[a("li",[e._v("Matrix Dimensionality Impact: Correlation analysis revealed significant relationships:")])]),e._v(" "),a("ul",[a("li",[e._v("Matrix volume (M × N × K) shows strong correlation with runtime (r = 0.98)")]),e._v(" "),a("li",[e._v("Output dimensions (M × N) demonstrate higher correlation with power consumption (r = 0.80) than compute\ndimension K")]),e._v(" "),a("li",[e._v("TFLOPS efficiency exhibits negative correlation with matrix size (r = −0.41), following:")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/aac518d6-4a49-4896-a2f6-8aa77b760356",alt:"image"}})]),e._v(" "),a("ol",{attrs:{start:"2"}},[a("li",[e._v("Tile Size Performance Impact: Analysis of tile size effects revealed optimization patterns:")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/7fe0ec65-da9c-4679-b530-9378219ac875",alt:"image"}})]),e._v(" "),a("p",[e._v("Key findings include:")]),e._v(" "),a("ul",[a("li",[e._v("Optimal tile size of 16 × 16 minimizes runtime across dimensions")]),e._v(" "),a("li",[e._v("Power consumption stabilization occurs with larger tile sizes")]),e._v(" "),a("li",[e._v("Performance plateau observed beyond 16 × 16 tiles due to shared memory constraints")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/e489e08c-57c5-496b-abee-7c0a26af305c",alt:"image"}})]),e._v(" "),a("p",[e._v("But if we calculate the shared memory usage for tiling 16"),a("em",[e._v("16, it would be 2")]),e._v("16"),a("em",[e._v("16")]),e._v("4 = 2kB and the threadblock per SM is 6, so "),a("strong",[e._v("it only use 12KB shared memory")]),e._v("...why?")])])}),[],!1,null,null,null);a.default=s.exports}}]);