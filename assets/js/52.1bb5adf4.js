(window.webpackJsonp=window.webpackJsonp||[]).push([[52],{507:function(e,t,r){"use strict";r.r(t);var a=r(9),i=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis [Citation 39]")]),e._v(" "),t("li",[e._v("Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA] üëç")]),e._v(" "),t("li",[e._v("[120 2015] Anatomy of GPU Memory System for Multi-Application Execution üëç")]),e._v(" "),t("li",[e._v("CRISP: Concurrent Rendering and Compute Simulation Platform for GPUs üëç")]),e._v(" "),t("li",[e._v("Parallelizing a modern GPU simulator [Rodrigo Huerta UPC]  üëç")]),e._v(" "),t("li",[e._v("Analyzing and Improving Hardware Modeling of Accel-Sim [Rodrigo Huerta UPC]  üëç")])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_1-delta-gpu-performance-model-for-deep-learning-applications-with-in-depth-memory-system-traffic-analysis"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-delta-gpu-performance-model-for-deep-learning-applications-with-in-depth-memory-system-traffic-analysis"}},[e._v("#")]),e._v(" 1.DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis")]),e._v(" "),t("p",[e._v("Still reading in process.")]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_2-lost-in-abstraction-pitfalls-of-analyzing-gpus-at-the-intermediate-language-level-hpca"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-lost-in-abstraction-pitfalls-of-analyzing-gpus-at-the-intermediate-language-level-hpca"}},[e._v("#")]),e._v(" 2.Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]")]),e._v(" "),t("p",[e._v("Done.\nGem5 GPU Introduction.")]),e._v(" "),t("p",[e._v("Reference Materials")]),e._v(" "),t("p",[e._v("AMD_gem5_APU_simulator_isca_2018_gem5_wiki.pdf")]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_3-120-anatomy-of-gpu-memory-system-for-multi-application-execution"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-120-anatomy-of-gpu-memory-system-for-multi-application-execution"}},[e._v("#")]),e._v(" 3. [120] Anatomy of GPU Memory System for Multi-Application Execution")]),e._v(" "),t("p",[t("strong",[e._v("MAFIA: Multiple Application Framework for GPUs")])]),e._v(" "),t("p",[e._v("This is a opensource work based on gpgpu-sim.")]),e._v(" "),t("p",[e._v("https://github.com/adwaitjog/mafia")]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_3-crisp-concurrent-rendering-and-compute-simulation-platform-for-gpus"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-crisp-concurrent-rendering-and-compute-simulation-platform-for-gpus"}},[e._v("#")]),e._v(" 3. CRISP: Concurrent Rendering and Compute Simulation Platform for GPUs üëç")]),e._v(" "),t("p",[e._v("üëç Source code: https://github.com/JRPan/crisp-artifact")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/07e94c89-67f2-4b96-8782-7b5204aa9682",alt:"image"}})]),e._v(" "),t("p",[e._v("We extended Accel-Sim and GPGPU-Sim to support advanced GPU partition methods."),t("br"),e._v("\nBy default, the simulator supports concurrent kernel execution but launches thread blocks (CTAs) from one kernel exhaustively before switching to the next kernel."),t("br"),e._v("\nThis means if a kernel is large enough and has enough warps to fill all the SMs, there is no concurrent execution."),t("br"),e._v("\nWe updated the CTA scheduler and added the following partition methods:\\")]),e._v(" "),t("ul",[t("li",[e._v("Multi-Process Service (MPS)")]),e._v(" "),t("li",[e._v("Multi-instance GPU (MiG)")]),e._v(" "),t("li",[e._v("a Fine-grained intra-SM Partition (FG) similar to the async compute feature in Vulkan.")])]),e._v(" "),t("p",[e._v("At a high level, MPS and MiG represent coarse-grained inter-SM partitioning methods where each SM is dedicated to either graphics rendering or general computing tasks."),t("br"),e._v("\nIn the MPS model, only the SMs are partitioned, while the L2 cache and higher-level memory spaces remain shared across tasks."),t("br"),e._v("\nThe MiG model partitions all resources, and each SM only accesses a designated subset of memory sub-partitions and controllers.")]),e._v(" "),t("p",[e._v("In fine-grained intra-SM partitioning (FG), each SM is partitioned to run both tasks."),t("br"),e._v("\nInstead of issuing as many CTAs as possible, the CTA scheduler only issues CTAs within the limits of partitioned resources."),t("br"),e._v("\nPartitioned resources include thread slots, shared memory, and registers."),t("br"),e._v("\nAt the CTA issue stage, the CTA scheduler checks the CTA‚Äôs resource requirements with the remaining resources on the SM."),t("br"),e._v("\nIf all resource constraints are met, the CTA is issued."),t("br"),e._v("\nAt CTA commit, resources occupied by the CTA are freed and can be used again for future CTAs."),t("br"),e._v("\nHowever, static partitioning leads to inefficiency since different kernel pairs exhibit divergent characteristics."),t("br"),e._v("\nFor example, one kernel may be register-heavy while another uses a lot of shared memory."),t("br"),e._v(" "),t("strong",[e._v("Therefore, the partition ratio can be changed dynamically to maximize resource utilization.")])]),e._v(" "),t("p",[e._v("When the partition ratio changes dynamically, and on-chip resources must be reassigned to reflect the updated ratio."),t("br"),e._v("\nFor example, each CTA from kernel A has 128 threads, while CTAs from kernel B have 256 threads."),t("br"),e._v("\nResources freed by one CTA from kernel A are not enough for a CTA from kernel B."),t("br"),e._v("\nIn this case, the CTA scheduler stops issuing CTAs from kernel A and waits until two CTAs from kernel A commit, then starts issuing CTAs from kernel B.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/ee6cd302-0e24-4a63-818f-c62bc6f9bd8c",alt:"image"}})])])}),[],!1,null,null,null);t.default=i.exports}}]);