(window.webpackJsonp=window.webpackJsonp||[]).push([[99],{552:function(e,n,t){"use strict";t.r(n);var i=t(9),a=Object(i.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("p",[e._v("Too many paper on llms...")]),e._v(" "),n("p",[n("strong",[e._v("Survey")])]),e._v(" "),n("ol",[n("li",[e._v("[C60 2024] Understanding LLMs: A Comprehensive Overview from Training to Inference")]),e._v(" "),n("li",[e._v("[C20 2024] Mobile Edge Intelligence for Large Language Models: A Contemporary Survey")]),e._v(" "),n("li",[e._v("[P 58] A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms")]),e._v(" "),n("li",[e._v("[P 30] A Comprehensive Evaluation of Quantization Strategies for Large Language Models")]),e._v(" "),n("li",[e._v("[P 8]  A Comprehensive Study on Quantization Techniques for Large Language Models")]),e._v(" "),n("li",[e._v("[C24] Model Compression and Efficient Inference for Large Language Models: A Survey")]),e._v(" "),n("li",[e._v("[C26 2024] Towards Better Chain-of-Thought Prompting Strategies: A Survey")]),e._v(" "),n("li",[e._v("[C24 2024] A Survey of Reasoning with Foundation Models")]),e._v(" "),n("li",[e._v("[C75 2025] Resource-efficient Algorithms and Systems of Foundation Models: A Survey üëç")]),e._v(" "),n("li",[e._v("[C4 2024] LLM for Mobile: An Initial Roadmap")]),e._v(" "),n("li",[e._v("[C1 2024] Achieving Peak Performance for Large Language Models: A Systematic Review")]),e._v(" "),n("li",[e._v("[C1 2024] A Survey: Collaborative Hardware and Software Design in the Era of Large Language Models")]),e._v(" "),n("li",[e._v("[C577 2014] Mixture of experts: a literature survey")]),e._v(" "),n("li",[e._v("[C187 2024] Large Language Model based Multi-Agents: A Survey of Progress and Challenges")]),e._v(" "),n("li",[e._v("[C80 2024] Understanding the planning of LLM agents: A survey")]),e._v(" "),n("li",[e._v("[C895 2023] A survey on large language model based autonomous agents\\")]),e._v(" "),n("li",[e._v("[C60 2024] Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects")]),e._v(" "),n("li",[e._v("[C59 2024] Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline")])]),e._v(" "),n("p",[n("strong",[e._v("KV Cache")])]),e._v(" "),n("ol",[n("li",[e._v("[C60 2024 USENIX] InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management  üëç")]),e._v(" "),n("li",[e._v("[C1 2024]LayerKV: Optimizing Large Language Model Serving with Layer-wise KV Cache Management")]),e._v(" "),n("li",[e._v("[C1 2024 wei] Unifying KV Cache Compression for Large Language Models with LeanKV")])]),e._v(" "),n("p",[n("strong",[e._v("Quantization")])]),e._v(" "),n("ol",[n("li",[e._v("[C8 Y2024] An Empirical Study of LLaMA3 Quantization: From LLMs to MLLMs")])]),e._v(" "),n("p",[n("strong",[e._v("Cross-layer Attention")])]),e._v(" "),n("ol",[n("li",[e._v("[C25 Y2024] "),n("strong",[e._v("Reducing Transformer Key-Value Cache Size")]),e._v(" with Cross-Layer Attention")]),e._v(" "),n("li",[e._v("[C4 2024] Cross-layer Attention Sharing for Large Language Models")])]),e._v(" "),n("p",[n("strong",[e._v("Attention")])]),e._v(" "),n("ol",[n("li",[e._v("[C1709 2020] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention")]),e._v(" "),n("li",[e._v("[C573 2018] Efficient Attention: Attention With Linear Complexities")]),e._v(" "),n("li",[e._v("[C93 2024] Gated Linear Attention Transformers with Hardware-Efficient Training")]),e._v(" "),n("li",[e._v("[C24 2024] Tensor Attention Training: Provably Efficient Learning of Higher-order Transformers")]),e._v(" "),n("li",[e._v("[2024] When Attention Sink Emerges in Language Models: An Empirical View üëç")]),e._v(" "),n("li",[e._v("[C40 2024] Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon")])]),e._v(" "),n("p",[n("strong",[e._v("Is All You Need")])]),e._v(" "),n("ol",[n("li",[e._v("[2025] Element-wise Attention Is All You Need")]),e._v(" "),n("li",[e._v("[2025] Tensor Product Attention Is All You Need")])]),e._v(" "),n("p",[n("strong",[e._v("Feedforward Layers")])]),e._v(" "),n("ol",[n("li",[e._v("[2024] Building on Efficient Foundations: Effectively Training LLMs with Structured Feedforward Layers")]),e._v(" "),n("li",[e._v("[C3 2024] FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference")])]),e._v(" "),n("p",[n("strong",[e._v("Attatch Memory")])]),e._v(" "),n("ol",[n("li",[e._v("[C2 2024] Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU")]),e._v(" "),n("li",[e._v("[C63 2023] LLM in a flash: Efficient Large Language Model Inference with Limited Memory")])]),e._v(" "),n("p",[n("strong",[e._v("Novel LLM")])]),e._v(" "),n("ol",[n("li",[e._v("[C2 2024] Larimar: Large Language Models with Episodic Memory Control")])]),e._v(" "),n("p",[n("strong",[e._v("Batch")])]),e._v(" "),n("ol",[n("li",[e._v("[C59 2023] Batch Prompting: Efficient Inference with Large Language Model APIs")])]),e._v(" "),n("p",[n("strong",[e._v("Pruning")])]),e._v(" "),n("ol",[n("li",[e._v("[C25 2024] ZipLM: Inference-Aware Structured Pruning of Language Models")])]),e._v(" "),n("p",[n("strong",[e._v("Speculative decoding")])]),e._v(" "),n("ol",[n("li",[e._v("[C25 2024] Efficient Inference for Large Language Model-based Generative Recommendation")]),e._v(" "),n("li",[e._v("[C39 2024] Enhancing Inference Efficiency and Accuracy in Large Language Models through Next-Phrase Prediction  üëç")]),e._v(" "),n("li",[e._v("[C5 2023] Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy")]),e._v(" "),n("li",[e._v("[C64 2024] Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding   üëç")]),e._v(" "),n("li",[e._v("[C4 2024] Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference")]),e._v(" "),n("li",[e._v("[C46 2023] SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference  üëç")])]),e._v(" "),n("p",[n("strong",[e._v("Interesting")])]),e._v(" "),n("ol",[n("li",[e._v("[C60 2024] SnapKV: LLM Knows What You Are Looking for before Generation  üëç")]),e._v(" "),n("li",[e._v("[C6 2024] Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training")]),e._v(" "),n("li",[e._v("[C165 2019] Efficient Training of BERT by Progressively Stacking")]),e._v(" "),n("li",[e._v("[C9 ISCA 2024] LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference")]),e._v(" "),n("li",[e._v("[C6 2024] Efficient Large Foundation Models Design: A Perspective From Model and System Co-Design")]),e._v(" "),n("li",[e._v("[C83 2023] Compressing Context to Enhance Inference Efficiency of Large Language Models  üëç")]),e._v(" "),n("li",[e._v("[C13 2024 OSDI] ServerlessLLM: Low-Latency Serverless Inference for Large Language Models")]),e._v(" "),n("li",[e._v("[C12 2024] From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models")]),e._v(" "),n("li",[e._v("[C418 2022] Transformers Learn In-Context by Gradient Descent üëç")]),e._v(" "),n("li",[e._v("[C51 2024] Massive Activations in Large Language Models üëç")]),e._v(" "),n("li",[e._v("[C2007 2019] Generating Long Sequences with Sparse Transformers üëç")]),e._v(" "),n("li",[e._v("[C1890 2019] What does BERT look at? An Analysis of BERT‚Äôs Attention")]),e._v(" "),n("li",[e._v("[C1798 2019] BERT Rediscovers the Classical NLP Pipeline")]),e._v(" "),n("li",[e._v("[C402 2019] Analyzing the Structure of Attention in a Transformer Language Model")]),e._v(" "),n("li",[e._v("[C347 2018] Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures")]),e._v(" "),n("li",[e._v("[C112 2021] MoEfication: Transformer Feed-forward Layers are Mixtures of Experts")]),e._v(" "),n("li",[e._v("[C1325 2019] Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Prune")]),e._v(" "),n("li",[e._v("[C101] An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation")]),e._v(" "),n("li",[e._v("[C38517 2020] Language Models are Few-Shot Learners üëç")]),e._v(" "),n("li",[e._v("[C24 2024] What can a Single Attention Layer Learn? A Study Through the Random Features Lens")]),e._v(" "),n("li",[e._v("[C9 2023] Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps")]),e._v(" "),n("li",[e._v("[C606 2021] Transformer Feed-Forward Layers Are Key-Value Memories üëç")]),e._v(" "),n("li",[e._v("[Blog] The Feedforward Demystified: A Core Operation of Transformers")]),e._v(" "),n("li",[e._v("[C211 2024] ConvBERT: Improving BERT with Span-based Dynamic Convolution  üëç")]),e._v(" "),n("li",[e._v("[C15 2020] Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models")]),e._v(" "),n("li",[e._v("[C147 2023] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective")]),e._v(" "),n("li",[e._v("[C24 2024] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")]),e._v(" "),n("li",[e._v("[C0 2024] Densing Law of LLMs")]),e._v(" "),n("li",[e._v("[C9 2024] Demystifying the Compression of Mixture-of-Experts Through a Unified Framework")]),e._v(" "),n("li",[e._v("[C7 2024] Configurable Foundation Models: Building LLMs from a Modular Perspective")]),e._v(" "),n("li",[e._v("[C2 2024] Task Scheduling for Efficient Inference of Large Language Models on Single Moderate GPU Systems")]),e._v(" "),n("li",[e._v("[C7 2024] Towards Sustainable Large Language Model Serving üëç")]),e._v(" "),n("li",[e._v("[C42 2024] LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding")]),e._v(" "),n("li",[e._v("[C10395 0221] GPT Understands,Too")]),e._v(" "),n("li",[e._v("[C36 2023] Simplifying Transformer Blocks")]),e._v(" "),n("li",[e._v("[C2 2024] When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1")]),e._v(" "),n("li",[e._v("[C188 2023] LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion")])]),e._v(" "),n("p",[n("strong",[e._v("Why Infer?")])]),e._v(" "),n("ol",[n("li",[e._v("[C78 2024] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU  üëç")]),e._v(" "),n("li",[e._v("[C25 2024] Powerinfer-2: fast large language model inference on a smartphone")]),e._v(" "),n("li",[e._v("[C6 2024] InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference")]),e._v(" "),n("li",[e._v("[C0 2024] CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation")]),e._v(" "),n("li",[e._v("[C3 2024] NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference")]),e._v(" "),n("li",[e._v("[C2 2024] MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs")]),e._v(" "),n("li",[e._v("[C2 2025] SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference")]),e._v(" "),n("li",[e._v("[C1 2024] BlendServe: Optimizing Offline Inference for Auto-regressive Large Models with Resource-aware Batching")]),e._v(" "),n("li",[e._v("[C5 2024] Efficient LLM Inference with Activation Checkpointing and Hybrid Caching")]),e._v(" "),n("li",[e._v("[C12 2024]Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache")]),e._v(" "),n("li",[e._v("[C2-24] HiRE: High Recall Approximate Top-k Estimation for Efficient LLM Inference*")]),e._v(" "),n("li",[e._v("[C1 2024] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference")])]),e._v(" "),n("p",[n("strong",[e._v("Chain of Thoughts")])]),e._v(" "),n("ol",[n("li",[e._v("[C1954 2024] Tree of Thoughts: Deliberate Problem Solving with Large Language Models üëç")]),e._v(" "),n("li",[e._v("[C795 2022] Automatic Chain of Thought Prompting in Large Language Models")]),e._v(" "),n("li",[e._v("[C109 2022] Iteratively Prompt Pre-trained Language Models for Chain of Thought")]),e._v(" "),n("li",[e._v("[C2 2024] Reducing Costs - The Path of Optimization for ChainofThought Reasoning via Sparse Attention Mechanism")]),e._v(" "),n("li",[e._v("[C17 2024] Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future")]),e._v(" "),n("li",[e._v("[C63 2024] Chain of Thought Empowers Transformers to Solve Inherently Serial Problems")])]),e._v(" "),n("p",[n("strong",[e._v("General Efficient")])]),e._v(" "),n("ol",[n("li",[e._v("[C44 2023] Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity")]),e._v(" "),n("li",[e._v("[C4 2024] Efficient Training and Inference: Techniques for Large Language Models Using Llama")]),e._v(" "),n("li",[e._v("[C226 2023] H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models")]),e._v(" "),n("li",[e._v("[C17 2024] EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism")])]),e._v(" "),n("p",[n("strong",[e._v("Big tech")])]),e._v(" "),n("ol",[n("li",[e._v("[NVIDIA 1868] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism")]),e._v(" "),n("li",[e._v("[2579] Scaling Laws for Neural Language Models")]),e._v(" "),n("li",[e._v("[C510 2021] Baidu ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation")]),e._v(" "),n("li",[e._v("[C6 2024] Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent")]),e._v(" "),n("li",[e._v("[C1671] Qwen Technical Report")]),e._v(" "),n("li",[e._v("[C1212] Constitutional AI: Harmlessness from AI Feedback")]),e._v(" "),n("li",[e._v("[C1082] Scaling Language Models: Methods, Analysis & Insights from Training Gopher")]),e._v(" "),n("li",[e._v("[C11289] Llama 2: Open Foundation and Fine-Tuned Chat Models")])]),e._v(" "),n("p",[n("strong",[e._v("Hyperparameter")])]),e._v(" "),n("ol",[n("li",[e._v("[C2070 2020] Designing Network Design Spaces")]),e._v(" "),n("li",[e._v("[C25389 2019] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks")]),e._v(" "),n("li",[e._v("[C1422 2018] A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay")])]),e._v(" "),n("p",[n("strong",[e._v("New Design")])]),e._v(" "),n("ol",[n("li",[e._v("[1719 2022] Mamba: Linear-Time Sequence Modeling with Selective State Spaces")]),e._v(" "),n("li",[e._v("[29 2024] Demystify Mamba in Vision: A Linear Attention Perspective"),n("br"),e._v("\nThis paper explains that "),n("strong",[e._v("Mamba and linear attention Transformer can be formulated within a unified framework")]),e._v(".")])]),e._v(" "),n("p",[n("strong",[e._v("Old Gold Time of Transformer")])]),e._v(" "),n("ol",[n("li",[e._v("[C4622 2019] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context")]),e._v(" "),n("li",[e._v("[C10646 XLNet 2020] XLNet: Generalized Autoregressive Pretraining for Language Understanding")])]),e._v(" "),n("p",[n("strong",[e._v("AI Agent")])]),e._v(" "),n("ol",[n("li",[e._v("[C147 2023] MemoryBank: Enhancing Large Language Models with Long-Term Memory")]),e._v(" "),n("li",[e._v("[C22 2023] Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges")]),e._v(" "),n("li",[e._v("[C2051 2022] ReAct: Synergizing Reasoning and Acting in Language Models")]),e._v(" "),n("li",[e._v("[C220 2023] OpenAGI: When LLM Meets Domain Experts")])]),e._v(" "),n("hr"),e._v(" "),n("h2",{attrs:{id:"attention"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#attention"}},[e._v("#")]),e._v(" Attention")]),e._v(" "),n("h3",{attrs:{id:"_1-transformers-are-rnns-fast-autoregressive-transformers-with-linear-attention"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_1-transformers-are-rnns-fast-autoregressive-transformers-with-linear-attention"}},[e._v("#")]),e._v(" 1. Transformers are RNNs:Fast Autoregressive Transformers with Linear Attention üëç üëç")]),e._v(" "),n("h3",{attrs:{id:"_2-efficient-attention-attention-with-linear-complexities"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2-efficient-attention-attention-with-linear-complexities"}},[e._v("#")]),e._v(" 2. Efficient Attention: Attention With Linear Complexities")]),e._v(" "),n("p",[e._v("Also another paper:\nGaussian‚ÄëLinearized Transformer with Tranquilized Time‚ÄëSeries Decomposition Methods for Fault Diagnosis and Forecasting of Methane Gas Sensor Arrays.")]),e._v(" "),n("p",[e._v("These two paper are almost the same.\nBasic idea:\nclassic calculation of attention:")]),e._v(" "),n("img",{attrs:{src:"https://github.com/user-attachments/assets/70208eed-9009-4677-a017-57a14773dfb8",width:"40%"}}),e._v(" "),n("p",[e._v("Instead of calculating QK, it calculates KV.")]),e._v(" "),n("img",{attrs:{src:"https://github.com/user-attachments/assets/9a6ec80b-8119-47fc-8198-851803dcff6d",width:"70%"}}),e._v(" "),n("img",{attrs:{src:"https://github.com/user-attachments/assets/f81039ea-43fd-43c5-a5d7-6b9787459a1b",width:"70%"}}),e._v(" "),n("p",[e._v("Figure from: Gaussian‚ÄëLinearized Transformer")]),e._v(" "),n("p",[e._v("Class calculation complexity:")]),e._v(" "),n("img",{attrs:{src:"https://github.com/user-attachments/assets/5902c7ec-ac3d-4bbd-92bb-fa95c82208f5",width:"50%"}}),e._v(" "),n("p",[e._v("Provided the model dimension is << sequence length,")]),e._v(" "),n("p",[e._v("compute complexity in linear attention:")]),e._v(" "),n("img",{attrs:{src:"https://github.com/user-attachments/assets/134b7825-1f53-4811-b101-25b6ce251316",width:"50%"}}),e._v(" "),n("p",[n("a",{attrs:{href:"https://towardsdatascience.com/linear-attention-is-all-you-need-5fa9c845c1b5",target:"_blank",rel:"noopener noreferrer"}},[e._v("Figure Source"),n("OutboundLink")],1)]),e._v(" "),n("img",{attrs:{src:"https://github.com/user-attachments/assets/e245f311-52fa-47ec-b183-2fd986a1001e",width:"50%"}}),e._v(" "),n("p",[e._v("Source:Demystify Mamba in Vision: A Linear Attention Perspective")]),e._v(" "),n("img",{attrs:{src:"https://github.com/user-attachments/assets/a2b0c73e-4d90-4e4f-954c-ccf83f8fe2ac",width:"50%"}}),e._v(" "),n("p",[n("a",{attrs:{href:"https://sandeshgh.com/post/transformers_rnn/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Figure Source"),n("OutboundLink")],1)]),e._v(" "),n("p",[e._v("Provided the model dimension is << sequence length, linear complexity is much lower compute complexity compared with classic attention.")]),e._v(" "),n("p",[e._v("Please notice that in  the KV result. The intermediate result is dmodel * dmodel, instead of N * N.\nN is context length and dmodel is model dimension.")]),e._v(" "),n("hr"),e._v(" "),n("h2",{attrs:{id:"is-all-you-need"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#is-all-you-need"}},[e._v("#")]),e._v(" Is All You Need")]),e._v(" "),n("h3",{attrs:{id:"tensor-product-attention-is-all-you-need"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#tensor-product-attention-is-all-you-need"}},[e._v("#")]),e._v(" Tensor Product Attention Is All You Need")]),e._v(" "),n("p",[e._v("Idea: factorizes Q, K, and V activations using contextual tensor-decompositions")]),e._v(" "),n("img",{attrs:{src:"https://github.com/user-attachments/assets/be3c6aaf-d33f-42d7-8f15-4b57a8fa9b15",width:"70%"}}),e._v(" "),n("img",{attrs:{src:"https://github.com/user-attachments/assets/0c345baa-5481-43f8-9f07-9d88dd9f7644",width:"70%"}}),e._v(" "),n("p",[e._v("Instead of caching Q,K,V, it could only cache A,B of Q and A,B of K.\nThis matrix factorization might be similar to LORA, using multiplication of two matrices A and B to represent Q.")]),e._v(" "),n("p",[e._v("By caching A,B, it reduces large memory required by KV cache.")]),e._v(" "),n("img",{attrs:{src:"https://github.com/user-attachments/assets/b64ad8ac-bbba-409d-919d-b438c1859b5a",width:"70%"}}),e._v(" "),n("h2",{attrs:{id:"interesting"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#interesting"}},[e._v("#")]),e._v(" Interesting")]),e._v(" "),n("h3",{attrs:{id:"_21-c21-2022-moefication-transformer-feed-forward-layers-are-mixtures-of-experts"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_21-c21-2022-moefication-transformer-feed-forward-layers-are-mixtures-of-experts"}},[e._v("#")]),e._v(" 21. [C21 2022] MoEfication: Transformer Feed-forward Layers are Mixtures of Experts")]),e._v(" "),n("p",[n("img",{attrs:{src:"https://github.com/user-attachments/assets/a809c124-b2f3-4b94-8db4-61731a793873",alt:"image"}}),n("br"),e._v(" "),n("em",[e._v("Source from the paper")])]),e._v(" "),n("h3",{attrs:{id:"_28-c10395-0221-gpt-understands-too"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_28-c10395-0221-gpt-understands-too"}},[e._v("#")]),e._v(" 28. [C10395 0221] GPT Understands,Too")]),e._v(" "),n("p",[e._v("Straightforward Idea."),n("br"),e._v("\nChanging a single word in the prompt might result in substantial performance drop."),n("br"),e._v("\nP-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts."),n("br"),e._v("\nEmpirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE."),n("br"),e._v("\nP-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.")]),e._v(" "),n("p",[n("img",{attrs:{src:"https://github.com/user-attachments/assets/0ad0e6ae-afc8-4c8f-9905-a1650f28a751",alt:"image"}})]),e._v(" "),n("h2",{attrs:{id:"old-gold-time-of-transformer"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#old-gold-time-of-transformer"}},[e._v("#")]),e._v(" Old Gold Time of Transformer")]),e._v(" "),n("h3",{attrs:{id:"_1-tansformer-xl"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_1-tansformer-xl"}},[e._v("#")]),e._v(" 1. tansformer-xl")]),e._v(" "),n("h4",{attrs:{id:"segment-level-recurrence-mechanism"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#segment-level-recurrence-mechanism"}},[e._v("#")]),e._v(" Segment-Level Recurrence Mechanism")]),e._v(" "),n("ul",[n("li",[e._v("Extends context length by reusing hidden states (memory) across segments.")]),e._v(" "),n("li",[e._v("Instead of processing fixed-length context windows independently, Transformer-XL introduces a "),n("strong",[e._v("memory module")]),e._v(":\n"),n("ul",[n("li",[n("p",[e._v("Memory at segment "),n("code",[e._v("t")]),e._v(" is represented as:")])]),e._v(" "),n("li",[n("p",[e._v("This allows the model to attend to representations from previous segments.")])])])]),e._v(" "),n("li",[e._v("Enables modeling of long-term dependencies without recomputation of overlapping segments.")])]),e._v(" "),n("h4",{attrs:{id:"relative-positional-encoding-rpe"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#relative-positional-encoding-rpe"}},[e._v("#")]),e._v(" Relative Positional Encoding (RPE):")]),e._v(" "),n("ul",[n("li",[e._v("Introduces position encodings relative to each query-key pair, improving generalization across sequence lengths.")]),e._v(" "),n("li",[e._v("The positional bias is calculated as:")])]),e._v(" "),n("p",[n("img",{attrs:{src:"https://github.com/user-attachments/assets/8d74f888-ef26-40d5-b12f-52300f54b251",alt:"image"}})]),e._v(" "),n("h4",{attrs:{id:"segment-level-attention"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#segment-level-attention"}},[e._v("#")]),e._v(" Segment-Level Attention:")]),e._v(" "),n("ul",[n("li",[e._v("The attention mechanism incorporates both the current segment and memory from previous segments:\n"),n("img",{attrs:{src:"https://github.com/user-attachments/assets/c11b0860-6b85-4c9f-b16c-7426c97b3605",alt:"image"}})])]),e._v(" "),n("h2",{attrs:{id:"response-length-prediction"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#response-length-prediction"}},[e._v("#")]),e._v(" Response Length Prediction")]),e._v(" "),n("h3",{attrs:{id:"_18-c59-2024-response-length-perception-and-sequence-scheduling-an-llm-empowered-llm-inference-pipeline"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_18-c59-2024-response-length-perception-and-sequence-scheduling-an-llm-empowered-llm-inference-pipeline"}},[e._v("#")]),e._v(" 18. [C59 2024] Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline")]),e._v(" "),n("p",[e._v("They finetune the model to get accurate length prediction.")]),e._v(" "),n("p",[e._v("The benefit is that they could group the request into batches, so requests could be grouped with similar length.")]),e._v(" "),n("p",[n("img",{attrs:{src:"https://github.com/user-attachments/assets/6795f63c-4a37-499d-8037-6a2f5977232f",alt:"image"}})])])}),[],!1,null,null,null);n.default=a.exports}}]);