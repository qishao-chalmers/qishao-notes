(window.webpackJsonp=window.webpackJsonp||[]).push([[31],{492:function(e,t,n){"use strict";n.r(t);var r=n(8),a=Object(r.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("hr"),e._v(" "),t("h2",{attrs:{id:"_0-feel-the-flow-of-tvm-compilation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_0-feel-the-flow-of-tvm-compilation"}},[e._v("#")]),e._v(" 0. Feel the flow of TVM compilation")]),e._v(" "),t("p",[t("strong",[e._v("Model Definition")])]),e._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('import tvm\nfrom tvm import relay, te\nimport numpy as np\n\n# Model parameters\nbatch_size, input_dim, output_dim = 32, 128, 64\n\n# Relay model\nx = relay.var("x", shape=(batch_size, input_dim), dtype="float32")\nw = relay.var("w", shape=(input_dim, output_dim), dtype="float32")\ny = relay.nn.dense(x, w)\nmodel = relay.Function([x, w], y)\n\n# Input data\nx_data = np.random.rand(batch_size, input_dim).astype("float32")\nw_data = np.random.rand(input_dim, output_dim).astype("float32")\nparams = {"w": w_data}\n')])]),e._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[e._v("1")]),t("br"),t("span",{staticClass:"line-number"},[e._v("2")]),t("br"),t("span",{staticClass:"line-number"},[e._v("3")]),t("br"),t("span",{staticClass:"line-number"},[e._v("4")]),t("br"),t("span",{staticClass:"line-number"},[e._v("5")]),t("br"),t("span",{staticClass:"line-number"},[e._v("6")]),t("br"),t("span",{staticClass:"line-number"},[e._v("7")]),t("br"),t("span",{staticClass:"line-number"},[e._v("8")]),t("br"),t("span",{staticClass:"line-number"},[e._v("9")]),t("br"),t("span",{staticClass:"line-number"},[e._v("10")]),t("br"),t("span",{staticClass:"line-number"},[e._v("11")]),t("br"),t("span",{staticClass:"line-number"},[e._v("12")]),t("br"),t("span",{staticClass:"line-number"},[e._v("13")]),t("br"),t("span",{staticClass:"line-number"},[e._v("14")]),t("br"),t("span",{staticClass:"line-number"},[e._v("15")]),t("br"),t("span",{staticClass:"line-number"},[e._v("16")]),t("br"),t("span",{staticClass:"line-number"},[e._v("17")]),t("br")])]),t("p",[t("strong",[e._v("Relay IR")]),e._v("\nThe relay.Function represents the high-level computational graph.")]),e._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("print(model)\n\n# Simplified Relay IR:\n# fn (%x: Tensor[(32, 128), float32], %w: Tensor[(128, 64), float32]) {\n#   nn.dense(%x, %w)\n# }\n\n")])]),e._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[e._v("1")]),t("br"),t("span",{staticClass:"line-number"},[e._v("2")]),t("br"),t("span",{staticClass:"line-number"},[e._v("3")]),t("br"),t("span",{staticClass:"line-number"},[e._v("4")]),t("br"),t("span",{staticClass:"line-number"},[e._v("5")]),t("br"),t("span",{staticClass:"line-number"},[e._v("6")]),t("br"),t("span",{staticClass:"line-number"},[e._v("7")]),t("br")])]),t("p",[t("strong",[e._v("Lowering to Tensor Expression (TE)")])]),e._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('with tvm.transform.PassContext(opt_level=3):\n    mod, params = relay.build_module.bind_params_by_name(model, params)\n    graph, lib, params = relay.build(mod, target="cuda", params=params)\n')])]),e._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[e._v("1")]),t("br"),t("span",{staticClass:"line-number"},[e._v("2")]),t("br"),t("span",{staticClass:"line-number"},[e._v("3")]),t("br")])]),t("p",[e._v("In Tensor Expression (TE), computations are represented using tensor operations:")]),e._v(" "),t("ul",[t("li",[e._v("Compute: C[i, j] = sum(A[i, k] * B[k, j] for k in range(input_dim))")]),e._v(" "),t("li",[e._v("Schedule: Operations like tiling, thread binding, and vectorization are applied.")])]),e._v(" "),t("p",[t("strong",[e._v("Example TE for matrix multiplication:")])]),e._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('A = te.placeholder((batch_size, input_dim), name="A")\nB = te.placeholder((input_dim, output_dim), name="B")\nk = te.reduce_axis((0, input_dim), name="k")\n\n# Compute definition\nC = te.compute(\n    (batch_size, output_dim),\n    lambda i, j: te.sum(A[i, k] * B[k, j], axis=k),\n    name="C"\n)\n\n')])]),e._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[e._v("1")]),t("br"),t("span",{staticClass:"line-number"},[e._v("2")]),t("br"),t("span",{staticClass:"line-number"},[e._v("3")]),t("br"),t("span",{staticClass:"line-number"},[e._v("4")]),t("br"),t("span",{staticClass:"line-number"},[e._v("5")]),t("br"),t("span",{staticClass:"line-number"},[e._v("6")]),t("br"),t("span",{staticClass:"line-number"},[e._v("7")]),t("br"),t("span",{staticClass:"line-number"},[e._v("8")]),t("br"),t("span",{staticClass:"line-number"},[e._v("9")]),t("br"),t("span",{staticClass:"line-number"},[e._v("10")]),t("br"),t("span",{staticClass:"line-number"},[e._v("11")]),t("br")])]),t("p",[t("strong",[e._v("TIR (Tensor IR)")])]),e._v(" "),t("p",[e._v("After applying schedules, TE is lowered to TIR. TIR is a low-level representation focusing on loops and memory hierarchy.")]),e._v(" "),t("p",[e._v("Example TIR (simplified):")]),e._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('@tvm.script.ir_module\nclass MyModule:\n    @tvm.tir.prim_func\n    def main(A: tvm.tir.Buffer[(32, 128), "float32"],\n             B: tvm.tir.Buffer[(128, 64), "float32"],\n             C: tvm.tir.Buffer[(32, 64), "float32"]):\n        for i in range(32):  # Outer loop for batch\n            for j in range(64):  # Outer loop for output_dim\n                with tvm.tir.block("C"):\n                    C[i, j] = 0.0\n                    for k in range(128):  # Reduction loop for input_dim\n                        C[i, j] += A[i, k] * B[k, j]\n\n')])]),e._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[e._v("1")]),t("br"),t("span",{staticClass:"line-number"},[e._v("2")]),t("br"),t("span",{staticClass:"line-number"},[e._v("3")]),t("br"),t("span",{staticClass:"line-number"},[e._v("4")]),t("br"),t("span",{staticClass:"line-number"},[e._v("5")]),t("br"),t("span",{staticClass:"line-number"},[e._v("6")]),t("br"),t("span",{staticClass:"line-number"},[e._v("7")]),t("br"),t("span",{staticClass:"line-number"},[e._v("8")]),t("br"),t("span",{staticClass:"line-number"},[e._v("9")]),t("br"),t("span",{staticClass:"line-number"},[e._v("10")]),t("br"),t("span",{staticClass:"line-number"},[e._v("11")]),t("br"),t("span",{staticClass:"line-number"},[e._v("12")]),t("br"),t("span",{staticClass:"line-number"},[e._v("13")]),t("br")])]),t("p",[t("strong",[e._v("CUDA Code Generation")])]),e._v(" "),t("p",[e._v("Finally, TIR is compiled into CUDA code:")]),e._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("print(lib.imported_modules[0].get_source())\n\n# Simplified CUDA Code:\n# __global__ void fused_dense(float* __restrict__ A, float* __restrict__ B, float* __restrict__ C) {\n#   int idx = threadIdx.x + blockIdx.x * blockDim.x;\n#   if (idx < 2048) {  // 32 * 64 = batch_size * output_dim\n#     int i = idx / 64;  // Batch index\n#     int j = idx % 64;  // Output index\n#     float result = 0.0;\n#     for (int k = 0; k < 128; ++k) {  // Reduction loop\n#       result += A[i * 128 + k] * B[k * 64 + j];\n#     }\n#     C[idx] = result;\n#   }\n# }\n\n")])]),e._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[e._v("1")]),t("br"),t("span",{staticClass:"line-number"},[e._v("2")]),t("br"),t("span",{staticClass:"line-number"},[e._v("3")]),t("br"),t("span",{staticClass:"line-number"},[e._v("4")]),t("br"),t("span",{staticClass:"line-number"},[e._v("5")]),t("br"),t("span",{staticClass:"line-number"},[e._v("6")]),t("br"),t("span",{staticClass:"line-number"},[e._v("7")]),t("br"),t("span",{staticClass:"line-number"},[e._v("8")]),t("br"),t("span",{staticClass:"line-number"},[e._v("9")]),t("br"),t("span",{staticClass:"line-number"},[e._v("10")]),t("br"),t("span",{staticClass:"line-number"},[e._v("11")]),t("br"),t("span",{staticClass:"line-number"},[e._v("12")]),t("br"),t("span",{staticClass:"line-number"},[e._v("13")]),t("br"),t("span",{staticClass:"line-number"},[e._v("14")]),t("br"),t("span",{staticClass:"line-number"},[e._v("15")]),t("br"),t("span",{staticClass:"line-number"},[e._v("16")]),t("br")])]),t("p",[t("strong",[e._v("Summary of Intermediate Representations")])]),e._v(" "),t("ol",[t("li",[e._v("Relay IR: High-level computational graph, defines operators like nn.dense.")]),e._v(" "),t("li",[e._v("TE: Abstracts computation using mathematical tensor operations and supports scheduling primitives.")]),e._v(" "),t("li",[e._v("TIR: Low-level, loop-based representation with explicit memory hierarchy.")]),e._v(" "),t("li",[e._v("CUDA Code: GPU kernel for matrix multiplication, including thread and block mappings.")])]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_0-1-model-parsing-and-relay-ir-construction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_0-1-model-parsing-and-relay-ir-construction"}},[e._v("#")]),e._v(" 0.1 Model Parsing and Relay IR Construction")]),e._v(" "),t("ul",[t("li",[e._v("Input\nModel in high-level frameworks like TensorFlow, PyTorch, or ONNX.")]),e._v(" "),t("li",[e._v("Process:\n"),t("ul",[t("li",[e._v("TVM parses the input model and converts it into Relay IR, a hig-h-level intermediate representation.")]),e._v(" "),t("li",[e._v("The Relay IR describes the computational graph with operator-level abstractions.")])])]),e._v(" "),t("li",[e._v("Key Functions:\nrelay.frontend.from_pytorch(), relay.frontend.from_onnx() in src/relay/frontend/.")])]),e._v(" "),t("h3",{attrs:{id:"_0-2-high-level-optimizations-in-relay"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_0-2-high-level-optimizations-in-relay"}},[e._v("#")]),e._v(" 0.2 High-Level Optimizations in Relay")]),e._v(" "),t("ul",[t("li",[e._v("Input: Relay IR.")]),e._v(" "),t("li",[e._v("Process:\n"),t("ul",[t("li",[e._v("Optimize the Relay IR for performance and hardware compatibility through:")]),e._v(" "),t("li",[e._v("Operator Fusion: Fuse adjacent operations.")]),e._v(" "),t("li",[e._v("Constant Folding: Precompute static expressions.")]),e._v(" "),t("li",[e._v("Layout Transformation: Adjust data layouts (e.g., NCHW → NCHWc).")]),e._v(" "),t("li",[e._v("Quantization: Lower precision where applicable.")]),e._v(" "),t("li",[e._v("Common Subexpression Elimination.\nFinalize the optimized Relay graph.")])])]),e._v(" "),t("li",[e._v("Key Functions:"),t("br"),e._v("\nsrc/relay/transforms/ for passes like fuse_ops.cc, alter_op_layout.cc.")])]),e._v(" "),t("h3",{attrs:{id:"_0-3-lowering-to-tensor-expression-te"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_0-3-lowering-to-tensor-expression-te"}},[e._v("#")]),e._v(" 0.3. Lowering to Tensor Expression (TE)")]),e._v(" "),t("ul",[t("li",[e._v("Input: Optimized Relay IR.")]),e._v(" "),t("li",[e._v("Process:\n"),t("ul",[t("li",[e._v("Translate high-level Relay operators into Tensor Expressions (TE).")]),e._v(" "),t("li",[e._v("TE represents computations as mathematical tensor operations and allows for:\n"),t("ul",[t("li",[e._v("Abstraction of computation patterns (e.g., matrix multiplication).")]),e._v(" "),t("li",[e._v("Introduction of scheduling primitives (e.g., tiling, unrolling, vectorization).")])])])])]),e._v(" "),t("li",[e._v("Key Functions:\n"),t("ul",[t("li",[e._v("src/relay/backend/te_compiler.cc: Bridges Relay IR and TE.")]),e._v(" "),t("li",[e._v("src/te/tensor.cc: Constructs tensor expressions.")])])])]),e._v(" "),t("h3",{attrs:{id:"_0-4-scheduling-in-te"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_0-4-scheduling-in-te"}},[e._v("#")]),e._v(" 0.4. Scheduling in TE")]),e._v(" "),t("ul",[t("li",[e._v("Input: Tensor Expressions.")]),e._v(" "),t("li",[e._v("Process:\n"),t("ul",[t("li",[e._v("Apply scheduling primitives to improve performance:")]),e._v(" "),t("li",[e._v("Tiling: Divide tensors into smaller chunks for parallelism.")]),e._v(" "),t("li",[e._v("Unrolling: Optimize loops for instruction pipelining.")]),e._v(" "),t("li",[e._v("Thread/Block Mapping: Map computations to GPU threads and blocks.")]),e._v(" "),t("li",[e._v("Vectorization: Use SIMD instructions where applicable.")]),e._v(" "),t("li",[e._v("Refines Tensor Expressions into Tensor Intermediate Representation (TIR).")])])]),e._v(" "),t("li",[e._v("Key Functions:\n"),t("ul",[t("li",[e._v("src/te/schedule/ for scheduling functions.")]),e._v(" "),t("li",[e._v("src/te/schedule/schedule_dataflow_rewrite.cc: Handles dataflow rewrite scheduling.")])])])]),e._v(" "),t("h3",{attrs:{id:"_0-5-lowering-to-tir"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_0-5-lowering-to-tir"}},[e._v("#")]),e._v(" 0.5. Lowering to TIR")]),e._v(" "),t("ul",[t("li",[e._v("Input: Tensor Expressions with schedules.")]),e._v(" "),t("li",[e._v("Process:\n"),t("ul",[t("li",[e._v("Convert TE into Tensor IR (TIR), a low-level IR closer to device execution.")]),e._v(" "),t("li",[e._v("Perform device-specific optimizations for CUDA (e.g., thread hierarchy mapping).")])])]),e._v(" "),t("li",[e._v("Key Functions:\n"),t("ul",[t("li",[e._v("src/tir/transform/ for device-specific passes like loop unrolling and thread binding.")])])])]),e._v(" "),t("h3",{attrs:{id:"_0-6-code-generation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_0-6-code-generation"}},[e._v("#")]),e._v(" 0.6. Code Generation")]),e._v(" "),t("ul",[t("li",[e._v("Input: TIR optimized for CUDA.")]),e._v(" "),t("li",[e._v("Process:\n"),t("ul",[t("li",[e._v("Code Generation:Translate TIR into CUDA kernels. Use TVM's built-in CUDA code generator.")]),e._v(" "),t("li",[e._v("Calling cuBLAS/cuDNN or CUTLASS:\n"),t("ul",[t("li",[e._v("For specific operations (e.g., GEMM), call external libraries.")]),e._v(" "),t("li",[e._v("Determine the sequence of library calls and parameters based on operator attributes.")])])]),e._v(" "),t("li",[e._v("Memory Allocation: Analyze dataflow to allocate memory efficiently on GPU.")])])]),e._v(" "),t("li",[e._v("Key Functions:\n"),t("ul",[t("li",[e._v("CUDA Codegen:\nsrc/target/source/codegen_cuda.cc: Generates CUDA source code.")]),e._v(" "),t("li",[e._v("External Libraries:\nsrc/runtime/contrib/cublas.cc: Integrates with cuBLAS.\nsrc/runtime/contrib/cudnn.cc: Integrates with cuDNN.\nsrc/contrib/cutlass/: Integrates with CUTLASS.")])])])]),e._v(" "),t("h3",{attrs:{id:"_0-7-final-compilation-and-deployment"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_0-7-final-compilation-and-deployment"}},[e._v("#")]),e._v(" 0.7. Final Compilation and Deployment")]),e._v(" "),t("ul",[t("li",[e._v("Input: CUDA source code.")]),e._v(" "),t("li",[e._v("Process:\n"),t("ul",[t("li",[e._v("Compile CUDA source code using NVCC or the TVM runtime.")]),e._v(" "),t("li",[e._v("Deploy the compiled kernel and runtime modules.")])])]),e._v(" "),t("li",[e._v("Key Functions:\n"),t("ul",[t("li",[e._v("src/target/source/: Handles code generation.")]),e._v(" "),t("li",[e._v("src/runtime/: Manages runtime execution and deployment.")])])])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_1-model-parsing-and-relay-ir-construction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-model-parsing-and-relay-ir-construction"}},[e._v("#")]),e._v(" 1. Model Parsing and Relay IR Construction")]),e._v(" "),t("p",[e._v("In TVM, high-level optimization in the Relay IR phase includes several graph-level optimizations, data layout transformations, and other functional passes.")]),e._v(" "),t("p",[e._v("These optimizations are implemented in various source files under src/relay/transform and src/relay/op directories.")]),e._v(" "),t("p",[e._v("Below is a categorized list of these optimizations along with their corresponding source code files and functions:")]),e._v(" "),t("h3",{attrs:{id:"_1-1-graph-level-optimizations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-graph-level-optimizations"}},[e._v("#")]),e._v(" 1.1 Graph-Level Optimizations")]),e._v(" "),t("p",[e._v("Graph-level optimizations restructure or simplify the computation graph for better performance.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Optimization\tSource")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Constant Folding")]),e._v(" "),t("td",[e._v("src/relay/transform/fold_constant.cc")]),e._v(" "),t("td",[e._v("FoldConstant, ConstantFolder")])]),e._v(" "),t("tr",[t("td",[e._v("Operator Fusion")]),e._v(" "),t("td",[e._v("src/relay/transform/fuse_ops.cc")]),e._v(" "),t("td",[e._v("FuseOps, FuseMutator, PatternMatcher")])]),e._v(" "),t("tr",[t("td",[e._v("Dead Code Elimination (DCE)")]),e._v(" "),t("td",[e._v("src/relay/transform/eliminate_common_subexpr.cc")]),e._v(" "),t("td",[e._v("EliminateCommonSubexpr")])]),e._v(" "),t("tr",[t("td",[e._v("Common Subexpression Elimination")]),e._v(" "),t("td",[e._v("src/relay/transform/eliminate_common_subexpr.cc")]),e._v(" "),t("td",[e._v("EliminateCommonSubexpr")])]),e._v(" "),t("tr",[t("td",[e._v("Simplify Inference")]),e._v(" "),t("td",[e._v("src/relay/transform/simplify_inference.cc")]),e._v(" "),t("td",[e._v("SimplifyInference, SimplifyInferenceMutator")])]),e._v(" "),t("tr",[t("td",[e._v("Call Folding")]),e._v(" "),t("td",[e._v("src/relay/transform/fold_call.cc")]),e._v(" "),t("td",[e._v("FoldCall")])]),e._v(" "),t("tr",[t("td",[e._v("Inline Functions")]),e._v(" "),t("td",[e._v("src/relay/transform/inline.cc")]),e._v(" "),t("td",[e._v("Inline, InlineMutator")])]),e._v(" "),t("tr",[t("td",[e._v("Prune Unused Functions")]),e._v(" "),t("td",[e._v("src/relay/transform/prune_unused_functions.cc")]),e._v(" "),t("td",[e._v("PruneUnusedFunctions")])])])]),e._v(" "),t("h3",{attrs:{id:"_1-2-data-layout-transformations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-data-layout-transformations"}},[e._v("#")]),e._v(" 1.2 Data Layout Transformations")]),e._v(" "),t("p",[e._v("These optimizations adjust the layout of tensors for better memory access patterns and compatibility with target hardware.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Transformation")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Alter Layout")]),e._v(" "),t("td",[e._v("src/relay/transform/alter_op_layout.cc")]),e._v(" "),t("td",[e._v("AlterOpLayout, AlterOpLayoutRewriter")])]),e._v(" "),t("tr",[t("td",[e._v("Convert Layout")]),e._v(" "),t("td",[e._v("s\tsrc/relay/transform/convert_layout.cc")]),e._v(" "),t("td",[e._v("ConvertLayout")])]),e._v(" "),t("tr",[t("td",[e._v("Fold Scale Axis")]),e._v(" "),t("td",[e._v("src/relay/transform/fold_scale_axis.cc")]),e._v(" "),t("td",[e._v("FoldScaleAxis, ScaleAxisSimplifier")])]),e._v(" "),t("tr",[t("td",[e._v("Layout Optimization")]),e._v(" "),t("td",[e._v("src/relay/transform/layout_rewrite.cc")]),e._v(" "),t("td",[e._v("LayoutRewrite")])])])]),e._v(" "),t("h3",{attrs:{id:"_1-3-quantization-and-precision-management"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-quantization-and-precision-management"}},[e._v("#")]),e._v(" 1.3 Quantization and Precision Management")]),e._v(" "),t("p",[e._v("TVM supports quantization optimizations for reduced precision operations.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Optimization")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Quantize")]),e._v(" "),t("td",[e._v("src/relay/quantize/quantize.cc")]),e._v(" "),t("td",[e._v("Quantize, CreateQuantizePass")])]),e._v(" "),t("tr",[t("td",[e._v("Dequantize")]),e._v(" "),t("td",[e._v("src/relay/quantize/dequantize.cc")]),e._v(" "),t("td",[e._v("Dequantize")])]),e._v(" "),t("tr",[t("td",[e._v("SimplifyQuantize")]),e._v(" "),t("td",[e._v("src/relay/transform/simplify_quantize.cc")]),e._v(" "),t("td",[e._v("SimplifyQuantize, SimplifyQuantizeRewriter")])])])]),e._v(" "),t("h3",{attrs:{id:"_1-4-automatic-differentiation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-4-automatic-differentiation"}},[e._v("#")]),e._v(" 1.4 Automatic Differentiation")]),e._v(" "),t("p",[e._v("TVM includes an autodiff system for neural networks.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Transformation")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Reverse Mode Autodiff")]),e._v(" "),t("td",[e._v("src/relay/transforms/gradient.cc")]),e._v(" "),t("td",[e._v("AutomaticDifferentiation, ReverseAD")])])])]),e._v(" "),t("h3",{attrs:{id:"_1-5-high-level-hardware-aware-optimizations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-5-high-level-hardware-aware-optimizations"}},[e._v("#")]),e._v(" 1.5 High-Level Hardware-Aware Optimizations")]),e._v(" "),t("p",[e._v("These optimizations modify operations based on the target hardware.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Optimization")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Annotate Target")]),e._v(" "),t("td",[e._v("src/relay/transform/annotate_target.cc")]),e._v(" "),t("td",[e._v("AnnotateTarget")])]),e._v(" "),t("tr",[t("td",[e._v("Partition Graph")]),e._v(" "),t("td",[e._v("src/relay/transform/partition_graph.cc")]),e._v(" "),t("td",[e._v("PartitionGraph")])]),e._v(" "),t("tr",[t("td",[e._v("Merge Compiler Regions")]),e._v(" "),t("td",[e._v("src/relay/transform/merge_compiler_regions.cc")]),e._v(" "),t("td",[e._v("MergeCompilerRegions")])])])]),e._v(" "),t("h3",{attrs:{id:"_1-6-device-placement"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-6-device-placement"}},[e._v("#")]),e._v(" 1.6 Device Placement")]),e._v(" "),t("p",[e._v("These passes assign operations to devices for heterogeneous execution.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Transformation")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Rewrite Annotated Ops")]),e._v(" "),t("td",[e._v("src/relay/transform/rewrite_annotated_ops.cc")]),e._v(" "),t("td",[e._v("RewriteAnnotatedOps")])]),e._v(" "),t("tr",[t("td",[e._v("Device Annotation")]),e._v(" "),t("td",[e._v("src/relay/transform/device_annotation.cc")]),e._v(" "),t("td",[e._v("DeviceAnnotation")])])])]),e._v(" "),t("h3",{attrs:{id:"_1-7-meta-pass-management"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-7-meta-pass-management"}},[e._v("#")]),e._v(" 1.7 Meta-Pass Management")]),e._v(" "),t("p",[e._v("Relay provides a meta-pass system to manage and sequence passes.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Meta-Pass")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Sequential Pass Manager")]),e._v(" "),t("td",[e._v("src/relay/transform/sequential.cc")]),e._v(" "),t("td",[e._v("Sequential, PassManager")])]),e._v(" "),t("tr",[t("td",[e._v("Pass Context")]),e._v(" "),t("td",[e._v("src/relay/transform/pass.cc")]),e._v(" "),t("td",[e._v("PassContext, WithPassContext")])])])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir"}},[e._v("#")]),e._v(" 2 Lowering to Tensor Expression (TE), Scheduing in TE and Lowering into TIR")]),e._v(" "),t("p",[e._v("The lowering process from Relay IR to Tensor Expression (TE) and Tensor IR (TIR) in TVM involves multiple phases.")]),e._v(" "),t("p",[e._v("These include converting Relay IR to TE, applying tensor computation abstractions, performing scheduling, and constructing device-specific low-level TIR.")]),e._v(" "),t("p",[e._v("Here’s a detailed breakdown of the corresponding TVM source code files and functions for these stages:")]),e._v(" "),t("h3",{attrs:{id:"_2-1-converting-relay-ir-to-tensor-expression-te"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-converting-relay-ir-to-tensor-expression-te"}},[e._v("#")]),e._v(" 2.1 Converting Relay IR to Tensor Expression (TE)")]),e._v(" "),t("p",[e._v("This phase converts high-level Relay IR into the computation abstractions provided by TE.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Process")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Relay to TE Lowering")]),e._v(" "),t("td",[e._v("src/relay/backend/te_compiler.cc")]),e._v(" "),t("td",[e._v("LowerToTE, CreateSchedule, ScheduleGetter")])]),e._v(" "),t("tr",[t("td",[e._v("Operator Strategy")]),e._v(" "),t("td",[e._v("src/relay/op/strategy/generic.cc")]),e._v(" "),t("td",[e._v("GenericFunc, OpStrategy")])]),e._v(" "),t("tr",[t("td",[e._v("Relay to TE Bridge")]),e._v(" "),t("td",[e._v("src/relay/backend/te_compiler_cache.cc")]),e._v(" "),t("td",[e._v("TECompiler, LowerTE")])]),e._v(" "),t("tr",[t("td",[e._v("Shape Function Lowering")]),e._v(" "),t("td",[e._v("src/relay/backend/te_compiler.cc")]),e._v(" "),t("td",[e._v("LowerShapeFunc")])])])]),e._v(" "),t("p",[e._v("Explanation:")]),e._v(" "),t("ul",[t("li",[e._v("The Relay IR graph is analyzed, and for each operator, TVM retrieves a corresponding TE function using OpStrategy.")]),e._v(" "),t("li",[e._v("TE functions define high-level operations like matrix multiplication, element-wise addition, etc.")])]),e._v(" "),t("h3",{attrs:{id:"_2-2-abstraction-of-computation-in-tensor-expression-te"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-abstraction-of-computation-in-tensor-expression-te"}},[e._v("#")]),e._v(" 2.2 Abstraction of Computation in Tensor Expression (TE)")]),e._v(" "),t("p",[e._v("TE provides a declarative way to express computation. This includes operations like tiling, unrolling, and vectorizing.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Process")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Tensor Expression Build")]),e._v(" "),t("td",[e._v("src/te/operation/create_primfunc.cc")]),e._v(" "),t("td",[e._v("CreatePrimFunc, ComputeBody, ScheduleOps")])]),e._v(" "),t("tr",[t("td",[e._v("Compute Definition")]),e._v(" "),t("td",[e._v("src/te/operation/compute_op.cc")]),e._v(" "),t("td",[e._v("ComputeOpNode, ComputeOp")])]),e._v(" "),t("tr",[t("td",[e._v("Tensor Compute Intrinsics")]),e._v(" "),t("td",[e._v("src/te/operation/tensorize.cc")]),e._v(" "),t("td",[e._v("Tensorize, CreateIntrinBody")])])])]),e._v(" "),t("p",[e._v("Explanation:")]),e._v(" "),t("ul",[t("li",[e._v("High-level computations are abstracted into a declarative format using ComputeOp.")]),e._v(" "),t("li",[e._v("Intrinsic support for tensorization is added for specialized hardware operations.")])]),e._v(" "),t("h3",{attrs:{id:"_2-3-scheduling-in-tensor-expression"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-scheduling-in-tensor-expression"}},[e._v("#")]),e._v(" 2.3 Scheduling in Tensor Expression")]),e._v(" "),t("p",[e._v("Scheduling is where TVM optimizes how computations are performed on the target device.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Process")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Tile, Unroll, Vectorize")]),e._v(" "),t("td",[e._v("src/te/schedule/schedule_dataflow_rewrite.cc")]),e._v(" "),t("td",[e._v("ScheduleDataFlowRewrite, Tile, Unroll, Vectorize")])]),e._v(" "),t("tr",[t("td",[e._v("Thread and Block Mapping")]),e._v(" "),t("td",[e._v("src/te/schedule/schedule_lang.cc")]),e._v(" "),t("td",[e._v("bind, split, reorder, fuse")])]),e._v(" "),t("tr",[t("td",[e._v("AutoScheduler Interface")]),e._v(" "),t("td",[e._v("src/auto_scheduler/compute_dag.cc")]),e._v(" "),t("td",[e._v("ComputeDAG, ApplySteps")])]),e._v(" "),t("tr",[t("td",[e._v("Lowering Schedule to TIR")]),e._v(" "),t("td",[e._v("src/te/schedule/graph.cc")]),e._v(" "),t("td",[e._v("ScheduleGraph, LowerSchedule")])])])]),e._v(" "),t("p",[e._v("Explanation:")]),e._v(" "),t("ul",[t("li",[e._v("This phase defines how computations should be split into smaller tiles, mapped to hardware threads and blocks, and optimized using unrolling and vectorization.")]),e._v(" "),t("li",[e._v("Tensor schedules are converted into lower-level forms through ScheduleGraph.")])]),e._v(" "),t("h3",{attrs:{id:"_2-4-constructing-low-level-tensor-ir-tir"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-constructing-low-level-tensor-ir-tir"}},[e._v("#")]),e._v(" 2.4 Constructing Low-Level Tensor IR (TIR)")]),e._v(" "),t("p",[e._v("TIR represents a low-level, device-specific IR used to generate target-specific code.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Process")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("TIR Construction")]),e._v(" "),t("td",[e._v("src/tir/stmt_functor.cc")]),e._v(" "),t("td",[e._v("StmtFunctor, VisitStmt, MakeStmt")])]),e._v(" "),t("tr",[t("td",[e._v("Lowering to TIR")]),e._v(" "),t("td",[e._v("src/tir/transforms/lower_tir.cc")]),e._v(" "),t("td",[e._v("LowerTIR, TransformTIR")])]),e._v(" "),t("tr",[t("td",[e._v("Memory Planning")]),e._v(" "),t("td",[e._v("src/tir/transforms/storage_rewrite.cc")]),e._v(" "),t("td",[e._v("StorageRewrite, PlanMemory")])]),e._v(" "),t("tr",[t("td",[e._v("Device-Specific TIR")]),e._v(" "),t("td",[e._v("src/target/codegen.cc")]),e._v(" "),t("td",[e._v("Build, BuildIRModule")])])])]),e._v(" "),t("p",[e._v("Explanation:")]),e._v(" "),t("ul",[t("li",[e._v("TE schedules are converted into TIR, which provides explicit control over memory accesses and device-specific optimizations.")]),e._v(" "),t("li",[e._v("StorageRewrite optimizes memory allocation and reuse.")])]),e._v(" "),t("h3",{attrs:{id:"_2-5-device-specific-optimizations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-device-specific-optimizations"}},[e._v("#")]),e._v(" 2.5 Device-Specific Optimizations")]),e._v(" "),t("p",[e._v("Device-specific optimizations tailor the generated code for the target platform (e.g., CUDA).")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Transformation")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Thread/Block Mapping")]),e._v(" "),t("td",[e._v("src/tir/transforms/thread_storage_sync.cc")]),e._v(" "),t("td",[e._v("ThreadStorageSync")])]),e._v(" "),t("tr",[t("td",[e._v("Loop Partitioning")]),e._v(" "),t("td",[e._v("src/tir/transforms/loop_partition.cc")]),e._v(" "),t("td",[e._v("LoopPartition")])]),e._v(" "),t("tr",[t("td",[e._v("Device Codegen")]),e._v(" "),t("td",[e._v("src/target/source/codegen_cuda.cc")]),e._v(" "),t("td",[e._v("CodeGenCUDA, PrintKernel")])])])]),e._v(" "),t("p",[e._v("High-Level Summary of the Workflow")]),e._v(" "),t("ul",[t("li",[e._v("Relay to TE:"),t("br"),e._v("\nConverts high-level operations into Tensor Expression (TE) definitions using strategies (src/relay/backend/te_compiler.cc).")]),e._v(" "),t("li",[e._v("Computation Abstraction:\nDefines computations in TE with ComputeOp (src/te/operation/compute_op.cc).")]),e._v(" "),t("li",[e._v("Scheduling:"),t("br"),e._v("\nApplies optimizations like tiling, unrolling, and mapping computations to threads/blocks (src/te/schedule/schedule_lang.cc).")]),e._v(" "),t("li",[e._v("Lowering to TIR:"),t("br"),e._v("\nTranslates the schedule into TIR, which explicitly handles device memory and control flow (src/tir/transforms/lower_tir.cc).")]),e._v(" "),t("li",[e._v("Device-Specific Codegen:"),t("br"),e._v("\nEmits target-specific code (e.g., CUDA) via CodeGenCUDA (src/target/source/codegen_cuda.cc).")])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_3-code-generation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-code-generation"}},[e._v("#")]),e._v(" 3. Code Generation")]),e._v(" "),t("h3",{attrs:{id:"_3-1-gpu-code-generation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-gpu-code-generation"}},[e._v("#")]),e._v(" 3.1 GPU Code Generation")]),e._v(" "),t("p",[e._v("This phase translates Tensor IR (TIR) into GPU-compatible low-level code, generating CUDA kernels and API calls.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Process")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("TIR to CUDA Kernel")]),e._v(" "),t("td",[e._v("src/target/source/codegen_cuda.cc")]),e._v(" "),t("td",[e._v("CodeGenCUDA, GenerateKernel, PrintStmt")])]),e._v(" "),t("tr",[t("td",[e._v("CodeGen Base Class")]),e._v(" "),t("td",[e._v("src/target/source/codegen_c.cc")]),e._v(" "),t("td",[e._v("CodeGenC, PrintExpr")])]),e._v(" "),t("tr",[t("td",[e._v("Shared Memory Handling")]),e._v(" "),t("td",[e._v("src/target/source/codegen_cuda.cc")]),e._v(" "),t("td",[e._v("PrintStorageScope, PrintStorageSync")])]),e._v(" "),t("tr",[t("td",[e._v("Thread/Block Synchronization")]),e._v(" "),t("td",[e._v("src/tir/transforms/thread_storage_sync.cc")]),e._v(" "),t("td",[e._v("ThreadStorageSync")])])])]),e._v(" "),t("p",[t("strong",[e._v("Explanation:")]),e._v("\nCodeGenCUDA translates TIR to CUDA kernels, emitting device-side code and managing constructs like thread/block mappings, shared memory, and synchronization.\nSynchronization points are inserted using PrintStorageSync.")]),e._v(" "),t("h3",{attrs:{id:"_3-2-kernel-construction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-kernel-construction"}},[e._v("#")]),e._v(" 3.2. Kernel Construction")]),e._v(" "),t("p",[e._v("Kernel construction involves creating CUDA device kernels and host-side launcher code to invoke them.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Process")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Kernel Emission")]),e._v(" "),t("td",[e._v("src/target/source/codegen_cuda.cc")]),e._v(" "),t("td",[e._v("PrintFuncBody, EmitFunction")])]),e._v(" "),t("tr",[t("td",[e._v("Kernel Launch Code")]),e._v(" "),t("td",[e._v("src/runtime/cuda/cuda_module.cc")]),e._v(" "),t("td",[e._v("CUDAWrappedFunc, LaunchKernel")])]),e._v(" "),t("tr",[t("td",[e._v("Kernel Metadata Management")]),e._v(" "),t("td",[e._v("src/runtime/module.cc")]),e._v(" "),t("td",[e._v("PackImports, ExportModule")])])])]),e._v(" "),t("p",[e._v("Explanation:\nThe EmitFunction generates kernel function declarations and definitions for execution on the GPU.\nHost-side kernel launchers are defined in cuda_module.cc.")]),e._v(" "),t("h3",{attrs:{id:"_3-3-cublas-cutlass-integration"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-cublas-cutlass-integration"}},[e._v("#")]),e._v(" 3.3. cuBLAS/CUTLASS Integration")]),e._v(" "),t("p",[e._v("When using cuBLAS or CUTLASS for tensor computations (e.g., GEMM), TVM generates calls to these libraries instead of writing explicit CUDA kernels.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Process")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("cuBLAS Integration")]),e._v(" "),t("td",[e._v("src/runtime/contrib/cublas/cublas.cc")]),e._v(" "),t("td",[e._v("CUBLASCall, InitCUBLASHandle, GemmOp")])]),e._v(" "),t("tr",[t("td",[e._v("CUTLASS Integration")]),e._v(" "),t("td",[e._v("src/contrib/cutlass/gen_cutlass_gemm.cc")]),e._v(" "),t("td",[e._v("GenerateCutlassGemm, EmitCutlassCode")])]),e._v(" "),t("tr",[t("td",[e._v("External Code Generation")]),e._v(" "),t("td",[e._v("src/relay/backend/contrib/cublas_codegen.cc")]),e._v(" "),t("td",[e._v("CUBLASFunction, CodegenCUBLAS")])])])]),e._v(" "),t("p",[e._v("Explanation:\ncublas.cc provides wrappers for cuBLAS API calls like cublasSgemm, with TVM handling data layout transformations as needed.\nCUTLASS integration uses template-based code generation in gen_cutlass_gemm.cc, emitting optimized kernels for matrix operations.")]),e._v(" "),t("h3",{attrs:{id:"_3-4-target-specific-optimizations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-target-specific-optimizations"}},[e._v("#")]),e._v(" 3.4. Target-Specific Optimizations")]),e._v(" "),t("p",[e._v("Target-specific optimizations fine-tune the generated CUDA code based on the GPU architecture and memory hierarchy.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Process")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Thread/Block Mapping")]),e._v(" "),t("td",[e._v("src/tir/transforms/thread_storage_sync.cc")]),e._v(" "),t("td",[e._v("ThreadStorageSync, OptimizeThreads")])]),e._v(" "),t("tr",[t("td",[e._v("Loop Partitioning")]),e._v(" "),t("td",[e._v("src/tir/transforms/loop_partition.cc")]),e._v(" "),t("td",[e._v("LoopPartition")])]),e._v(" "),t("tr",[t("td",[e._v("Memory Planning")]),e._v(" "),t("td",[e._v("src/tir/transforms/storage_rewrite.cc")]),e._v(" "),t("td",[e._v("StorageRewrite, PlanMemory")])]),e._v(" "),t("tr",[t("td",[e._v("Warp-Level Optimization")]),e._v(" "),t("td",[e._v("src/tir/transforms/vectorize_loop.cc")]),e._v(" "),t("td",[e._v("VectorizeLoop, Vectorizer")])])])]),e._v(" "),t("p",[e._v("Explanation:\nThread and block mapping ensures optimal utilization of GPU threads and memory.\nLoop partitioning and vectorization optimize data access patterns for warp-level efficiency.\nStorageRewrite minimizes memory usage by analyzing reuse patterns and adjusting allocation.")]),e._v(" "),t("h3",{attrs:{id:"_3-5-memory-management"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-memory-management"}},[e._v("#")]),e._v(" 3.5. Memory Management")]),e._v(" "),t("p",[e._v("Efficient memory management involves optimizing shared/global memory usage and enabling memory reuse.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Process")]),e._v(" "),t("th",[e._v("File")]),e._v(" "),t("th",[e._v("Key Functions/Classes")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Shared Memory Usage")]),e._v(" "),t("td",[e._v("src/target/source/codegen_cuda.cc")]),e._v(" "),t("td",[e._v("PrintStorageScope, EmitSharedMemory")])]),e._v(" "),t("tr",[t("td",[e._v("Memory Allocation")]),e._v(" "),t("td",[e._v("src/tir/transforms/storage_rewrite.cc")]),e._v(" "),t("td",[e._v("PlanMemory, ReuseMemory")])]),e._v(" "),t("tr",[t("td",[e._v("Memory Alignment")]),e._v(" "),t("td",[e._v("src/target/source/codegen_cuda.cc")]),e._v(" "),t("td",[e._v("PrintStorageAlloc")])])])]),e._v(" "),t("p",[e._v("Explanation:\nShared memory scopes are explicitly emitted during CUDA codegen (EmitSharedMemory).\nPlanMemory optimizes allocation to minimize fragmentation and overhead.")]),e._v(" "),t("h3",{attrs:{id:"_3-6-overall-codegen-workflow"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-6-overall-codegen-workflow"}},[e._v("#")]),e._v(" 3.6. Overall Codegen Workflow")]),e._v(" "),t("p",[e._v("Key Stages and Their Files")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("TIR Lowering:"),t("br"),e._v("\nFile: src/tir/transforms/lower_tir.cc"),t("br"),e._v("\nFunction: LowerTIR, TransformTIR")])]),e._v(" "),t("li",[t("p",[e._v("CUDA Kernel Emission:"),t("br"),e._v("\nFile: src/target/source/codegen_cuda.cc"),t("br"),e._v("\nFunction: EmitFunction, GenerateKernel")])]),e._v(" "),t("li",[t("p",[e._v("cuBLAS Integration:"),t("br"),e._v("\nFile: src/runtime/contrib/cublas/cublas.cc"),t("br"),e._v("\nFunction: CUBLASCall, InitCUBLASHandle")])]),e._v(" "),t("li",[t("p",[e._v("CUTLASS Integration:"),t("br"),e._v("\nFile: src/contrib/cutlass/gen_cutlass_gemm.cc"),t("br"),e._v("\nFunction: GenerateCutlassGemm, EmitCutlassCode")])]),e._v(" "),t("li",[t("p",[e._v("Target-Specific Optimizations:"),t("br"),e._v("\nFile: src/tir/transforms/thread_storage_sync.cc"),t("br"),e._v("\nFunction: ThreadStorageSync, OptimizeThreads")])])])])}),[],!1,null,null,null);t.default=a.exports}}]);