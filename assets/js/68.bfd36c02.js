(window.webpackJsonp=window.webpackJsonp||[]).push([[68],{522:function(e,t,a){"use strict";a.r(t);var s=a(9),i=Object(s.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("[3] Static Cost Estimation for Data Layout Selection on GPUs")]),e._v(" "),t("li",[e._v("[118] Managing DRAM Latency Divergence in Irregular GPGPU Applications")])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_1-static-cost-estimation-for-data-layout-selection-on-gpus"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-static-cost-estimation-for-data-layout-selection-on-gpus"}},[e._v("#")]),e._v(" 1. Static Cost Estimation for Data Layout Selection on GPUs")]),e._v(" "),t("p",[e._v("on the CPU, using Array-of-Struct (AoS) would be more efficient in general, because using AoS would encourage spatial locality. "),t("br"),e._v("\nin old GPU models before the L1 and L2 cache were introduced, Struct-of-Array (SoA) would usually be preferred since it would achieve better coalescing of memory access."),t("br"),e._v("\nThe GPU L1 and L2 cache brought back the opportunity of cache reuse, which means merging fields stored in discrete arrays in SoA into a single AoS might lead to better performance.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/e66b1b8b-4b92-4825-8de2-0f1b2d693a73",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"background"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#background"}},[e._v("#")]),e._v(" Background")]),e._v(" "),t("p",[e._v("Accessing one field will also bring the adjacent fields into the cache."),t("br"),e._v("\nOn the other hand, SoA may not utilize the cache well, as accessing x[i] will not result in loading y[i] into the cache since x[i] and y[i] are unlikely to be in the same cache line.")]),e._v(" "),t("p",[e._v("on the GPU, using SoA will be more likely to result in more coalesced memory accesses, all threads in the same warp are accessing the same field."),t("br"),e._v("\nfor different logical data points simultaneously, "),t("strong",[e._v("using SoA will lead to fewer memory transactions as the same fields are declared in adjacent memory locations.")])]),e._v(" "),t("p",[e._v("AoS might lead to worse coalesced accesses, as declaring different fields in the same struct will waste memory bandwidth when accessing only a single field. Thus, SoA was preferred\non the GPU before the GPU cache was introduced.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/47be4998-06ec-4edf-81bc-555a4638e264",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/2e9f4e11-0ebf-4922-bc78-b27bdce58880",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"framework"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#framework"}},[e._v("#")]),e._v(" Framework")]),e._v(" "),t("ol",[t("li",[t("em",[e._v("Block Size, Grid Size, Number of Blocks per SM, and Thread ID")]),e._v(":"),t("br"),e._v("\nFor a GPU kernel, the block size is the number of threads inside each CUDA thread block, and the grid size is the number of CUDA thread blocks in total."),t("br"),e._v("\nThe number of blocks per SM is the maximum number of CUDA thread blocks each SM can have when executing the kernel, and it is bounded by")])]),e._v(" "),t("ul",[t("li",[e._v("the GPU architecture")]),e._v(" "),t("li",[e._v("the amount of resources each block is requesting.\nFor each thread, the thread ID (tid) is its one-dimensional rank inside the global thread pool.")])]),e._v(" "),t("ol",{attrs:{start:"2"}},[t("li",[t("p",[t("em",[e._v("Structure Size")]),e._v("\nThe structure size of a structure is the number of bytes of the structure including the padding bytes.")])]),e._v(" "),t("li",[t("p",[t("em",[e._v("Array Index")])])]),e._v(" "),t("li",[t("p",[t("em",[e._v("Stride")]),t("br"),e._v("\nFor a global access to data field x, the stride between a pair of adjacent threads in the same warp is the difference between the two memory addresses of the field x these two threads are accessing."),t("br"),e._v("\nThe value of the stride depends on the difference of the array indices accessed between adjacent threads, as well as the structure size of the structure field x belongs to."),t("br"),e._v("\nStrides can be different for different pairs of adjacent threads.")])]),e._v(" "),t("li",[t("p",[t("em",[e._v("Instruction Distance")]),t("br"),e._v("\nWe define the L1 and L2 instruction distance between "),t("em",[e._v("two memory access instructions I1 and I2")]),e._v(", as the "),t("em",[e._v("number of unique memory locations accessed\nby all threads sharing the L1 or L2 cache between I1 and I2")]),e._v(".\\")])])]),e._v(" "),t("blockquote",[t("p",[e._v("It is like memory footprint?")])]),e._v(" "),t("p",[e._v("They estimate the best memory layout based on cost function.\n"),t("img",{attrs:{src:"https://github.com/user-attachments/assets/39d8ac2f-109f-419e-9430-742fb3456603",alt:"image"}})]),e._v(" "),t("ul",[t("li",[e._v("Estimating the Cost Coefficient")]),e._v(" "),t("li",[e._v("Estimating the Number of Blocks per SM")]),e._v(" "),t("li",[e._v("Estimating the Cache Hit Ratio")]),e._v(" "),t("li",[e._v("Estimating the Cost")]),e._v(" "),t("li",[e._v("Handling Dynamic Loop Lengths")])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_2-managing-dram-latency-divergence-in-irregular-gpgpu-applications"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-managing-dram-latency-divergence-in-irregular-gpgpu-applications"}},[e._v("#")]),e._v(" 2. Managing DRAM Latency Divergence in Irregular GPGPU Applications")]),e._v(" "),t("ul",[t("li",[e._v("we propose memory scheduling mechanisms that avoid inter-warp interference in the DRAM system to reduce the average memory stall latency experienced by warps.")]),e._v(" "),t("li",[e._v("we reduce latency divergence through mechanisms that coordinate scheduling decisions across multiple independent memory channels.")]),e._v(" "),t("li",[e._v("we show that carefully orchestrating the memory scheduling policy can achieve low average latency for warps, without compromising bandwidth utilization.")])]),e._v(" "),t("p",[e._v("Irregular Workloads:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/ebd339c6-76ef-4e66-b10e-a146df1dab5e",alt:"image"}})]),e._v(" "),t("p",[e._v("Memory Divergence:")]),e._v(" "),t("p",[e._v("Following Figure  shows that 56% of loads (the black bar) issued by irregular programs result in more than one memory request and that on average each load generates 5.9 memory requests after coalescing (benchmarks and evaluation techniques explained in Section V).\n"),t("img",{attrs:{src:"https://github.com/user-attachments/assets/1e14846f-f5ce-40af-bce6-13bee8508898",alt:"image"}})])])}),[],!1,null,null,null);t.default=i.exports}}]);