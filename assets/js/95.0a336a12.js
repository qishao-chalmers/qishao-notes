(window.webpackJsonp=window.webpackJsonp||[]).push([[95],{550:function(s,a,n){"use strict";n.r(a);var e=n(8),t=Object(e.a)({},(function(){var s=this,a=s._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h1",{attrs:{id:"pytorch-cuda"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pytorch-cuda"}},[s._v("#")]),s._v(" Pytorch CUDA")]),s._v(" "),a("p",[s._v("before dive into deeper source code, this is the instinct of pytorch source code:")]),s._v(" "),a("p",[s._v("gradually lower code from python to cuda code")]),s._v(" "),a("ul",[a("li",[a("strong",[s._v("torch")]),s._v(" "),a("ul",[a("li",[s._v("pytorch interface, python oriented\n-./csrc."),a("strong",[s._v("autograd mechanism")]),s._v(" "),a("ul",[a("li",[s._v("engine")]),s._v(" "),a("li",[s._v("function")]),s._v(" "),a("li",[s._v("variable")])])])])]),s._v(" "),a("li",[a("strong",[s._v("c10")]),s._v(" core functions\n-"),a("strong",[s._v("cuda memory management")]),s._v("\n-"),a("strong",[s._v("dispatcher mechanism")])]),s._v(" "),a("li",[a("strong",[s._v("aten")]),s._v(" "),a("ul",[a("li",[s._v("native_functions.yaml\n"),a("ul",[a("li",[s._v("register all implementations with backend specification")])])])])])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("    - func: _slow_conv2d_forward.output(Tensor self, Tensor weight, SymInt[2] kernel_size,\n      Tensor? bias, SymInt[2] stride, SymInt[2] padding, *, Tensor(a!) output) -> Tensor(a!)\n    python_module: nn\n    dispatch:\n    CPU: slow_conv2d_forward_out_cpu\n    CUDA: slow_conv2d_forward_out_cuda\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("ul",[a("li",[s._v("src/ATen/Native/core/boxing/\n"),a("ul",[a("li",[s._v("WrapFunctionIntoFunctor.h support dispatcher, unbox function and call into real implementation")])])]),s._v(" "),a("li",[s._v("./src/ATen/cuda\n"),a("ul",[a("li",[a("p",[s._v("general functions")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/0e78c6b5-ac3e-488c-9296-670f1e8558f6",alt:"image"}})])])])]),s._v(" "),a("li",[s._v("./src/ATen/native/cuda\n"),a("ul",[a("li",[a("p",[s._v("implementation of different layers")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/41d8b9a7-60cf-4c4b-8f6f-45570df2c49e",alt:"image"}})])])])])]),s._v(" "),a("h2",{attrs:{id:"pytorch-cuda-memory-allocation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pytorch-cuda-memory-allocation"}},[s._v("#")]),s._v(" Pytorch CUDA Memory Allocation")]),s._v(" "),a("h3",{attrs:{id:"lower-level"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lower-level"}},[s._v("#")]),s._v(" Lower level")]),s._v(" "),a("p",[s._v("./c10/cuda/CUDACachingAllocator.cpp")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("Block* malloc(\n      c10::DeviceIndex device,\n      size_t orig_size,\n      cudaStream_t stream) {\n    ...\n    // First, try to get a block from the existing pool.\n    bool block_found =\n        // Search pool\n        get_free_block(params)\n        // Trigger callbacks and retry search\n        || (trigger_free_memory_callbacks(params) && get_free_block(params));\n    ...\n    if (!block_found) {\n      // Do garbage collection if the flag is set.\n      if (C10_UNLIKELY(\n              set_fraction &&\n              CUDAAllocatorConfig::garbage_collection_threshold() > 0.0)) {\n        garbage_collect_cached_blocks(context);\n      }\n    ...\n      // Attempt allocate\n      // WARNING: alloc_block may release the allocator lock when calling\n      // cudaMalloc. So far this function has not modified allocator state, but\n      // keep in mind that any observed allocator state may change across calls\n      // to alloc_block since it may release the lock.\n      block_found = alloc_block(params, false, context, lock)\n          // Free enough available cached blocks to satisfy alloc and retry\n          // alloc.\n          || (release_available_cached_blocks(params, context) &&\n              alloc_block(params, false, context, lock))\n          // Free all non-split cached blocks and retry alloc.\n          || (C10_LIKELY(captures_underway.size() == 0) &&\n              release_cached_blocks(context) &&\n              alloc_block(params, true, context, lock));\n...\n}\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br")])]),a("p",[s._v("./aten/src/ATen/cuda")]),s._v(" "),a("ul",[a("li",[s._v("CachingHostAllocator.h")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("inline TORCH_CUDA_CPP_API at::DataPtr HostAlloc(size_t size) {\n  return getCachingHostAllocator()->allocate(size);\n}\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("ul",[a("li",[s._v("CachingHostAllocator.cpp")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("struct CUDACachingHostAllocatorImpl\n    : public CachingHostAllocatorImpl<CUDAStream, EventPool::Event> {\n private:\n  void allocate_host_memory(size_t size, void** ptr) override {\n    // Pinned memory pointers allocated by any device can be directly used by\n    // any other device, regardless of the current device at the time of\n    // allocation, since we assume unified addressing. So we grab any existing\n    // primary context, if available. See pytorch/pytorch#21081.\n    ...\n    // Use cudaHostAlloc for allocating pinned memory (global lock in driver)\n    C10_CUDA_CHECK(cudaHostAlloc(ptr, size, cudaHostAllocDefault));\n    ...\n    }\n  }\n\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br")])]),a("h3",{attrs:{id:"intermediate-level"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#intermediate-level"}},[s._v("#")]),s._v(" intermediate level")]),s._v(" "),a("p",[s._v("code like cudnn use get_workspace_size to allocate space")]),s._v(" "),a("p",[s._v("./aten/src/ATen/native/cuda/MixedDtypesLinear.cu")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("  // Allocate workspace for CUTLASS mixed datatypes GEMM kernel.\n  const auto workspace_size = Gemm::get_workspace_size(arguments);\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("code like:\n./aten/src/ATen/native/cuda/ForeachReduceOp.cu")]),s._v(" "),a("p",[s._v("allocate memory by using at::zeros or at::empty")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("  auto output_per_tensor = at::zeros(\n      {static_cast<int64_t>(ntensors) * max_chunks_per_tensor}, options);\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("at::zeros is based on:"),a("br"),s._v("\n./aten/src/ATen/native/cuda/EmptyTensor.cpp")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("TensorBase empty_cuda(\n    IntArrayRef size,\n    ScalarType dtype,\n    std::optional<Device> device_opt,\n    std::optional<c10::MemoryFormat> memory_format_opt) {\n  at::globalContext().lazyInitDevice(c10::DeviceType::CUDA);\n  const auto device = device_or_default(device_opt);\n  TORCH_INTERNAL_ASSERT(device.is_cuda());\n  const DeviceGuard device_guard(device);\n  auto* allocator = at::cuda::getCUDADeviceAllocator();\n  constexpr c10::DispatchKeySet cuda_dks(c10::DispatchKey::CUDA);\n  return at::detail::empty_generic(\n      size, allocator, cuda_dks, dtype, memory_format_opt);\n}\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br")])]),a("p",[s._v("./aten/src/ATen/EmptyTensor.cpp")]),s._v(" "),a("p",[s._v("We have specify allocator and size, so we will call cuda caching allocator to allocate memory.")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("TensorBase empty_generic(\n    IntArrayRef size,\n    c10::Allocator* allocator,\n    c10::DispatchKeySet ks,\n    ScalarType scalar_type,\n    std::optional<c10::MemoryFormat> memory_format_opt) {\n  return _empty_generic(size, allocator, ks, scalar_type, memory_format_opt);\n}\n\ntemplate <typename T>\nTensorBase _empty_generic(\n    ArrayRef<T> size,\n    c10::Allocator* allocator,\n    c10::DispatchKeySet ks,\n    ScalarType scalar_type,\n    std::optional<c10::MemoryFormat> memory_format_opt) {\n  at::detail::check_size_nonnegative(size);\n  at::detail::raise_warning_for_complex_half(scalar_type);\n  caffe2::TypeMeta dtype = scalarTypeToTypeMeta(scalar_type);\n  auto size_bytes = computeStorageNbytesContiguous(size, dtype.itemsize());\n  auto storage_impl = c10::make_intrusive<StorageImpl>(\n      c10::StorageImpl::use_byte_size_t(),\n      size_bytes,\n      allocator,\n      /*resizeable=*/true);\n\n  auto tensor = detail::make_tensor_base<TensorImpl>(\n      std::move(storage_impl), ks, dtype);\n  // Default TensorImpl has size [0]\n  // NB: test for meta dispatch key to avoid guarding on zero-ness\n  if (ks.has(c10::DispatchKey::Meta) || size.size() != 1 || size[0] != 0) {\n    tensor.unsafeGetTensorImpl()->generic_set_sizes_contiguous(size);\n  }\n\n  if (memory_format_opt.has_value()) {\n    // Restriding a just-created empty contiguous tensor does nothing.\n    if (*memory_format_opt != MemoryFormat::Contiguous) {\n      tensor.unsafeGetTensorImpl()->empty_tensor_restride(*memory_format_opt);\n    }\n  }\n\n  return tensor;\n}\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br"),a("span",{staticClass:"line-number"},[s._v("41")]),a("br"),a("span",{staticClass:"line-number"},[s._v("42")]),a("br"),a("span",{staticClass:"line-number"},[s._v("43")]),a("br")])]),a("h3",{attrs:{id:"python-level"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#python-level"}},[s._v("#")]),s._v(" python level")]),s._v(" "),a("p",[a("strong",[s._v("init")]),s._v(".pyi.in")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("class _cuda_CUDAAllocator: ...\n\ndef _cuda_customAllocator(alloc_fn: _int, free_fn: _int) -> _cuda_CUDAAllocator: ...\ndef _cuda_changeCurrentAllocator(allocator: _cuda_CUDAAllocator) -> None: ...\ndef _cuda_getAllocator() -> _cuda_CUDAAllocator: ...\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("./torch/cuda/memory.py")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('__all__ = [\n    "caching_allocator_alloc",\n    "caching_allocator_delete",\n    "caching_allocator_enable",\n    ...\n    "memory_allocated",\n    ...\n]\n\ndef caching_allocator_alloc(size, device: Union[Device, int] = None, stream=None):\n    r"""Perform a memory allocation using the CUDA memory allocator.\n\n    Memory is allocated for a given device and a stream, this\n    function is intended to be used for interoperability with other\n    frameworks. Allocated memory is released through\n    :func:`~torch.cuda.caching_allocator_delete`.\n\n    Args:\n        size (int): number of bytes to be allocated.\n        device (torch.device or int, optional): selected device. If it is\n            ``None`` the default CUDA device is used.\n        stream (torch.cuda.Stream or int, optional): selected stream. If is ``None`` then\n            the default stream for the selected device is used.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    """\n    if device is None:\n        device = torch.cuda.current_device()\n    device = _get_device_index(device)\n    if stream is None:\n        stream = torch.cuda.current_stream(device)\n    if isinstance(stream, torch.cuda.streams.Stream):\n        stream = stream.cuda_stream\n    ...\n    with torch.cuda.device(device):\n        return torch._C._cuda_cudaCachingAllocator_raw_alloc(size, stream)\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br")])]),a("p",[s._v("./torch/csrc/cuda/Module.cpp")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('    {"_cuda_cudaCachingAllocator_raw_alloc",\n     THCPModule_cudaCachingAllocator_raw_alloc,\n     METH_VARARGS,\n     nullptr},\n\nPyObject* THCPModule_cudaCachingAllocator_raw_alloc(\n    PyObject* _unused,\n    PyObject* args) {\n  HANDLE_TH_ERRORS\n  PyObject* size_o = nullptr;\n  PyObject* stream_o = nullptr;\n  if (!PyArg_ParseTuple(args, "OO", &size_o, &stream_o)) {\n    THPUtils_invalidArguments(\n        args,\n        nullptr,\n        "caching_allocator_alloc",\n        1,\n        "(ssize_t size, intptr_t stream);");\n    return nullptr;\n  }\n  auto size = PyLong_AsSsize_t(size_o);\n  cudaStream_t stream = static_cast<cudaStream_t>(PyLong_AsVoidPtr(stream_o));\n  void* mem = nullptr;\n  {\n    pybind11::gil_scoped_release no_gil;\n    mem = c10::cuda::CUDACachingAllocator::raw_alloc_with_stream(size, stream);\n  }\n  return PyLong_FromVoidPtr(mem);\n  END_HANDLE_TH_ERRORS\n}\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br")])]),a("h3",{attrs:{id:"code-gen-for-memory-allocation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#code-gen-for-memory-allocation"}},[s._v("#")]),s._v(" code gen for memory allocation")]),s._v(" "),a("p",[s._v("./torch/_inductor/codegen/cuda/cuda_kernel.py")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('    def call_kernel(\n        self,\n        name: str,\n        node: "CUDATemplateBuffer",  # type: ignore[name-defined]\n    ) -> None:\n        if node.get_workspace_size() > 0:\n            ws = WorkspaceArg(\n                count=node.get_workspace_size(),\n                device=V.graph.get_current_device_or_throw(),\n                zero_mode=WorkspaceZeroMode.UNINITIALIZED,\n                outer_name=WorkspaceArg.unique_name(),\n            )\n            wrapper.generate_workspace_allocation(ws)\n            workspace = str(ws.outer_name)\n            call_args.append(\n                workspace\n                if V.graph.cpp_wrapper\n                else f"c_void_p({workspace}.data_ptr())"\n            )\n      .....\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br")])]),a("p",[s._v("./torch/_inductor/codegen/wrapper.py")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("    def generate_workspace_allocation(self, ws: WorkspaceArg):\n        name = ws.get_name()\n        line = AllocateLine(self, ws)\n        if ws.zero_mode == WorkspaceZeroMode.UNINITIALIZED:\n            self.writeline(line)\n        elif ws.zero_mode == WorkspaceZeroMode.ZERO_ON_CALL:\n            self.writeline(line)\n            self.writeline(self.make_zero_buffer(name))\n        elif ws.zero_mode == WorkspaceZeroMode.ZERO_PER_GRAPH:\n            prior = self.allocated_workspaces.get(name)\n            if prior:\n                assert isinstance(prior, AllocateLine)\n                # expand existing allocation\n                prior.node = WorkspaceArg.maximum(prior.node, ws)\n            else:\n                self.writeline(line)\n                self.writeline(self.make_zero_buffer(name))\n                self.allocated_workspaces[name] = line\n        else:\n            raise AssertionError(ws.zero_mode)\n\n        if config.triton.autotune_at_compile_time:\n            self.kernel_autotune_calls.writeline(\n                PythonWrapperCodegen.make_allocation(\n                    self,\n                    name,\n                    ws.device,\n                    ws.dtype,\n                    shape=(V.graph.sizevars.size_hint(ws.count),),\n                    stride=(1,),\n                )\n            )\n            if ws.zero_mode != WorkspaceZeroMode.UNINITIALIZED:\n                self.kernel_autotune_calls.writeline(\n                    PythonWrapperCodegen.make_zero_buffer(self, name)\n                )\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br")])]),a("h2",{attrs:{id:"pytorch-adam-cuda-kernel"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pytorch-adam-cuda-kernel"}},[s._v("#")]),s._v(" Pytorch Adam CUDA Kernel")]),s._v(" "),a("h3",{attrs:{id:"native-functions-yaml"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#native-functions-yaml"}},[s._v("#")]),s._v(" native_functions.yaml")]),s._v(" "),a("p",[s._v("./aten/src/ATen/native/native_functions.yaml")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('- func: _fused_adam_(Tensor(a!)[] self,\nTensor(b!)[] grads,\nTensor(c!)[] exp_avgs,\nTensor(d!)[] exp_avg_sqs,\nTensor(e!)[] max_exp_avg_sqs,\nTensor[] state_steps, *,\nfloat lr, float beta1, float beta2,\nfloat weight_decay, float eps, bool amsgrad,\nbool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()\n  # Unlike "foreach" functions, lists of tensors should be guaranteed to be on the same device (for now).\n  variants: function\n  dispatch:\n    CPU: _fused_adam_kernel_cpu_\n    CUDA: _fused_adam_kernel_cuda_\n    MPS: _fused_adam_kernel_mps_\n  autogen: _fused_adam, _fused_adam.out\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br")])]),a("h3",{attrs:{id:"fusedadamkernel-cu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fusedadamkernel-cu"}},[s._v("#")]),s._v(" FusedAdamKernel.cu")]),s._v(" "),a("p",[s._v("./aten/arc/ATen/native/cuda")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('// The following overload simply has a Tensor lr\nvoid _fused_adam_kernel_cuda_(\n    at::TensorList params,\n    at::TensorList grads,\n    at::TensorList exp_avgs,\n    at::TensorList exp_avg_sqs,\n    at::TensorList max_exp_avg_sqs,\n    at::TensorList state_steps,\n    const at::Tensor& lr,\n    const double beta1,\n    const double beta2,\n    const double weight_decay,\n    const double eps,\n    const bool amsgrad,\n    const bool maximize,\n    const std::optional<at::Tensor>& grad_scale,\n    const std::optional<at::Tensor>& found_inf) {\n  if (lr.is_cpu()) {\n    _fused_adam_kernel_cuda_(\n        params,\n        grads,\n        exp_avgs,\n        exp_avg_sqs,\n        max_exp_avg_sqs,\n        state_steps,\n        lr.item<double>(),\n        beta1,\n        beta2,\n        weight_decay,\n        eps,\n        amsgrad,\n        maximize,\n        grad_scale,\n        found_inf);\n    return;\n  }\n\n  // Manually check devices since we specify no device check in\n  // native_functions.yaml\n  ...\n\n  if (amsgrad) {\n    ...\n  } else {\n    TORCH_CHECK(\n        at::native::check_fast_path_restrictions(\n            {params, grads, exp_avgs, exp_avg_sqs}),\n        "params, grads, exp_avgs, and exp_avg_sqs must have same dtype, device, and layout");\n    _fused_adam_cuda_impl_(\n        params,\n        grads,\n        exp_avgs,\n        exp_avg_sqs,\n        state_steps,\n        lr,\n        beta1,\n        beta2,\n        weight_decay,\n        eps,\n        maximize,\n        grad_scale,\n        found_inf);\n  }\n}\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br"),a("span",{staticClass:"line-number"},[s._v("41")]),a("br"),a("span",{staticClass:"line-number"},[s._v("42")]),a("br"),a("span",{staticClass:"line-number"},[s._v("43")]),a("br"),a("span",{staticClass:"line-number"},[s._v("44")]),a("br"),a("span",{staticClass:"line-number"},[s._v("45")]),a("br"),a("span",{staticClass:"line-number"},[s._v("46")]),a("br"),a("span",{staticClass:"line-number"},[s._v("47")]),a("br"),a("span",{staticClass:"line-number"},[s._v("48")]),a("br"),a("span",{staticClass:"line-number"},[s._v("49")]),a("br"),a("span",{staticClass:"line-number"},[s._v("50")]),a("br"),a("span",{staticClass:"line-number"},[s._v("51")]),a("br"),a("span",{staticClass:"line-number"},[s._v("52")]),a("br"),a("span",{staticClass:"line-number"},[s._v("53")]),a("br"),a("span",{staticClass:"line-number"},[s._v("54")]),a("br"),a("span",{staticClass:"line-number"},[s._v("55")]),a("br"),a("span",{staticClass:"line-number"},[s._v("56")]),a("br"),a("span",{staticClass:"line-number"},[s._v("57")]),a("br"),a("span",{staticClass:"line-number"},[s._v("58")]),a("br"),a("span",{staticClass:"line-number"},[s._v("59")]),a("br"),a("span",{staticClass:"line-number"},[s._v("60")]),a("br"),a("span",{staticClass:"line-number"},[s._v("61")]),a("br"),a("span",{staticClass:"line-number"},[s._v("62")]),a("br"),a("span",{staticClass:"line-number"},[s._v("63")]),a("br"),a("span",{staticClass:"line-number"},[s._v("64")]),a("br")])]),a("h3",{attrs:{id:"fused-adam-impl-cu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fused-adam-impl-cu"}},[s._v("#")]),s._v(" fused_adam_impl.cu")]),s._v(" "),a("p",[s._v("./aten/arc/ATen/native/cuda")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('void _fused_adam_cuda_impl_(\n    at::TensorList params,\n    at::TensorList grads,\n    at::TensorList exp_avgs,\n    at::TensorList exp_avg_sqs,\n    at::TensorList state_steps,\n    const double lr,\n    const double beta1,\n    const double beta2,\n    const double weight_decay,\n    const double eps,\n    const bool maximize,\n    const std::optional<at::Tensor>& grad_scale,\n    const std::optional<at::Tensor>& found_inf) {\n  std::vector<std::vector<at::Tensor>> tensor_lists{\n      params.vec(), grads.vec(), exp_avgs.vec(), exp_avg_sqs.vec()};\n\n  const float* grad_scale_ptr =\n      grad_scale.has_value() ? grad_scale->data_ptr<float>() : nullptr;\n  const float* found_inf_ptr =\n      found_inf.has_value() ? found_inf->data_ptr<float>() : nullptr;\n  const float* lr_ptr = nullptr;\n\n  AT_DISPATCH_FLOATING_TYPES_AND2(\n      kHalf,\n      kBFloat16,\n      params[0].scalar_type(),\n      "fused_adam_kernel_cuda",\n      [&]() {\n        multi_tensor_apply_for_fused_optimizer<4>(\n            tensor_lists,\n            state_steps,\n            FusedAdamMathFunctor<scalar_t, 4, ADAM_MODE::ORIGINAL, false>(),\n            lr_ptr, // unused\n            lr,\n            beta1,\n            beta2,\n            weight_decay,\n            eps,\n            maximize,\n            grad_scale_ptr,\n            found_inf_ptr);\n      });\n}\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br"),a("span",{staticClass:"line-number"},[s._v("41")]),a("br"),a("span",{staticClass:"line-number"},[s._v("42")]),a("br"),a("span",{staticClass:"line-number"},[s._v("43")]),a("br"),a("span",{staticClass:"line-number"},[s._v("44")]),a("br")])]),a("p",[s._v("The callable function is "),a("strong",[s._v("FusedAdamMathFunctor<scalar_t, 4, ADAM_MODE::ORIGINAL, false>()")]),s._v(".")]),s._v(" "),a("h3",{attrs:{id:"multitensorapply-cuh"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#multitensorapply-cuh"}},[s._v("#")]),s._v(" MultiTensorApply.cuh")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("\ntemplate <typename T, typename U, typename... ArgTypes>\nC10_LAUNCH_BOUNDS_1(kBlockSize)\n__global__ void multi_tensor_apply_kernel(\n    T tensorListMeta,\n    U callable,\n    ArgTypes... args) {\n  // Hand the chunk information to the user-supplied functor to process however\n  // it likes.\n  callable(kChunkSize, tensorListMeta, args...);\n}\n\ntemplate <int depth, typename T, typename... ArgTypes>\nvoid multi_tensor_apply_for_fused_optimizer(\n    std::vector<std::vector<at::Tensor>>& tensor_lists,\n    at::TensorList state_steps,\n    T callable,\n    ArgTypes... args) {\n    ...\n    for (const auto& chunk : c10::irange(chunks)) {\n            multi_tensor_apply_kernel<<<\n            loc_block_info,\n            kBlockSize,\n            0,\n            at::cuda::getCurrentCUDAStream()>>>(\n            tensorListMeta, callable, args...);\n        C10_CUDA_KERNEL_LAUNCH_CHECK();\n    }\n}\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br")])]),a("hr"),s._v(" "),a("h2",{attrs:{id:"pytorch-linear-lowering"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pytorch-linear-lowering"}},[s._v("#")]),s._v(" Pytorch linear lowering")]),s._v(" "),a("h3",{attrs:{id:"python-nn-functions-cpp"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#python-nn-functions-cpp"}},[s._v("#")]),s._v(" python_nn_functions.cpp")]),s._v(" "),a("p",[s._v("Depth 31/29")]),s._v(" "),a("p",[s._v("./torch/csrc/autograd/generated/python_nn_functions.cpp")]),s._v(" "),a("p",[s._v("operator() (__closure=0x7ffc7dcf9e88, input=..., weight=..., bias=...)")]),s._v(" "),a("p",[s._v("torch::autograd::THPVariable_linear")]),s._v(" "),a("h3",{attrs:{id:"linear-h"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#linear-h"}},[s._v("#")]),s._v(" linear.h")]),s._v(" "),a("p",[s._v("Depth 29")]),s._v(" "),a("p",[s._v("./build/aten/src/ATen/ops/linear.h")]),s._v(" "),a("p",[s._v("at::linear (input=..., weight=..., bias=...)")]),s._v(" "),a("h3",{attrs:{id:"operators-0-cpp"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#operators-0-cpp"}},[s._v("#")]),s._v(" Operators_0.cpp")]),s._v(" "),a("p",[s._v("at namespace\nDepth 28")]),s._v(" "),a("p",[s._v("at::_ops::linear::call (input=..., weight=..., bias=...)\nbuild/aten/src/ATen/Operators_0.cpp:3601")]),s._v(" "),a("h3",{attrs:{id:"dispatch"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dispatch"}},[s._v("#")]),s._v(" dispatch")]),s._v(" "),a("p",[s._v("c10 namespace")]),s._v(" "),a("p",[s._v("./aten/src/ATen/Core/dispatch/Dispatcher.h dispatch")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/cf488804-1b5d-4ade-ba21-fa0ec46733f7",alt:"image"}})]),s._v(" "),a("h3",{attrs:{id:"boring-part-traversing-from-dispatcher-to-actual-kernel-call"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#boring-part-traversing-from-dispatcher-to-actual-kernel-call"}},[s._v("#")]),s._v(" boring part, traversing from dispatcher to actual kernel call")]),s._v(" "),a("h4",{attrs:{id:"boxing-and-unbox"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#boxing-and-unbox"}},[s._v("#")]),s._v(" boxing and unbox")]),s._v(" "),a("p",[s._v("Depth 23")]),s._v(" "),a("p",[s._v("c10 namespace")]),s._v(" "),a("p",[s._v("aten/src/ATen/core/boxing/KernelFunction_impl.h 105")]),s._v(" "),a("p",[s._v("c10::callUnboxedKernelFunction at")]),s._v(" "),a("p",[s._v("aten/src/ATen/core/boxing/KernelFunction_impl.h:53")]),s._v(" "),a("p",[s._v("c10::impl::wrap_kernel_functor_unboxed_ at")]),s._v(" "),a("p",[s._v("aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:468")]),s._v(" "),a("h4",{attrs:{id:"from-c10-to-at-namespace"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#from-c10-to-at-namespace"}},[s._v("#")]),s._v(" From c10 to at namespace")]),s._v(" "),a("p",[s._v("Please notice the dispatchkeyset, that is related to dispatch mechanism in pytorch.")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("c10::impl::detail::WrapFunctionIntoFunctor_ in aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13\\\nto\nat::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__linear in\nbuild/aten/src/ATen/RegisterCompositeImplicitAutograd.cpp:2620\n\n\nDepth 20\n\nat::native::linear (input=..., weight=..., bias_opt=...) at aten/src/ATen/native/Linear.cpp:95\n\nTensor linear(const Tensor& input, const Tensor& weight, const c10::optional<Tensor>& bias_opt) {\n  ...\n  if (input_dim == 2 && bias->defined()) {\n    // Fused op is marginally faster.\n    return at::addmm(*bias, input, weight.t());\n  }\n  ...\n}\n\n\nDepth 19\n\nat::addmm (self=..., mat1=..., mat2=..., beta=..., alpha=...) at build/aten/src/ATen/ops/addmm.h:36\n\nDepth 18\n\nat::_ops::addmm::call (self=..., mat1=..., mat2=..., beta=..., alpha=...) at aten/src/ATen/Operators_0.cpp:7151\n\nDepth 12\n\nc10::impl::detail::WrapFunctionIntoFunctor_<\nc10::CompileTimeFunctionPointer<\nat::Tensor(c10::DispatchKeySet, const at::Tensor&, const at::Tensor&, const at::Tensor&, const c10::Scalar&, const c10::Scalar&),\ntorch::autograd::VariableType::(anonymous namespace)::addmm>,\nat::Tensor,\nc10::guts::typelist::typelist<c10::DispatchKeySet, const at::Tensor&, const at::Tensor&, const at::Tensor&, const c10::Scalar&, const c10::Scalar&>\n>::operator() (args#5=..., args#4=..., args#3=..., args#2=..., args#1=..., args#0=..., this=0x30c7a930)\naten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13\n\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br")])]),a("p",[s._v("to torch::autograd::VariableType::(anonymous namespace)::addmm (ks=..., self=..., mat1=..., mat2=..., beta=..., alpha=...) at aten/src/autograd/generated/VariableType_0.cpp:6898")]),s._v(" "),a("h3",{attrs:{id:"redispatchfunction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#redispatchfunction"}},[s._v("#")]),s._v(" RedispatchFunction")]),s._v(" "),a("p",[s._v("This is generated function, but I guess most of redispatch function will be generated here.")]),s._v(" "),a("p",[s._v("")]),s._v(" "),a("p",[s._v("at::redispatch::addmm (dispatchKeySet=..., self=..., mat1=..., mat2=..., beta=..., alpha=...) at build/aten/src/ATen/RedispatchFunctions.h:8517")]),s._v(" "),a("p",[s._v("And then:\nat::(anonymous namespace)::wrapper_CUDA_addmm build/aten/src/ATen/RegisterCUDA.cpp")]),s._v(" "),a("p",[a("strong",[s._v("linear")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("    // aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor\n    inline at::Tensor linear(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias={}) {\n        return at::_ops::linear::redispatch(dispatchKeySet, input, weight, bias);\n    }\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[a("strong",[s._v("relu")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("    inline at::Tensor relu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {\n        return at::_ops::relu::redispatch(dispatchKeySet, self);\n    }\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[a("strong",[s._v("softmax")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("    inline at::Tensor softmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype=c10::nullopt) {\n        return at::_ops::softmax_int::redispatch(dispatchKeySet, self, dim, dtype);\n    }\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("h3",{attrs:{id:"blas-cpp"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#blas-cpp"}},[s._v("#")]),s._v(" Blas.cpp")]),s._v(" "),a("p",[s._v("Blas.cpp calls to at::cuda::blas::gemm function.")]),s._v(" "),a("p",[s._v("./aten/src/ATen/native/cuda/Blas.cpp")]),s._v(" "),a("p",[s._v("Notice at::cuda::blas::gemm function. it calls into gemm function.")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(\n        at::ScalarType::Half,\n        at::ScalarType::BFloat16,\n        scalar_type,\n        "addmm_cuda",\n        [&] {\n          using opmath_t = at::opmath_type<scalar_t>;\n          opmath_t alpha_val = alpha.to<opmath_t>();\n          opmath_t beta_val = beta.to<opmath_t>();\n          const scalar_t* mat1_ptr = args.mata->const_data_ptr<scalar_t>();\n          const scalar_t* mat2_ptr = args.matb->const_data_ptr<scalar_t>();\n          scalar_t* result_ptr = args.result->mutable_data_ptr<scalar_t>();\n          at::cuda::blas::gemm<scalar_t>(\n              args.transa,\n              args.transb,\n              args.m,\n              args.n,\n              args.k,\n              alpha_val,\n              mat1_ptr,\n              args.lda,\n              mat2_ptr,\n              args.ldb,\n              beta_val,\n              result_ptr,\n              args.result_ld);\n        });\n    switch (activation) {\n      case Activation::RELU:\n        at::relu_(const_cast<Tensor&>(*args.result));\n        break;\n      case Activation::GELU:\n        at::gelu_(const_cast<Tensor&>(*args.result), "tanh");\n        break;\n      default: break;\n    }\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br")])])])}),[],!1,null,null,null);a.default=t.exports}}]);