(window.webpackJsonp=window.webpackJsonp||[]).push([[118],{570:function(e,t,a){"use strict";a.r(t);var s=a(9),i=Object(s.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"sarathi-efficient-llm-inference-by-piggybacking-decodes-with-chunked-prefills"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#sarathi-efficient-llm-inference-by-piggybacking-decodes-with-chunked-prefills"}},[e._v("#")]),e._v(" SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills")]),e._v(" "),t("p",[e._v("For example, on an A6000 GPU, for the LLaMA-13B model, a prefill with a sequence length of 512 tokens saturates GPU compute even at a batch size of just one.")]),e._v(" "),t("p",[e._v("The decode phase results in very low GPU utilization at low batch sizes.")]),e._v(" "),t("p",[e._v("For example, our experiments reveal that, at small batch sizes, the decode cost per token can be as high as ∼ 200 times the prefill cost per token.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/36abc78b-ee57-46d1-9fb1-f35a51494583",alt:"image"}})]),e._v(" "),t("p",[e._v("Figure 3 shows the per-token cost of each of the six transformer operations (§2.1) for prefill and decode at various batch sizes for a fixed sequence length (prefill+decode) of 1024.")]),e._v(" "),t("p",[e._v("First, we observe that prefill has almost constant pertoken cost across various batch sizes, indicating that prefill saturates the GPU even at batch size of 1.")]),e._v(" "),t("p",[e._v("Second, we see that decode behaves very differently from prefill as the pertoken cost reduces significantly when the batch size increases.")]),e._v(" "),t("p",[e._v("Third, we see that the decode cost per-token is "),t("strong",[e._v("200×, 100×, and 16.7× that of prefill at batch size of 1, 2 and 18")]),e._v(", respectively. Thus, it is clear that optimizing decodes is critical for\nefficient LLM inference.")]),e._v(" "),t("p",[e._v("We observe that the throughput of the prefill phase saturates at about 180 tokens/millisecond when B × L ≥ 512: e.g., a\nsingle prefill request can achieve peak throughput at L ≥ 512.")]),e._v(" "),t("p",[e._v("In contrast, the decode throughput increases linearly with small batch sizes.")]),e._v(" "),t("p",[e._v("To further understand the saturation point of decode phase, we profile a single layer as opposed to the 40 layers of the full model.")]),e._v(" "),t("p",[e._v("This enables us to fit 40× larger batches on the GPU due to the reduced memory footprint of model weights and KV caches.")]),e._v(" "),t("p",[e._v("We find that decode saturates at a much larger batch (e.g., 256 with 1024 sequence length).")]),e._v(" "),t("p",[e._v("Such large batches are infeasible to run with the full model.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/74f86857-3bad-4d18-88af-e13d686b1137",alt:"image"}})]),e._v(" "),t("blockquote",[t("p",[e._v("Please note that in (b) batch size with 256, even arithmetic intensity is large and become compute-intensive. However, scaling up the batch size to such high values is infeasible due to the KV-cache footprint of each request. For instance, we can fit a maximum batch size of 18 requests at a sequence length of 1K for the LLaMA-13B model on an A6000 GPU. Therefore, in the range of batch sizes that are practical today, the decode phase remains memory-bound.")])]),e._v(" "),t("h2",{attrs:{id:"distserve-disaggregating-prefill-and-decoding-for-goodput-optimized-large-language-model-serving"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#distserve-disaggregating-prefill-and-decoding-for-goodput-optimized-large-language-model-serving"}},[e._v("#")]),e._v(" DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/860b33d2-d07f-4cc7-9a9c-09c5dbacb889",alt:"image"}})]),e._v(" "),t("p",[e._v("The maximum achievable goodput on a single A100 GPU, which is constrained by the more stringent one of TTFT and TPOT requirements, is about 1.6 requests per second (rps).")]),e._v(" "),t("p",[e._v("The performance contrasts sharply when each phase is served independently on a separate GPU, shown by the orange and green curves, which achieve per-GPU goodput of 5.6 rps for the prefill phase and 10 rps for decoding.")]),e._v(" "),t("p",[e._v("Ideally, by allocating 2 GPUs for prefill and 1 GPU for decoding, we can effectively serve the model with an overall goodput\nof 10 rps, or equally 3.3 rps per GPU, which is 2.1x higher than existing systems.")]),e._v(" "),t("p",[e._v("Ideally, by allocating 2 GPUs for prefill and 1 GPU for decoding, we can effectively serve the model with an overall goodput\nof 10 rps, or equally 3.3 rps per GPU, which is 2.1x higher than existing systems.")]),e._v(" "),t("p",[e._v("First, colocation leads to strong prefill-decoding interference.")]),e._v(" "),t("p",[t("strong",[e._v("A prefill step often takes much longer than a decoding step.")])]),e._v(" "),t("p",[e._v("When batched together, decoding steps in the batch are delayed by the prefill steps, significantly elongating their TPOT.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/c0dabf55-49ae-48de-81a3-015756710bbd",alt:"image"}})]),e._v(" "),t("p",[e._v("As Figure 2 shows, adding a single prefill job to a batch of decoding requests significantly slows down both processes, leading to a marked increase in TTFT and TPOT.")]),e._v(" "),t("p",[e._v("Specifically, the decoding tasks in the batch must wait for lengthier prefill jobs to complete, thus extending TPOT; the slowdown intensifies with a longer prefill, shown in Figure 2(b).")]),e._v(" "),t("p",[e._v("Adding decoding jobs to prefill also increases the time to complete the prefill task, particularly when the GPU is already at capacity (Figure 2 blue curves).")]),e._v(" "),t("h3",{attrs:{id:"analysis-for-prefill-instance"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#analysis-for-prefill-instance"}},[e._v("#")]),e._v(" Analysis for Prefill Instance")]),e._v(" "),t("h4",{attrs:{id:"batching-strategy"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#batching-strategy"}},[e._v("#")]),e._v(" Batching Strategy")]),e._v(" "),t("p",[e._v("Assuming a given arrival rate, our goal is to fulfill the service’s latency requirement on TTFT using the least resources.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/9988fe53-8108-47e3-855b-74eeab999c14",alt:"image"}})]),e._v(" "),t("p",[e._v("For a 13B parameter LLM, processing a single sequence of 512 tokens can fully engage an A100 GPU; larger models require\nshorter sequences to reach GPU saturation.")]),e._v(" "),t("p",[e._v("Once the GPU becomes compute-bound, adding more requests to the batch no longer improves GPU efficiency.")]),e._v(" "),t("p",[e._v("Hence, for prefill instances, it is necessary to profile the specific LLM and GPUs in advance to identify a critical input length threshold, denoted as Lm, beyond which the prefill phase becomes compute-bound.")]),e._v(" "),t("p",[t("strong",[e._v("In practice, user prompts typically average over hundreds of tokens [7]. Batch sizes for the prefill instance are generally kept small.")])]),e._v(" "),t("h4",{attrs:{id:"parallelism-plan"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#parallelism-plan"}},[e._v("#")]),e._v(" Parallelism plan")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/bfbb2743-f67e-4d77-bc91-0272dce038d8",alt:"image"}})]),e._v(" "),t("p",[e._v("A more stringent SLO will make intra-op parallelism more advantageous, due to its ability to support higher request rates while adhering to SLOs.")]),e._v(" "),t("p",[e._v("The value of K depends on factors such as the input length, model architecture, communication bandwidth, and placement.")]),e._v(" "),t("h3",{attrs:{id:"analysis-for-decoding-instance"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#analysis-for-decoding-instance"}},[e._v("#")]),e._v(" Analysis for Decoding Instance")]),e._v(" "),t("p",[e._v("It receives the intermediate states (KV caches) and the first token from the prefill instance and generates subsequent tokens one at a time.")]),e._v(" "),t("p",[e._v("For decoding instances, our optimization goal is to satisfy the application’s TPOT requirement using minimal computing resources.")]),e._v(" "),t("p",[e._v("Post-disaggregation, the batch size for decoding may be constrained by GPU memory capacity, as it is necessary to maintain the KV caches for all active requests.")]),e._v(" "),t("blockquote",[t("p",[e._v("Intra-operator parallelism partitions computationally intensive operators, such as matrix multiplications, across multiple GPUs, accelerating computation but causing substantial communication.\nInter-operator parallelism organizes LLM layers into stages, each running on a GPU to form pipelines. It moderately increases execution time due to inter-stage communication, but linearly scales the system’s rate capacity with each added GPU.")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/a1da67f1-d51d-47c3-99b0-49bdd09d1d4d",alt:"image"}})]),e._v(" "),t("p",[e._v("Intra-op parallelism reduces latency with diminishing returns, caused by communication and reduced utilization\nafter partitioning.")]),e._v(" "),t("p",[e._v("Inter-op parallelism can almost linearly scale the throughput.")]),e._v(" "),t("p",[e._v("Hence, when the TPOT SLO is stringent, intra-op parallelism is essential to reduce TPOT to meet latency goals. Beyond this, inter-op parallelism is preferable to enhance throughput linearly.")]),e._v(" "),t("h3",{attrs:{id:"practical-problems"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#practical-problems"}},[e._v("#")]),e._v(" Practical Problems")]),e._v(" "),t("ul",[t("li",[e._v("Variable prefill length")]),e._v(" "),t("li",[e._v("Communication overhead")])]),e._v(" "),t("p",[e._v("The KV cache size of a single 512-token request on OPT-66B is approximately 1.13GB.")]),e._v(" "),t("p",[e._v("Assuming an average arrival rate of 10 requests per second, we need to transfer 1.13 × 10 = 11.3 GB data – or equivalently 90Gbps bandwidth to render the overhead invisible.")]),e._v(" "),t("p",[e._v("The size of the KV caches increases with average input length and arrival rate.")]),e._v(" "),t("p",[e._v("While many modern GPU clusters for LLMs are equipped with Infiniband (e.g., 800 Gbps), in cases where cross-node bandwidth is limited, disaggregation relies on the commonly available intra-node NVLINK, where the peak bandwidth between A100 GPUs is 600 GB/s, again "),t("strong",[e._v("rendering the transmission overhead negligible")]),e._v(".")])])}),[],!1,null,null,null);t.default=i.exports}}]);