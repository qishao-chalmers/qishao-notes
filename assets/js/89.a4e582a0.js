(window.webpackJsonp=window.webpackJsonp||[]).push([[89],{549:function(e,a,t){"use strict";t.r(a);var n=t(8),s=Object(n.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"aten"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aten"}},[e._v("#")]),e._v(" aten")]),e._v(" "),a("p",[a("strong",[e._v("Purpose")]),e._v('\nATen (short for "A Tensor Library") is the core tensor library in PyTorch. It provides the following:')]),e._v(" "),a("ul",[a("li",[e._v("Tensor Abstraction: Defines the Tensor class and provides fundamental tensor operations.")]),e._v(" "),a("li",[e._v("Backend Management: Handles the execution of operations on various backends (e.g., CPU, CUDA).")]),e._v(" "),a("li",[e._v("Dispatcher Integration: Works with the dispatcher to route tensor operations to the correct implementation.")])]),e._v(" "),a("p",[a("strong",[e._v("Key Roles")])]),e._v(" "),a("ul",[a("li",[a("p",[e._v("Tensor Definition:")]),e._v(" "),a("ul",[a("li",[e._v("Provides the Tensor class, which is the foundational data structure in PyTorch.")]),e._v(" "),a("li",[e._v("Source: aten/src/ATen/Tensor.h.")])])]),e._v(" "),a("li",[a("p",[e._v("Tensor Operations:")]),e._v(" "),a("ul",[a("li",[e._v("Implements a wide variety of tensor operations, such as matrix multiplication, element-wise addition, and reductions.")]),e._v(" "),a("li",[e._v("Operations are defined in:\n"),a("ul",[a("li",[e._v("aten/src/ATen/native/ (backend-specific implementations like cpu, cuda, or quantized).")]),e._v(" "),a("li",[e._v("Example: aten/src/ATen/native/LinearAlgebra.cpp (matrix-related ops).")])])])])]),e._v(" "),a("li",[a("p",[e._v("Dispatcher Entry Points:")]),e._v(" "),a("ul",[a("li",[e._v("Provides entry points for the PyTorch dispatcher system, which routes tensor operations to their specific implementations.")]),e._v(" "),a("li",[e._v("Function signatures for tensor operations are declared in aten/src/ATen/Functions.h and implemented in backend-specific files.")])])]),e._v(" "),a("li",[a("p",[e._v("Backend Management:")]),e._v(" "),a("ul",[a("li",[e._v("Supports multiple device backends, such as CPU, CUDA, ROCm, XLA, and others.")]),e._v(" "),a("li",[e._v("Backend-specific code is implemented in native directories (e.g., aten/src/ATen/native/cuda/ for CUDA).")])])]),e._v(" "),a("li",[a("p",[e._v("Default Fallbacks:")]),e._v(" "),a("ul",[a("li",[e._v("Provides fallback implementations for some operations when a specific backend implementation is not available.")])])])]),e._v(" "),a("h2",{attrs:{id:"aten-src-aten-core-dispatch"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aten-src-aten-core-dispatch"}},[e._v("#")]),e._v(" ./aten/src/ATen/core/dispatch")]),e._v(" "),a("p",[e._v("Dispatcher")]),e._v(" "),a("details",[a("summary",[e._v("Code")]),e._v(" "),a("div",{staticClass:"language-cpp line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-cpp"}},[a("code",[e._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("/**\n   * Register a new operator schema.\n   *\n   * If a schema with the same operator name and overload name already exists,\n   * this function will check that both schemas are exactly identical.\n   */")]),e._v("\n  RegistrationHandleRAII "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("registerDef")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("FunctionSchema schema"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" std"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[e._v("::")]),e._v("string debug"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" std"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[e._v("::")]),e._v("vector"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("<")]),e._v("at"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[e._v("::")]),e._v("Tag"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" tags "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(";")]),e._v("\n\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("/**\n   * Register a kernel to the dispatch table for an operator.\n   * If dispatch_key is nullopt, then this registers a fallback kernel.\n   *\n   * @return A RAII object that manages the lifetime of the registration.\n   *         Once that object is destructed, the kernel will be deregistered.\n   */")]),e._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("// NB: steals the inferred function schema, as we may need to hold on to")]),e._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("// it for a bit until the real schema turns up")]),e._v("\n  RegistrationHandleRAII "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("registerImpl")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("OperatorName op_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" c10"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[e._v("::")]),e._v("optional"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("<")]),e._v("DispatchKey"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" dispatch_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" KernelFunction kernel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" c10"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[e._v("::")]),e._v("optional"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("<")]),e._v("impl"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[e._v("::")]),e._v("CppSignature"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" cpp_signature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" std"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[e._v("::")]),e._v("unique_ptr"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("<")]),e._v("FunctionSchema"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" inferred_function_schema"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" std"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[e._v("::")]),e._v("string debug"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(";")]),e._v("\n\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("/**\n   * Register a new operator by name.\n   */")]),e._v("\n  RegistrationHandleRAII "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("registerName")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("OperatorName op_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(";")]),e._v("\n\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("/**\n   * Register a fallback kernel for a backend.\n   * If an operator is called but there is no concrete kernel for the dispatch\n   * key of the given operator arguments, it will check if there is such a\n   * fallback kernel for the given dispatch key and, if yes, call that one.\n   */")]),e._v("\n  RegistrationHandleRAII "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("registerFallback")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("DispatchKey dispatch_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" KernelFunction kernel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" std"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[e._v("::")]),e._v("string debug"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(";")]),e._v("\n\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("/**\n   * Use to register whenever we had a TORCH_LIBRARY declaration in the frontend\n   * API.  These invocations are only permitted once per program, so we raise\n   * an error if this is called again for the same namespace.\n   */")]),e._v("\n  RegistrationHandleRAII "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("registerLibrary")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("std"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[e._v("::")]),e._v("string ns"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" std"),a("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[e._v("::")]),e._v("string debug"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(";")]),e._v("\n")])]),e._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[e._v("1")]),a("br"),a("span",{staticClass:"line-number"},[e._v("2")]),a("br"),a("span",{staticClass:"line-number"},[e._v("3")]),a("br"),a("span",{staticClass:"line-number"},[e._v("4")]),a("br"),a("span",{staticClass:"line-number"},[e._v("5")]),a("br"),a("span",{staticClass:"line-number"},[e._v("6")]),a("br"),a("span",{staticClass:"line-number"},[e._v("7")]),a("br"),a("span",{staticClass:"line-number"},[e._v("8")]),a("br"),a("span",{staticClass:"line-number"},[e._v("9")]),a("br"),a("span",{staticClass:"line-number"},[e._v("10")]),a("br"),a("span",{staticClass:"line-number"},[e._v("11")]),a("br"),a("span",{staticClass:"line-number"},[e._v("12")]),a("br"),a("span",{staticClass:"line-number"},[e._v("13")]),a("br"),a("span",{staticClass:"line-number"},[e._v("14")]),a("br"),a("span",{staticClass:"line-number"},[e._v("15")]),a("br"),a("span",{staticClass:"line-number"},[e._v("16")]),a("br"),a("span",{staticClass:"line-number"},[e._v("17")]),a("br"),a("span",{staticClass:"line-number"},[e._v("18")]),a("br"),a("span",{staticClass:"line-number"},[e._v("19")]),a("br"),a("span",{staticClass:"line-number"},[e._v("20")]),a("br"),a("span",{staticClass:"line-number"},[e._v("21")]),a("br"),a("span",{staticClass:"line-number"},[e._v("22")]),a("br"),a("span",{staticClass:"line-number"},[e._v("23")]),a("br"),a("span",{staticClass:"line-number"},[e._v("24")]),a("br"),a("span",{staticClass:"line-number"},[e._v("25")]),a("br"),a("span",{staticClass:"line-number"},[e._v("26")]),a("br"),a("span",{staticClass:"line-number"},[e._v("27")]),a("br"),a("span",{staticClass:"line-number"},[e._v("28")]),a("br"),a("span",{staticClass:"line-number"},[e._v("29")]),a("br"),a("span",{staticClass:"line-number"},[e._v("30")]),a("br"),a("span",{staticClass:"line-number"},[e._v("31")]),a("br"),a("span",{staticClass:"line-number"},[e._v("32")]),a("br"),a("span",{staticClass:"line-number"},[e._v("33")]),a("br"),a("span",{staticClass:"line-number"},[e._v("34")]),a("br"),a("span",{staticClass:"line-number"},[e._v("35")]),a("br"),a("span",{staticClass:"line-number"},[e._v("36")]),a("br"),a("span",{staticClass:"line-number"},[e._v("37")]),a("br"),a("span",{staticClass:"line-number"},[e._v("38")]),a("br")])])]),e._v(" "),a("h1",{attrs:{id:"c10"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#c10"}},[e._v("#")]),e._v(" C10")]),e._v(" "),a("p",[a("strong",[e._v("Purpose")]),e._v('\nC10 (short for "Caffe 2.0") is a lower-level, foundational library within PyTorch. It provides the following:')]),e._v(" "),a("ul",[a("li",[e._v("Device Abstraction: Defines device and tensor metadata.")]),e._v(" "),a("li",[e._v("Dispatcher System: Implements the core dispatch mechanism for routing tensor operations.")]),e._v(" "),a("li",[e._v("Type Management: Handles scalar types, tensor data types, and other utilities.")]),e._v(" "),a("li",[e._v("Utilities: Provides logging, threading, and memory management.")])]),e._v(" "),a("p",[a("strong",[e._v("Key Roles")])]),e._v(" "),a("ul",[a("li",[a("p",[e._v("Dispatcher:")]),e._v(" "),a("ul",[a("li",[e._v("C10's dispatcher is the routing mechanism for PyTorch tensor operations.")]),e._v(" "),a("li",[e._v("Routes function calls (like torch.add) to backend-specific implementations.")]),e._v(" "),a("li",[e._v("Files:\n"),a("ul",[a("li",[e._v("c10/Dispatcher.h: Core dispatcher implementation.")]),e._v(" "),a("li",[e._v("c10/core/KernelFunction.h: Manages kernel function registration.")])])])])]),e._v(" "),a("li",[a("p",[e._v("Device Abstraction:")]),e._v(" "),a("ul",[a("li",[e._v("Provides c10::Device and c10::DeviceType to manage devices like CPU, CUDA, XLA, etc.")]),e._v(" "),a("li",[e._v("Files:\n"),a("ul",[a("li",[e._v("c10/core/Device.h: Device abstraction.")]),e._v(" "),a("li",[e._v("c10/core/DeviceType.h: Enumerates device types.")])])])])]),e._v(" "),a("li",[a("p",[e._v("Type Management:")]),e._v(" "),a("ul",[a("li",[e._v("Handles scalar and tensor data types.")]),e._v(" "),a("li",[e._v("Provides abstractions for c10::ScalarType (e.g., float, int, bool).")]),e._v(" "),a("li",[e._v("Files:\n"),a("ul",[a("li",[e._v("c10/core/ScalarType.h: Scalar type definitions.")]),e._v(" "),a("li",[e._v("c10/core/TensorTypeId.h: Tensor type identifiers.")])])])])]),e._v(" "),a("li",[a("p",[e._v("Memory Management:")]),e._v(" "),a("ul",[a("li",[e._v("Abstracts memory allocation across devices.")]),e._v(" "),a("li",[e._v("Supports custom allocators for tensors (e.g., caching allocators for CUDA).")]),e._v(" "),a("li",[e._v("Files:\n"),a("ul",[a("li",[e._v("c10/core/Allocator.h: Base allocator interface.")]),e._v(" "),a("li",[a("strong",[e._v("c10/cuda/CUDACachingAllocator.cpp: CUDA memory management")]),e._v(".")])])])])]),e._v(" "),a("li",[a("p",[e._v("Threading:")]),e._v(" "),a("ul",[a("li",[e._v("Provides utilities for multi-threaded execution.")]),e._v(" "),a("li",[e._v("Files:\n"),a("ul",[a("li",[e._v("c10/util/ThreadPool.h: Thread pool implementation.")]),e._v(" "),a("li",[e._v("c10/util/ThreadLocal.h: Thread-local storage utilities.")])])])])]),e._v(" "),a("li",[a("p",[e._v("Utilities:")]),e._v(" "),a("ul",[a("li",[e._v("Provides logging, error handling, and common utilities.")]),e._v(" "),a("li",[e._v("Files:\n"),a("ul",[a("li",[e._v("c10/util/Logging.h: Logging macros.")]),e._v(" "),a("li",[e._v("c10/util/Exception.h: Exception handling utilities.")])])])])])]),e._v(" "),a("h2",{attrs:{id:"core"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#core"}},[e._v("#")]),e._v(" core")]),e._v(" "),a("p",[a("strong",[e._v("GeneratorImpl.h")])]),e._v(" "),a("p",[e._v("Random Number Generator")]),e._v(" "),a("blockquote",[a("p",[e._v("A Pseudo Random Number Generator (PRNG) is an engine that uses an algorithm to generate a seemingly random sequence of numbers, that may be later be used in creating a random distribution. "),a("br"),e._v("\nSuch an engine almost always maintains a state and requires a seed to start off the creation of random numbers."),a("br"),e._v("\nOften times, users have found it beneficial to be able to explicitly create, retain, and destroy PRNG states and also be able to have control over the seed value.")])]),e._v(" "),a("h1",{attrs:{id:"torch"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#torch"}},[e._v("#")]),e._v(" torch")]),e._v(" "),a("h2",{attrs:{id:"csrc"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#csrc"}},[e._v("#")]),e._v(" csrc")]),e._v(" "),a("h3",{attrs:{id:"autograd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#autograd"}},[e._v("#")]),e._v(" autograd")]),e._v(" "),a("p",[a("strong",[e._v("variable.h")])]),e._v(" "),a("p",[e._v("Variable")]),e._v(" "),a("p",[e._v("A "),a("code",[e._v("Variable")]),e._v(" augments a "),a("code",[e._v("Tensor")]),e._v(" with the ability to interact in our autograd machinery."),a("br"),e._v("\nConceptually, "),a("code",[e._v("Variable")]),e._v("s travel along "),a("code",[e._v("Edge")]),e._v("s between "),a("code",[e._v("Node")]),e._v("s in the autograd graph."),a("br"),e._v("\nA "),a("code",[e._v("Variable")]),e._v(" can either be a leaf, like a weight in a neural network, or an interior variable, when it is the result of an operation between variables. Every "),a("code",[e._v("Variable")]),e._v(" also stores another "),a("code",[e._v("Variable")]),e._v(" called its "),a("code",[e._v("grad")]),e._v(" (gradient)."),a("br"),e._v("\nIf the variable is a leaf, its\ngradient will be accumulated into this variable.")]),e._v(" "),a("p",[e._v("Every Tensor is a Variable, but sometimes we colloquially refer to Variables that don't require gradients as Tensors (since none of the autograd machinery for Variables applies)."),a("br"),e._v("\nHistorically, Variables and Tensors were separate concepts, but now they are exactly the same (i.e. we have using Variable = at::Tensor`).")]),e._v(" "),a("p",[e._v("Gradient Edges")]),e._v(" "),a("p",[e._v("Furthermore, "),a("code",[e._v("Variable")]),e._v("s have the notion of a "),a("code",[e._v("gradient_edge")]),e._v(", which is the edge in the autograd graph that connects the variable to a particular input\nof the gradient function that will be invoked with the variable during the backward pass.")]),e._v(" "),a("p",[e._v("More precisely, this gradient function can be one of two things:")]),e._v(" "),a("ul",[a("li",[a("ol",[a("li",[e._v("A "),a("code",[e._v("grad_fn")]),e._v(", if the variable is in the interior of the graph. This is the gradient of the function that produced the variable.")])])]),e._v(" "),a("li",[a("ol",{attrs:{start:"2"}},[a("li",[e._v("A "),a("code",[e._v("grad_accumulator")]),e._v(", if the variable is a leaf, which accumulates a scalar gradient value into its "),a("code",[e._v("grad")]),e._v(" variable.")])])])]),e._v(" "),a("p",[e._v("Versioning")]),e._v(" "),a("p",[e._v("Another major feature of "),a("code",[e._v("Variable")]),e._v("s are "),a("em",[e._v("versions")]),e._v(". "),a("br"),e._v("\nVersions are incremented when an in-place mutation of a variable occurs."),a("br"),e._v("\nVersions are useful when constructing "),a("code",[e._v("SavedVariable")]),e._v("s, which take a snapshot of a "),a("code",[e._v("Variable")]),e._v(" at a certain version."),a("br"),e._v("\nYou can retrieve a "),a("code",[e._v("Variable")]),e._v("'s version through its "),a("code",[e._v("current_version()")]),e._v(" method.")]),e._v(" "),a("p",[e._v("Views")]),e._v(" "),a("p",[e._v("It is possible for a  "),a("code",[e._v("Variable")]),e._v(" to be a "),a("em",[e._v("view")]),e._v(" of another "),a("code",[e._v("Variable")]),e._v(", in which case it tracks that "),a("code",[e._v("Variable")]),e._v("'s data and autograd history. Beyond\nconstruction, the interface of a view is identical to that of a regular "),a("code",[e._v("Variable")]),e._v(". "),a("br"),e._v("\nYou can determine whether "),a("code",[e._v("Variable")]),e._v(" is in fact a view by probing its "),a("code",[e._v("is_view()")]),e._v(" method. "),a("br"),e._v("\nNote that the "),a("em",[e._v("view")]),e._v(" semantics are only meaningful for "),a("code",[e._v("Variable")]),e._v(" relations that are relevant to autograd.")])])}),[],!1,null,null,null);a.default=s.exports}}]);