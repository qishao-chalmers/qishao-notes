(window.webpackJsonp=window.webpackJsonp||[]).push([[114],{567:function(t,e,i){"use strict";i.r(e);var n=i(9),s=Object(n.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("ol",[e("li",[t._v("[596] Efficient Steaming Language Models with Attention Sinks")])]),t._v(" "),e("h2",{attrs:{id:"_1-596-2023-efficient-steaming-language-models-with-attention-sinks"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-596-2023-efficient-steaming-language-models-with-attention-sinks"}},[t._v("#")]),t._v(" 1. [596 2023] Efficient Steaming Language Models with Attention Sinks")]),t._v(" "),e("p",[t._v("Attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention.")]),t._v(" "),e("p",[t._v("In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a “sink” even if they are not semantically important.")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/e78d12d3-80c5-4771-bb09-4a39440d089c"}}),t._v(" "),e("p",[t._v("The results show the insufficiency of introducing merely one or two initial tokens, whereas a threshold of four initial tokens appears enough, with subsequent additions contributing marginal effects.")]),t._v(" "),e("p",[t._v("The nature of the SoftMax function prevents all attended tokens from having zero values.")]),t._v(" "),e("p",[t._v("This requires aggregating some information from other tokens across all heads in all layers, even if the current embedding has sufficient self-contained information for its prediction.")]),t._v(" "),e("p",[t._v("Consequently, the model tends to dump unnecessary attention values to specific tokens.")]),t._v(" "),e("p",[t._v("This result justifies our choice of introducing 4 initial tokens as attention sinks in StreamingLLM.")]),t._v(" "),e("p",[t._v("We’ve noted that LLMs are typically trained to utilize multiple initial tokens as attention sinks rather than just one.")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/fb405010-15cb-4e44-aac4-95a91b98df55"}}),t._v(" "),e("p",[t._v("The KV cache in StreamingLLM can be conceptually divided into two parts, as illustrated in Figure 4: (1) Attention sinks (four initial tokens) stabilize the attention computation; 2) Rolling KV Cache retains the most recent tokens, crucial for language modeling.")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/ff71fbb0-f72d-42e1-8dd7-b6cf41f7d3df"}}),t._v(" "),e("p",[t._v("They also test pretraining with a single sink token.")]),t._v(" "),e("p",[t._v("The vanilla model requires the addition of multiple tokens as attention sinks to maintain stable streaming perplexity.")]),t._v(" "),e("p",[t._v("In contrast, the model trained with a sink token achieves satisfactory streaming performance "),e("strong",[t._v("using just the sink token")]),t._v(".")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/96760ee8-9989-483f-8a2e-af2f2894eed2"}})])}),[],!1,null,null,null);e.default=s.exports}}]);