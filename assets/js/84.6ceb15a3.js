(window.webpackJsonp=window.webpackJsonp||[]).push([[84],{539:function(e,t,a){"use strict";a.r(t);var i=a(9),n=Object(i.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("[164] GSLICE: Controlled Spatial Sharing of GPUs for a Scalable Inference Platform — Comprehensive Summary & Analysis")])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_1-gslice-controlled-spatial-sharing-of-gpus-for-a-scalable-inference-platform-comprehensive-summary-analysis"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-gslice-controlled-spatial-sharing-of-gpus-for-a-scalable-inference-platform-comprehensive-summary-analysis"}},[e._v("#")]),e._v(" "),t("strong",[e._v("1. GSLICE: Controlled Spatial Sharing of GPUs for a Scalable Inference Platform — Comprehensive Summary & Analysis")])]),e._v(" "),t("p",[t("em",[e._v("Summarized by Qwen")])]),e._v(" "),t("h3",{attrs:{id:"_1-problem-statement-the-gpu-inference-bottleneck"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-problem-statement-the-gpu-inference-bottleneck"}},[e._v("#")]),e._v(" "),t("strong",[e._v("1. Problem Statement: The GPU Inference Bottleneck")])]),e._v(" "),t("p",[e._v("Modern GPUs (e.g., NVIDIA V100) offer immense computational power (~125 TFLOPS) and memory bandwidth (~900 GBps). However, most Deep Neural Network (DNN) inference models (e.g., ResNet-50, VGG-19) are computationally lightweight in comparison and cannot fully saturate this hardware.")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Traditional Solutions Fail:")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Temporal Sharing (Time-Slicing):")]),e._v(" Runs one model at a time → Low utilization, low throughput.")]),e._v(" "),t("li",[t("strong",[e._v("Batching:")]),e._v(" Improves throughput but increases latency → Violates Service Level Objectives (SLOs) for real-time apps.")]),e._v(" "),t("li",[t("strong",[e._v("Default CUDA MPS (Multi-Process Service):")]),e._v(" Allows concurrent execution but provides "),t("strong",[e._v("uncontrolled spatial sharing")]),e._v(" → Heavy models monopolize the GPU, starving lighter ones → Unpredictable performance.")]),e._v(" "),t("li",[t("strong",[e._v("Static GPU% in MPS:")]),e._v(" Static allocation → Wastes resources when workload is light or starves models when demand spikes.")])])])]),e._v(" "),t("blockquote",[t("p",[e._v("✅ "),t("strong",[e._v("Core Challenge")]),e._v(": Achieve "),t("strong",[e._v("high GPU utilization")]),e._v(", "),t("strong",[e._v("low latency (SLO compliance)")]),e._v(", "),t("strong",[e._v("performance isolation")]),e._v(", and "),t("strong",[e._v("scalability")]),e._v(" simultaneously across diverse, concurrent inference functions (IFs).")])]),e._v(" "),t("h3",{attrs:{id:"_2-gslice-the-solution-intelligent-dynamic-spatial-multiplexing"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-gslice-the-solution-intelligent-dynamic-spatial-multiplexing"}},[e._v("#")]),e._v(" "),t("strong",[e._v("2. GSLICE: The Solution — Intelligent, Dynamic Spatial Multiplexing")])]),e._v(" "),t("p",[e._v("GSLICE is a "),t("strong",[e._v("DPDK-based, framework-agnostic inference platform")]),e._v(" that overcomes these limitations by introducing "),t("strong",[e._v("controlled, adaptive, and isolated spatial sharing")]),e._v(" of the GPU via enhanced CUDA MPS.")]),e._v(" "),t("h4",{attrs:{id:"key-innovations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#key-innovations"}},[e._v("#")]),e._v(" "),t("strong",[e._v("Key Innovations")])]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",{staticStyle:{"text-align":"left"}},[e._v("Innovation")]),e._v(" "),t("th",{staticStyle:{"text-align":"left"}},[e._v("Description")]),e._v(" "),t("th",{staticStyle:{"text-align":"left"}},[e._v("Why It Matters")])])]),e._v(" "),t("tbody",[t("tr",[t("td",{staticStyle:{"text-align":"left"}},[t("strong",[e._v("Self-Tuning GPU Resource Allocation")])]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Dynamically adjusts each IF’s GPU % based on real-time metrics: "),t("br"),e._v(" • "),t("code",[e._v("Residual Latency Capacity = SLO - Observed Latency")]),t("br"),e._v(" • "),t("code",[e._v("Residual Throughput Capacity = Achieved Throughput - Request Arrival Rate")]),t("br"),e._v(" • "),t("strong",[e._v("Logic")]),e._v(": Increase GPU% if either metric is negative; decrease if both are positive and large (>5%).")]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Replaces static, wasteful provisioning with intelligent, feedback-driven resource management. Ensures every IF gets “just enough” to meet its SLO and demand.")])]),e._v(" "),t("tr",[t("td",{staticStyle:{"text-align":"left"}},[t("strong",[e._v("Max-Min Fairness Algorithm")])]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("When reallocating resources, GSLICE prioritizes the IF with the "),t("em",[e._v("lowest")]),e._v(" current GPU% relative to its need.")]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Prevents starvation of lighter or less demanding models. Guarantees fairness while maximizing aggregate system throughput.")])]),e._v(" "),t("tr",[t("td",{staticStyle:{"text-align":"left"}},[t("strong",[e._v("Shadow IF + Overlapped Execution")])]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("For every active IF, GSLICE maintains a "),t("strong",[e._v("shadow (hot-standby) IF")]),e._v(" on the same CPU core. When GPU% needs adjustment:"),t("br"),e._v(" 1. Configure shadow IF with new GPU%."),t("br"),e._v(" 2. Load model onto GPU on shadow."),t("br"),e._v(" 3. Perform seamless switchover (< 100 µs) from primary to shadow IF."),t("br"),e._v(" 4. Terminate old primary IF.")]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Solves the critical problem of "),t("strong",[e._v("~2–15s restart overhead")]),e._v(" in CUDA MPS. Enables dynamic re-provisioning with "),t("strong",[e._v("near-zero downtime")]),e._v(", making adaptation practical.")])]),e._v(" "),t("tr",[t("td",{staticStyle:{"text-align":"left"}},[t("strong",[e._v("Self-Learning Adaptive Batching (SLAB)")])]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Dynamically adjusts batch size based on observed latency, arrival rate, and SLO headroom. Uses EWMA for smoothing.")]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Balances the trade-off between batching’s throughput gains and latency costs. Converges to optimal batch size "),t("strong",[e._v("10x faster")]),e._v(" than systems like Clipper.")])]),e._v(" "),t("tr",[t("td",{staticStyle:{"text-align":"left"}},[t("strong",[e._v("Parameter Sharing Across IFs")])]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Model weights (the largest component of GPU memory) are loaded "),t("strong",[e._v("once")]),e._v(" onto the GPU. Multiple instances of the "),t("em",[e._v("same")]),e._v(" model share these identical parameters.")]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Reduces per-IF GPU memory footprint by 20-50%. Enables multiplexing "),t("strong",[e._v("5–54% more IFs")]),e._v(" on the same GPU. Dramatically reduces model load times (8–10x faster).")])]),e._v(" "),t("tr",[t("td",{staticStyle:{"text-align":"left"}},[t("strong",[e._v("Zero-Copy Data Transfer")])]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Uses DPDK’s pinned memory and GPU DMA to scatter-gather network packets directly into GPU memory, bypassing the CPU.")]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Eliminates costly CPU data copy overheads, especially critical for high-throughput streaming data.")])])])]),e._v(" "),t("h3",{attrs:{id:"_3-the-critical-architectural-choice-why-mps-streams"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-the-critical-architectural-choice-why-mps-streams"}},[e._v("#")]),e._v(" "),t("strong",[e._v("3. The Critical Architectural Choice: Why MPS > Streams")])]),e._v(" "),t("p",[e._v("This is where GSLICE makes a "),t("strong",[e._v("foundational, deliberate, and highly impactful decision")]),e._v(".")]),e._v(" "),t("h4",{attrs:{id:"the-two-paths-to-spatial-sharing"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#the-two-paths-to-spatial-sharing"}},[e._v("#")]),e._v(" "),t("strong",[e._v("The Two Paths to Spatial Sharing")])]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",{staticStyle:{"text-align":"left"}},[e._v("Feature")]),e._v(" "),t("th",{staticStyle:{"text-align":"left"}},[e._v("CUDA Streams")]),e._v(" "),t("th",{staticStyle:{"text-align":"left"}},[e._v("CUDA MPS")])])]),e._v(" "),t("tbody",[t("tr",[t("td",{staticStyle:{"text-align":"left"}},[t("strong",[e._v("Scope")])]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Within a single process.")]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Across multiple processes.")])]),e._v(" "),t("tr",[t("td",{staticStyle:{"text-align":"left"}},[t("strong",[e._v("Mechanism")])]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Multiple command queues within one app. Kernels from different streams can overlap "),t("em",[e._v("if")]),e._v(" they don’t compete for the same resources.")]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Multiple independent applications (processes) run concurrently on the same GPU. Each has its own context.")])]),e._v(" "),t("tr",[t("td",{staticStyle:{"text-align":"left"}},[t("strong",[e._v("Resource Control")])]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("No built-in mechanism to limit compute/memory usage per stream.")]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Can enforce "),t("strong",[e._v("GPU% limits")]),e._v(" (SM allocation) per process via environment variables.")])]),e._v(" "),t("tr",[t("td",{staticStyle:{"text-align":"left"}},[t("strong",[e._v("Isolation")])]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("❌ Poor. Streams from the same app can interfere. One slow kernel blocks others.")]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("✅ Good. Processes are isolated by design. Resource limits provide hard boundaries.")])]),e._v(" "),t("tr",[t("td",{staticStyle:{"text-align":"left"}},[t("strong",[e._v("Use Case Fit for Inference")])]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Mixing different models? Impossible. Only suitable for batching "),t("em",[e._v("within")]),e._v(" one model.")]),e._v(" "),t("td",{staticStyle:{"text-align":"left"}},[e._v("Perfect. Enables running AlexNet, ResNet-50, and VGG-19 as separate processes on one GPU.")])])])]),e._v(" "),t("h4",{attrs:{id:"gslice-s-experimental-findings-streams-are-not-the-answer"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gslice-s-experimental-findings-streams-are-not-the-answer"}},[e._v("#")]),e._v(" "),t("strong",[e._v("GSLICE’s Experimental Findings: Streams Are Not the Answer")])]),e._v(" "),t("p",[e._v("The authors rigorously tested both approaches:")]),e._v(" "),t("ol",[t("li",[t("p",[t("strong",[e._v("Streams Don't Enable True Heterogeneous Multiplexing")]),e._v(":")]),e._v(" "),t("ul",[t("li",[e._v("Experiments showed that when running "),t("strong",[e._v("ResNet-50 and AlexNet concurrently using streams")]),e._v(", their kernels "),t("strong",[e._v("did not overlap meaningfully")]),e._v(" (Figure 6).")]),e._v(" "),t("li",[e._v("The heavier ResNet-50 kernels dominated the SMs, forcing AlexNet to wait. This was effectively "),t("strong",[e._v("time-sharing disguised as spatial sharing")]),e._v(".")]),e._v(" "),t("li",[e._v("Result: Increased latency, no real throughput gain, and poor isolation.")])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Streams Harm Latency")]),e._v(":")]),e._v(" "),t("ul",[t("li",[e._v("Adding more streams beyond 2 consistently increased latency without improving throughput for most models (Figure 5a).")]),e._v(" "),t("li",[e._v("Even for Alexnet, which saw some throughput gain with more streams, latency still increased significantly.")]),e._v(" "),t("li",[e._v("The overhead of managing many streams and context switches negated the benefits.")])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("MPS with Resource Limits Is the Right Tool")]),e._v(":")]),e._v(" "),t("ul",[t("li",[e._v("Figure 7 demonstrates the key insight: By running AlexNet and ResNet-50 as "),t("strong",[e._v("separate MPS processes")]),e._v(", each limited to a fixed GPU% (e.g., 40% and 60%), their kernels "),t("strong",[e._v("could execute concurrently without interference")]),e._v(".")]),e._v(" "),t("li",[e._v("The GPU's SMs were partitioned, ensuring each model had guaranteed access to its allocated compute units.")]),e._v(" "),t("li",[e._v("This provided true "),t("strong",[e._v("performance isolation")]),e._v(' — the latency of one model was unaffected by the other, as long as its allocated GPU% was above its "knee point".')])])])]),e._v(" "),t("blockquote",[t("p",[e._v("🚫 "),t("strong",[e._v("Conclusion from the Paper")]),e._v(":"),t("br"),e._v(" "),t("em",[e._v("“Streams are problematic while multiplexing models with unequal compute requirement... streams do not provide any meaningful overlap... resulting in increased inference task completion time... Therefore, in GSLICE we prefer using MPS with resource provisioning instead of CUDA streams, whenever possible.”")])])]),e._v(" "),t("h4",{attrs:{id:"why-this-decision-was-revolutionary"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#why-this-decision-was-revolutionary"}},[e._v("#")]),e._v(" "),t("strong",[e._v("Why This Decision Was Revolutionary")])]),e._v(" "),t("p",[e._v("Choosing MPS over streams wasn't just a technical preference — it was an "),t("strong",[e._v("architectural commitment to isolation and control")]),e._v(".")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Streams")]),e._v(" optimize for "),t("em",[e._v("parallelism within a single application")]),e._v(".")]),e._v(" "),t("li",[t("strong",[e._v("MPS with limits")]),e._v(" optimizes for "),t("em",[e._v("fair, multi-tenant, heterogeneous resource sharing")]),e._v(" — the exact problem cloud inference platforms face.")])]),e._v(" "),t("p",[e._v("By choosing MPS, GSLICE could leverage NVIDIA’s built-in mechanism for "),t("strong",[e._v("resource partitioning")]),e._v(" (GPU%) and build its self-tuning logic "),t("em",[e._v("on top of it")]),e._v(", rather than fighting against the inherent limitations of streams. This allowed them to deliver "),t("strong",[e._v("guaranteed SLOs")]),e._v(" — something streams fundamentally cannot provide in a mixed-model environment.")]),e._v(" "),t("h3",{attrs:{id:"_4-the-foundational-insight-the-knee-point"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-the-foundational-insight-the-knee-point"}},[e._v("#")]),e._v(" "),t("strong",[e._v('4. The Foundational Insight: The "Knee Point"')])]),e._v(" "),t("p",[e._v("Through extensive profiling of models (AlexNet, ResNet-50, VGG-19, GNMT, Jasper), the authors made a profound discovery:")]),e._v(" "),t("img",{attrs:{width:"564",height:"544",alt:"image",src:"https://github.com/user-attachments/assets/c08c68d8-075d-4272-bc88-410b6f7cb533"}}),e._v(" "),t("blockquote",[t("p",[e._v("✅ "),t("strong",[e._v('Every DNN model has a "knee point"')]),e._v(" — a specific percentage of GPU resources (e.g., 60% for ResNet-50, 70% for VGG-19) beyond which increasing allocation yields only "),t("strong",[e._v("diminishing returns")]),e._v(" in throughput or latency improvement.")])]),e._v(" "),t("h4",{attrs:{id:"why-this-changes-everything"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#why-this-changes-everything"}},[e._v("#")]),e._v(" "),t("strong",[e._v("Why This Changes Everything")])]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Traditional Assumption")]),e._v(" "),t("th",[e._v("GSLICE's Discovery")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("Give each model 100% GPU to maximize its performance.")]),e._v(" "),t("td",[e._v("Even the heaviest models plateau at ~70% GPU.")])]),e._v(" "),t("tr",[t("td",[e._v("Static allocation: “We’ll give VGG-19 80%, ResNet-50 20%.”")]),e._v(" "),t("td",[e._v("The optimal allocation is "),t("strong",[e._v("model-specific")]),e._v(", "),t("strong",[e._v("non-linear")]),e._v(", and "),t("strong",[e._v("not intuitive")]),e._v(".")])]),e._v(" "),t("tr",[t("td",[e._v("Resource waste is inevitable.")]),e._v(" "),t("td",[e._v("You can "),t("strong",[e._v("save 30–40% GPU per model")]),e._v(" without hurting performance.")])])])]),e._v(" "),t("p",[t("strong",[e._v("The Impact:")])]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Massive Efficiency Gain")]),e._v(": If you run 5 models on one GPU, and each only needs 60% instead of 100%, you’re effectively saving "),t("strong",[e._v("200% worth of GPU capacity")]),e._v(".")]),e._v(" "),t("li",[t("strong",[e._v("Enables Multiplexing")]),e._v(": Without knowing the knee point, spatial sharing would be guesswork. With it, you know exactly how much “headroom” each model has — making controlled multiplexing "),t("em",[e._v("scientific")]),e._v(", not heuristic.")]),e._v(" "),t("li",[t("strong",[e._v("Foundation for Self-Tuning")]),e._v(": The knee point isn’t just a number — it’s the "),t("strong",[e._v("baseline demand")]),e._v(" used by GSLICE’s self-tuning algorithm to determine if an IF is over- or under-provisioned.")])]),e._v(" "),t("blockquote",[t("p",[e._v("✅ "),t("strong",[e._v("Think of it like this")]),e._v(":"),t("br"),e._v("\nYou wouldn’t give a sedan the full engine power of a Formula 1 car — even if it could handle it. The “knee point” tells you "),t("em",[e._v("exactly")]),e._v(" how much engine power each car needs to reach its top speed — no more, no less.")])]),e._v(" "),t("h3",{attrs:{id:"_5-the-engineering-masterpiece-shadow-if"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-the-engineering-masterpiece-shadow-if"}},[e._v("#")]),e._v(" "),t("strong",[e._v("5. The Engineering Masterpiece: Shadow IF")])]),e._v(" "),t("h4",{attrs:{id:"the-problem"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#the-problem"}},[e._v("#")]),e._v(" "),t("strong",[e._v("The Problem")])]),e._v(" "),t("p",[e._v("In CUDA MPS, changing a process’s GPU% requires "),t("strong",[e._v("restarting the entire process")]),e._v(". Restarting a PyTorch/TensorFlow/TensorRT inference process takes "),t("strong",[e._v("2–15 seconds")]),e._v(" — completely unacceptable for real-time systems.")]),e._v(" "),t("p",[t("strong",[e._v("Result:")]),e._v(" Dynamic resource allocation was a theoretical dream.")]),e._v(" "),t("h4",{attrs:{id:"gslice-s-solution-shadow-if-overlapped-execution"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gslice-s-solution-shadow-if-overlapped-execution"}},[e._v("#")]),e._v(" "),t("strong",[e._v("GSLICE’s Solution: Shadow IF + Overlapped Execution")])]),e._v(" "),t("p",[e._v("Here’s the genius:")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Step")]),e._v(" "),t("th",[e._v("What Happens")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[t("strong",[e._v("1. Active IF")])]),e._v(" "),t("td",[e._v("Running normally, serving requests, using 30% GPU.")])]),e._v(" "),t("tr",[t("td",[t("strong",[e._v("2. Shadow IF")])]),e._v(" "),t("td",[e._v("A "),t("em",[e._v("hot standby")]),e._v(" instance running on the same CPU core, with identical buffers, framework, and code — "),t("strong",[e._v("but NOT accessing GPU")]),e._v(". Zero overhead.")])]),e._v(" "),t("tr",[t("td",[t("strong",[e._v("3. Need to Increase GPU%?")])]),e._v(" "),t("td",[e._v("GSLICE configures the "),t("em",[e._v("shadow")]),e._v(" IF with the new target (e.g., 70%). It begins loading the model weights onto the GPU… "),t("strong",[e._v("while the active IF keeps serving requests")]),e._v(".")])]),e._v(" "),t("tr",[t("td",[t("strong",[e._v("4. Switchover")])]),e._v(" "),t("td",[e._v("When the shadow IF is ready, the IF Manager does a near-instantaneous (<100µs) handoff: "),t("br"),e._v(" - Stops the active IF after its current batch. "),t("br"),e._v(" - Promotes the shadow IF to active. "),t("br"),e._v(" - Terminates the old active IF.")])]),e._v(" "),t("tr",[t("td",[t("strong",[e._v("5. Result")])]),e._v(" "),t("td",[e._v("GPU% changed from 30% → 70%. "),t("strong",[e._v("No downtime. No dropped requests. SLO intact.")])])])])]),e._v(" "),t("h4",{attrs:{id:"why-this-is-so-impressive"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#why-this-is-so-impressive"}},[e._v("#")]),e._v(" "),t("strong",[e._v("Why This Is So Impressive")])]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("Feature")]),e._v(" "),t("th",[e._v("Why It Matters")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[t("strong",[e._v("Zero-Downtime")])]),e._v(" "),t("td",[e._v("Achieves what was thought impossible: dynamic reconfiguration of GPU resources without service interruption.")])]),e._v(" "),t("tr",[t("td",[t("strong",[e._v("Hidden Latency")])]),e._v(" "),t("td",[e._v("Masks the 2–15s model load time by overlapping it with live traffic.")])]),e._v(" "),t("tr",[t("td",[t("strong",[e._v("Minimal Overhead")])]),e._v(" "),t("td",[e._v("Shadow IF uses only CPU memory and cores — no extra GPU, no network, no data copies.")])]),e._v(" "),t("tr",[t("td",[t("strong",[e._v("Scalable")])]),e._v(" "),t("td",[e._v("Can be applied to any ML framework (PyTorch, TF, TensorRT) with minimal changes (<30 lines).")])]),e._v(" "),t("tr",[t("td",[t("strong",[e._v("Elegant Simplicity")])]),e._v(" "),t("td",[e._v("No kernel-level hacking. No CUDA driver modifications. Just smart software orchestration.")])])])]),e._v(" "),t("blockquote",[t("p",[e._v("✅ "),t("strong",[e._v("Think of it like upgrading a jet engine mid-flight:")]),t("br"),e._v("\nYou don’t land the plane. You have a second, identical engine already spinning up on the wing. At the perfect moment, you flip the switch — and the plane continues flying, now with 2x thrust.")])]),e._v(" "),t("p",[e._v("This is "),t("strong",[e._v("systems engineering at its finest")]),e._v(": solving a hardware limitation with pure software ingenuity.")]),e._v(" "),t("h3",{attrs:{id:"_6-synergy-the-triad-of-brilliance"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-synergy-the-triad-of-brilliance"}},[e._v("#")]),e._v(" "),t("strong",[e._v("6. Synergy: The Triad of Brilliance")])]),e._v(" "),t("p",[e._v("These three innovations don't just coexist — they "),t("strong",[e._v("amplify each other")]),e._v(":")]),e._v(" "),t("ol",[t("li",[t("strong",[e._v("Knee Point")]),e._v(" tells you "),t("strong",[e._v("how much")]),e._v(" GPU each model "),t("em",[e._v("needs")]),e._v(".")]),e._v(" "),t("li",[t("strong",[e._v("Shadow IF")]),e._v(" lets you "),t("strong",[e._v("dynamically give it that amount")]),e._v(", instantly and safely.")]),e._v(" "),t("li",[t("strong",[e._v("MPS with Resource Limits")]),e._v(" provides the "),t("strong",[e._v("underlying, controllable infrastructure")]),e._v(" that makes #1 and #2 possible.")])]),e._v(" "),t("p",[e._v("Together, they enable "),t("strong",[e._v("true elasticity")]),e._v(" for GPU inference:")]),e._v(" "),t("ul",[t("li",[e._v("A sudden spike in image requests? → GSLICE detects latency pressure → increases VGG-19’s GPU% from 60% → 75% via shadow IF → SLO maintained.")]),e._v(" "),t("li",[e._v("Traffic drops? → Reduces GPU% → frees capacity for a new IF.")]),e._v(" "),t("li",[e._v("New model added? → Load its weights once → share parameters → spin up shadow IF → deploy with zero delay.")])]),e._v(" "),t("h3",{attrs:{id:"_7-architecture-overview"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-architecture-overview"}},[e._v("#")]),e._v(" "),t("strong",[e._v("7. Architecture Overview")])]),e._v(" "),t("img",{attrs:{width:"645",height:"439",alt:"image",src:"https://github.com/user-attachments/assets/b83bca76-6759-41f4-9902-a6a1daa6fafd"}}),e._v(" "),t("ul",[t("li",[t("strong",[e._v("IF Manager")]),e._v(": DPDK process handling network I/O and coordinating IF lifecycle.")]),e._v(" "),t("li",[t("strong",[e._v("libml")]),e._v(": Lightweight C/C++ library providing core services (batching, zero-copy, resource mgmt) abstracted from ML frameworks.")]),e._v(" "),t("li",[t("strong",[e._v("Orchestrator")]),e._v(": Manages creation/destruction of IFs.")]),e._v(" "),t("li",[t("strong",[e._v("Shadow IFs")]),e._v(": Hot-standby instances enabling zero-downtime re-provisioning.")]),e._v(" "),t("li",[t("strong",[e._v("Supports")]),e._v(": PyTorch, TensorFlow, TensorRT, CNTK, MXNet, Darknet (minimal changes required).")])]),e._v(" "),t("h3",{attrs:{id:"_9-conclusion-a-paradigm-shift"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_9-conclusion-a-paradigm-shift"}},[e._v("#")]),e._v(" "),t("strong",[e._v("9. Conclusion: A Paradigm Shift")])]),e._v(" "),t("p",[e._v("GSLICE is not merely an incremental improvement—it represents a "),t("strong",[e._v("paradigm shift")]),e._v(" in GPU inference infrastructure.")]),e._v(" "),t("p",[e._v("It successfully solves the long-standing trilemma:")]),e._v(" "),t("blockquote",[t("p",[t("strong",[e._v("High Utilization")]),e._v(" + "),t("strong",[e._v("Low Latency (SLO Compliance)")]),e._v(" + "),t("strong",[e._v("Performance Isolation")])])]),e._v(" "),t("p",[e._v("By combining "),t("strong",[e._v("self-tuning resource allocation")]),e._v(", "),t("strong",[e._v("zero-downtime re-provisioning")]),e._v(", "),t("strong",[e._v("adaptive batching")]),e._v(", "),t("strong",[e._v("parameter sharing")]),e._v(", and "),t("strong",[e._v("zero-copy I/O")]),e._v(", GSLICE achieves unprecedented levels of efficiency and scalability. It transforms the GPU from a static, underutilized asset into a "),t("strong",[e._v("dynamic, intelligent, multi-tenant, high-performance computing engine")]),e._v(" capable of serving hundreds of diverse, real-time inference applications concurrently.")]),e._v(" "),t("p",[e._v("Its impact lies not only in its performance numbers but in proving that "),t("strong",[e._v("intelligent, software-defined control")]),e._v(" can unlock the full potential of commodity hardware for the next generation of cloud AI services.")]),e._v(" "),t("h3",{attrs:{id:"✅-final-takeaway"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#✅-final-takeaway"}},[e._v("#")]),e._v(" ✅ "),t("strong",[e._v("Final Takeaway")])]),e._v(" "),t("blockquote",[t("p",[t("strong",[e._v("The Knee Point")]),e._v(" revealed a hidden truth about DNN efficiency — "),t("strong",[e._v("less is more")]),e._v("."),t("br"),e._v(" "),t("strong",[e._v("The Shadow IF")]),e._v(" solved a seemingly unsolvable systems problem — "),t("strong",[e._v("zero-downtime reconfiguration")]),e._v("."),t("br"),e._v(" "),t("strong",[e._v("The MPS > Streams Choice")]),e._v(" was the foundational architectural decision that made all of this possible — "),t("strong",[e._v("true isolation for heterogeneous workloads")]),e._v(".")])]),e._v(" "),t("hr"),e._v(" "),t("p",[e._v("These three elements together form a masterpiece of systems research: elegant, practical, transformative, and deeply insightful.")])])}),[],!1,null,null,null);t.default=n.exports}}]);