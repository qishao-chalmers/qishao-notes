(window.webpackJsonp=window.webpackJsonp||[]).push([[122],{574:function(t,e,i){"use strict";i.r(e);var a=i(9),s=Object(a.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("ol",[e("li",[t._v("[596 Y2023] Efficient Steaming Language Models with Attention Sinks")]),t._v(" "),e("li",[t._v("[Y2025] When Attention Sink Emerges in Language Models")]),t._v(" "),e("li",[t._v("[192 Y2024] Massive Activations in Large Language Models")]),t._v(" "),e("li",[t._v("[2025] A Refined Analysis of Massive Activations in LLMs")])]),t._v(" "),e("h2",{attrs:{id:"_1-596-y2023-efficient-steaming-language-models-with-attention-sinks"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-596-y2023-efficient-steaming-language-models-with-attention-sinks"}},[t._v("#")]),t._v(" 1. [596 Y2023] Efficient Steaming Language Models with Attention Sinks")]),t._v(" "),e("p",[t._v("Attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention.")]),t._v(" "),e("p",[t._v("In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a “sink” even if they are not semantically important.")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/e78d12d3-80c5-4771-bb09-4a39440d089c"}}),t._v(" "),e("p",[t._v("The results show the insufficiency of introducing merely one or two initial tokens, whereas a threshold of four initial tokens appears enough, with subsequent additions contributing marginal effects.")]),t._v(" "),e("p",[t._v("The nature of the SoftMax function prevents all attended tokens from having zero values.")]),t._v(" "),e("p",[t._v("This requires aggregating some information from other tokens across all heads in all layers, even if the current embedding has sufficient self-contained information for its prediction.")]),t._v(" "),e("p",[t._v("Consequently, the model tends to dump unnecessary attention values to specific tokens.")]),t._v(" "),e("p",[t._v("This result justifies our choice of introducing 4 initial tokens as attention sinks in StreamingLLM.")]),t._v(" "),e("p",[t._v("We’ve noted that LLMs are typically trained to utilize multiple initial tokens as attention sinks rather than just one.")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/fb405010-15cb-4e44-aac4-95a91b98df55"}}),t._v(" "),e("p",[t._v("The KV cache in StreamingLLM can be conceptually divided into two parts, as illustrated in Figure 4: (1) Attention sinks (four initial tokens) stabilize the attention computation; 2) Rolling KV Cache retains the most recent tokens, crucial for language modeling.")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/ff71fbb0-f72d-42e1-8dd7-b6cf41f7d3df"}}),t._v(" "),e("p",[t._v("They also test pretraining with a single sink token.")]),t._v(" "),e("p",[t._v("The vanilla model requires the addition of multiple tokens as attention sinks to maintain stable streaming perplexity.")]),t._v(" "),e("p",[t._v("In contrast, the model trained with a sink token achieves satisfactory streaming performance "),e("strong",[t._v("using just the sink token")]),t._v(".")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/96760ee8-9989-483f-8a2e-af2f2894eed2"}}),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"_2-y2025-when-attention-sink-emerges-in-language-models"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-y2025-when-attention-sink-emerges-in-language-models"}},[t._v("#")]),t._v(" 2. [Y2025] When Attention Sink Emerges in Language Models")]),t._v(" "),e("p",[e("strong",[t._v("Background")])]),t._v(" "),e("p",[t._v("Xiao et al. (2023b) revealed that LLMs allocate significant attention scores to specific token positions, e.g. the first token (not necessary to be a BOS token), resulting in “vertical” attention patterns.")]),t._v(" "),e("p",[e("strong",[t._v("Contribution")])]),t._v(" "),e("ul",[e("li",[t._v("Attention sink emerges after LMs are trained effectively on sufficient training data.\nIt appears less obvious in LMs trained with small learning rates."),e("br"),t._v("\nWhile weight decay encourages the emergence of attention sink.")]),t._v(" "),e("li",[t._v("The sink position is highly related to the loss function and data distribution and can be shifted to other positions rather than the first token.")]),t._v(" "),e("li",[t._v("Attention sink acts more like key biases, storing extra attention and meanwhile not contributing to the value computation.\nThis phenomenon (at least partially) stems from tokens’inner dependence on attention scores due to the softmax normalization."),e("br"),t._v("\nAfter relaxing such dependence by "),e("strong",[t._v("replacing softmax attention with other attention operations, e.g., sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters")]),t._v(".")])]),t._v(" "),e("h3",{attrs:{id:"what-is-attention-sink"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#what-is-attention-sink"}},[t._v("#")]),t._v(" What is Attention Sink?")]),t._v(" "),e("p",[t._v("An attention sink is when tokens disproportionately attend to the first token (often the BOS token or first word), even when it holds little semantic importance.")]),t._v(" "),e("p",[t._v("This creates a vertical attention pattern, frequently leveraged for efficiency in:")]),t._v(" "),e("ul",[e("li",[t._v("Streaming inference")]),t._v(" "),e("li",[t._v("KV cache optimization")]),t._v(" "),e("li",[t._v("Quantization-aware training")])]),t._v(" "),e("h3",{attrs:{id:"main-contributions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#main-contributions"}},[t._v("#")]),t._v(" Main Contributions")]),t._v(" "),e("h4",{attrs:{id:"empirical-demonstration-of-universality"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#empirical-demonstration-of-universality"}},[t._v("#")]),t._v(" Empirical Demonstration of Universality")]),t._v(" "),e("p",[t._v("Attention sink appears across LMs (GPT2-XL, LLaMA2/3, Mistral) — even with:")]),t._v(" "),e("ul",[e("li",[t._v("Random token sequences")]),t._v(" "),e("li",[t._v("Small-scale models\nThis suggests the phenomenon is model-agnostic and tied to the training process, not the data or size.")])]),t._v(" "),e("h4",{attrs:{id:"mechanistic-understanding"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#mechanistic-understanding"}},[t._v("#")]),t._v(" Mechanistic Understanding")]),t._v(" "),e("p",[t._v("First token’s key vector acts like a bias: due to angle alignment with other queries, not due to large vector norms.")]),t._v(" "),e("p",[t._v("The cosine similarity, not norm product, drives attention sink — meaning high attention persists even with small key norms.")]),t._v(" "),e("h4",{attrs:{id:"emergence-during-pretraining"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#emergence-during-pretraining"}},[t._v("#")]),t._v(" Emergence During Pretraining")]),t._v(" "),e("p",[t._v("Attention sink becomes prominent after sufficient optimization.")]),t._v(" "),e("p",[t._v("It's less obvious with:")]),t._v(" "),e("ul",[e("li",[t._v("Fewer training steps")]),t._v(" "),e("li",[t._v("Smaller learning rates")]),t._v(" "),e("li",[t._v("Less training data")])]),t._v(" "),e("h4",{attrs:{id:"influence-of-data-distribution"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#influence-of-data-distribution"}},[t._v("#")]),t._v(" Influence of Data Distribution")]),t._v(" "),e("p",[t._v("Sink position can shift:")]),t._v(" "),e("ul",[e("li",[t._v("From token 1 to 2 if token 1 is randomized")]),t._v(" "),e("li",[t._v("To fixed tokens if injected intentionally")]),t._v(" "),e("li",[e("strong",[t._v("repeated tokens in input suppress attention sink in relative positional encoding models (e.g., LLaMA)")]),t._v(".")])]),t._v(" "),e("h4",{attrs:{id:"loss-function-optimization-effects"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#loss-function-optimization-effects"}},[t._v("#")]),t._v(" Loss Function & Optimization Effects")]),t._v(" "),e("ul",[e("li",[t._v("Weight decay encourages attention sink, but excessive decay suppresses it by harming learning.")]),t._v(" "),e("li",[t._v("In prefix LM tasks, sink moves from token 1 to the prefix span.")]),t._v(" "),e("li",[t._v("Shifted window attention (e.g., in Mistral) localizes the sink to absolute positions — not relative ones.")])]),t._v(" "),e("h4",{attrs:{id:"attention-sink-key-biases"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#attention-sink-key-biases"}},[t._v("#")]),t._v(" Attention Sink = Key Biases")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/1faf375f-a01c-4b7d-af36-8d41e2f5b1fa"}}),t._v(" "),e("p",[t._v("We further show that due to the different manifold of $\\mathbf{k}_1^{l,h}$, the angles between $\\mathbf{k}_1^{l,h}$ and $\\mathbf{q}_t^{l,h}$ play an important role. Considering\n$\\mathbf{q}_t^{l,h} {\\mathbf{k}_j^{l,h}}^\\top = |\\mathbf{q}_t^{l,h}| \\cdot |\\mathbf{k}_j^{l,h}| \\cdot \\cos(\\mathbf{q}_t^{l,h}, \\mathbf{k}_j^{l,h}),$")]),t._v(" "),e("p",[t._v("we visualize the cosine similarity between keys and values, and the product of $\\ell_2$-norm between keys and values in Figure 2("),e("em",[t._v("Bottom")]),t._v(")** .")]),t._v(" "),e("blockquote",[e("p",[e("strong",[t._v("Although")]),t._v("  $$|\\mathbf{q}_t^{l,h}| \\cdot |\\mathbf{k}_1^{l,h}|$$ "),e("strong",[t._v("is comparatively small,")])])]),t._v(" "),e("p",[t._v("$$\\cos(\\mathbf{q}_t^{l,h}, \\mathbf{k}_1^{l,h})$$ "),e("strong",[t._v("is significantly large, leading to attention sink.")])]),t._v(" "),e("p",[t._v("This explains why attention sink exists despite the small $$\\ell_2$$-norm of keys of the first token.")]),t._v(" "),e("p",[t._v("To conclude, the first token leverages its keys to act as biases, thus minimizing the angles between $$\\mathbf{k}_1^{l,h}$$ and $$\\mathbf{q}_t^{l,h}$$, and "),e("strong",[t._v("exhibiting attention sink")]),t._v(" .")]),t._v(" "),e("ul",[e("li",[t._v("Attention sink does not contribute to value computation, acting more like a "),e("strong",[t._v("key-space artifact")]),t._v(".")]),t._v(" "),e("li",[t._v("Simply "),e("strong",[t._v("adding key biases (even without value biases) shifts the sink away from real tokens, proving it’s an optimization artifact")]),t._v(".")])]),t._v(" "),e("h4",{attrs:{id:"attention-sink-under-different-inputs"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#attention-sink-under-different-inputs"}},[t._v("#")]),t._v(" Attention Sink Under Different Inputs")]),t._v(" "),e("ul",[e("li",[t._v("input domains have negligible effects on our attention sink metric Sinkϵ1")]),t._v(" "),e("li",[t._v("rrandomly sample T tokens from the tokenizer vocabulary V to construct a sequence")]),t._v(" "),e("li",[t._v("randomly sample 1 token from the tokenizer V and repeat it T times.\nAs present in Table 1(Left), attention sink still exists when the inputs are "),e("strong",[t._v("random tokens instead of natural language")]),t._v(".")])]),t._v(" "),e("p",[t._v("However, with repeated tokens, attention sink in Mistral (Jiang et al., 2023) and LLaMA models disappears.\n"),e("strong",[t._v("we prove that for LMs with NoPE/relative PE/ALiBI/Rotary, if the first T tokens are the same, their corresponding hidden states are the same. They all have massive activations, thus dispersing the attention sink.")])]),t._v(" "),e("h4",{attrs:{id:"effects-of-optimization-on-attention-sink"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#effects-of-optimization-on-attention-sink"}},[t._v("#")]),t._v(" Effects of Optimization on Attention Sink")]),t._v(" "),e("ul",[e("li",[t._v("Attention sink emerges after LMs are trained effectively.")]),t._v(" "),e("li",[t._v("Attention sink appears less obvious in LMs trained with small learning rates.")])]),t._v(" "),e("h4",{attrs:{id:"effects-of-data-distribution-pdata-on-attention-sink"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#effects-of-data-distribution-pdata-on-attention-sink"}},[t._v("#")]),t._v(" Effects of Data Distribution pData on Attention Sink")]),t._v(" "),e("ol",[e("li",[t._v("Attention sink emerges after LMs are trained on sufficient training data.")]),t._v(" "),e("li",[t._v("Attention sink could be shifted to other positions rather than the first token if modifying pdata.")])]),t._v(" "),e("h4",{attrs:{id:"effects-of-loss-function-on-attention-sink"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#effects-of-loss-function-on-attention-sink"}},[t._v("#")]),t._v(" Effects of Loss Function on Attention Sink")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/c2dbbdd3-68f5-4b76-8e5f-e3cc788fd0ae"}}),t._v(" "),e("ol",[e("li",[t._v("Weight decay encourages the emergence of attention sink.")]),t._v(" "),e("li",[t._v("With prefix language modeling, attention sink appears among the prefix tokens rather than the first token only.")]),t._v(" "),e("li",[t._v("With shifted window attention, attention sink appears on the “absolute”, not the “relative” first token. Smaller window size prevents the emergence of attention sink.")])]),t._v(" "),e("h4",{attrs:{id:"effects-of-model-architecture-on-attention-sink"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#effects-of-model-architecture-on-attention-sink"}},[t._v("#")]),t._v(" Effects of Model Architecture on Attention Sink")]),t._v(" "),e("ul",[e("li",[t._v("we note that all these LMs, even the one without explicit PE (NoPE), have attention sink.")])]),t._v(" "),e("h5",{attrs:{id:"attention-bias"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#attention-bias"}},[t._v("#")]),t._v(" Attention Bias")]),t._v(" "),e("p",[t._v("considered a learnable sink token in each chunk before the input tokens during LM pre-training."),e("br"),t._v("\nAs this token is fixed in the first token, this could be considered as implicitly introducing biases k,v,q in attention."),e("br"),t._v("\nas long as there are key biases k*"),e("sup",[t._v("l,h")]),t._v(" attention sink disappears on the first token but on the biases.")]),t._v(" "),e("p",[e("strong",[t._v("So they prove that v*"),e("sup",[t._v("l,h")]),t._v(" is not important, could just be zero.")])]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/3f9afb16-8169-4262-a8a8-665574b1d971"}}),t._v(" "),e("ol",[e("li",[t._v("Positional embedding, FFN design, LN location, and multi-head design do not affect the emergence of attention sink.")]),t._v(" "),e("li",[t._v("Attention sink acts more like key biases, storing extra attention and meanwhile not contributing to the value computation.")]),t._v(" "),e("li",[e("strong",[t._v("When relaxing tokens’ inner dependence on attention scores, attention sink does not emerge in LMs.")]),t._v(" "),e("strong",[t._v("We note that the LMs with no attention sink typically relax tokens’ inner dependence on attention scores.")]),e("br"),t._v("\nTheir attention scores during pre-training could be negative or not add up to one."),e("br"),t._v("\nThis indicated that attention sink (at least partially) stems from such inner dependence."),e("br"),t._v("\nBesides the attention metric computed by proxy attention scores, we also observe that the above LMs also have no massive activations.")])]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/4b0d5fe4-940b-45b9-a9bd-6f151dd163b5"}}),t._v(" "),e("h4",{attrs:{id:"role-of-attention-normalization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#role-of-attention-normalization"}},[t._v("#")]),t._v(" Role of Attention Normalization")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Softmax normalization creates inter-token dependence, reinforcing attention sink")])]),t._v(" "),e("li",[e("strong",[t._v("Replacing softmax with sigmoid or non-normalized attention (e.g., ELU+1) eliminates attention sink — even in 1B-parameter models")])])]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"_3-192-y2024-massive-activations-in-large-language-models"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-192-y2024-massive-activations-in-large-language-models"}},[t._v("#")]),t._v(" 3. [192 Y2024] Massive Activations in Large Language Models")]),t._v(" "),e("ul",[e("li",[t._v("Certain activations exhibit huge magnitudes, e.g., more than 4 orders of magnitude larger than the median.\n"),e("ul",[e("li",[t._v("These activations are also extremely rare, often numbering fewer than 10 among tens of millions of total activations.")])])]),t._v(" "),e("li",[t._v("Regarding the depth dimension of LLMs, the appearance of massive activations is mostly abrupt: they emerge suddenly after a single layer of computation, and diminish at the last few layers.\n"),e("ul",[e("li",[t._v("Further, we find massive activations occur in a small number of feature dimensions that are input agnostic.")]),t._v(" "),e("li",[t._v("Many of these activations are found within the starting word token and delimiter tokens.")]),t._v(" "),e("li",[t._v("Additionally, we show that massive activations are not the same as outlier features (Dettmers et al., 2022), a previously known phenomenon in LLMs.")])])]),t._v(" "),e("li",[t._v("Massive activations act as fixed but crucial bias terms in LLMs.\n"),e("ul",[e("li",[t._v("Certain internal states of the models that are independent from the inputs, analogous to the bias term b in a linear layer y = W x + b.")]),t._v(" "),e("li",[t._v("First, we show that massive activations play a critical role in LLMs’ capabilities. For instance, in LLaMA2-7B, setting merely four massive activations (out of millions of activations) to zero would result in catastrophic collapse in model performance.")]),t._v(" "),e("li",[t._v("Further, "),e("strong",[t._v("setting them to their mean values does not hurt the model, suggesting their role is equivalent to simple constant biases")]),t._v(".")]),t._v(" "),e("li",[t._v("Our analysis reveals that after the initial layers, LLMs repurpose the tokens linked with massive activations to store these important biases.")])])]),t._v(" "),e("li",[t._v("Massive activations are closely connected with self-attention.\n"),e("ul",[e("li",[t._v("In particular, we show massive activations cause attention to be attracted to the tokens associated with them.")]),t._v(" "),e("li",[t._v("Our findings extend the observations from “attention sinks” (Xiao et al., 2023b)—we demonstrate that LLMs allocate excessive attention to more than just the first token, and provide an in-depth analysis on how such attention concentration patterns arise.")]),t._v(" "),e("li",[t._v("Our analysis suggests that LLMs try to learn implicit bias components in self-attention via massive activations, during their pretraining phase.")]),t._v(" "),e("li",[t._v("We thus experiment with augmenting self-attention with additional key and value embeddings that are explicitly designed as biases.")]),t._v(" "),e("li",[t._v("Remarkably, we demonstrate that training with them eliminates the need for LLMs to learn massive activations.")])])])]),t._v(" "),e("p",[t._v('<img src="https://github.com/user-attachments/assets/96b97b21-81a5-41fb-af6d-dd382b7beaa1)')]),t._v(" "),e("p",[t._v("The most notable property is that these activations possess massive values and their magnitudes are significantly larger than other activations, often several orders of magnitude larger than the median value.")]),t._v(" "),e("p",[t._v("Another property is that they are exceptionally few in number."),e("br"),t._v("\nFor LLaMA2-7B in Figure 1, there are approximately 40,000 total activations in each presented hidden state but at most four massive activations can be identified.")]),t._v(" "),e("p",[e("strong",[t._v("an activation qualifies as a massive activation if its magnitude surpasses 100 and is at least or around 1,000 times larger than the median magnitude of its hidden state.")])]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/efa19455-5a67-4705-aa35-50879093f7fc"}}),t._v(" "),e("h3",{attrs:{id:"which-layers"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#which-layers"}},[t._v("#")]),t._v(" Which Layers?")]),t._v(" "),e("p",[e("em",[t._v("Massive activations exist and remain as largely constant values throughout most of the intermediate layers.")])]),t._v(" "),e("p",[e("em",[t._v("They emerge in the initial layers and start to diminish in the last few layers.")])]),t._v(" "),e("h3",{attrs:{id:"which-feature-and-sequence-dimensions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#which-feature-and-sequence-dimensions"}},[t._v("#")]),t._v(" Which Feature and Sequence Dimensions?")]),t._v(" "),e("p",[t._v("We find that massive activations appear at:")]),t._v(" "),e("ol",[e("li",[t._v("the starting word token,")]),t._v(" "),e("li",[t._v("the token representing the first period (.) or newline token (\\n) in the sequence.")]),t._v(" "),e("li",[t._v("These activations are exclusively located within the starting token of the sequence, regardless of its semantics.")]),t._v(" "),e("li",[t._v("They are associated with the starting token, delimiter tokens and also certain word tokens, e.g., token “and” and token “of”.")])]),t._v(" "),e("ul",[e("li",[t._v("For feature dimensions, massive activations are consistently present in very few fixed dimensions.")]),t._v(" "),e("li",[t._v("For sequence dimensions, we classify LLMs into three categories based on massive activations’ locations:\n"),e("ul",[e("li",[t._v("Starting token only.\nModels include LLaMA2-13B, MPT and GPT-2.")]),t._v(" "),e("li",[t._v("Starting token and the first “strong” delimiter token (i.e., “.” or “\\n”)\nModels include LLaMA2-7B and LLaMA2-7B-Chat.")]),t._v(" "),e("li",[t._v("Starting token, delimiter tokens (such as “.”, “\\n”, “’” or “,”), and certain word tokens with weak semantics (such as “and”, “from”, “of” or “2”2) "),e("br"),t._v("\nModels include LLaMA2-70B, Mistral-7B, Mixtral-8x7B, Falcon-40B and Phi-2.")])])])]),t._v(" "),e("h3",{attrs:{id:"difference-from-outlier"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#difference-from-outlier"}},[t._v("#")]),t._v(" Difference from Outlier")]),t._v(" "),e("p",[e("em",[t._v("Conceptually, a massive activation is a scalar value, determined jointly by the sequence and feature dimensions ; in contrast, an outlier feature is a vector, corresponding to activations at all tokens. Further, massive activations are present at extremely few tokens, while outlier features expect most activations in them to be large. In practice, we find that massive activations do not overlap with outlier feature dimensions.")])]),t._v(" "),e("h3",{attrs:{id:"massive-activations-act-as-biases-in-llms"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#massive-activations-act-as-biases-in-llms"}},[t._v("#")]),t._v(" Massive Activations Act as Biases in LLMs")]),t._v(" "),e("p",[t._v("Variances of massive activations are considerably smaller relative to their mean values when compared to other activations.")]),t._v(" "),e("p",[t._v("Massive activations act as fixed but important biases in LLMs.")]),t._v(" "),e("p",[t._v("Why these layers and tokens? The fact that these activations act as biases may explain why LLMs store them at certain layers and tokens:")]),t._v(" "),e("ul",[e("li",[t._v("The tendency of these activations to appear at the starting token could be attributed to "),e("strong",[t._v("the fact that every autoregressive training instance contains an initial token")]),t._v(".\n"),e("ul",[e("li",[t._v("Since LLMs are based on next word prediction, the starting token is the only token used in all forward passes within a sequence.")])])]),t._v(" "),e("li",[t._v("The existence of these activations in "),e("strong",[t._v("delimiter tokens might be due to the relatively low semantic value of these tokens, rendering them a low-cost option for storing such biases")]),t._v(".\n"),e("ul",[e("li",[t._v("Conversely, tokens with rich semantics would risk significant loss of input information, if they are repurposed to store biases.")])])]),t._v(" "),e("li",[t._v("The fact that massive activations emerge only after a few initial layers may be because LLMs would require "),e("strong",[t._v("some initial layers")]),t._v(" to process the meaning of the tokens associated with massive activations.\n"),e("ul",[e("li",[t._v("At these layers, their semantics may be transferred to other token positions via self-attention, and preserved moving forward.")])])])]),t._v(" "),e("h3",{attrs:{id:"attention-is-concentrated-on-massive-activations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#attention-is-concentrated-on-massive-activations"}},[t._v("#")]),t._v(" Attention is Concentrated on Massive Activations")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/77194c6f-677f-4c9a-ac21-84b64b3e9101"}}),t._v(" "),e("p",[t._v("We find that in layer 3 and deeper layers (e.g., layer 31), attention is mostly concentrated on the two tokens associated with massive activations.")]),t._v(" "),e("h3",{attrs:{id:"massive-activations-impose-implicit-attention-biases"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#massive-activations-impose-implicit-attention-biases"}},[t._v("#")]),t._v(" Massive Activations Impose Implicit Attention Biases")]),t._v(" "),e("p",[t._v("We find that at all stages, features of the two tokens associated with massive activations are drastically different from other tokens.")]),t._v(" "),e("p",[t._v("Specifically, after the first “normalize” step, the embeddings of these two tokens appear as a sparse vector with two distinct non-zero elements.")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/8084bcb2-a03a-468d-b026-8f5e13f560cb"}}),t._v(" "),e("p",[t._v("We can see that the value updates from C are nearly identical across tokens, i.e., they serve as additive bias terms, although not explicitly imposed.")]),t._v(" "),e("p",[t._v("Furthermore, we note that this pattern of value update is strikingly similar across various inputs.")]),t._v(" "),e("p",[t._v("Overall, our results indicate that LLMs use massive activations to allocate substantial attention at certain tokens.")]),t._v(" "),e("p",[t._v("These tokens are then utilized to form a constant bias term when computing the attention output.")]),t._v(" "),e("h3",{attrs:{id:"explicit-attention-biases-eliminate-massive-activations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#explicit-attention-biases-eliminate-massive-activations"}},[t._v("#")]),t._v(" Explicit Attention Biases Eliminate Massive Activations")]),t._v(" "),e("p",[t._v("Introduce additional learnable parameters k′, v′ ∈ Rd for each head.")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/61137f43-91f7-4b81-af72-6da99aecbf9f"}}),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/81552ba0-c3b8-4dcf-866b-f8156a24b057"}}),t._v(" "),e("p",[e("em",[t._v("Massive activations are connected to self-attention.")])]),t._v(" "),e("p",[e("em",[t._v("LLMs use massive activations to concentrate substantial attention on very few tokens, injecting implicit bias terms in the attention computation.")])]),t._v(" "),e("p",[e("em",[t._v("Further, massive activations can be eliminated by augmenting LLMs with explicit attention biases.")])]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"_4-2025-a-refined-analysis-of-massive-activations-in-llms"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-2025-a-refined-analysis-of-massive-activations-in-llms"}},[t._v("#")]),t._v(" 4. [2025] A Refined Analysis of Massive Activations in LLMs")]),t._v(" "),e("h3",{attrs:{id:"abstract"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#abstract"}},[t._v("#")]),t._v(" Abstract")]),t._v(" "),e("p",[t._v("(1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance;"),e("br"),t._v("\n(2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases.")]),t._v(" "),e("p",[t._v("We consequently investigate novel hybrid mitigation strategies; in particular "),e("strong",[t._v("pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)")]),t._v(" successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated.")]),t._v(" "),e("h3",{attrs:{id:"introduction-prior-work"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#introduction-prior-work"}},[t._v("#")]),t._v(" Introduction & Prior Work")]),t._v(" "),e("p",[t._v("Activation spikes occur in the FFNs of specific layers, particularly in early and late layers, and are dedicated to a few tokens rather than being spread across sequences.")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/e569b0be-b54f-4a9d-8c05-46ef8c5da467"}}),t._v(" "),e("h4",{attrs:{id:"constant-values-across-layers"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#constant-values-across-layers"}},[t._v("#")]),t._v(" Constant Values Across Layers")]),t._v(" "),e("p",[t._v("Massive activations remain largely constant throughout most intermediate layers."),e("br"),t._v("\nThey emerge in the initial layers and diminish in the final layers."),e("br"),t._v("\nFor some models (e.g., LLaMA2-7B and LLaMA2-13B [9]), they emerge rapidly within a single layer, while for others (e.g., Phi-2 [10] and OPT [11]), they accumulate gradually over many layers.")]),t._v(" "),e("h4",{attrs:{id:"fixed-location"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#fixed-location"}},[t._v("#")]),t._v(" Fixed Location")]),t._v(" "),e("p",[t._v("In terms of the sequence dimension, massive activations are located on specific types of tokens."),e("br"),t._v("\nTokens associated with massive activations are typically starting word tokens, delimiter tokens, or tokens with weak semantics (e.g., “and,” “of,” “from”)."),e("br"),t._v("\nThese tokens are few in number, but play a critical role in shaping model behavior."),e("br"),t._v("\nIn terms of the feature dimension, massive activations are consistently present in very few fixed dimensions.")]),t._v(" "),e("h4",{attrs:{id:"fixed-bias"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#fixed-bias"}},[t._v("#")]),t._v(" Fixed Bias")]),t._v(" "),e("p",[t._v("Massive activations act as important bias terms, not just as redundant activations with no effect; the model seems to re-purposing the tokens linked to them to store these biases."),e("br"),t._v("\nThis phenomenon is observed through intervention analysis:")]),t._v(" "),e("ul",[e("li",[t._v("setting massive activation values to zero severely degrades model performance, while replacing them with their mean values (computed over N input samples) does not cause significant harm.")]),t._v(" "),e("li",[t._v("This indicates that their values are constant and input-agnostic.")])]),t._v(" "),e("p",[t._v("In layers following the first occurrence of massive activations, "),e("strong",[t._v("attention becomes concentrated on tokens associated with these activations")]),t._v(".")]),t._v(" "),e("p",[t._v("For a multi-token input sequence which triggers massive activations, the tokens associated with massive activations exhibit drastically different key and value states compared to the key and value states of other tokens in the same sequence.")]),t._v(" "),e("p",[t._v("It’s important to note that by default prior work [1] conducted analyses without including the BOS token.\nHowever, this approach may not align with how most models are trained and supposed to be configured for inference, as inputs typically include the BOS token.")]),t._v(" "),e("h4",{attrs:{id:"results"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#results"}},[t._v("#")]),t._v(" Results")]),t._v(" "),e("h5",{attrs:{id:"not-all-massive-activations-are-detrimental"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#not-all-massive-activations-are-detrimental"}},[t._v("#")]),t._v(" Not all Massive Activations are detrimental")]),t._v(" "),e("p",[t._v("While some models experienced significantly worse perplexity when massive activations were suppressed by setting their values to zero, others showed minimal or no degradation in performance.")]),t._v(" "),e("p",[t._v("However, "),e("strong",[t._v("one weak pattern emerges")]),t._v(": models which have quickly increasing massive activations across the first few layers, quickly decreasing massive activations for the final few layers, and almost constant massive activations for the middle layers, tend to exhibit more detrimental effects.")]),t._v(" "),e("ul",[e("li",[t._v("Falcon-7B displays extreme emergence and diminishment, but with slightly gradual steps; its massive activations are only moderately detrimental.")]),t._v(" "),e("li",[t._v("Falcon-2-11B exhibits multiple step-wise increases on intermediate layers; its massive activations appear to be non-detrimental.")])]),t._v(" "),e("p",[e("strong",[t._v("Notice that in c) there is step-wise increase in layer 15th and 25th")])]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/c09d87ac-9dd6-49bf-a746-880597b43f8b"}}),t._v(" "),e("h5",{attrs:{id:"bos-token-is-significant-for-some-llms"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#bos-token-is-significant-for-some-llms"}},[t._v("#")]),t._v(" BOS Token is significant for some LLMs")]),t._v(" "),e("p",[t._v("Notably, from a perplexity perspective (see Tables 1 and 6), excluding the BOS token also leads to worsened performance across all Gemma models, underscoring the importance of the BOS token in this model family.")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/d303668e-0dd1-4110-b73b-4160ca4f6ebd"}}),t._v(" "),e("p",[e("strong",[t._v("When the BOS token is excluded, massive activations are not observed in Gemma-7B, Gemma-2-2B, and Gemma-2-9B. However, when the BOS token is included, massive activations emerge in these models.")])]),t._v(" "),e("h5",{attrs:{id:"attention-kv-bias-is-not-a-general-mitigation-strategy"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#attention-kv-bias-is-not-a-general-mitigation-strategy"}},[t._v("#")]),t._v(" Attention KV Bias is not a general Mitigation Strategy")]),t._v(" "),e("p",[t._v("Significant reduction in top activation magnitudes when KV bias is applied "),e("strong",[t._v("in GPT2")]),t._v(".")]),t._v(" "),e("p",[t._v("However, downstream task performance on benchmarks such as HellaSwag consistently underperformed throughout the whole training process.")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/48ab80b7-aa64-4f5a-9de4-28c958cf72ae"}}),t._v(" "),e("p",[t._v("The results reveal that attention becomes concentrated on the BOS token starting from layer 3 (note that massive activations first appear in layer 2; see Figure 3a).")]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/e8d65634-5f77-4ee4-bc84-b5f95872fe6c"}}),t._v(" "),e("p",[t._v("📢 "),e("strong",[t._v("While KV bias successfully reduces the magnitude of massive activations, it does not eliminate attention concentration.")])]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/2117c4b7-ed2a-4f9b-90ef-331a23450a6c"}}),t._v(" "),e("p",[t._v("Instead, the extra “token” continues to attract a significant proportion of attention across layers")]),t._v(" "),e("p",[e("strong",[t._v("In contrast, the effect of KV bias on LLaMA-1B differs from its impact on GPT-2, particularly when the BOS token is included.")])]),t._v(" "),e("p",[t._v("For LLaMA-1B, attention is initially concentrated on the extra “token” in the layers immediately following the emergence of massive activations (specifically, from layer 3 to layer 6).")]),t._v(" "),e("p",[e("strong",[t._v("Beyond layer 6, attention shifts back to the BOS token.")])]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/023d2141-c3dc-4509-94f6-8d649eda5a3d"}}),t._v(" "),e("p",[e("strong",[t._v("This highlights the key role played by the BOS token in LLaMA-1B.")])]),t._v(" "),e("p",[e("strong",[t._v("When the BOS token is included, the baseline model(without KV bias) exhibits strong attention concentration on the BOS token across all layers.")])]),t._v(" "),e("img",{staticStyle:{width:"600px",height:"auto"},attrs:{src:"https://github.com/user-attachments/assets/cefff18f-2ff6-4684-91d4-a21366a08eb8"}}),t._v(" "),e("p",[t._v("However, this pattern changes when the BOS token is excluded. As shown in Figure 15, the attention distribution becomes less concentrated and splits between two tokens: the starting token and the full-stop token.")]),t._v(" "),e("h4",{attrs:{id:"target-variance-rescaling-tvr-kv-bias"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#target-variance-rescaling-tvr-kv-bias"}},[t._v("#")]),t._v(" Target Variance Rescaling (TVR) & KV Bias")]),t._v(" "),e("p",[t._v("These results demonstrate that combining TVR and KV bias can effectively mitigate some undesirable effects of massive activations without compromising model performance.")]),t._v(" "),e("h4",{attrs:{id:"other-characteristics"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#other-characteristics"}},[t._v("#")]),t._v(" Other Characteristics")]),t._v(" "),e("p",[t._v("Our analysis of massive activations reveals both consistencies and contradictions with prior findings across various LLM architectures.")]),t._v(" "),e("p",[t._v("One key characteristic previously identified is that massive activation values tend to be mostly constant across layers [1].")]),t._v(" "),e("p",[t._v("However, we observed deviations from this pattern in certain models."),e("br"),t._v("\nFor instance, models such as Gemma-2-2B, Gemma-3-1B, Gemma-3-4B, and Gemma-3-14B display a “uptrend” increase in activation magnitudes across layers (see Figures 8 and 10)."),e("br"),t._v("\nThis behavior might be influenced by architectural design choices, as these models employ both pre- and post-layernorm operations in their MLP and attention modules."),e("br"),t._v("\nIn contrast, other models that rely solely on pre-layernorm do not exhibit this trend, suggesting that normalization strategies could play a role in shaping activation dynamics.")]),t._v(" "),e("p",[t._v("Another characteristic pertains to the fixed sequence dimension locations of massive activations.")]),t._v(" "),e("p",[t._v("Prior study [1] suggested that these activations are primarily associated with starting, delimiter, or weak semantic\ntokens.")]),t._v(" "),e("p",[t._v("However, our findings reveal exceptions in models like Gemma-3-4B, Gemma-3-12B, and Falcon-7B."),e("br"),t._v("\nFor these models, massive activations are linked not only to starting or delimiter tokens, but also to other\ntokens within the sequence, such as "),e("strong",[t._v('"polished", "mass", "cold"')]),t._v("."),e("br"),t._v("\nThis challenges the assumption that massive activations are strictly tied to specific token types, highlighting the need for architecture-specific investigations.")]),t._v(" "),e("p",[t._v("Despite these contradictions, several characteristics remain consistent with prior observations."),e("br"),t._v("\nMassive activations are consistently small in number, and occur in very few fixed embedding positions across different models."),e("br"),t._v("\nThis alignment reinforces the robustness of earlier findings while underscoring the importance of understanding how these activations interact with model architecture and training dynamics.")])])}),[],!1,null,null,null);e.default=s.exports}}]);