(window.webpackJsonp=window.webpackJsonp||[]).push([[96],{549:function(e,t,a){"use strict";a.r(t);var r=a(8),o=Object(r.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"memory-optimizations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#memory-optimizations"}},[e._v("#")]),e._v(" "),t("strong",[e._v("Memory Optimizations")])]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Activation Checkpointing")]),t("br"),e._v("\nRecomputation during backward pass.")]),e._v(" "),t("li",[t("strong",[e._v("Quantization-Aware Training (QAT)")]),t("br"),e._v("\nTrain with INT8/FP8 precision.")]),e._v(" "),t("li",[t("strong",[e._v("Dynamic Memory Allocation")]),t("br"),e._v("\nBuffer reuse to avoid fragmentation.")]),e._v(" "),t("li",[t("strong",[e._v("Low-Rank Gradient Projection (GaLore)")]),t("br"),e._v(" "),t("strong",[e._v("NEW")]),e._v(" Compress gradients via low-rank approximations during training.")])]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"c99-y2024-full-parameter-fine-tuning-for-large-language-models-with-limited-resources"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#c99-y2024-full-parameter-fine-tuning-for-large-language-models-with-limited-resources"}},[e._v("#")]),e._v(" [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources")]),e._v(" "),t("ul",[t("li",[e._v("Use SGD instead of Adam for fine-tuning weights.")]),e._v(" "),t("li",[e._v("Update layer by layer in backward pass. Traditional Adam will backward probgation all layers and then update weigths.")]),e._v(" "),t("li",[e._v("SGD also avoid state memory of ADAM.")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/c1e7d018-84e2-49f7-bd74-cdf4edaa0343",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/a98a808f-1cb1-4432-b0d0-7c51e9f4ee48",alt:"image"}})]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"c25-y2024-flora-low-rank-adapters-are-secretly-gradient-compressors"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#c25-y2024-flora-low-rank-adapters-are-secretly-gradient-compressors"}},[e._v("#")]),e._v(" [C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors")]),e._v(" "),t("p",[e._v("This paper discovers that LORA can be approximated by a random projection.")]),e._v(" "),t("p",[e._v("LORA restricts overall weights update matrices to be low-rank.")]),e._v(" "),t("p",[e._v("FLORA use "),t("em",[e._v("random projection matrix")]),e._v(", which allows high-rank update gradients.")]),e._v(" "),t("blockquote",[t("p",[e._v("Our intuition arises from investigating LoRA and observing that a LoRA update is dominated by a random projection, which compresses the gradient into a\nlower-dimensional space.\nOur FLORA resamples the random projection and is able to mitigate the low-rank limitation of LoRA. Further, our approach only stores the compressed gradient\naccumulation and momentum, thus saving the memory usage of optimization states to the sublinear level.")])]),e._v(" "),t("p",[e._v("Gradident Accumulation:")]),e._v(" "),t("ul",[t("li",[e._v("Gradient accumulation stores the sum of gradients over multiple batches to simulate a larger effective batch size (helpful when memory limits prevent using large batches).")]),e._v(" "),t("li",[e._v("Normally, this requires a memory buffer equal to the model size to store the full gradient matrix.")])]),e._v(" "),t("p",[e._v("Momentum")]),e._v(" "),t("ul",[t("li",[e._v("Momentum smooths gradient updates by keeping an exponentially weighted moving average (EMA) of past gradients.")]),e._v(" "),t("li",[e._v("Maintaining momentum for large models requires significant memory since an additional buffer equal to the model size is needed.")])]),e._v(" "),t("p",[e._v("FLORA Compression:")]),e._v(" "),t("ul",[t("li",[e._v("compress gradients accumulation: Applying a random projection matrix A to reduce the dimensionality of the gradients.")]),e._v(" "),t("li",[e._v("compress momentum: Using random projection to compress the momentum term M.")])]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"c42-y2024-galore-memory-efficient-llm-training-by-gradient-low-rank-projection"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#c42-y2024-galore-memory-efficient-llm-training-by-gradient-low-rank-projection"}},[e._v("#")]),e._v(" [C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/3ddb7188-8d90-4232-8be1-cb570a74bc56",alt:"image"}})]),e._v(" "),t("blockquote",[t("p",[e._v("Galore: gradient Low-Rank Projection (GaLore), a training strategy that allows fullparameter learning but is more memory-efficient than common low-rank adaptation  methods such as LoRA.\nKey idea is to leverage the slowchanging low-rank structure of the gradient G(m√ón) of the weight matrix W, rather than trying to approximate the weight matrix itself as low rank.\nwhile the weight matrices are not necessarily low-rank, the gradient indeed becomes low-rank during the training for certain gradient forms and associated network\narchitectures.")])]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"c0-2024-compact-compressed-activations-for-memory-efficient-llm-training"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#c0-2024-compact-compressed-activations-for-memory-efficient-llm-training"}},[e._v("#")]),e._v(" [C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/37a40cf7-5a3b-4c55-b847-1fb1e9c732a5",alt:"image"}})]),e._v(" "),t("blockquote",[t("p",[e._v("By storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters.\nCompAct saves low-rank compressed activations during the forward pass, instead of the full activation tensors.\nThe resulting gradients are low-rank as well, also reducing the size of optimizer states.\nAs CompAct decompresses the gradients back to full size only for the update step, it compresses a large part of the compute graph, which in turn translates to major memory savings.")])]),e._v(" "),t("p",[e._v("CompAct is a logical next step from previous work, moving from "),t("strong",[e._v("low-rank parameters")]),e._v(", through "),t("strong",[e._v("compressed low-rank gradients")]),e._v(" , to "),t("strong",[e._v("compressed activations")]),e._v(".")]),e._v(" "),t("blockquote",[t("p",[e._v("compared to GaLore, our approach may be viewed as a simple change in the order of operations, applying the compression one step before GaLore does, to the "),t("strong",[e._v("activations")]),e._v(" rather than to the "),t("strong",[e._v("gradients")]),e._v(".")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/c0e05d1b-b19b-4bb0-92df-4842010b6502",alt:"image"}})])])}),[],!1,null,null,null);t.default=o.exports}}]);