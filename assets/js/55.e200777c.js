(window.webpackJsonp=window.webpackJsonp||[]).push([[55],{469:function(e,t,a){"use strict";a.r(t);var r=a(5),i=Object(r.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("[2023] Evaluating Unified Memory Performance in HIP")]),e._v(" "),t("li",[e._v("[97] Unlocking Bandwidth for GPUs in CC-NUMA Systems")]),e._v(" "),t("li",[e._v("[ASPLOS] DeepUM: Tensor Migration and Prefetching in Unified Memory")]),e._v(" "),t("li",[e._v("[ASPLOS] Capuchin: Tensor-based GPU Memory Management for Deep Learning")]),e._v(" "),t("li",[e._v("PUMP Profiling-free Unified Memory Prefetcher for Large DNN Model Support")]),e._v(" "),t("li",[e._v("[MICRO] G10 Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations")])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_1-2023-evaluating-unified-memory-performance-in-hip"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-2023-evaluating-unified-memory-performance-in-hip"}},[e._v("#")]),e._v(" 1.[2023] Evaluating Unified Memory Performance in HIP")]),e._v(" "),t("p",[e._v("UM only works on recent AMD GPUs, including Vega10 and MI100."),t("br"),e._v("\nThere are two flavors of the support: XNACK-enabled and XNACK-disabled.\\")]),e._v(" "),t("ul",[t("li",[e._v("In the XNACK-enabled mode,a GPU can handle retry of a memory access after page-faults, which enables mapping and migrating data on demand, as well\nas memory overcommitment.")]),e._v(" "),t("li",[e._v("In the XNACK-disabled mode, all memory must be resident and mapped in GPU page tables when the GPU is executing application code."),t("br"),e._v("\nThe XNACK-enabled mode only has experimental support.")])]),e._v(" "),t("p",[e._v("The experimental results show that the performance of the applications using UM is closely related to data transfer size and memory accesses of a kernel. Compared to ‚ÄúUM‚Äù, prefetching\nmemory as a memory usage hint leads to significant data transfers between the host and device.")]),e._v(" "),t("p",[e._v("Compared to ‚ÄúUM‚Äù, prefetching memory as a memory usage hint leads to significant data transfers between the host and device.")]),e._v(" "),t("ul",[t("li",[e._v("‚ÄúUM-hint‚Äù and ‚ÄúUM‚Äù indicate unified memory with and without memory usage hints, respectively.")]),e._v(" "),t("li",[e._v("‚ÄúZeroCopy‚Äù uses zero-copy buffers for data migration.")]),e._v(" "),t("li",[e._v("‚ÄúPageableCopy‚Äù copies data from pageable host memory to device memory")]),e._v(" "),t("li",[e._v("‚ÄúPageLockedCopy‚Äù transfers data from page-locked host memory to device memory")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/76a68408-3cee-458e-b1d0-ad3a2fc7ae0a",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"result-analysis"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#result-analysis"}},[e._v("#")]),e._v(" Result Analysis")]),e._v(" "),t("p",[e._v("The result shows that the stall rate is highly sensitive to the increase of memory size in UM.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/84ec16fc-6579-407a-b624-17365e348d9b",alt:"image"}})]),e._v(" "),t("p",[e._v("The decrease of the kernel execution time ranges from approximately 1.1X to 2.8X with respect to the vector length for the three optimization techniques.„ÄÅ\nHowever, the execution time is still approximately 1.4X to 74.8X "),t("strong",[e._v("longer than that of the kernel that takes the copy-then-execute")]),e._v(" approach.")]),e._v(" "),t("p",[e._v("In [28], the authors present 32 open-source UM benchmarks in CUDA and evaluate their performance on an NVIDIA Pascal GPU."),t("br"),e._v("\nThey find that across the benchmarks the performance of the UM benchmarks is on average "),t("strong",[e._v("34.2%")]),e._v(" slower compared with the benchmarks without UM due to the cost of page fault\nhandling")]),e._v(" "),t("blockquote",[t("p",[e._v("[28] "),t("em",[e._v("UVMBench: A Comprehensive Benchmark Suite for Researching Unified Virtual Memory in GPU")])])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_2-97-unlocking-bandwidth-for-gpus-in-cc-numa-systems"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-97-unlocking-bandwidth-for-gpus-in-cc-numa-systems"}},[e._v("#")]),e._v(" 2.[97] Unlocking Bandwidth for GPUs in CC-NUMA Systems")]),e._v(" "),t("p",[t("em",[e._v("Nvidia with umich")])]),e._v(" "),t("h3",{attrs:{id:"main-idea-in-short"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#main-idea-in-short"}},[e._v("#")]),e._v(" Main Idea in Short")]),e._v(" "),t("ul",[t("li",[e._v("Mainly focus on how many pages that covers the page-fault pages should be migrated.")]),e._v(" "),t("li",[e._v("Prefetching with upgraded range, which balance the prefetching and also reduce the number of TLB shootdowns")]),e._v(" "),t("li",[e._v("TLB shootdown is estimated at 100 cycles")]),e._v(" "),t("li",[t("strong",[e._v("Memory Oversubscription and Eviction is not considered.")])]),e._v(" "),t("li",[e._v("Page Migration Threshold accustomed to each workload is complex. And not worth it. It is better to just migrate on first touch.")])]),e._v(" "),t("h3",{attrs:{id:"introduction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/369a641b-18b1-4eb2-8c4e-3d83c1861ade",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"contribution"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#contribution"}},[e._v("#")]),e._v(" Contribution")]),e._v(" "),t("ul",[t("li",[e._v("Counter-based metrics to determine when to migrate pages from the CPU to GPU are insufficient for finding an optimal migration policy to exploit GPU memory bandwidth."),t("br"),e._v("\nIn streaming workloads, where each page may be accessed only a few times, waiting for N accesses to occur before migrating a page will actually limit the number of accesses that occur after migration, reducing the efficacy of the page migration operation.")])]),e._v(" "),t("ol",{attrs:{start:"2"}},[t("li",[e._v("TLB shootdown and refill overhead can significantly degrade the performance of any page migration policy for GPUs."),t("br"),e._v("\nWe show that combining reactive migration with virtual address locality information to aggressively prefetch pages can mitigate much of this overhead, resulting in increased GPU throughput.")])]),e._v(" "),t("h3",{attrs:{id:"interesting-experiment"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#interesting-experiment"}},[e._v("#")]),e._v(" Interesting Experiment")]),e._v(" "),t("p",[e._v("Performance comparson of DDR and GDDR Experiments")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/e5c30717-100c-4792-b0b9-6d15d3f72144",alt:"image"}})]),e._v(" "),t("p",[e._v("This choice is motivated by our observation that the performance of some GPU compute workloads would degrade by as much as 66% if the traditional GDDR memory on a GPU were replaced with standard DDR memory, as seen in Figure 2.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/7eaec0c6-bc35-4f6d-b9aa-ae31d8b23f06",alt:"image"}})]),e._v(" "),t("p",[t("em",[e._v("Still confused about the following Figure.")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/76b5a722-f83b-458d-a8ab-bd03022702ff",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"interesting-finding"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#interesting-finding"}},[e._v("#")]),e._v(" Interesting Finding")]),e._v(" "),t("h4",{attrs:{id:"clustered-page"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#clustered-page"}},[e._v("#")]),e._v(" Clustered Page")]),e._v(" "),t("p",[e._v("Page Accessing is clusted by memory arranges."),t("br"),e._v("\nPart of continuous virtual address is hot."),t("br"),e._v("\nThis clustering is key to range expansion because it suggests that if a page is identified for migration, then other neighboring pages in the virtual address space are likely to have a similar number of total touches.")]),e._v(" "),t("h4",{attrs:{id:"threshold-to-trigger-page-migration"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#threshold-to-trigger-page-migration"}},[e._v("#")]),e._v(" Threshold to trigger page migration")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/dced8b99-52e5-4390-9b4c-f825afe21cf2",alt:"image"}})]),e._v(" "),t("p",[e._v("a first touch policy (threshold-1) requires no tracking information and can be trivially implemented by migrating a page the first time the GPU\ntranslates an address for the page.")]),e._v(" "),t("p",[e._v("Considering the performance differential seen across thresholds, we believe the overhead of implementing the necessary hardware counters to track all pages within a system to differentiate their access counts is not worth the improvement over a vastly simpler first-touch migration policy.")]),e._v(" "),t("h3",{attrs:{id:"tlb"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#tlb"}},[e._v("#")]),e._v(" TLB")]),e._v(" "),t("p",[e._v("The runtime system also must be cognizant that performing TLB invalidations (an integral part of page migration) on a GPU does not just halt a single processor, but thousands of compute pipelines that may be accessing these pages through a large shared TLB structure."),t("br"),e._v("\nThis shared TLB structure makes page migrations between a CPU and GPU potentially much more costly (in terms of the opportunity cost of lost execution throughput) than in CPU-only systems.")]),e._v(" "),t("p",[e._v("Recent papers have provided proposals about how to efficiently implement general purpose TLBs that are, or could be, optimized for a GPU‚Äôs needs [28]‚Äì[30]."),t("br"),e._v("\nOthers have recently looked at improving TLB reach by exploiting locality within the virtual to physical memory remapping, or avoiding this layer completely [31]‚Äì[33]."),t("br"),e._v("\nFinally, Gerofi et al. [34] recently examined TLB performance of the Xeon Phi for applications with large footprints, while McCurdy et al. [35]\ninvestigated the effect of superpages and TLB coverage for HPC applications in the context of CPUs.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/7365332a-8dc1-4314-9eca-11ae29d117c2",alt:"image"}})]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_3-asplos-deepum-tensor-migration-and-prefetching-in-unified-memory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-asplos-deepum-tensor-migration-and-prefetching-in-unified-memory"}},[e._v("#")]),e._v(" 3. [ASPLOS] DeepUM: Tensor Migration and Prefetching in Unified Memory")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/a50fe29a-1100-4b36-9924-8c0b49ce28a1",alt:"image"}})]),e._v(" "),t("p",[e._v("DeepUM automatically prefetches data using correlation prefetching.")]),e._v(" "),t("p",[e._v("Two correlation table records the history of kenrel execution and page access patterns during training of prefetching.")]),e._v(" "),t("p",[e._v("DeepUM‚Äôs correlation tables record the history of the kernel executions and their page accesses during the training phase of a DNN."),t("br"),e._v("\nIt prefetches pages based on the information in the correlation tables by predicting which kernel will execute next."),t("br"),e._v("\nWhile traditional correlation prefetching uses a single table to store history and records the relationship between the CPU cache lines, DeepUM correlation prefetching uses two different table structures.")]),e._v(" "),t("p",[e._v("Assign each kernel with a kernel ID and maitian kernel ID with its page access history.")]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_4-asplos-capuchin-tensor-based-gpu-memory-management-for-deep-learning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-asplos-capuchin-tensor-based-gpu-memory-management-for-deep-learning"}},[e._v("#")]),e._v(" 4. [ASPLOS] Capuchin: Tensor-based GPU Memory Management for Deep Learning")]),e._v(" "),t("p",[e._v("üëç üëç üëç üëç üëç")]),e._v(" "),t("p",[e._v("The key feature of Capuchin is that it makes memory management decisions based on dynamic tensor access pattern tracked at runtime. "),t("br"),e._v("\nThis design is motivated by the observation that the access pattern to tensors is regular during training iterations. "),t("br"),e._v("\nBased on the identified patterns, one can exploit the total memory optimization space and offer the fine-grain and flexible control of when and how to perform\nmemory optimization techniques.")]),e._v(" "),t("h4",{attrs:{id:"introduction-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#introduction-2"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),t("p",[e._v("BERT 768 hiddenlayers 73GB memory int training."),t("br"),e._v("\nV100 32GB on-board memory and P100 16GB on-board memory.")]),e._v(" "),t("p",[e._v("Feature maps are produced in the forward propagation and used again in the backward propagation."),t("br"),e._v("\nMajor deep learning frameworks such as Tensorflow , MXNet and Pytorch usually maintain these feature maps in GPU memory until they are no longer\nneeded in backward propagation computation. "),t("br"),e._v("\nHowever, there is usually a large gap between two accesses to the same feature map in forward and backward propagation, which incurs high memory consumption to store the intermediate results.")]),e._v(" "),t("p",[e._v("To reduce memory consumption:")]),e._v(" "),t("ul",[t("li",[e._v("swapping")]),e._v(" "),t("li",[e._v("recomputting")])]),e._v(" "),t("p",[e._v("Key observaions:")]),e._v(" "),t("ul",[t("li",[e._v("we believe that dynamically tracking fine-grained tensor accesses is a fundamental and general technique that enables effective memory management optimizations."),t("br"),e._v("\nThis paper demonstrates that this essential idea can be implemented efficiently on top of major deep learning frameworks.")]),e._v(" "),t("li",[e._v("The training process is composed of millions of iterations with clear boundaries, and the tensor accesses have regular and repeated access patterns across iterations."),t("br"),e._v("\nThis means that analyzing the timing and tensor access patterns can easily reveal the memory optimization opportunities with concrete guidance")])]),e._v(" "),t("h4",{attrs:{id:"background"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#background"}},[e._v("#")]),e._v(" Background")]),e._v(" "),t("p",[t("strong",[e._v("Forward propagation")])]),e._v(" "),t("p",[e._v("input feature maps, current layer‚Äôs weights and bias produce the output feature maps which become the next layer‚Äôs input data."),t("br"),e._v("\nThe forward propagation concludes with the calculation of loss by comparing the output with ground truth label at the output layer.")]),e._v(" "),t("p",[t("strong",[e._v("Backward propagation")])]),e._v(" "),t("p",[e._v("The backward propagation starts from the output layer and reversely traverses layers to optimize the weights and bias.")]),e._v(" "),t("p",[t("strong",[e._v("Memory Usage")])]),e._v(" "),t("ul",[t("li",[e._v("feature maps:output in the forward propagation")]),e._v(" "),t("li",[e._v("gradient maps:output in the backward propagtaion")]),e._v(" "),t("li",[e._v("convolution workspace: extra memory space needed by convolution algorithm.")])]),e._v(" "),t("p",[e._v("Model weights consume very small amount of memory and are usually persistent in GPU memory to be continuously updated.")]),e._v(" "),t("p",[e._v("the latter two are temporary memory usage which can be released immediately after current computations are finished.")]),e._v(" "),t("p",[e._v("The feature maps are needed in both forward and backward propagation. However, there exists "),t("em",[t("strong",[e._v("a large time gap between the two usage points for computations in forward and backward phase")])]),e._v(".")]),e._v(" "),t("p",[t("strong",[e._v("Deap learning framework execution modes")])]),e._v(" "),t("ul",[t("li",[e._v("eager mode: dynamic graph")]),e._v(" "),t("li",[e._v("graph mode: static graph")])]),e._v(" "),t("h3",{attrs:{id:"framework"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#framework"}},[e._v("#")]),e._v(" Framework")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/bd0b963e-743e-4e1b-95b1-d2f77a4544a3",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/9a812147-f1a2-4ef9-9c43-fac1d9df67d9",alt:"image"}})]),e._v(" "),t("p",[e._v("The idea is clear, simple and classic.")]),e._v(" "),t("p",[e._v("Profile based on tensor reuse pattern.")]),e._v(" "),t("p",[e._v("Based on history information choose early prefetch or recompute.")]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_5-pump-profiling-free-unified-memory-prefetcher-for-large-dnn-model-support"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-pump-profiling-free-unified-memory-prefetcher-for-large-dnn-model-support"}},[e._v("#")]),e._v(" 5. PUMP Profiling-free Unified Memory Prefetcher for Large DNN Model Support")]),e._v(" "),t("p",[e._v("Year: 2022")]),e._v(" "),t("p",[e._v("üëç üëç üëç üëç üëç")]),e._v(" "),t("p",[t("strong",[e._v("This paper is published with source code. Real Stuff!")])]),e._v(" "),t("p",[e._v("PUMP exploits GPU asynchronous execution for prefetch; that is, there exists a delay between the time that CPU launches a kernel and the time the kernel executes in GPU.")]),e._v(" "),t("p",[e._v("PUMP extracts memory blocks accessed by the kernel when launching and swaps these blocks into GPU memory.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/8f1ec193-9be7-4be0-be6a-1fe7c735ab4e",alt:"image"}})]),e._v(" "),t("p",[e._v("The idea behind this paper is also simple and classic.")]),e._v(" "),t("p",[e._v("Before launch the kernel, launch the prefetch cudaEvent.")]),e._v(" "),t("p",[e._v("They exploit dynamic linker of linux, with LD_PRELOAD, they could call their version of cuda wrapper.")]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_6-micro-g10-enabling-an-efficient-unified-gpu-memory-and-storage-architecture-with-smart-tensor-migrations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-micro-g10-enabling-an-efficient-unified-gpu-memory-and-storage-architecture-with-smart-tensor-migrations"}},[e._v("#")]),e._v(" 6. [MICRO] G10 Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations")]),e._v(" "),t("p",[e._v("üëç üëç üëç üëç üëç")]),e._v(" "),t("p",[t("strong",[e._v("Paper with Code")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/4f27bd28-1c3f-4752-9d71-ecc76bc544dd",alt:"image"}})]),e._v(" "),t("p",[e._v("Key Observations: tensors are not active for long time.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/1b914427-3254-48d1-8e48-02dc1cc4cf01",alt:"image"}})]),e._v(" "),t("p",[e._v("Main Idea: insert pre-prefetch and evict function in CUDA code.\n"),t("img",{attrs:{src:"https://github.com/user-attachments/assets/4c0da2dd-190f-46c7-be33-8ce1a6545204",alt:"image"}})])])}),[],!1,null,null,null);t.default=i.exports}}]);