(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{412:function(e,a,t){"use strict";t.r(a);var i=t(5),s=Object(i.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("ol",[a("li",[e._v("Baryon: Efficient Hybrid Memory Management with Compression and Sub-Blocking")]),e._v(" "),a("li",[e._v("CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache")]),e._v(" "),a("li",[e._v("Hybrid2: Combining Caching and Migration in Hybrid Memory Systems")]),e._v(" "),a("li",[e._v("SILC-FM: Subblocked InterLeaved Cache-Like Flat Memory Organization")]),e._v(" "),a("li",[e._v("MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories")]),e._v(" "),a("li",[e._v("Transparent Hardware Management of Stacked DRAM as Part of Memory")]),e._v(" "),a("li",[e._v("CHAMELEON: A Dynamically Reconfigurable Heterogeneous Memory System")]),e._v(" "),a("li",[e._v("BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM")]),e._v(" "),a("li",[e._v("Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories")]),e._v(" "),a("li",[e._v("Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems")]),e._v(" "),a("li",[e._v("Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation")]),e._v(" "),a("li",[e._v("Die-Stacked DRAM: Memory, Cache, or MemCache?")]),e._v(" "),a("li",[e._v("Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache")]),e._v(" "),a("li",[e._v("Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors")]),e._v(" "),a("li",[e._v("On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems")]),e._v(" "),a("li",[e._v("Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems [DAC]")]),e._v(" "),a("li",[e._v("TicToc: Enabling Bandwidth-Efficient DRAM Caching for both Hits and Misses in Hybrid Memory Systems International Conference on Computer Design (ICCD)")])]),e._v(" "),a("p",[e._v("DRAM NVM")]),e._v(" "),a("ol",[a("li",[e._v("An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture")]),e._v(" "),a("li",[e._v("CLOCK-DWF: A Write-History-Aware Page Replacement Algorithm for Hybrid PCM and DRAM Memory Architectures")]),e._v(" "),a("li",[e._v("APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method")]),e._v(" "),a("li",[e._v("Page Placement in Hybrid Memory Systems")])]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_2-cameo-a-two-level-memory-organization-with-capacity-of-main-memory-and-flexibility-of-hardware-managed-cache"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-cameo-a-two-level-memory-organization-with-capacity-of-main-memory-and-flexibility-of-hardware-managed-cache"}},[e._v("#")]),e._v(" 2. CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache")]),e._v(" "),a("h5",{attrs:{id:"mempod"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mempod"}},[e._v("#")]),e._v(" MemPod:")]),e._v(" "),a("p",[e._v("CAMEO [13] proposes a cache-like flat address space memory management scheme in an attempt to close the gap between cache and flat memory organizations. CAMEO operates similarly to THM, however it does so at the granularity of cache lines (64B). Migrations are restricted within segments with one fast line location per segment. Its bookkeeping structures are entirely stored in memory, while a “Line Location Predictor” attempts to save some bookkeeping-related accesses by predicting the location of a line.")]),e._v(" "),a("p",[e._v("CAMEO initiates a line migration upon every access to slow memory.")]),e._v(" "),a("p",[e._v("CAMEO can incur high migration traffic as every access could induce a migration.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/aeefa981-341b-48fb-82bd-27d356bc5ea6",alt:"image"}})]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_5-mempod-a-clustered-architecture-for-efficient-and-scalable-migration-in-flat-address-space-multi-level-memories"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-mempod-a-clustered-architecture-for-efficient-and-scalable-migration-in-flat-address-space-multi-level-memories"}},[e._v("#")]),e._v(" 5. MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories")]),e._v(" "),a("p",[e._v("Year: 2017")]),e._v(" "),a("p",[e._v("MemPod uses MEA counters to track page access activity and identify hot pages. They are dramatically smaller than prior tracking mechanisms while capturing activity counts and temporal recency in a way that provides more effective prediction of future page access.")]),e._v(" "),a("p",[e._v("What makes MEA most useful, though, is its failure mode – when it fails to find the most-accessed pages, it does so by favoring recency over quantity. That is, a page accessed several times near the end of an interval can easily knock out a page accessed many more times early in the interval. As a result, it combines both access counting and temporal locality, at a fraction of the cost of access counting alone.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://user-images.githubusercontent.com/23403286/237034027-921f15ba-0cf5-4a55-9221-deffea837202.png",alt:"image"}})]),e._v(" "),a("p",[e._v("Three triggers are most commonly used whenever state must be updated based on tracking information (MC scheduling, migrations, dynamic voltage and frequency scaling etc.). Interval-based (or epoch-based) triggers occur with a set frequency, while threshold-based solutions trigger whenever a predetermined criterion is met. Finally, event-based triggers react to predefined events. Both interval-based and threshold-based approaches face the same challenge of identifying the optimal interval or threshold value.")]),e._v(" "),a("p",[e._v("MemPod achieves the best performance (lower AMMAT) with 50us intervals and 64 counters per Pod. MemPod’s lightweight operation allows for such small intervals. For comparison purposes, HMA [14] identi-fied the best epoch length to be 100ms (2000x larger) in order to support all the lengthy processes that take place during a migration event for that method.")]),e._v(" "),a("p",[e._v("Based on these results, we use 64 MEA 2-bit counters over 50us intervals for subsequent results in this paper. Each one of the 64 MEA entries needs 21 bits for addressing the 1.1M pages per Pod and 2 bits for its counter, leading to an area cost of only 184B per Pod and 736B total. Compared to the state of the art, MemPod’s activity tracking requirement is ∼712x smaller than THM’s (512KB) and ∼12800x smaller than HMA’s (9MB).")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://user-images.githubusercontent.com/23403286/237031049-f4cbb08e-5ee3-4ab5-9ffd-d4abc43306db.png",alt:"image"}})]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_6-transparent-hardware-management-of-stacked-dram-as-part-of-memory"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-transparent-hardware-management-of-stacked-dram-as-part-of-memory"}},[e._v("#")]),e._v(" 6. Transparent Hardware Management of Stacked DRAM as Part of Memory")]),e._v(" "),a("h5",{attrs:{id:"mempod-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mempod-2"}},[e._v("#")]),e._v(" MemPod:")]),e._v(" "),a("p",[e._v("Sim, et al. proposed a technique for transparent hardware management of a hybrid memory system [17],which we will refer to as “THM”. THM does not require OS intervention while managing migrations. In order to keep bookkeeping costs manageable, THM allows migrations only within sets of pages (called segments). Each segment includes one fast memory page and a set of slow memory pages. The slow pages of each segment can only migrate to the one fast page location, and any such migration results in the eviction of the currently-residing page. THM monitors memory accesses with one “competing counter” per segment resulting in a low cost profiling solution. Finally, THM supports caching part of its structures on chip while the rest is stored in memory.")]),e._v(" "),a("p",[e._v("THM’s competing counters can lead to false positives, allowing a cold page to migrate to fast memory.")]),e._v(" "),a("p",[e._v("THM offers significantly limited flexibility by restricting migrations withing segments, however this decision reduces bookkeeping costs significantly. Competing counters in each segment are used for activity tracking, occasionally leading to false (threshold-based) migration triggering if a cold page gets accessed at the right time. Identifying migration candidates incurs very little overhead since there is exactly one fast memory location for each slow memory page that triggers migration.")]),e._v(" "),a("p",[a("strong",[e._v("Cite from paper")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/1d619878-f38a-43c6-bb45-c5838601f1e2",alt:"image"}})]),e._v(" "),a("p",[e._v("Similar to set dueling, they adopted sample region.\nThe locations in fast memory are grouped into 32 distinct regions in an interleaving fashion, and four regions are dedicated to sampling, while other 28 regions follow the threshold decision from sampling.")]),e._v(" "),a("p",[e._v("• Nstatic: # of memory requests serviced from fast memory with static mapping\n• Ndynamic: # of memory requests expected to be serviced from fast memory when swapping with a given threshold\n• Nswap: # of expected swaps for a given threshold.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/bd34b14e-774b-4598-9145-7a523965547e",alt:"image"}})]),e._v(" "),a("p",[e._v("K differs depending on the relative latency of fast and slow memory.\nThe cost of a single fast swap is about 1200 cycles, and the difference in access latency between fast and slow memory is 72 cycles.Thus, in general, the swapped-in segment needs to get at least 17 more (future) hits than the swapped-out segment for swapping to be valuable. K is computed in hardware at boot time.")]),e._v(" "),a("p",[a("strong",[e._v("Me")]),e._v("\nCompeting counter needs a threshold to invoke swap.")]),e._v(" "),a("p",[e._v("Sample region will have different threshold. Based on different threshold, Nswap is different. Thus they choose the the best threshold with max Bexpected as candidate threshold.")]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_8-batman-techniques-for-maximizing-system-bandwidth-of-memory-systems-with-stacked-dram"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_8-batman-techniques-for-maximizing-system-bandwidth-of-memory-systems-with-stacked-dram"}},[e._v("#")]),e._v(" 8.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM")]),e._v(" "),a("p",[a("strong",[e._v("Insights")])]),e._v(" "),a("ul",[a("li",[e._v("bandwidth distribution")]),e._v(" "),a("li",[e._v("dram and hbm similar latency")])]),e._v(" "),a("p",[e._v("As the NM simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. We observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.")]),e._v(" "),a("p",[e._v("We leverage our key insight on controlling data movement and propose Bandwidth-Aware Tiered-Memory Management (BATMAN), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the NM and the FM.\nWe define the desired access rate of the NM as the target access rate (TAR). TAR is the fraction of memory accesses serviced by the NM when memory accesses to both memories are proportional to the respective bandwidth.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/a307ab63-9871-4e66-bf4c-792a391022ae",alt:"image"}})]),e._v(" "),a("p",[e._v("2X 2/3\n4X 4/5\n8X 8/9")]),e._v(" "),a("p",[a("strong",[e._v("Me")]),e._v("\nBandwidth-Aware Tired-Memory Management tries to distritube memory according to HBM and DRAM bandwidth ratio. And also treat it as a threshold to refuse page migration.")]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_9-heterogeneous-memory-architectures-a-hw-sw-approach-for-mixing-die-stacked-and-off-package-memories"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_9-heterogeneous-memory-architectures-a-hw-sw-approach-for-mixing-die-stacked-and-off-package-memories"}},[e._v("#")]),e._v(" 9. Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories")]),e._v(" "),a("h5",{attrs:{id:"mempod-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mempod-3"}},[e._v("#")]),e._v(" MemPod:")]),e._v(" "),a("p",[e._v("HMA [14] is a HW/SW mechanism that attempts to predict frequently accessed pages in memory and, at predefined intervals, migrate those pages to fast memory. HW support is required for profiling memory accesses using counters for each memory page, while the migration is handled by the OS. Due to the costly OS involvement, HMA’s intervals are kept large. Additionally, the hardware cost of its profiling counters is high. However, HMA is capable of managing migrations in a flat address space without the need of additional bookkeeping for finding migrated pages as the OS can update page tables and TLBs to reflect migrations.")]),e._v(" "),a("p",[e._v("HMA does not require a remap table due to the OS updating the existing system’s structures. For activity tracking it uses Full Counters. The costly OS involvement and the high penalty for sorting all its counters force HMA to operate at very large intervals, weakening its adaptability to phase changes. However, HMA offers full flexibility for migrations.")]),e._v(" "),a("p",[a("strong",[e._v("Cite from org paper")]),e._v("\nThis first-touch hot-page (FTHP) policy is effectively a generalization of both the history-based and first-touch algorithms.")]),e._v(" "),a("p",[e._v("We propose a dynamic feedback-directed HMA policy that can dynamically adjust the hotness threshold θ to achieve a best-of-both-worlds approach between history-based and first-touch policies.")]),e._v(" "),a("ul",[a("li",[e._v("At the start of each epoch, the size of the hot set is compared to the size of the die-stacked DRAM (N).")]),e._v(" "),a("li",[e._v("If the hot set is too small to fill the fast memory, then θ is lowered which causes more pages to be classified as hot.")]),e._v(" "),a("li",[e._v("Likewise, if the hot set is too large, θ is increased which causes fewer pages to be put in the hot set.")]),e._v(" "),a("li",[e._v("If the feedback mechanism works well, then the size of the hot set should converge to N.")])]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_10-challenges-in-heterogeneous-die-stacked-and-off-chip-memory-systems"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_10-challenges-in-heterogeneous-die-stacked-and-off-chip-memory-systems"}},[e._v("#")]),e._v(" 10.Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems")]),e._v(" "),a("p",[e._v("Year: 2012\nSoftware OS management\nTo prevent the mapping of pages with insufficient miss traffic, we employ a threshold θ such that any page with fewer than θ LLC misses is not considered for mapping into stacked DRAM. The application of a threshold may result in cases when the list of most frequently missed pages has only k < P items. In this case, we\nsimply keep a random set of P − k of the existing pages from the previous epoch already in stacked DRAM to avoid consuming bandwidth to swap out the page back to the off-chip memory.")]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_11-banshee-bandwidth-efficient-dram-caching-via-software-hardware-cooperation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_11-banshee-bandwidth-efficient-dram-caching-via-software-hardware-cooperation"}},[e._v("#")]),e._v(" 11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation")]),e._v(" "),a("p",[e._v("Year: 2017\nSoftware aovid tag look up by storing DRAM Cache presence information in the page table and tlbs.")]),e._v(" "),a("p",[e._v("Specifically, Banshee uses a hardwaremanaged frequency-based replacement (FBR) policy that only caches hot pages to reduce unnecessary data replacement traffic.\nTo reduce the cost of accessing/updating frequency counters (which are stored in in-package DRAM), Banshee uses a new sampling approach to only read/write counters\nfor a fraction of memory accesses.")]),e._v(" "),a("p",[a("strong",[e._v("Background")])]),e._v(" "),a("ul",[a("li",[e._v("Using tags  Alloy Cache | Unison Cache")]),e._v(" "),a("li",[e._v("Using address remapping Heterogeneous Memory Architecture (HMA) | Tagless DRAM Cache (TDC)")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/aed3c7e9-2ccb-48d4-b165-308467935a79",alt:"image"}})]),e._v(" "),a("p",[e._v("They reuse "),a("strong",[e._v("reverse maping mechanism")]),e._v(".")]),e._v(" "),a("p",[e._v("Banshee tracks each page’s access frequency with a counter, stored in the metadata.\nWe store counters not only for the pages in the DRAM cache, but also for some pages not in cache, which are candidates to bring into the cache.")]),e._v(" "),a("p",[e._v("Instead, an access in Banshee only updates a page’s frequency counter with a certain sample rate. For a sample rate of 10%, for example, the frequency counters are accessed/updated only once for every 10 DRAM accesses.")]),e._v(" "),a("p",[e._v("Frequency-based replacement may lead to thrashing problem.")]),e._v(" "),a("p",[e._v("Banshee solves this problem by only replacing a page when the candidate’s counter is greater than the victim’s counter by a certain threshold. This ensures that a page just evicted from the DRAM cache must be accessed for at least 2·threshold/sampling rate times before it can enter the cache again, thus preventing a page from entering and leaving frequently.")]),e._v(" "),a("p",[e._v("By default, the threshold is the product of the number of cachelines in a page and the sampling coefficient divided by two (threshold = page_size x sampling_coeff / 2). Intuitively, this means replacement can happen only if the benefit of swapping the pages outweighs the cost of the replacement operation.")]),e._v(" "),a("p",[e._v("If a counter saturates after being incremented, all counters in the metadata will be reduced by half using a shift operation in hardware.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/0e9e90cd-e67c-480e-a081-03f1b9a9920e",alt:"image"}})]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_13-unison-cache-a-scalable-and-effective-die-stacked-dram-cache"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_13-unison-cache-a-scalable-and-effective-die-stacked-dram-cache"}},[e._v("#")]),e._v(" 13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache")]),e._v(" "),a("p",[e._v("The state-of-the-art block-based design, called Alloy Cache, colocates a tag with each data block (e.g., 64B) in the stacked DRAM to provide fast access to data in a single DRAM access. However, such a design suffers from low hit rates due to poor temporal locality in the DRAM cache. In contrast, the state-of-the-art page-based design, called Footprint Cache, organizes the DRAM cache at page granularity (e.g., 4KB), but fetches only the blocks that will likely be touched within a page. In doing so, the Footprint Cache achieves high hit rates with moderate on-chip tag storage and reasonable lookup latency. However, multi-gigabyte stacked DRAM caches will soon be practical and needed by server applications, thereby mandating tens of MBs of tag storage even for page-based DRAM caches.")]),e._v(" "),a("p",[e._v("We introduce a novel stacked-DRAM cache design, Unison Cache. Similar to Alloy Cache’s approach, Unison Cache incorporates the tag metadata directly into the stacked DRAM to enable scalability to arbitrary stacked-DRAM capacities. Then, leveraging the insights from the Footprint Cache design, Unison Cache employs large, page-sized cache allocation units to achieve high hit rates and reduction in tag overheads, while predicting and fetching only the useful blocks within each page to minimize the off-chip traffic. Our evaluation using server workloads and caches of up to 8GB reveals that Unison cache improves performance by 14% compared to Alloy Cache due to its high hit rate, while outperforming the state-of-the art page-based designs that require impractical SRAM-based tags of around 50MB.")]),e._v(" "),a("p",[a("strong",[e._v("Me")]),e._v("\nBlock-based high miss ratio, page-based high migration penalty when fetching useless data.\nIf footprint meta data is adopted to collect data trace, FootCache meta table cannot scale with increasing capactiy of HBM.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://user-images.githubusercontent.com/23403286/237041324-e1d78ea0-4ff7-4b2c-8870-03b5b3cb9c4a.png",alt:"image"}})]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_14-dynamically-adapting-page-migration-policies-based-on-applications-memory-access-behaviors"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_14-dynamically-adapting-page-migration-policies-based-on-applications-memory-access-behaviors"}},[e._v("#")]),e._v(" 14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://hitqshao.github.io/qishao-notes/pages/24769f/",title:"Link to notes",target:"_blank",rel:"noopener noreferrer"}},[e._v("Link to notes for Dynamically Adapting..."),a("OutboundLink")],1)]),e._v(" "),a("h3",{attrs:{id:"_15-on-the-fly-page-migration-and-address-reconciliation-for-heterogeneous-memory-systems"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_15-on-the-fly-page-migration-and-address-reconciliation-for-heterogeneous-memory-systems"}},[e._v("#")]),e._v(" 15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems")]),e._v(" "),a("p",[e._v("“on-the-fly” migration performs better than epoch-based page migration techniques, since we migrate recent hot pages.")]),e._v(" "),a("p",[e._v("Instead of relying completely on OS to perform AR for the evicted entries, as done in [Ramoset al. 2011], we propose a hardware-based AR, where the MigC hardware initiates TLB shootdown and cache flushing without explicitly stopping the user program.")]),e._v(" "),a("p",[a("strong",[e._v("Like previous studies [Meswani et al. 2015; Prodromou et al. 2017; Su et al. 2015], we observe that not all applications benefit from page migration, since page migration incurs performance overheads due to extra data movement.")])]),e._v(" "),a("p",[e._v("The model works with the principle that, to get performance benefit, one should migrate the smallest set of pages from slow to fast memory that yields in the largest increase in memory accesses to fast memory to amortize the migration overhead.")]),e._v(" "),a("p",[a("strong",[e._v("Me")]),e._v("\nTry to use 80% principle.\nIn our study, we look at the 80-percentile accesses and identify the “set of top-accessed pages” that contribute to more than 80% of all memory accesses.")]),e._v(" "),a("p",[e._v("In our proposal, we migrate a page immediately when it receives sufficient number of memory accesses, unlike any epoch-based schemes. We allow full flexibility in page relocation like HMAHS [Meswani et al. 2015] and keep a remap table for address redirection. We keep this table small by periodically evicting entries and it is placed on-chip.")]),e._v(" "),a("p",[e._v("The particular access count, which can separate such top-accessed pages from other pages, is referred to as “filter count.” Note that, filter count indicates an upper bound for hotness threshold")]),e._v(" "),a("p",[a("strong",[e._v("Me")]),e._v("\nThis paper explain the address reconciliation AR in detail.")]),e._v(" "),a("p",[a("strong",[e._v("Flow of AR")]),e._v("\n-First, all cache lines from these pages, which are currently residing in the cache hierarchies and tagged with OS-visible PA, must be invalidated (and dirty lines written back), since the current OS-visible PA will be replaced with the new PA. All future accesses to these pages will only have access to the new PA.\n-Next,corresponding page table entries (PTEs) for A and B need to be updated with new PAs.\n-The TLB entries in all cores using the old PA must also be invalidated (as well as any other OS structures that contain the physical page addresses).")]),e._v(" "),a("p",[e._v("(i) flush_cache_page()\n(ii) change PTE,\n(iii) flush_tlb_page()")]),e._v(" "),a("p",[e._v("We use a "),a("strong",[e._v("Migration Benefit Quotient (MBQ)")]),e._v(", by calculating the difference between the total number of accesses to any page and the filter count used in classifying applications’ memory locality, as described.\nMBQ indicates how many of the future accesses of a page may go to fast memory if the page were migrated from slow to fast memory using the filter count as a hotness\nthreshold.")]),e._v(" "),a("p",[a("strong",[e._v("Sacturation account")]),e._v("\nthe sum of access counts of all pages with 32,909 accesses or less (the bars corresponding to x-axis 0 to 3,290) accounts for 98% of all accesses. For our purposes, we use this access count as a saturation count (or assume that the maximum number of accesses any page can receive).")]),e._v(" "),a("p",[a("strong",[e._v("For each workload, we use the difference between the saturation count and the filter count to determine MBQ.")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/e942285a-16d6-4b39-9b75-662513ac34f8",alt:"image"}})]),e._v(" "),a("ol",[a("li",[e._v("Low MBQ, difference is less than 1K. For example, in Figure 5(b) xalanc, pages with memory access count 609 or less (the bars corresponding to x-axis 0 to 60) provide 98% of the memory accesses. Hence, the difference between saturation count (609) and filter count (70) is 539. These workloads may not achieve significant increase in accesses to fast memory after page migration.")]),e._v(" "),a("li",[e._v("Medium MBQ, difference is in between 1K to K (e.g., Figure 5(c) omnetpp). These workloads may receive moderate benefits, depending the migration overheads.")]),e._v(" "),a("li",[e._v("High MBQ, difference is more than 8K (e.g., Figure 5(a) mcf). These workloads are likely to receive higher hits in faster memory as a result of page migration.")])]),e._v(" "),a("p",[a("strong",[e._v("Me")]),e._v("\nIn short")]),e._v(" "),a("ul",[a("li",[e._v("sactuartion account. The sum of access counts fo all pages with sactuartion account accounts for 98% of all accesses.")]),e._v(" "),a("li",[e._v("filter account. The “set of top-accessed pages” that contribute to more than 80% of all memory accesses.\nIf they are close, it means that this is a uniform benchmark.\nIf they have a large difference, this means that there is a subset of pages that has far more memory accesses.")])]),e._v(" "),a("hr"),e._v(" "),a("h4",{attrs:{id:"_17-tictoc-enabling-bandwidth-efficient-dram-caching-for-both-hits-and-misses-in-hybrid-memory-systems-international-conference-on-computer-design-iccd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_17-tictoc-enabling-bandwidth-efficient-dram-caching-for-both-hits-and-misses-in-hybrid-memory-systems-international-conference-on-computer-design-iccd"}},[e._v("#")]),e._v(" 17. TicToc: Enabling Bandwidth-Efficient DRAM Caching for both Hits and Misses in Hybrid Memory Systems International Conference on Computer Design (ICCD)")]),e._v(" "),a("p",[e._v("Year: 2019")])])}),[],!1,null,null,null);a.default=s.exports}}]);