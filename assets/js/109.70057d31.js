(window.webpackJsonp=window.webpackJsonp||[]).push([[109],{560:function(e,t,a){"use strict";a.r(t);var i=a(9),n=Object(i.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("[1] BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models")]),e._v(" "),t("li",[e._v("[1075] Adafactor Adaptive Learning Rates with Sublinear Memory Cost")]),e._v(" "),t("li",[e._v("[3] Adam Accumulation to Reduce Memory Footprints of both Activations and Gradients for Large-scale DNN Training")])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_1-1-badam-a-memory-efficient-full-parameter-optimization-method-for-large-language-models"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-badam-a-memory-efficient-full-parameter-optimization-method-for-large-language-models"}},[e._v("#")]),e._v(" 1. [1] BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models")]),e._v(" "),t("p",[t("strong",[e._v("It change the finetuing into block coordinate descent (BCD)-type optimization")]),e._v(" which I dont understand.")]),e._v(" "),t("p",[e._v("For instance, to finetune an LLM with M billion parameters, Adam [23] necessitates roughly 18M GB of GPU memory for successful training, and\nthis estimate does not even account for the storage of activations used in the backpropagation (BP) process.")]),e._v(" "),t("p",[e._v("Despite the success of PEFT methods, finetuning within a substantially lower-dimensional subspace may potentially "),t("strong",[e._v("limit downstream performance")]),e._v(".")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/8def8b42-d493-4358-80bc-515746a2cc17",alt:"image"}})]),e._v(" "),t("p",[e._v("We first analyze the memory cost of Adam with mixed precision training.")]),e._v(" "),t("p",[e._v("One needs to store the FP16 model parameters for the BP process, which costs 2M memory.")]),e._v(" "),t("p",[e._v("For a more precise update, the optimizer also maintains a master copy of a FP32 model, which costs 4M memory.")]),e._v(" "),t("p",[e._v("Then, it comes to store the "),t("strong",[e._v("gradient (converted to FP32), momentum, and second moment")]),e._v(" in FP32 precision, costing "),t("strong",[e._v("4M + 4M + 4M = 12M")]),e._v(" memory.")]),e._v(" "),t("p",[e._v("In total, Adam needs roughly "),t("strong",[e._v("18M memory")]),e._v(".")]),e._v(" "),t("p",[e._v("In terms of BAdam, it needs to store the up-to-date model parameters (see Figure 1) in FP16 precision, which costs 2M memory. Importantly, since BAdam only updates the active block at one time, we\ncan store the model parameters, gradient, momentum, and second moment only for the active block θπi in FP32 precision, where the FP32 model parameters and gradient of the active block can be\nobtained by transforming their FP16 versions to the FP32 versions.")]),e._v(" "),t("p",[e._v("Let us consider the simple case where the partitioned D blocks are equal-sized. Then, BAdam only needs in total")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/f0ee0fbd-cf93-4b2b-a1b0-f7df945d5d16",alt:"image"}})]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_2-1075-adafactor-adaptive-learning-rates-with-sublinear-memory-cost"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-1075-adafactor-adaptive-learning-rates-with-sublinear-memory-cost"}},[e._v("#")]),e._v(" 2. [1075] Adafactor Adaptive Learning Rates with Sublinear Memory Cost")]),e._v(" "),t("p",[e._v("This paper introduces "),t("strong",[e._v("Adafactor")]),e._v(" , an optimization algorithm designed to provide "),t("strong",[e._v("adaptive learning rates with significantly reduced memory overhead")]),e._v("  compared to traditional methods like Adam.")]),e._v(" "),t("p",[e._v("The core innovation is replacing full per-parameter second-moment estimators (used to scale gradients) with "),t("strong",[e._v("factored approximations")]),e._v("  based on the "),t("strong",[e._v("row and column sums")]),e._v("  of squared gradients.")]),e._v(" "),t("p",[t("strong",[e._v("Similar to LORA, using only the per-row and percolumn sums of these moving averages, and estimating the per-parameter second moments based on these sums.")])]),e._v(" "),t("p",[e._v("This change reduces memory usage from O(nm) to O(n + m) for matrix-shaped parameters.")]),e._v(" "),t("p",[e._v("Key contributions and details include:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Factored Second Moment Estimation")]),e._v(" :")]),e._v(" "),t("ul",[t("li",[e._v("Adafactor leverages a rank-1 approximation using the generalized Kullback-Leibler divergence to estimate the second moment of gradients.")]),e._v(" "),t("li",[e._v("Instead of storing full-size accumulators, it maintains exponential moving averages of row and column sums, achieving memory savings while preserving empirical performance.")]),e._v(" "),t("li",[e._v("A closed-form solution for the rank-1 approximation ensures computational efficiency and compatibility with exponential smoothing.")])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Removing Momentum")]),e._v(" :")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Momentum (first moment) is omitted (β₁ = 0)")]),e._v(" to further reduce memory cost. This change initially causes instability, especially without learning rate warmup.")])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Stabilizing Updates")]),e._v(" :")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Update Clipping")]),e._v(" : Caps the RMS of unscaled updates to avoid large, destabilizing parameter jumps due to outdated second-moment estimators.")]),e._v(" "),t("li",[t("strong",[e._v("Increasing β₂ Schedule")]),e._v(" : Proposes a decay schedule like β̂₂ₜ = 1 − t^(-c), which adapts over time and avoids the need for bias correction.")]),e._v(" "),t("li",[e._v("These methods independently and jointly stabilize training in the absence of momentum and warmup.")])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Relative Step Sizes")]),e._v(" :")]),e._v(" "),t("ul",[t("li",[e._v("Instead of fixed absolute learning rates, Adafactor uses "),t("strong",[e._v("parameter-relative step sizes")]),e._v(" , scaling updates based on the parameter norm, making it more resilient to differing parameter magnitudes (e.g., in embeddings).")])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Experiments")]),e._v(" :")]),e._v(" "),t("ul",[t("li",[e._v("Conducted on Transformer models for WMT’14 En→De machine translation.")]),e._v(" "),t("li",[e._v("Adafactor with factored moments, no momentum, update clipping, increasing decay rate, and relative step sizes performs comparably to Adam while using less memory.")]),e._v(" "),t("li",[e._v("Results show robustness to poor parameter initialization and scaling, unlike Adam.")])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Practical Use")]),e._v(" :")]),e._v(" "),t("ul",[t("li",[e._v("Adafactor enables training larger models on memory-constrained hardware.")]),e._v(" "),t("li",[e._v("Implementation is available in the Tensor2Tensor library.")])])])]),e._v(" "),t("p",[t("strong",[e._v("Three-Sentence Summary:")])]),e._v(" "),t("p",[e._v("The paper proposes "),t("strong",[e._v("Adafactor")]),e._v(" , a memory-efficient optimizer that approximates second-moment estimators using factored row and column sums, drastically reducing auxiliary memory usage from O(nm) to O(n + m).")]),e._v(" "),t("p",[e._v("To address instability caused by omitting momentum and slow-decaying estimators, the authors introduce "),t("strong",[e._v("update clipping")]),e._v("  and an "),t("strong",[e._v("increasing decay schedule")]),e._v(" , both of which stabilize training.")]),e._v(" "),t("p",[e._v("Combined with "),t("strong",[e._v("relative step sizes")]),e._v(" , Adafactor achieves comparable performance to Adam on large-scale Transformer tasks while enabling significantly larger models on limited hardware.")]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_3-adam-accumulation-to-reduce-memory-footprints-of-both-activations-and-gradients-for-large-scale-dnn-training"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-adam-accumulation-to-reduce-memory-footprints-of-both-activations-and-gradients-for-large-scale-dnn-training"}},[e._v("#")]),e._v(" [3] Adam Accumulation to Reduce Memory Footprints of both Activations and Gradients for Large-scale DNN Training")]),e._v(" "),t("h3",{attrs:{id:"gradient-accumulation-gradient-release"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gradient-accumulation-gradient-release"}},[e._v("#")]),e._v(" Gradient Accumulation & Gradient Release")]),e._v(" "),t("p",[t("strong",[e._v("Gradient accumulation")]),e._v(" reduces the activation memory by splitting a mini-batch into a sequence of micro batches and accumulating the gradients of all micro-batches.")]),e._v(" "),t("p",[t("strong",[e._v("Gradient accumulation")]),e._v(" must preserve accumulated value of gradients until the last micro-batch.")]),e._v(" "),t("p",[t("strong",[e._v("Gradient release")]),e._v(" reduces the gradient memory by freeing up the gradient-occupied space in a layer-by-layer manner.")]),e._v(" "),t("p",[t("strong",[e._v("Gradient release")]),e._v(" releases the gradients immediately after use.")]),e._v(" "),t("h3",{attrs:{id:"adam-accumulation-adama"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#adam-accumulation-adama"}},[e._v("#")]),e._v(" Adam Accumulation (AdamA)")]),e._v(" "),t("p",[e._v("Specifically, instead of accumulating gradients, AdamA integrates gradients into optimizer states (m and v in Adam)\nimmediately after the gradients are produced, and accumulates optimizer states sequentially over micro-batches.")]),e._v(" "),t("p",[e._v("This subtle change of directly integrating gradients to optimizer states makes the memory space for whole model gradients no longer needed, eliminating the aforementioned contradiction between preserving gradients and releasing gradients.")]),e._v(" "),t("p",[e._v("Consequently, AdamA can reduce the gradient memory to 1/M of the original (M is the number of layers), and the activation memory to 1/N of the original (N is the number of micro-batches).")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/1dfb4486-d838-42d9-a412-5c5f818a2fb8",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"elaboration"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#elaboration"}},[e._v("#")]),e._v(" Elaboration")]),e._v(" "),t("p",[e._v("The key idea behind gradient accumulation is to split a mini-batch into several micro-batches.")]),e._v(" "),t("p",[e._v("This method computes the gradients of micro-batches sequentially and accumulates them to reduce the memory footprint of activations as well as to keep the same convergence properties as the original mini-batch.")]),e._v(" "),t("p",[e._v("Gradient release executes the backward process in a layer-by-layer manner, which immediately releases the gradient-occupied\nmemory after the weight updating is finished, so that the memory allocated for gradients can be reduced from the size of whole model size to the size of the maximum layer.")]),e._v(" "),t("h3",{attrs:{id:"mechanism"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#mechanism"}},[e._v("#")]),e._v(" Mechanism")]),e._v(" "),t("p",[e._v("Intuitively, as gradients are eventually used to update the optimizer states (m and v in Adam), if we can integrate gradients\ninto optimizer states in advance, the gradients memory can be released, thus resolving this dilemma.")]),e._v(" "),t("p",[e._v("Inspired by this insight, we for the first time propose an optimizer accumulation method, namely AdamA, that integrates gradients into optimizer states immediately after produced and then accumulates optimizer states sequentially over micro-batches.")])])}),[],!1,null,null,null);t.default=n.exports}}]);