(window.webpackJsonp=window.webpackJsonp||[]).push([[83],{537:function(e,a,t){"use strict";t.r(a);var s=t(9),i=Object(s.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("ol",[a("li",[e._v("[22] Adaptive Memory-Side Last-Level GPU Caching")]),e._v(" "),a("li",[e._v("[83 2015] Locality-Driven Dynamic GPU Cache Bypassing")]),e._v(" "),a("li",[e._v("[9 2021 HPCA] Analyzing and Leveraging Decoupled L1 Caches in GPUs")])]),e._v(" "),a("hr"),e._v(" "),a("h2",{attrs:{id:"_1-22-adaptive-memory-side-last-level-gpu-caching"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-22-adaptive-memory-side-last-level-gpu-caching"}},[e._v("#")]),e._v(" 1. [22] Adaptive Memory-Side Last-Level GPU Caching")]),e._v(" "),a("ul",[a("li",[e._v("Private LLCs, which replicate shared data across multiple slices, provide higher bandwidth but suffer from higher miss rates.")]),e._v(" "),a("li",[e._v("Shared LLCs avoid redundancy, reducing miss rates, but suffer bandwidth contention under high sharing.")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/fe7ab2e6-f43a-4881-b154-0782bb84bafc",alt:"image"}})]),e._v(" "),a("p",[e._v("In the shared LLC organization, an LLC slice is shared by all SMs.")]),e._v(" "),a("p",[e._v("The LLC slice for a given cache line is determined by a few address bits.")]),e._v(" "),a("p",[e._v("Collectively, all LLC slices associated with a given memory controller cache the entire memory address space served by the memory controller.")]),e._v(" "),a("p",[e._v("In the private LLC organization, an LLC slice is private to a cluster of SMs.")]),e._v(" "),a("p",[e._v("An LLC slice caches the entire memory partition served by the respective memory controller for only a single cluster of SMs.")]),e._v(" "),a("p",[e._v("The LLC slice for a cache line is thus determined by the cluster ID.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/86193f2f-2ad3-45c3-a78d-4b4c79db370d",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"dynamic-reconfiguration-rules"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dynamic-reconfiguration-rules"}},[e._v("#")]),e._v(" Dynamic Reconfiguration Rules")]),e._v(" "),a("ul",[a("li",[e._v("Switch to private if:\n"),a("ul",[a("li",[e._v("Miss rate remains comparable (within 2%)")]),e._v(" "),a("li",[e._v("Bandwidth gain outweighs miss rate penalty")])])]),e._v(" "),a("li",[e._v("Revert to shared:\n"),a("ul",[a("li",[e._v("At new kernel or time epoch")])])])]),e._v(" "),a("p",[e._v("How to profile/")]),e._v(" "),a("p",[e._v("Set Dueling")]),e._v(" "),a("hr"),e._v(" "),a("h2",{attrs:{id:"_2-83-2015-locality-driven-dynamic-gpu-cache-bypassing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-83-2015-locality-driven-dynamic-gpu-cache-bypassing"}},[e._v("#")]),e._v(" 2. [83 2015] Locality-Driven Dynamic GPU Cache Bypassing")]),e._v(" "),a("h3",{attrs:{id:"categories-of-applications"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#categories-of-applications"}},[e._v("#")]),e._v(" Categories of Applications")]),e._v(" "),a("p",[e._v("The paper classifies GPU applications into three categories based on how they benefit (or suffer) from the L1 D-cache:")]),e._v(" "),a("h4",{attrs:{id:"_1-cache-unfriendly-cnf"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-cache-unfriendly-cnf"}},[e._v("#")]),e._v(" 1. Cache-Unfriendly (CNF)")]),e._v(" "),a("p",[e._v("Definition: Applications that perform better when L1 D-cache is bypassed.")]),e._v(" "),a("p",[e._v("Cause:")]),e._v(" "),a("ul",[a("li",[e._v("Low data reuse.")]),e._v(" "),a("li",[e._v("Long reuse distances.")])]),e._v(" "),a("p",[e._v("Leads to cache pollution and resource contention.")]),e._v(" "),a("p",[e._v("Impact:")]),e._v(" "),a("ul",[a("li",[e._v("Memory pipeline stalls.")]),e._v(" "),a("li",[e._v("Unnecessary eviction of useful lines.")])]),e._v(" "),a("p",[e._v("Examples:")]),e._v(" "),a("ul",[a("li",[e._v("NW, SD2, LUD, HS, PTF, BH, SSSP")])]),e._v(" "),a("p",[e._v("Performance gain: Up to 36% IPC improvement by bypassing L1.")]),e._v(" "),a("h4",{attrs:{id:"_2-cache-insensitive-ci"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-cache-insensitive-ci"}},[e._v("#")]),e._v(" 2. Cache-Insensitive (CI)")]),e._v(" "),a("p",[e._v("Definition: Applications for which enabling/disabling L1 D-cache has little to no effect.")]),e._v(" "),a("p",[e._v("Cause:")]),e._v(" "),a("ul",[a("li",[e._v("Heavy use of shared memory.")]),e._v(" "),a("li",[e._v("Minimal or no global memory accesses.")])]),e._v(" "),a("p",[e._v("Low memory intensity or high control divergence.")]),e._v(" "),a("p",[e._v("Impact:")]),e._v(" "),a("ul",[a("li",[e._v("Cache behavior does not affect IPC.")])]),e._v(" "),a("p",[e._v("Examples:")]),e._v(" "),a("ul",[a("li",[e._v("CFD, MYC, FFT, GS, PF, LFK")])]),e._v(" "),a("h4",{attrs:{id:"_3-cache-friendly-cf"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-cache-friendly-cf"}},[e._v("#")]),e._v(" 3. Cache-Friendly (CF)")]),e._v(" "),a("p",[e._v("Definition: Applications that benefit from L1 D-cache.")]),e._v(" "),a("p",[e._v("Cause:")]),e._v(" "),a("ul",[a("li",[e._v("High data reuse.")]),e._v(" "),a("li",[e._v("Short reuse distances.")])]),e._v(" "),a("p",[e._v("Impact:")]),e._v(" "),a("ul",[a("li",[e._v("Disabling L1 severely degrades performance.")])]),e._v(" "),a("p",[e._v("Examples:")]),e._v(" "),a("ul",[a("li",[a("strong",[e._v("MM, HT, SD1, BT, BP")])])]),e._v(" "),a("p",[e._v("Performance loss: Up to 77% IPC drop when bypassed.")]),e._v(" "),a("ul",[a("li",[e._v("🔁 Reuse Behavior Analysis")]),e._v(" "),a("li",[e._v("🔢 Reuse Count")])]),e._v(" "),a("p",[e._v("What it shows: Number of times a memory address is reused.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/fcc652e0-5464-423b-af2b-e7dcf4b4c21b",alt:"image"}})]),e._v(" "),a("p",[e._v("Observation:")]),e._v(" "),a("ul",[a("li",[e._v("CNF apps have few high-reuse accesses.")]),e._v(" "),a("li",[e._v("Example: >60% of accesses in NW, LUD are reused fewer than 3 times.")])]),e._v(" "),a("h4",{attrs:{id:"📏-reuse-distance-figure-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#📏-reuse-distance-figure-4"}},[e._v("#")]),e._v(" 📏 Reuse Distance (Figure 4)")]),e._v(" "),a("p",[e._v("Definition: The number of unique memory accesses between two accesses to the same address.")]),e._v(" "),a("p",[e._v("Example: Pattern A–B–C–A → reuse distance = 2.")]),e._v(" "),a("p",[e._v("Observation:")]),e._v(" "),a("ul",[a("li",[e._v("CNF apps have long reuse distances: often 512–2048.")])]),e._v(" "),a("p",[a("strong",[e._v("These accesses cannot fit in L1 (e.g., 128B lines × 512 = 64KB).")])]),e._v(" "),a("p",[e._v("🧠 L1 vs. L2 Cache Bottlenecks\nExperimental Setup (Figure 2)")]),e._v(" "),a("ul",[a("li",[e._v("The authors increase associativity and capacity of both L1 and L2 caches.")])]),e._v(" "),a("p",[e._v("Findings:")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/4dc6d668-e824-43ee-b0fd-ca229beb6ced",alt:"image"}})]),e._v(" "),a("p",[e._v("Cache Level\tObservation for CNF Apps")]),e._v(" "),a("ul",[a("li",[a("strong",[e._v("L2\tPerformance is insensitive to L2 size and associativity. L2 is not a bottleneck.")])]),e._v(" "),a("li",[a("strong",[e._v("L1\tPerformance improves with larger/more associative L1. But needs impractically large L1 (e.g., 128-way, 16MB) to be effective. Still insufficient for some apps")]),e._v(".")])]),e._v(" "),a("p",[e._v("Conclusion: "),a("strong",[e._v("The L1 D-cache is the performance bottleneck for CNF workloads, not L2.")])]),e._v(" "),a("h3",{attrs:{id:"bypassing-logic"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bypassing-logic"}},[e._v("#")]),e._v(" Bypassing logic")]),e._v(" "),a("ul",[a("li",[e._v("On a tag store miss → insert into tag store with RC = 1 → bypass data store.")]),e._v(" "),a("li",[e._v("On subsequent hits:\n"),a("ul",[a("li",[e._v("RC incremented.")]),e._v(" "),a("li",[e._v("If RC > threshold (e.g., 2), allocate data in the data store.")])])]),e._v(" "),a("li",[e._v("Replacement:\n"),a("ul",[a("li",[e._v("Tag store uses LFU with aging (decays RC to evict stale entries).")])])]),e._v(" "),a("li",[e._v("Data store uses existing GPU policies like LRU, RRIP.")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/01bb388a-a287-4514-b5b6-a628fc2453c2",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"📊-summary-table"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#📊-summary-table"}},[e._v("#")]),e._v(" 📊 Summary Table")]),e._v(" "),a("p",[e._v("Category\tBehavior\tReuse Count\tReuse Distance\tL1 Role\tL2 Role")]),e._v(" "),a("ul",[a("li",[e._v("CNF\tCache hurts\tMostly low (1–2)\tLong (512–2048)\tBottleneck, polluted\tNot bottleneck")]),e._v(" "),a("li",[e._v("CI\tCache irrelevant\tN/A\tN/A\tIrrelevant\tIrrelevant")]),e._v(" "),a("li",[e._v("CF\tCache helps\tHigh\tShort\tCritical\tLess relevant")])]),e._v(" "),a("h3",{attrs:{id:"🧠-design-implications"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#🧠-design-implications"}},[e._v("#")]),e._v(" 🧠 Design Implications")]),e._v(" "),a("ul",[a("li",[e._v("L1 D-cache should selectively cache data.")]),e._v(" "),a("li",[e._v("A naive insert-everything policy causes: Thrashing in CNF apps.")]),e._v(" "),a("li",[e._v("Performance degradation in CF apps if cache is bypassed.")])]),e._v(" "),a("p",[e._v("The paper proposes a reuse-aware dynamic bypass mechanism that:")]),e._v(" "),a("ul",[a("li",[e._v("Tracks Reference Count (RC).")]),e._v(" "),a("li",[e._v("Filters accesses based on reuse patterns.")])]),e._v(" "),a("p",[e._v("Add extra information in the tag. But not the data.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/192f1f93-4aa2-4957-a8b8-fe96ffacc85f",alt:"image"}})]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_3-9-2021-analyzing-and-leveraging-decoupled-l1-caches-in-gpus"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-9-2021-analyzing-and-leveraging-decoupled-l1-caches-in-gpus"}},[e._v("#")]),e._v(" 3. [9 2021] Analyzing and Leveraging Decoupled L1 Caches in GPUs")]),e._v(" "),a("h4",{attrs:{id:"📌-motivation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#📌-motivation"}},[e._v("#")]),e._v(" 📌 Motivation")]),e._v(" "),a("p",[e._v("Modern GPUs use private, tightly-coupled L1 caches per core. While this cache hierarchy helps address the memory wall by providing on-chip bandwidth, it introduces two key inefficiencies:")]),e._v(" "),a("p",[e._v("Data Replication: Multiple cores may cache the same data, wasting total L1 capacity.")]),e._v(" "),a("p",[e._v("Underutilized L1 Bandwidth: Many-to-few traffic from many L1s to fewer L2s causes poor L1 bandwidth utilization.")]),e._v(" "),a("h4",{attrs:{id:"🚀-proposed-solution-dc-l1-decoupled-l1-cache-architecture"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#🚀-proposed-solution-dc-l1-decoupled-l1-cache-architecture"}},[e._v("#")]),e._v(" 🚀 Proposed Solution: DC-L1 (DeCoupled L1) Cache Architecture")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/9e418409-59b4-4b91-8157-527bdf49be7c",alt:"image"}})]),e._v(" "),a("p",[e._v("Core Idea:")]),e._v(" "),a("p",[a("strong",[e._v("Physically decouple L1 caches from the cores and aggregate them into a flexible organization called DC-L1, enabling:")])]),e._v(" "),a("ul",[a("li",[e._v("Reduced data replication.")]),e._v(" "),a("li",[e._v("Improved bandwidth utilization.")]),e._v(" "),a("li",[e._v("Tunable design between latency and capacity benefits.")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/6d38f1d0-873b-440e-88ed-90b7f166f151",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"📐-architectural-designs-explored"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#📐-architectural-designs-explored"}},[e._v("#")]),e._v(" 📐 Architectural Designs Explored")]),e._v(" "),a("ol",[a("li",[a("strong",[e._v("Private DC-L1 (PrY):")])])]),e._v(" "),a("p",[e._v("Each DC-L1 is shared by a group of cores.")]),e._v(" "),a("p",[e._v("Configurable aggregation granularity: e.g., Pr80 (1 core per DC-L1), Pr40 (2 cores/DC-L1), … Pr10 (8 cores/DC-L1).")]),e._v(" "),a("p",[e._v("Benefit: reduces replication as cores share cache lines.")]),e._v(" "),a("p",[e._v("🧠 Tradeoff: Larger group size → better replication reduction but worse bandwidth (more contention, fewer ports).")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/d0cfb8b3-43c1-406f-824f-dd599d34952f",alt:"image"}})]),e._v(" "),a("ol",{attrs:{start:"2"}},[a("li",[a("strong",[e._v("Fully-Shared DC-L1 (Sh40):")])])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/ff16cb91-c07a-4e60-8649-14067003b670",alt:"image"}})]),e._v(" "),a("p",[e._v("All DC-L1 caches are part of a single logically shared cache space.")]),e._v(" "),a("p",[e._v("Each DC-L1 owns a distinct part of the address space (similar to L2 bank slicing).")]),e._v(" "),a("p",[e._v("Eliminates replication completely.")]),e._v(" "),a("p",[e._v("🔴 Problem: Requires a large 80×40 NoC crossbar → high area and power overheads.")]),e._v(" "),a("ol",{attrs:{start:"3"}},[a("li",[a("strong",[e._v("Clustered Shared DC-L1 (ShY+CZ):")])])]),e._v(" "),a("p",[e._v("Divide GPU cores and DC-L1s into clusters (e.g., 10 clusters of 8 cores and 4 DC-L1s each).")]),e._v(" "),a("p",[e._v("Enable intra-cluster sharing, but allow inter-cluster replication.")]),e._v(" "),a("p",[e._v("Tradeoff: reduced replication and reduced NoC cost.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/3b3352e5-4cf3-4407-8b84-d28c647191cc",alt:"image"}})]),e._v(" "),a("p",[e._v("✅ Chosen configuration: Sh40+C10 (40 DC-L1s, 10 clusters) → balanced performance and cost.")]),e._v(" "),a("ol",{attrs:{start:"4"}},[a("li",[a("strong",[e._v("Sh40+C10+Boost:")])])]),e._v(" "),a("p",[e._v("Same as above, but doubles the frequency of small crossbars in NoC#1.")]),e._v(" "),a("p",[e._v("Leverages the small crossbar size to overcome latency/bandwidth loss from decoupling.")]),e._v(" "),a("h4",{attrs:{id:"simulation-set-up"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#simulation-set-up"}},[e._v("#")]),e._v(" Simulation Set up")]),e._v(" "),a("p",[e._v("We estimated such latency under the evaluated applications with Sh40+C10+Boost, and observed an overhead of "),a("strong",[e._v("54 cycles")]),e._v(", on average.")]),e._v(" "),a("p",[e._v("Another source of latency overhead is the aggregation of the DC-L1s.")]),e._v(" "),a("p",[e._v("Specifically, with Sh40+C10+Boost, each DC-L1 cache is double the size of the baseline L1 cache, which adds a 7% increase in the DC-L1 access latency.")]),e._v(" "),a("p",[e._v("Specifically, the DC-L1s with Sh40+C10+Boost have an access latency of 30 cycles, compared to "),a("strong",[e._v("28 cycles")]),e._v(" L1 access latency in the baseline.")]),e._v(" "),a("h4",{attrs:{id:"insights-analysis"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#insights-analysis"}},[e._v("#")]),e._v(" Insights & Analysis")]),e._v(" "),a("ul",[a("li",[e._v("Replication Sensitivity Classification")]),e._v(" "),a("li",[e._v("Applications were deemed replication-sensitive if:")]),e._v(" "),a("li",[e._v("Replication ratio > 25%")]),e._v(" "),a("li",[e._v("L1 miss rate > 50%")]),e._v(" "),a("li",[e._v("5% IPC gain with 16× L1 size")])]),e._v(" "),a("p",[e._v("12 such applications (e.g., T-AlexNet, C-BFS) showed substantial gains from reducing L1 replication.")]),e._v(" "),a("h3",{attrs:{id:"ideas"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ideas"}},[e._v("#")]),e._v(" Ideas")]),e._v(" "),a("h4",{attrs:{id:"_1-workload-aware-dc-l1-partitioning-for-multi-programmed-gpus"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-workload-aware-dc-l1-partitioning-for-multi-programmed-gpus"}},[e._v("#")]),e._v(" 1. Workload-aware DC-L1 Partitioning for Multi-programmed GPUs")]),e._v(" "),a("p",[a("strong",[e._v("Problem")]),e._v(": The paper evaluates replication and bandwidth under single workloads. In multi-tenant settings, different applications compete for DC-L1 capacity and bandwidth.")]),e._v(" "),a("p",[a("strong",[e._v("Research Idea")]),e._v(": Dynamically partition DC-L1 clusters based on workload characteristics (e.g., memory intensity, data reuse, prefetch aggressiveness).")]),e._v(" "),a("p",[e._v("Use runtime metrics (e.g., replication rate, MPKI) to drive partition reconfiguration.")]),e._v(" "),a("p",[a("strong",[e._v("Contribution")]),e._v(": Design a low-overhead runtime or compiler hint system for adaptive DC-L1 clustering and mapping under multi-workload scenarios.")]),e._v(" "),a("h4",{attrs:{id:"_2-replication-aware-compressed-dc-l1-caches"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-replication-aware-compressed-dc-l1-caches"}},[e._v("#")]),e._v(" 2. Replication-aware Compressed DC-L1 Caches")]),e._v(" "),a("p",[a("strong",[e._v("Problem")]),e._v(": Even clustered DC-L1s have limited capacity and still experience intra-cluster replication.")]),e._v(" "),a("p",[a("strong",[e._v("Research Idea")]),e._v(": Implement base-delta or frequent-value compression in DC-L1 caches.")]),e._v(" "),a("ul",[a("li",[e._v("Exploit inter-core redundancy for cross-core dictionary compression or address-delta reuse.")])]),e._v(" "),a("p",[a("strong",[e._v("Twist")]),e._v(": Co-design compression with replication-tracking to avoid storing multiple compressed versions of the same line.")]),e._v(" "),a("p",[a("strong",[e._v("Accel-Sim Angle")]),e._v(": Add lightweight modeling of decompression latency and tag expansion overheads in the L1 cache pipeline.")]),e._v(" "),a("h4",{attrs:{id:"_3-prefetch-filtering-and-scheduling-in-dc-l1s-for-irregular-workloads"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-prefetch-filtering-and-scheduling-in-dc-l1s-for-irregular-workloads"}},[e._v("#")]),e._v(" 3. Prefetch Filtering and Scheduling in DC-L1s for Irregular Workloads")]),e._v(" "),a("p",[a("strong",[e._v("Problem:")]),e._v(" DC-L1 nodes create new prefetch timing and routing bottlenecks.")]),e._v(" "),a("p",[a("strong",[e._v("Research Idea:")]),e._v(" Introduce a prefetch scheduler in DC-L1 nodes that:")]),e._v(" "),a("ul",[a("li",[e._v('Differentiates between "core-locality" and "remote-sharing" prefetches.')]),e._v(" "),a("li",[e._v("Uses reuse-distance or MSHR saturation tracking to decide eviction or forwarding.")])]),e._v(" "),a("p",[a("strong",[e._v("New Angle")]),e._v(": Combine prefetch confidence with replication sensitivity to filter harmful prefetches that pollute shared DC-L1s.")]),e._v(" "),a("h4",{attrs:{id:"_4-topology-aware-workload-to-dc-l1-mapping-in-multi-gpu-systems"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-topology-aware-workload-to-dc-l1-mapping-in-multi-gpu-systems"}},[e._v("#")]),e._v(" 4. Topology-aware Workload-to-DC-L1 Mapping in Multi-GPU Systems")]),e._v(" "),a("p",[a("strong",[e._v("Problem:")]),e._v(" DC-L1s require networked routing; current cluster mapping is static.")]),e._v(" "),a("p",[a("strong",[e._v("Research Idea:")]),e._v(" In a multi-GPU setup (Accel-Sim or MCM-GPU-style), design dynamic workload mapping policies that:")]),e._v(" "),a("ul",[a("li",[e._v("Cluster memory-sharing CTAs to minimize DC-L1 hop distance.")]),e._v(" "),a("li",[e._v("Balance NoC load across shared DC-L1s.")])]),e._v(" "),a("p",[a("strong",[e._v("Extension:")]),e._v(" Apply machine learning (e.g., reinforcement learning) to learn optimal DC-L1 routing and core affinity over time.")])])}),[],!1,null,null,null);a.default=i.exports}}]);