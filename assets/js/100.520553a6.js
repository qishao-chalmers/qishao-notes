(window.webpackJsonp=window.webpackJsonp||[]).push([[100],{553:function(e,r,t){"use strict";t.r(r);var s=t(8),a=Object(s.a)({},(function(){var e=this,r=e._self._c;return r("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[r("ol",[r("li",[e._v("[C606 2021] Transformer Feed-Forward Layers Are Key-Value Memories")])]),e._v(" "),r("hr"),e._v(" "),r("h2",{attrs:{id:"_1-c606-2021-transformer-feed-forward-layers-are-key-value-memories"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-c606-2021-transformer-feed-forward-layers-are-key-value-memories"}},[e._v("#")]),e._v(" 1. [C606 2021] Transformer Feed-Forward Layers Are Key-Value Memories")]),e._v(" "),r("h3",{attrs:{id:"overview"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#overview"}},[e._v("#")]),e._v(" "),r("strong",[e._v("Overview")])]),e._v(" "),r("p",[e._v("The paper investigates the under-explored role of feed-forward layers in transformer-based language models.")]),e._v(" "),r("p",[e._v("While self-attention has been extensively studied, feed-forward layers contain two-thirds of a transformer's parameters.")]),e._v(" "),r("p",[e._v("The authors propose that these layers function as "),r("strong",[e._v("key-value (KV) memories")]),e._v(" , where:")]),e._v(" "),r("ul",[r("li",[r("strong",[e._v("Keys")]),e._v("  capture "),r("strong",[e._v("textual patterns")]),e._v("  in training examples.")]),e._v(" "),r("li",[r("strong",[e._v("Values")]),e._v("  induce "),r("strong",[e._v("distributions over the output vocabulary")]),e._v(" .")])]),e._v(" "),r("p",[e._v("Through experiments, the paper demonstrates that:")]),e._v(" "),r("ul",[r("li",[e._v("Lower-layer keys capture "),r("strong",[e._v("shallow syntactic patterns")]),e._v(" , while upper layers learn "),r("strong",[e._v("semantic patterns")]),e._v(" .")]),e._v(" "),r("li",[e._v("Values "),r("strong",[e._v("predict next-token distributions")]),e._v(" , aligning more strongly in upper layers.")]),e._v(" "),r("li",[e._v("The final model prediction is an "),r("strong",[e._v("aggregation of memories")]),e._v(" , refined by "),r("strong",[e._v("residual connections")]),e._v(" .")])]),e._v(" "),r("h3",{attrs:{id:"key-contributions"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#key-contributions"}},[e._v("#")]),e._v(" "),r("strong",[e._v("Key Contributions")])]),e._v(" "),r("ol",[r("li",[r("strong",[e._v("Feed-forward layers operate as key-value memories")])])]),e._v(" "),r("ul",[r("li",[e._v("The first parameter matrix ("),r("strong",[e._v("The first parameter matrix (keys, $K$")]),e._v(" ) interacts with inputs to compute a weighted sum of the second parameter matrix ("),r("strong",[e._v("The first parameter matrix ("),r("strong",[e._v("The first parameter matrix (keys, $K$")]),e._v(" ) interacts with inputs to compute a weighted sum of the second parameter matrix (values, $V$")]),e._v(" ).")]),e._v(" "),r("li",[e._v("This mirrors neural memory models like those from "),r("strong",[e._v("Sukhbaatar et al. (2015, 2019)")]),e._v(" .")])]),e._v(" "),r("ol",{attrs:{start:"2"}},[r("li",[r("strong",[e._v("Keys capture human-interpretable textual patterns")])])]),e._v(" "),r("ul",[r("li",[e._v("Keys correspond to "),r("strong",[e._v("n-grams, phrase structures, and semantic themes")]),e._v(" .")]),e._v(" "),r("li",[e._v("Patterns vary across layers:\n"),r("ul",[r("li",[r("strong",[e._v("Lower layers → Shallow linguistic patterns (e.g., common words, syntactic structures)")])]),e._v(" "),r("li",[r("strong",[e._v("Upper layers → Semantic relationships (e.g., “a part of,” “military base”)")])])])]),e._v(" "),r("li",[e._v("Removing the "),r("strong",[e._v("last word")]),e._v("  in a sentence affects activations more than removing the "),r("strong",[e._v("first word")]),e._v(" , indicating that later words are more salient.")])]),e._v(" "),r("ol",{attrs:{start:"3"}},[r("li",[r("strong",[e._v("Values store next-token probability distributions")])])]),e._v(" "),r("ul",[r("li",[e._v("Values represent "),r("strong",[e._v("un-normalized probability distributions")]),e._v("  over the output vocabulary.")]),e._v(" "),r("li",[e._v("Agreement between "),r("strong",[e._v("keys and values increases in upper layers")]),e._v(" , meaning values encode likely next words based on key patterns.")])]),e._v(" "),r("ol",{attrs:{start:"4"}},[r("li",[r("strong",[e._v("Prediction refinement across layers")])])]),e._v(" "),r("ul",[r("li",[e._v("The final output results from an "),r("strong",[e._v("aggregation of multiple memories")]),e._v("  at each layer.")]),e._v(" "),r("li",[e._v("Residual connections act as a "),r("strong",[e._v("refinement mechanism")]),e._v(" , tuning predictions layer-by-layer.")]),e._v(" "),r("li",[e._v("In lower layers, residuals dominate the prediction, while in "),r("strong",[e._v("upper layers")]),e._v(" , feed-forward layers play a bigger role.")])]),e._v(" "),r("h3",{attrs:{id:"key-insights-and-findings"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#key-insights-and-findings"}},[e._v("#")]),e._v(" "),r("strong",[e._v("Key Insights and Findings")])]),e._v(" "),r("h4",{attrs:{id:"_1-feed-forward-layers-as-unnormalized-key-value-memories"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-feed-forward-layers-as-unnormalized-key-value-memories"}},[e._v("#")]),e._v(" "),r("strong",[e._v("1. Feed-Forward Layers as Unnormalized Key-Value Memories")])]),e._v(" "),r("ul",[r("li",[e._v("Each "),r("strong",[e._v("feed-forward layer resembles a key-value memory network")]),e._v(" .")]),e._v(" "),r("li",[e._v("The equation governing a feed-forward layer:\n$$FF(x) = f(x \\cdot K^T) \\cdot V$$")])]),e._v(" "),r("p",[e._v("closely resembles memory-based networks:\n$$MN(x) = \\text{softmax}(x \\cdot K^T) \\cdot V$$")]),e._v(" "),r("ul",[r("li",[e._v("The primary difference is the absence of "),r("strong",[e._v("softmax normalization")]),e._v("  in transformer feed-forward layers.")])]),e._v(" "),r("h4",{attrs:{id:"_2-keys-detect-patterns-in-training-data"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-keys-detect-patterns-in-training-data"}},[e._v("#")]),e._v(" "),r("strong",[e._v("2. Keys Detect Patterns in Training Data")])]),e._v(" "),r("ul",[r("li",[e._v("Experiments show that "),r("strong",[e._v("keys correlate with specific input patterns")]),e._v(" .")]),e._v(" "),r("li",[r("strong",[e._v("Methodology")]),e._v(" :\n"),r("ul",[r("li",[r("strong",[e._v("Trained a 16-layer transformer (Baevski & Auli, 2019) on WikiText-103")]),e._v(" .")]),e._v(" "),r("li",[r("strong",[e._v("Extracted the most activating input sentences for each key")]),e._v(" .")]),e._v(" "),r("li",[r("strong",[e._v("Human annotators")]),e._v("  identified patterns.")])])]),e._v(" "),r("li",[r("strong",[e._v("Findings")]),e._v(" :\n"),r("ul",[r("li",[r("strong",[e._v("Lower-layer keys")]),e._v('  detect surface-level syntax (e.g., "words ending in -ing").')]),e._v(" "),r("li",[r("strong",[e._v("Upper-layer keys")]),e._v("  capture deeper semantics (e.g., “military bases,” “a part of” relationships).")]),e._v(" "),r("li",[r("strong",[e._v("Removing the last word in a sentence impacts activations more than removing the first")]),e._v(" .")])])])]),e._v(" "),r("h4",{attrs:{id:"_3-values-represent-next-token-distributions"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3-values-represent-next-token-distributions"}},[e._v("#")]),e._v(" "),r("strong",[e._v("3. Values Represent Next-Token Distributions")])]),e._v(" "),r("ul",[r("li",[e._v("Values store distributions over output words.")]),e._v(" "),r("li",[r("strong",[e._v("In lower layers, values are uncorrelated with key patterns")]),e._v(" .")]),e._v(" "),r("li",[r("strong",[e._v("In upper layers, values strongly correlate with likely next tokens")]),e._v(" .")]),e._v(" "),r("li",[e._v("Agreement rate between "),r("strong",[e._v("key activations and value-predicted tokens rises in deeper layers")]),e._v(" .")])]),e._v(" "),r("h4",{attrs:{id:"_4-aggregation-of-memories-produces-final-prediction"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-aggregation-of-memories-produces-final-prediction"}},[e._v("#")]),e._v(" "),r("strong",[e._v("4. Aggregation of Memories Produces Final Prediction")])]),e._v(" "),r("ul",[r("li",[e._v("Each layer "),r("strong",[e._v("combines multiple key-value pairs")]),e._v(" , producing a "),r("strong",[e._v("distribution different from any individual memory")]),e._v(" .")]),e._v(" "),r("li",[e._v("Prediction "),r("strong",[e._v("refinement occurs layer-by-layer")]),e._v("  via residual connections.")]),e._v(" "),r("li",[r("strong",[e._v("At least 30% of predictions are already determined in the lower layers")]),e._v(" .")]),e._v(" "),r("li",[r("strong",[e._v("Upper layers refine predictions, sometimes vetoing earlier predictions")]),e._v(" .")])]),e._v(" "),r("h3",{attrs:{id:"unintuitive-findings"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#unintuitive-findings"}},[e._v("#")]),e._v(" "),r("strong",[e._v("Unintuitive Findings")])]),e._v(" "),r("ol",[r("li",[r("strong",[e._v("Lower layers do not predict tokens well, but upper layers do")])])]),e._v(" "),r("ul",[r("li",[e._v("Unlike self-attention, which is useful throughout the model, "),r("strong",[e._v("feed-forward layers do not contribute much to token prediction in early layers")]),e._v(" .")]),e._v(" "),r("li",[e._v("Instead, early layers "),r("strong",[e._v("store")]),e._v("  patterns that are later "),r("strong",[e._v("utilized in upper layers")]),e._v(" .")])]),e._v(" "),r("ol",{attrs:{start:"2"}},[r("li",[r("strong",[e._v("Memories do not act independently but are composed layer-wise")])])]),e._v(" "),r("ul",[r("li",[e._v("Individual key-value memories are not direct predictors.")]),e._v(" "),r("li",[e._v("Instead, the model "),r("strong",[e._v("aggregates multiple memories")]),e._v("  at each layer to form a final output distribution.")])]),e._v(" "),r("ol",{attrs:{start:"3"}},[r("li",[r("strong",[e._v("Residual connections mostly retain earlier predictions rather than overriding them")])])]),e._v(" "),r("ul",[r("li",[e._v("In most cases, the "),r("strong",[e._v("residual connection output remains the same")]),e._v("  through layers, with minor refinements.")]),e._v(" "),r("li",[e._v("Occasionally, "),r("strong",[e._v("feed-forward layers override residual outputs")]),e._v(" , leading to large shifts in predictions.")])]),e._v(" "),r("h3",{attrs:{id:"practical-implications"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#practical-implications"}},[e._v("#")]),e._v(" "),r("strong",[e._v("Practical Implications")])]),e._v(" "),r("ul",[r("li",[r("strong",[e._v("Better model interpretability:")]),e._v(" "),r("ul",[r("li",[e._v("Automating pattern detection in keys could improve transparency in transformers.")])])]),e._v(" "),r("li",[r("strong",[e._v("Memory efficiency:")]),e._v(" "),r("ul",[r("li",[e._v("Understanding how transformers use feed-forward layers could enable "),r("strong",[e._v("parameter reduction techniques")]),e._v(" .")])])]),e._v(" "),r("li",[r("strong",[e._v("Security concerns:")]),e._v(" "),r("ul",[r("li",[e._v("Key-value memories "),r("strong",[e._v("store training data patterns")]),e._v(" , posing "),r("strong",[e._v("data leakage risks")]),e._v(" .")])])])]),e._v(" "),r("h3",{attrs:{id:"future-research-directions"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#future-research-directions"}},[e._v("#")]),e._v(" "),r("strong",[e._v("Future Research Directions")])]),e._v(" "),r("ol",[r("li",[r("strong",[e._v("Embedding Space Transformations Across Layers")])])]),e._v(" "),r("ul",[r("li",[e._v("How does the representation space evolve through layers?")]),e._v(" "),r("li",[e._v("Why does token prediction improve only in upper layers?")])]),e._v(" "),r("ol",{attrs:{start:"2"}},[r("li",[r("strong",[e._v("Extending to Other Transformer Architectures")])])]),e._v(" "),r("ul",[r("li",[e._v("Do BERT, T5, or vision transformers exhibit the same key-value behavior?")])]),e._v(" "),r("ol",{attrs:{start:"3"}},[r("li",[r("strong",[e._v("Improving Transformer Efficiency")])])]),e._v(" "),r("ul",[r("li",[e._v("Can we "),r("strong",[e._v("prune redundant keys")]),e._v("  while preserving performance?")])]),e._v(" "),r("h3",{attrs:{id:"conclusion"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[e._v("#")]),e._v(" "),r("strong",[e._v("Conclusion")])]),e._v(" "),r("p",[e._v("This paper "),r("strong",[e._v("demystifies the function of feed-forward layers in transformers")]),e._v("  by revealing their "),r("strong",[e._v("role as key-value memories")]),e._v(".")]),e._v(" "),r("p",[e._v("The insights gained provide a foundation for:")]),e._v(" "),r("ul",[r("li",[r("strong",[e._v("Understanding how transformers make predictions.")])]),e._v(" "),r("li",[r("strong",[e._v("Optimizing transformer architectures for efficiency.")])]),e._v(" "),r("li",[r("strong",[e._v("Developing interpretability and security mechanisms for language models.")])])]),e._v(" "),r("p",[r("strong",[e._v("Final Thoughts")]),e._v("\nThis paper is particularly valuable for researchers "),r("strong",[e._v("exploring transformer internals")]),e._v(".\nIts key contribution is a "),r("strong",[e._v("paradigm shift")]),e._v("  in understanding feed-forward layers—not just as nonlinear projections, but as "),r("strong",[e._v("dynamic memory components that store patterns and predict next-token distributions")]),e._v(" .")]),e._v(" "),r("hr")])}),[],!1,null,null,null);r.default=a.exports}}]);