<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Memory Optimizations in LLM | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.ba1036cc.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.6d8a25ce.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/97.1f728656.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.a4e38144.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.e2b30df6.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.23638719.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.63faa9af.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.e14139ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.87054a33.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.da764b86.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.68726083.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.5827ba3f.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.e5798645.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.deb9a6dc.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.898f2980.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.a2683c1e.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.d91212af.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.91accf9b.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.0207ca27.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.9a9464c6.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.b8ef3fcb.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.08f9c0ee.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.2c1eefa4.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.f3a7838a.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.e5d2b02f.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.2db9abc0.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.66a036a6.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.b2d875fd.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.e2a67539.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.a896d6d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.f3404c41.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.c77b97f1.js"><link rel="prefetch" href="/qishao-notes/assets/js/126.1ef622ca.js"><link rel="prefetch" href="/qishao-notes/assets/js/127.7a3764ce.js"><link rel="prefetch" href="/qishao-notes/assets/js/128.a0d1ab6b.js"><link rel="prefetch" href="/qishao-notes/assets/js/129.5ed5954f.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.33791185.js"><link rel="prefetch" href="/qishao-notes/assets/js/130.659a73de.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.35cdaa5c.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.ba5a7695.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.8dd9af70.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.b688542f.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.ba9d4baf.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.42f21f4f.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.d41d755a.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.49321259.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.96a79aa3.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.53082807.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.f485ff78.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.b7629e59.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.ff0533c0.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.596c7d2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.dfda1a33.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.e1097275.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.4b34b005.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.a37871e7.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.9c58429f.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.1c45955e.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.7f08612e.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.0b761d24.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.b14f6e3a.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.21d1843c.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.c95eff2e.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.1118972c.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.c924945e.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.bae99c2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.89b382c5.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.c81aff9d.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.0ae91820.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.f5fc0a4e.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.685b325b.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.dde88fd3.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.c879de96.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.12a88187.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.4a1ed3f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.b95dad0e.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.8b03d117.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.b803c07a.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.4a8d3a68.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.14864548.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.bfe1dda8.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.9480e434.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.9a082998.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.a3b517e3.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.e6a3f4e4.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.e9d80e88.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.861f9f7a.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.f0fbf90b.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.3c9be637.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.65de7c88.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.744e3e66.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.71606e3b.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.b0f3a491.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.0e13cd4c.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.021a0ef2.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.747af7e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.7d99e80d.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.0166a661.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.1e07ff8f.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.3b4ae10c.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.4853193a.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.8f7da019.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.3a589a2f.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.8deb1a9c.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.3e2fcd7c.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.9480ce48.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.9c8ec88e.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.68538f63.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.fcb5bc6d.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.08df02ed.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.7280a683.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.8c457734.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.5d9b51d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.1452152d.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.1fb87437.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.53c6d81d.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.e4e19469.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.2d5d9b41.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.d0a4995a.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.e59c5ce4.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.7d0e7d9f.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.201ebb72.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.4e398438.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.8e749192.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.9dfed52c.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.66a19165.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.2bba59c2.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.f04fc93e.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.747a996c.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.99181d1e.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Paper List</a></li><li><a href="/qishao-notes/pages/dc7043/" class="sidebar-link">Efficient LLM</a></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" class="sidebar-link">Summery of Inner Workings of LLM</a></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" aria-current="page" class="active sidebar-link">Memory Optimizations in LLM</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7048/#memory-optimizations" class="sidebar-link">Memory Optimizations</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7048/#_1-c99-y2024-full-parameter-fine-tuning-for-large-language-models-with-limited-resources" class="sidebar-link">1. [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7048/#_2-c25-y2024-flora-low-rank-adapters-are-secretly-gradient-compressors" class="sidebar-link">2. [C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7048/#_3-c42-y2024-galore-memory-efficient-llm-training-by-gradient-low-rank-projection" class="sidebar-link">3. [C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7048/#_4-c0-2024-compact-compressed-activations-for-memory-efficient-llm-training" class="sidebar-link">4. [C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7048/#_5-c2-2024-protrain-efficient-llm-training-via-adaptive-memory-management" class="sidebar-link">5. [C2 2024] ProTrain: Efficient LLM Training via Adaptive Memory Management :+1:</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7048/#background" class="sidebar-link">Background</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7048/#contribution" class="sidebar-link">Contribution</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7048/#_6-c6-2024-memo-fine-grained-tensor-management-for-ultra-long-context-llm-training" class="sidebar-link">6. [C6 2024] Memo: Fine-grained Tensor Management For Ultra-long Context LLM Training</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7048/#_7-2025-efficient-llm-inference-with-activation-checkpointing-and-hybrid-caching" class="sidebar-link">7. [2025] Efficient LLM Inference with Activation Checkpointing and Hybrid Caching</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7048/#_8-6-elixir-train-a-large-language-model-on-a-small-gpu-cluster" class="sidebar-link">8. [6] Elixir: Train a Large Language Model on a Small GPU Cluster</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7048/#background-2" class="sidebar-link">Background</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7048/#_7-llmem-estimating-gpu-memory-usage-for-fine-tuning-pre-trained-llms" class="sidebar-link">[7] LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7048/#_12-asplos-vattention-dynamic-memory-management-for-serving-llms-without-pagedattention" class="sidebar-link">[12 ASPLOS] vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention</a></li></ul></li><li><a href="/qishao-notes/pages/dc7049/" class="sidebar-link">Reasoning in LLM</a></li><li><a href="/qishao-notes/pages/dc7050/" class="sidebar-link">LLM Mixed Precision &amp; Quantization &amp; Outlier</a></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" class="sidebar-link">LLM Scaling Law</a></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7059/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li><li><a href="/qishao-notes/pages/dc7060/" class="sidebar-link">LLM MOE Inference</a></li><li><a href="/qishao-notes/pages/dc7061/" class="sidebar-link">LLM Compression</a></li><li><a href="/qishao-notes/pages/dc7062/" class="sidebar-link">LLM Optimizer Optimization</a></li><li><a href="/qishao-notes/pages/dc7063/" class="sidebar-link">LLM Posttraining</a></li><li><a href="/qishao-notes/pages/dc7064/" class="sidebar-link">LLM MICRO - ISCA - HPCA</a></li><li><a href="/qishao-notes/pages/dc7065/" class="sidebar-link">LLM Prefilling &amp; Decoding Split</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-01-26</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">Memory Optimizations in LLM<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources</li> <li>[C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors</li> <li>[C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</li> <li>[C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training</li> <li>[C2 2024] ProTrain: Efficient LLM Training via Adaptive Memory Management</li> <li>[C6 2024] Memo: Fine-grained Tensor Management For Ultra-long Context LLM Training</li> <li>[2025] Efficient LLM Inference with Activation Checkpointing and Hybrid Caching</li> <li>[6] Elixir: Train a Large Language Model on a Small GPU Cluster</li> <li>[7] LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs <br> This is a very good paper that estimate memory usage based on different parallism. 👍</li> <li>[12] vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention</li></ol> <hr> <h2 id="memory-optimizations"><a href="#memory-optimizations" class="header-anchor">#</a> <strong>Memory Optimizations</strong></h2> <ul><li><strong>Activation Checkpointing</strong><br>
Recomputation during backward pass.</li> <li><strong>Quantization-Aware Training (QAT)</strong><br>
Train with INT8/FP8 precision.</li> <li><strong>Dynamic Memory Allocation</strong><br>
Buffer reuse to avoid fragmentation.</li> <li><strong>Low-Rank Gradient Projection (GaLore)</strong><br> <strong>NEW</strong> Compress gradients via low-rank approximations during training.</li></ul> <hr> <h2 id="_1-c99-y2024-full-parameter-fine-tuning-for-large-language-models-with-limited-resources"><a href="#_1-c99-y2024-full-parameter-fine-tuning-for-large-language-models-with-limited-resources" class="header-anchor">#</a> 1. [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources</h2> <ul><li>Use SGD instead of Adam for fine-tuning weights.</li> <li>Update layer by layer in backward pass. Traditional Adam will backward probgation all layers and then update weigths.</li> <li>SGD also avoid state memory of ADAM.</li></ul> <p><img src="https://github.com/user-attachments/assets/c1e7d018-84e2-49f7-bd74-cdf4edaa0343" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/a98a808f-1cb1-4432-b0d0-7c51e9f4ee48" alt="image"></p> <hr> <h2 id="_2-c25-y2024-flora-low-rank-adapters-are-secretly-gradient-compressors"><a href="#_2-c25-y2024-flora-low-rank-adapters-are-secretly-gradient-compressors" class="header-anchor">#</a> 2. [C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors</h2> <p>This paper discovers that LORA can be approximated by a random projection.</p> <p>LORA restricts overall weights update matrices to be low-rank.</p> <p>FLORA use <em>random projection matrix</em>, which allows high-rank update gradients.</p> <blockquote><p>Our intuition arises from investigating LoRA and observing that a LoRA update is dominated by a random projection, which compresses the gradient into a
lower-dimensional space.
Our FLORA resamples the random projection and is able to mitigate the low-rank limitation of LoRA. Further, our approach only stores the compressed gradient
accumulation and momentum, thus saving the memory usage of optimization states to the sublinear level.</p></blockquote> <p>Gradident Accumulation:</p> <ul><li>Gradient accumulation stores the sum of gradients over multiple batches to simulate a larger effective batch size (helpful when memory limits prevent using large batches).</li> <li>Normally, this requires a memory buffer equal to the model size to store the full gradient matrix.</li></ul> <p>Momentum</p> <ul><li>Momentum smooths gradient updates by keeping an exponentially weighted moving average (EMA) of past gradients.</li> <li>Maintaining momentum for large models requires significant memory since an additional buffer equal to the model size is needed.</li></ul> <p>FLORA Compression:</p> <ul><li>compress gradients accumulation: Applying a random projection matrix A to reduce the dimensionality of the gradients.</li> <li>compress momentum: Using random projection to compress the momentum term M.</li></ul> <hr> <h2 id="_3-c42-y2024-galore-memory-efficient-llm-training-by-gradient-low-rank-projection"><a href="#_3-c42-y2024-galore-memory-efficient-llm-training-by-gradient-low-rank-projection" class="header-anchor">#</a> 3. [C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</h2> <p><img src="https://github.com/user-attachments/assets/3ddb7188-8d90-4232-8be1-cb570a74bc56" alt="image"></p> <blockquote><p>Galore: gradient Low-Rank Projection (GaLore), a training strategy that allows fullparameter learning but is more memory-efficient than common low-rank adaptation  methods such as LoRA.
Key idea is to leverage the slowchanging low-rank structure of the gradient G(m×n) of the weight matrix W, rather than trying to approximate the weight matrix itself as low rank.
while the weight matrices are not necessarily low-rank, the gradient indeed becomes low-rank during the training for certain gradient forms and associated network
architectures.</p></blockquote> <hr> <h2 id="_4-c0-2024-compact-compressed-activations-for-memory-efficient-llm-training"><a href="#_4-c0-2024-compact-compressed-activations-for-memory-efficient-llm-training" class="header-anchor">#</a> 4. [C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training</h2> <img src="https://github.com/user-attachments/assets/37a40cf7-5a3b-4c55-b847-1fb1e9c732a5" style="width:600px;height:auto;"> <blockquote><p>By storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters.
CompAct saves low-rank compressed activations during the forward pass, instead of the full activation tensors.
The resulting gradients are low-rank as well, also reducing the size of optimizer states.
As CompAct decompresses the gradients back to full size only for the update step, it compresses a large part of the compute graph, which in turn translates to major memory savings.</p></blockquote> <p>CompAct is a logical next step from previous work, moving from <strong>low-rank parameters</strong>, through <strong>compressed low-rank gradients</strong> , to <strong>compressed activations</strong>.</p> <blockquote><p>compared to GaLore, our approach may be viewed as a simple change in the order of operations, applying the compression one step before GaLore does, to the <strong>activations</strong> rather than to the <strong>gradients</strong>.</p></blockquote> <p><img src="https://github.com/user-attachments/assets/c0e05d1b-b19b-4bb0-92df-4842010b6502" alt="image"></p> <hr> <h2 id="_5-c2-2024-protrain-efficient-llm-training-via-adaptive-memory-management"><a href="#_5-c2-2024-protrain-efficient-llm-training-via-adaptive-memory-management" class="header-anchor">#</a> 5. [C2 2024] ProTrain: Efficient LLM Training via Adaptive Memory Management 👍</h2> <h3 id="background"><a href="#background" class="header-anchor">#</a> Background</h3> <p><strong>Model State</strong>: Zero Redundancy Optimizer (ZeRO) (37; 51) distributes them across multiple GPUs, leveraging aggregated memory capacity to accommodate large models in data parallelism.</p> <p><strong>activations, gradient checkpointing</strong> reduces memory consumption by discarding certain activations during the forward pass and recomputing them during the backward pass.</p> <h3 id="contribution"><a href="#contribution" class="header-anchor">#</a> Contribution</h3> <ul><li><p>To reduce memory consumption, ProTrain adaptively decides whether to use offloading or gradient checkpointing, determines the amount of model states and activations to offload and the number of transformer blocks to apply gradient checkpointing, all without user inputs.</p></li> <li><p>For computation, ProTrain keeps forward/backward computation on the GPU for efficiency, while dynamically determining the portion of parameter updates to be performed on the CPU and GPU.
Additionally, ProTrain performs CPU parameter updates concurrently with backward computation on the GPU to hide the overhead of CPU updates.</p></li> <li><p>ProTrain overlaps IO communication with computation by proactively prefetching future parameters during forward/backward computation, parallelizing gradient offloading with backward computation, and swapping activations only when
the overhead can be hidden by computation.</p></li> <li><p>ProTrain proposes a Chunk-Based Model State Management system that organizes model states into uniformly sized chunks</p></li> <li><p>ProTrain also proposes <strong>Block-Wise Activation Management</strong> to handle activations at the <strong>transformer block</strong> level, performing swapping or gradient checkpointing as needed for each block.</p></li> <li><p>To hide the swapping overhead, ProTrain applies interleaved swapping and checkpointing, where each block of swapping is typically followed by multiple blocks of checkpointing.
This ensures that ProTrain’s swapping reduces memory usage without compromising performance.</p></li></ul> <p><strong>Discussion about zero</strong></p> <p>ZeRO operates in three stages</p> <ul><li>ZeRO-1 partitions optimizer states across GPUs</li> <li>ZeRO-2 extends this by also distributing gradients</li> <li>ZeRO-3 further divides the parameters, which are required to be gathered before forward/backward computation.</li></ul> <p><em>The most interesting contribution of this work to me, it is the interleaved gradient checkpointing and swapping.</em></p> <p>Since the training follows the specific sequence:</p> <ul><li>the last layer: no optimization</li> <li>the second last layer: gradient checkpointing</li> <li>the third last layer: swapping</li></ul> <p><strong>In the backward pass, blocks without optimization are processed first, consuming activations and freeing memory for subsequent checkpointing and swapping.</strong></p> <img src="https://github.com/user-attachments/assets/b0daba0b-50b0-4d9d-b94b-1697910afedb" style="width:600px;height:auto;"> <hr> <h2 id="_6-c6-2024-memo-fine-grained-tensor-management-for-ultra-long-context-llm-training"><a href="#_6-c6-2024-memo-fine-grained-tensor-management-for-ultra-long-context-llm-training" class="header-anchor">#</a> 6. [C6 2024] Memo: Fine-grained Tensor Management For Ultra-long Context LLM Training</h2> <p>I feel this paper shares the same idea from [C2 2024] ProTrain: Efficient LLM Training via Adaptive Memory Management.</p> <p>like the balance between swapping and recomputing gradient, here they argue that for long context llm training, swapping might be more economic compared with gradient checkpointing.</p> <p><strong>Feel the trend on the right... Full offload might be more economic when length grows larger.</strong></p> <img src="https://github.com/user-attachments/assets/43d4b157-76a2-4022-8620-c393ae0b0a70" style="width:600px;height:auto;"> <img src="https://github.com/user-attachments/assets/2147e9ea-ed8e-4609-a03c-200dbc85842c" style="width:600px;height:auto;"> <p>Contemporary mainstream LLM training frameworks such as Megatron-LM and DeepSpeed prefer activation recomputation to swapping, which is due to the fact that the GPU computing ability has a far more rapid growth than the
connectivity between CPU and GPU memory in the past few years.</p> <p>However, we find that the situation is a bit different in long context training of LLMs. Denote (𝑠) as the sequence length. The computation complexity of one transformer layer is 𝑂(𝑠^2), while the activation memory complexity is 𝑂(𝑠) thanks to FlashAttention.</p> <p>During GPU computation, we can leverage the idle CPU-GPU bandwidth, offloading activations to CPU memory during the forward
pass, and fetching the activations during the backward pass.</p> <p>As the sequence length increases, there is greater potential for overlapping computation and communication, given that their time requirements scale <strong>quadratically</strong> and <strong>linearly</strong> with the sequence length.</p> <p>We introduce a fine-grained activation recomputation and swapping mechanism to manage the skeletal activations. We consider both tensor-level and token-level activation management.</p> <img src="https://github.com/user-attachments/assets/7491aeab-f900-4d96-866b-aa7d68d94586" style="width:600px;height:auto;"> <hr> <h2 id="_7-2025-efficient-llm-inference-with-activation-checkpointing-and-hybrid-caching"><a href="#_7-2025-efficient-llm-inference-with-activation-checkpointing-and-hybrid-caching" class="header-anchor">#</a> 7. [2025] Efficient LLM Inference with Activation Checkpointing and Hybrid Caching</h2> <p>The activation cache stores activation checkpoints generated during intermediate inference stages, allowing the fast recomputation of KV cache while model parameters are transferred to GPU from host memory.</p> <p>Unlike conventional methods that recompute the KV cache from scratch using token IDs, the activation cache allows bypassing projection and FFN operations.</p> <p>To address the memory and communication challenges of the host memory offloading, this study proposes a novel activation checkpointing scheme based on Activation caching.</p> <p>Unlike the token recomputation relying on a full prefill step, activation values of all layers are stored in the memory. Computing the keys and values of a layer from the activation values requires a modest computation capacity.</p> <p><strong>Keeping activation checkpoints</strong> instead of keys and values reduce the memory capacity consumption and the communication traffic by half.</p> <blockquote><p>The major arguement: activation memory is less than K&amp;V, K&amp;V could be recomputed.</p></blockquote> <img src="https://github.com/user-attachments/assets/f7fd3b57-a94f-4429-82f2-47805c3f535d" style="width:600px;height:auto;"> <p>since completely removing the KV cache incurs excessively repetitive computations across token generations, maintaining only a portion of the context as KV cache and the rest as token IDs is more promising.</p> <p>As balancing computation and communication is a key challenge for improving throughput, the ratio of KVs to token IDs must be
adjusted carefully.</p> <img src="https://github.com/user-attachments/assets/74164b54-d369-4d97-91ca-41026c7d563c" style="width:600px;height:auto;"> <hr> <h2 id="_8-6-elixir-train-a-large-language-model-on-a-small-gpu-cluster"><a href="#_8-6-elixir-train-a-large-language-model-on-a-small-gpu-cluster" class="header-anchor">#</a> 8. [6] Elixir: Train a Large Language Model on a Small GPU Cluster</h2> <img src="https://github.com/user-attachments/assets/7e8a9e6b-c565-4bf9-8584-f6bb0154b79b" style="width:600px;height:auto;"> <p><strong>We argue that using free GPU memory to store optimizer states or retaining gathered parameters during training can improve training throughput.</strong></p> <p>• We build a pre-runtime profiler designed for large models. It is capable of obtaining the computation graph and the memory usage of the model before training. We bring this powerful tool to support large model profiling.
• We introduce rCache to control the degree of memory redundancy. Moreover, we build a search engine to find the optimal configuration, maximizing training efficiency automatically.
Different from previous works, our optimal configuration considers both memory partitioning and memory offloading.</p> <h3 id="background-2"><a href="#background-2" class="header-anchor">#</a> Background</h3> <h4 id="memory-usage"><a href="#memory-usage" class="header-anchor">#</a> Memory Usage</h4> <p>Memory usage during training primarily consists of five components: parameters, gradients, optimizer states, activations, and buffers.</p> <p>Optimizer states are the extra memory footprint consumed by the optimizer.</p> <p>For example, Adam [26] needs to store averaged momentum and variance of gradients.</p> <p>We refer to parameters, gradients, and optimizer states collectively as <strong>model states</strong>.</p> <p>Activations are the intermediate temporary variables generated during training.</p> <p>Typically,  activations are stored for the backward pass to compute gradients.</p> <p>However, their memory usage may vary depending on the training framework.</p> <p>In PyTorch, the <strong>temporary gradients of intermediate tensor variables</strong> can also be viewed as <strong>activations</strong>.</p> <p>Compared to other components, buffers consume a relatively small amount of memory.</p> <p>We assume that buffers are always stored in the GPU for subsequent analysis.</p> <h4 id="mixed-precision-training"><a href="#mixed-precision-training" class="header-anchor">#</a> Mixed Precision Training</h4> <p>The SOTA approach to train large models utilizes both the halfprecision floating-point (FP16) format and the single-precision floating-point (FP32) format during training.</p> <p><strong>Parameters, gradients, and activations</strong> are stored and computed in FP16 to reduce memory usage and improve efficiency.</p> <p>Meanwhile, the accumulation operator in the optimizer update is sensitive to underflow in low-precision formats. The master weight, which is an FP32 copy of the parameters, is used to accumulate gradients in each optimizer update and is rounded to FP16 parameters before the forward pass.</p> <p>In this case, the memory usage of parameters, gradients, and activations is halved, but the memory usage of optimizer states is increased due to the addition of the master weight.</p> <p>For example, if we use Adam and the model size is M, training requires 2M bytes for parameters, 2M bytes for gradients, and 12M bytes for optimizer states.</p> <img src="https://github.com/user-attachments/assets/44c48f9f-1b69-41f9-8e5b-a3a58a034b95" style="width:600px;height:auto;"> <img src="https://github.com/user-attachments/assets/ff98c9bf-fcec-4d6e-94ea-99a4f145c445" style="width:600px;height:auto;"> <hr> <h2 id="_7-llmem-estimating-gpu-memory-usage-for-fine-tuning-pre-trained-llms"><a href="#_7-llmem-estimating-gpu-memory-usage-for-fine-tuning-pre-trained-llms" class="header-anchor">#</a> [7] LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs</h2> <p>Determining the most effective method for achieving rapid fine-tuning while preventing GPU outof-memory issues in a given environment remains unclear.</p> <p>To address this challenge, we introduce LLMem, a solution that estimates the GPU memory consumption when applying distributed finetuning methods across multiple GPUs and identifies the optimal method.</p> <hr> <h2 id="_12-asplos-vattention-dynamic-memory-management-for-serving-llms-without-pagedattention"><a href="#_12-asplos-vattention-dynamic-memory-management-for-serving-llms-without-pagedattention" class="header-anchor">#</a> [12 ASPLOS] vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention</h2> <p><img src="https://github.com/user-attachments/assets/0689a237-6fcb-4d37-959f-f8aaa402d223" alt="My Image" style="width:300px;height:auto;"></p> <p>Pageattention needs two level software mapping.</p> <blockquote><p>For example, the vLLM paper acknowledges that the PagedAttention-based implementation was 20−26% slower than the corresponding none-paged FasterTransformer kernel, primarily due to the overhead of looking up Block-Tables and executing extra branches.</p></blockquote> <blockquote><p>Our analysis reveals that the number of instructions executed in PagedAttention kernels is 7 − 13% higher than the non-paged kernels. Caching page indices also increases register pressure, causing register spilling.</p></blockquote> <p>In vAttention, they use CUDA VMM API to avoid the two level mapping.</p> <p><img src="https://github.com/user-attachments/assets/513e1257-1a54-4667-a656-4fe40161d621" style="width:300px;height:auto;"></p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/13.llm_mem_opt.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/04/06, 23:46:42</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7047/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">List of LLM Optimization Techniques</div></a> <a href="/qishao-notes/pages/dc7049/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Reasoning in LLM</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7047/" class="prev">List of LLM Optimization Techniques</a></span> <span class="next"><a href="/qishao-notes/pages/dc7049/">Reasoning in LLM</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.ba1036cc.js" defer></script><script src="/qishao-notes/assets/js/2.6d8a25ce.js" defer></script><script src="/qishao-notes/assets/js/97.1f728656.js" defer></script>
  </body>
</html>
