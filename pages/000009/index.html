<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Learn TVM | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.a069b46c.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.4d9050ab.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/31.8f3dd2fd.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.8425daf7.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.af60b514.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.a3660bdb.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.2dbed768.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.e9d435f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.70ac7ea8.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.9b548611.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.0bb8780c.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.2d2fedc3.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.57778c1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.0f5a297c.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.0a71d251.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.5328be02.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.4d9c97dd.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.bd8fde5f.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.076566df.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.bdfec127.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.8e797622.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.7f406c2d.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.8d06719f.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.0cd75376.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.e5ecc544.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.6060ae90.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.21a59f86.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.cbad6fad.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.95c9419d.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.cb179c60.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.ba4d690c.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.16ff58d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/126.9f9613b0.js"><link rel="prefetch" href="/qishao-notes/assets/js/127.7dbd2148.js"><link rel="prefetch" href="/qishao-notes/assets/js/128.8f6c7fdc.js"><link rel="prefetch" href="/qishao-notes/assets/js/129.ad89e0c8.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.5efab44e.js"><link rel="prefetch" href="/qishao-notes/assets/js/130.1dc602e8.js"><link rel="prefetch" href="/qishao-notes/assets/js/131.e6be1556.js"><link rel="prefetch" href="/qishao-notes/assets/js/132.eae82686.js"><link rel="prefetch" href="/qishao-notes/assets/js/133.9f779220.js"><link rel="prefetch" href="/qishao-notes/assets/js/134.d18ab66c.js"><link rel="prefetch" href="/qishao-notes/assets/js/135.806bb8c1.js"><link rel="prefetch" href="/qishao-notes/assets/js/136.27418249.js"><link rel="prefetch" href="/qishao-notes/assets/js/137.7f43f2c1.js"><link rel="prefetch" href="/qishao-notes/assets/js/138.01eeee55.js"><link rel="prefetch" href="/qishao-notes/assets/js/139.2b575d84.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.c969a0b3.js"><link rel="prefetch" href="/qishao-notes/assets/js/140.5064fd4b.js"><link rel="prefetch" href="/qishao-notes/assets/js/141.07b81e95.js"><link rel="prefetch" href="/qishao-notes/assets/js/142.b94b0c94.js"><link rel="prefetch" href="/qishao-notes/assets/js/143.0e1ce0b9.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.df53a168.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.67008b70.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.861e1d5d.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.c9558048.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.880240a3.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.61609b8a.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.72b53498.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.05a251e9.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.637bf54a.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.9538dbb9.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.8c76174c.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.8faeebe9.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.914a6398.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.b0085c20.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.7232bd8c.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.dea2283c.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.a0eefaec.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.feb23e67.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.8ffc297f.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.f099fafe.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.83d06270.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.093d9d1a.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.fe8a50b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.072efd5d.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.60eee2cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.c90ef165.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.6aebda8b.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.4afd8139.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.350453bb.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.460fc31f.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.ad822e3a.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.61455ccc.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.9aaf5a2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.3c3f4c37.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.5afe5ead.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.79517f64.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.e4bdf3ee.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.d28e2cac.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.26a65cae.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.5887d547.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.e63d9095.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.92826847.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.87dcb106.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.661d07a0.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.ea5b377a.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.a1fbdc11.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.8745291b.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.897efcf6.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.919fc4e0.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.d2bb5053.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.5459930a.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.52c7ee65.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.57f3eb17.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.750cd420.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.c8443dd9.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.3ecd1c5a.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.76a443ae.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.e05fe896.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.40baa929.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.07170ed6.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.358dd4b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.8574c005.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.0275e1fe.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.88825c83.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.b3660535.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.ca8f05c8.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.0b200c98.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.0075faa9.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.3966b649.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.b83bf513.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.3986868c.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.05ea99eb.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.29f0a0a6.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.c78de5ac.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.f1159767.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.2daca6b4.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.9ef2f78a.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.35751831.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.7b898713.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.59ead3d9.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.036a5ba4.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.36fcf661.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.1559c11a.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.f84f0d29.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.e3d512b0.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.8f36ccf1.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.8ca7fe23.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.88e64e8f.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.4c13edd1.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.5755107c.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.a39d9526.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/000001/" class="sidebar-link">llvm flow</a></li><li><a href="/qishao-notes/pages/000002/" class="sidebar-link">Getting Started with LLVM Core Libraries Chap5 IR</a></li><li><a href="/qishao-notes/pages/000003/" class="sidebar-link">Getting Started with LLVM Core Libraries Chap6 Backend</a></li><li><a href="/qishao-notes/pages/000004/" class="sidebar-link">Learning LLVM Notes</a></li><li><a href="/qishao-notes/pages/000005/" class="sidebar-link">Add New DIY Instruction ACE to LLVM</a></li><li><a href="/qishao-notes/pages/000006/" class="sidebar-link">How does LLVM perform instruction combine</a></li><li><a href="/qishao-notes/pages/000007/" class="sidebar-link">Understand llvm with its source code</a></li><li><a href="/qishao-notes/pages/000008/" class="sidebar-link">Writing TinyRISCV Backend</a></li><li><a href="/qishao-notes/pages/000009/" aria-current="page" class="active sidebar-link">Learn TVM</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/000009/#_0-feel-the-flow-of-tvm-compilation" class="sidebar-link">0. Feel the flow of TVM compilation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-1-model-parsing-and-relay-ir-construction" class="sidebar-link">0.1 Model Parsing and Relay IR Construction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-2-high-level-optimizations-in-relay" class="sidebar-link">0.2 High-Level Optimizations in Relay</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-3-lowering-to-tensor-expression-te" class="sidebar-link">0.3. Lowering to Tensor Expression (TE)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-4-scheduling-in-te" class="sidebar-link">0.4. Scheduling in TE</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-5-lowering-to-tir" class="sidebar-link">0.5. Lowering to TIR</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-6-code-generation" class="sidebar-link">0.6. Code Generation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-7-final-compilation-and-deployment" class="sidebar-link">0.7. Final Compilation and Deployment</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/000009/#_1-model-parsing-and-relay-ir-construction" class="sidebar-link">1. Model Parsing and Relay IR Construction</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-1-graph-level-optimizations" class="sidebar-link">1.1 Graph-Level Optimizations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-2-data-layout-transformations" class="sidebar-link">1.2 Data Layout Transformations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-3-quantization-and-precision-management" class="sidebar-link">1.3 Quantization and Precision Management</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-4-automatic-differentiation" class="sidebar-link">1.4 Automatic Differentiation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-5-high-level-hardware-aware-optimizations" class="sidebar-link">1.5 High-Level Hardware-Aware Optimizations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-6-device-placement" class="sidebar-link">1.6 Device Placement</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-7-meta-pass-management" class="sidebar-link">1.7 Meta-Pass Management</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/000009/#_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir" class="sidebar-link">2 Lowering to Tensor Expression (TE), Scheduing in TE and Lowering into TIR</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-1-converting-relay-ir-to-tensor-expression-te" class="sidebar-link">2.1 Converting Relay IR to Tensor Expression (TE)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-2-abstraction-of-computation-in-tensor-expression-te" class="sidebar-link">2.2 Abstraction of Computation in Tensor Expression (TE)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-3-scheduling-in-tensor-expression" class="sidebar-link">2.3 Scheduling in Tensor Expression</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-4-constructing-low-level-tensor-ir-tir" class="sidebar-link">2.4 Constructing Low-Level Tensor IR (TIR)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-5-device-specific-optimizations" class="sidebar-link">2.5 Device-Specific Optimizations</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/000009/#_3-code-generation" class="sidebar-link">3. Code Generation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-1-gpu-code-generation" class="sidebar-link">3.1 GPU Code Generation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-2-kernel-construction" class="sidebar-link">3.2. Kernel Construction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-3-cublas-cutlass-integration" class="sidebar-link">3.3. cuBLAS/CUTLASS Integration</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-4-target-specific-optimizations" class="sidebar-link">3.4. Target-Specific Optimizations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-5-memory-management" class="sidebar-link">3.5. Memory Management</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-6-overall-codegen-workflow" class="sidebar-link">3.6. Overall Codegen Workflow</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/000010/" class="sidebar-link">Learn TPU_MLIR</a></li><li><a href="/qishao-notes/pages/000011/" class="sidebar-link">MLIR TOY Tutorial</a></li><li><a href="/qishao-notes/pages/000012/" class="sidebar-link">Auto Differentiation in Compiler</a></li><li><a href="/qishao-notes/pages/000013/" class="sidebar-link">MLIR Open Meeting Notes The Torch MLIR Project</a></li><li><a href="/qishao-notes/pages/000014/" class="sidebar-link">MLIR Compiling Flow of Conv2D</a></li><li><a href="/qishao-notes/pages/000015/" class="sidebar-link">MLIR Compiling Flow of Conv2D</a></li><li><a href="/qishao-notes/pages/000016/" class="sidebar-link">MLIR Compiling Flow of Transformer-Decoder</a></li><li><a href="/qishao-notes/pages/000017/" class="sidebar-link">MLIR Essential Concepts</a></li><li><a href="/qishao-notes/pages/000018/" class="sidebar-link">MLIR NVGPU Dialect</a></li><li><a href="/qishao-notes/pages/000019/" class="sidebar-link">MLIR Linalg Dialect</a></li><li><a href="/qishao-notes/pages/000020/" class="sidebar-link">MLIR Bufferization</a></li><li><a href="/qishao-notes/pages/000021/" class="sidebar-link">MLIR Bufferization Passes</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/compiler/#compiler" data-v-06225672>compiler</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2024-12-08</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">Learn TVM<!----></h1> <!----> <div class="theme-vdoing-content content__default"><hr> <h2 id="_0-feel-the-flow-of-tvm-compilation"><a href="#_0-feel-the-flow-of-tvm-compilation" class="header-anchor">#</a> 0. Feel the flow of TVM compilation</h2> <p><strong>Model Definition</strong></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>import tvm
from tvm import relay, te
import numpy as np

# Model parameters
batch_size, input_dim, output_dim = 32, 128, 64

# Relay model
x = relay.var(&quot;x&quot;, shape=(batch_size, input_dim), dtype=&quot;float32&quot;)
w = relay.var(&quot;w&quot;, shape=(input_dim, output_dim), dtype=&quot;float32&quot;)
y = relay.nn.dense(x, w)
model = relay.Function([x, w], y)

# Input data
x_data = np.random.rand(batch_size, input_dim).astype(&quot;float32&quot;)
w_data = np.random.rand(input_dim, output_dim).astype(&quot;float32&quot;)
params = {&quot;w&quot;: w_data}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p><strong>Relay IR</strong>
The relay.Function represents the high-level computational graph.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>print(model)

# Simplified Relay IR:
# fn (%x: Tensor[(32, 128), float32], %w: Tensor[(128, 64), float32]) {
#   nn.dense(%x, %w)
# }

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p><strong>Lowering to Tensor Expression (TE)</strong></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>with tvm.transform.PassContext(opt_level=3):
    mod, params = relay.build_module.bind_params_by_name(model, params)
    graph, lib, params = relay.build(mod, target=&quot;cuda&quot;, params=params)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>In Tensor Expression (TE), computations are represented using tensor operations:</p> <ul><li>Compute: C[i, j] = sum(A[i, k] * B[k, j] for k in range(input_dim))</li> <li>Schedule: Operations like tiling, thread binding, and vectorization are applied.</li></ul> <p><strong>Example TE for matrix multiplication:</strong></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>A = te.placeholder((batch_size, input_dim), name=&quot;A&quot;)
B = te.placeholder((input_dim, output_dim), name=&quot;B&quot;)
k = te.reduce_axis((0, input_dim), name=&quot;k&quot;)

# Compute definition
C = te.compute(
    (batch_size, output_dim),
    lambda i, j: te.sum(A[i, k] * B[k, j], axis=k),
    name=&quot;C&quot;
)

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p><strong>TIR (Tensor IR)</strong></p> <p>After applying schedules, TE is lowered to TIR. TIR is a low-level representation focusing on loops and memory hierarchy.</p> <p>Example TIR (simplified):</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>@tvm.script.ir_module
class MyModule:
    @tvm.tir.prim_func
    def main(A: tvm.tir.Buffer[(32, 128), &quot;float32&quot;],
             B: tvm.tir.Buffer[(128, 64), &quot;float32&quot;],
             C: tvm.tir.Buffer[(32, 64), &quot;float32&quot;]):
        for i in range(32):  # Outer loop for batch
            for j in range(64):  # Outer loop for output_dim
                with tvm.tir.block(&quot;C&quot;):
                    C[i, j] = 0.0
                    for k in range(128):  # Reduction loop for input_dim
                        C[i, j] += A[i, k] * B[k, j]

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p><strong>CUDA Code Generation</strong></p> <p>Finally, TIR is compiled into CUDA code:</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>print(lib.imported_modules[0].get_source())

# Simplified CUDA Code:
# __global__ void fused_dense(float* __restrict__ A, float* __restrict__ B, float* __restrict__ C) {
#   int idx = threadIdx.x + blockIdx.x * blockDim.x;
#   if (idx &lt; 2048) {  // 32 * 64 = batch_size * output_dim
#     int i = idx / 64;  // Batch index
#     int j = idx % 64;  // Output index
#     float result = 0.0;
#     for (int k = 0; k &lt; 128; ++k) {  // Reduction loop
#       result += A[i * 128 + k] * B[k * 64 + j];
#     }
#     C[idx] = result;
#   }
# }

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p><strong>Summary of Intermediate Representations</strong></p> <ol><li>Relay IR: High-level computational graph, defines operators like nn.dense.</li> <li>TE: Abstracts computation using mathematical tensor operations and supports scheduling primitives.</li> <li>TIR: Low-level, loop-based representation with explicit memory hierarchy.</li> <li>CUDA Code: GPU kernel for matrix multiplication, including thread and block mappings.</li></ol> <hr> <h3 id="_0-1-model-parsing-and-relay-ir-construction"><a href="#_0-1-model-parsing-and-relay-ir-construction" class="header-anchor">#</a> 0.1 Model Parsing and Relay IR Construction</h3> <ul><li>Input
Model in high-level frameworks like TensorFlow, PyTorch, or ONNX.</li> <li>Process:
<ul><li>TVM parses the input model and converts it into Relay IR, a hig-h-level intermediate representation.</li> <li>The Relay IR describes the computational graph with operator-level abstractions.</li></ul></li> <li>Key Functions:
relay.frontend.from_pytorch(), relay.frontend.from_onnx() in src/relay/frontend/.</li></ul> <h3 id="_0-2-high-level-optimizations-in-relay"><a href="#_0-2-high-level-optimizations-in-relay" class="header-anchor">#</a> 0.2 High-Level Optimizations in Relay</h3> <ul><li>Input: Relay IR.</li> <li>Process:
<ul><li>Optimize the Relay IR for performance and hardware compatibility through:</li> <li>Operator Fusion: Fuse adjacent operations.</li> <li>Constant Folding: Precompute static expressions.</li> <li>Layout Transformation: Adjust data layouts (e.g., NCHW → NCHWc).</li> <li>Quantization: Lower precision where applicable.</li> <li>Common Subexpression Elimination.
Finalize the optimized Relay graph.</li></ul></li> <li>Key Functions:<br>
src/relay/transforms/ for passes like fuse_ops.cc, alter_op_layout.cc.</li></ul> <h3 id="_0-3-lowering-to-tensor-expression-te"><a href="#_0-3-lowering-to-tensor-expression-te" class="header-anchor">#</a> 0.3. Lowering to Tensor Expression (TE)</h3> <ul><li>Input: Optimized Relay IR.</li> <li>Process:
<ul><li>Translate high-level Relay operators into Tensor Expressions (TE).</li> <li>TE represents computations as mathematical tensor operations and allows for:
<ul><li>Abstraction of computation patterns (e.g., matrix multiplication).</li> <li>Introduction of scheduling primitives (e.g., tiling, unrolling, vectorization).</li></ul></li></ul></li> <li>Key Functions:
<ul><li>src/relay/backend/te_compiler.cc: Bridges Relay IR and TE.</li> <li>src/te/tensor.cc: Constructs tensor expressions.</li></ul></li></ul> <h3 id="_0-4-scheduling-in-te"><a href="#_0-4-scheduling-in-te" class="header-anchor">#</a> 0.4. Scheduling in TE</h3> <ul><li>Input: Tensor Expressions.</li> <li>Process:
<ul><li>Apply scheduling primitives to improve performance:</li> <li>Tiling: Divide tensors into smaller chunks for parallelism.</li> <li>Unrolling: Optimize loops for instruction pipelining.</li> <li>Thread/Block Mapping: Map computations to GPU threads and blocks.</li> <li>Vectorization: Use SIMD instructions where applicable.</li> <li>Refines Tensor Expressions into Tensor Intermediate Representation (TIR).</li></ul></li> <li>Key Functions:
<ul><li>src/te/schedule/ for scheduling functions.</li> <li>src/te/schedule/schedule_dataflow_rewrite.cc: Handles dataflow rewrite scheduling.</li></ul></li></ul> <h3 id="_0-5-lowering-to-tir"><a href="#_0-5-lowering-to-tir" class="header-anchor">#</a> 0.5. Lowering to TIR</h3> <ul><li>Input: Tensor Expressions with schedules.</li> <li>Process:
<ul><li>Convert TE into Tensor IR (TIR), a low-level IR closer to device execution.</li> <li>Perform device-specific optimizations for CUDA (e.g., thread hierarchy mapping).</li></ul></li> <li>Key Functions:
<ul><li>src/tir/transform/ for device-specific passes like loop unrolling and thread binding.</li></ul></li></ul> <h3 id="_0-6-code-generation"><a href="#_0-6-code-generation" class="header-anchor">#</a> 0.6. Code Generation</h3> <ul><li>Input: TIR optimized for CUDA.</li> <li>Process:
<ul><li>Code Generation:Translate TIR into CUDA kernels. Use TVM's built-in CUDA code generator.</li> <li>Calling cuBLAS/cuDNN or CUTLASS:
<ul><li>For specific operations (e.g., GEMM), call external libraries.</li> <li>Determine the sequence of library calls and parameters based on operator attributes.</li></ul></li> <li>Memory Allocation: Analyze dataflow to allocate memory efficiently on GPU.</li></ul></li> <li>Key Functions:
<ul><li>CUDA Codegen:
src/target/source/codegen_cuda.cc: Generates CUDA source code.</li> <li>External Libraries:
src/runtime/contrib/cublas.cc: Integrates with cuBLAS.
src/runtime/contrib/cudnn.cc: Integrates with cuDNN.
src/contrib/cutlass/: Integrates with CUTLASS.</li></ul></li></ul> <h3 id="_0-7-final-compilation-and-deployment"><a href="#_0-7-final-compilation-and-deployment" class="header-anchor">#</a> 0.7. Final Compilation and Deployment</h3> <ul><li>Input: CUDA source code.</li> <li>Process:
<ul><li>Compile CUDA source code using NVCC or the TVM runtime.</li> <li>Deploy the compiled kernel and runtime modules.</li></ul></li> <li>Key Functions:
<ul><li>src/target/source/: Handles code generation.</li> <li>src/runtime/: Manages runtime execution and deployment.</li></ul></li></ul> <hr> <h2 id="_1-model-parsing-and-relay-ir-construction"><a href="#_1-model-parsing-and-relay-ir-construction" class="header-anchor">#</a> 1. Model Parsing and Relay IR Construction</h2> <p>In TVM, high-level optimization in the Relay IR phase includes several graph-level optimizations, data layout transformations, and other functional passes.</p> <p>These optimizations are implemented in various source files under src/relay/transform and src/relay/op directories.</p> <p>Below is a categorized list of these optimizations along with their corresponding source code files and functions:</p> <h3 id="_1-1-graph-level-optimizations"><a href="#_1-1-graph-level-optimizations" class="header-anchor">#</a> 1.1 Graph-Level Optimizations</h3> <p>Graph-level optimizations restructure or simplify the computation graph for better performance.</p> <table><thead><tr><th>Optimization	Source</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Constant Folding</td> <td>src/relay/transform/fold_constant.cc</td> <td>FoldConstant, ConstantFolder</td></tr> <tr><td>Operator Fusion</td> <td>src/relay/transform/fuse_ops.cc</td> <td>FuseOps, FuseMutator, PatternMatcher</td></tr> <tr><td>Dead Code Elimination (DCE)</td> <td>src/relay/transform/eliminate_common_subexpr.cc</td> <td>EliminateCommonSubexpr</td></tr> <tr><td>Common Subexpression Elimination</td> <td>src/relay/transform/eliminate_common_subexpr.cc</td> <td>EliminateCommonSubexpr</td></tr> <tr><td>Simplify Inference</td> <td>src/relay/transform/simplify_inference.cc</td> <td>SimplifyInference, SimplifyInferenceMutator</td></tr> <tr><td>Call Folding</td> <td>src/relay/transform/fold_call.cc</td> <td>FoldCall</td></tr> <tr><td>Inline Functions</td> <td>src/relay/transform/inline.cc</td> <td>Inline, InlineMutator</td></tr> <tr><td>Prune Unused Functions</td> <td>src/relay/transform/prune_unused_functions.cc</td> <td>PruneUnusedFunctions</td></tr></tbody></table> <h3 id="_1-2-data-layout-transformations"><a href="#_1-2-data-layout-transformations" class="header-anchor">#</a> 1.2 Data Layout Transformations</h3> <p>These optimizations adjust the layout of tensors for better memory access patterns and compatibility with target hardware.</p> <table><thead><tr><th>Transformation</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Alter Layout</td> <td>src/relay/transform/alter_op_layout.cc</td> <td>AlterOpLayout, AlterOpLayoutRewriter</td></tr> <tr><td>Convert Layout</td> <td>s	src/relay/transform/convert_layout.cc</td> <td>ConvertLayout</td></tr> <tr><td>Fold Scale Axis</td> <td>src/relay/transform/fold_scale_axis.cc</td> <td>FoldScaleAxis, ScaleAxisSimplifier</td></tr> <tr><td>Layout Optimization</td> <td>src/relay/transform/layout_rewrite.cc</td> <td>LayoutRewrite</td></tr></tbody></table> <h3 id="_1-3-quantization-and-precision-management"><a href="#_1-3-quantization-and-precision-management" class="header-anchor">#</a> 1.3 Quantization and Precision Management</h3> <p>TVM supports quantization optimizations for reduced precision operations.</p> <table><thead><tr><th>Optimization</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Quantize</td> <td>src/relay/quantize/quantize.cc</td> <td>Quantize, CreateQuantizePass</td></tr> <tr><td>Dequantize</td> <td>src/relay/quantize/dequantize.cc</td> <td>Dequantize</td></tr> <tr><td>SimplifyQuantize</td> <td>src/relay/transform/simplify_quantize.cc</td> <td>SimplifyQuantize, SimplifyQuantizeRewriter</td></tr></tbody></table> <h3 id="_1-4-automatic-differentiation"><a href="#_1-4-automatic-differentiation" class="header-anchor">#</a> 1.4 Automatic Differentiation</h3> <p>TVM includes an autodiff system for neural networks.</p> <table><thead><tr><th>Transformation</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Reverse Mode Autodiff</td> <td>src/relay/transforms/gradient.cc</td> <td>AutomaticDifferentiation, ReverseAD</td></tr></tbody></table> <h3 id="_1-5-high-level-hardware-aware-optimizations"><a href="#_1-5-high-level-hardware-aware-optimizations" class="header-anchor">#</a> 1.5 High-Level Hardware-Aware Optimizations</h3> <p>These optimizations modify operations based on the target hardware.</p> <table><thead><tr><th>Optimization</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Annotate Target</td> <td>src/relay/transform/annotate_target.cc</td> <td>AnnotateTarget</td></tr> <tr><td>Partition Graph</td> <td>src/relay/transform/partition_graph.cc</td> <td>PartitionGraph</td></tr> <tr><td>Merge Compiler Regions</td> <td>src/relay/transform/merge_compiler_regions.cc</td> <td>MergeCompilerRegions</td></tr></tbody></table> <h3 id="_1-6-device-placement"><a href="#_1-6-device-placement" class="header-anchor">#</a> 1.6 Device Placement</h3> <p>These passes assign operations to devices for heterogeneous execution.</p> <table><thead><tr><th>Transformation</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Rewrite Annotated Ops</td> <td>src/relay/transform/rewrite_annotated_ops.cc</td> <td>RewriteAnnotatedOps</td></tr> <tr><td>Device Annotation</td> <td>src/relay/transform/device_annotation.cc</td> <td>DeviceAnnotation</td></tr></tbody></table> <h3 id="_1-7-meta-pass-management"><a href="#_1-7-meta-pass-management" class="header-anchor">#</a> 1.7 Meta-Pass Management</h3> <p>Relay provides a meta-pass system to manage and sequence passes.</p> <table><thead><tr><th>Meta-Pass</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Sequential Pass Manager</td> <td>src/relay/transform/sequential.cc</td> <td>Sequential, PassManager</td></tr> <tr><td>Pass Context</td> <td>src/relay/transform/pass.cc</td> <td>PassContext, WithPassContext</td></tr></tbody></table> <hr> <h2 id="_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir"><a href="#_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir" class="header-anchor">#</a> 2 Lowering to Tensor Expression (TE), Scheduing in TE and Lowering into TIR</h2> <p>The lowering process from Relay IR to Tensor Expression (TE) and Tensor IR (TIR) in TVM involves multiple phases.</p> <p>These include converting Relay IR to TE, applying tensor computation abstractions, performing scheduling, and constructing device-specific low-level TIR.</p> <p>Here’s a detailed breakdown of the corresponding TVM source code files and functions for these stages:</p> <h3 id="_2-1-converting-relay-ir-to-tensor-expression-te"><a href="#_2-1-converting-relay-ir-to-tensor-expression-te" class="header-anchor">#</a> 2.1 Converting Relay IR to Tensor Expression (TE)</h3> <p>This phase converts high-level Relay IR into the computation abstractions provided by TE.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Relay to TE Lowering</td> <td>src/relay/backend/te_compiler.cc</td> <td>LowerToTE, CreateSchedule, ScheduleGetter</td></tr> <tr><td>Operator Strategy</td> <td>src/relay/op/strategy/generic.cc</td> <td>GenericFunc, OpStrategy</td></tr> <tr><td>Relay to TE Bridge</td> <td>src/relay/backend/te_compiler_cache.cc</td> <td>TECompiler, LowerTE</td></tr> <tr><td>Shape Function Lowering</td> <td>src/relay/backend/te_compiler.cc</td> <td>LowerShapeFunc</td></tr></tbody></table> <p>Explanation:</p> <ul><li>The Relay IR graph is analyzed, and for each operator, TVM retrieves a corresponding TE function using OpStrategy.</li> <li>TE functions define high-level operations like matrix multiplication, element-wise addition, etc.</li></ul> <h3 id="_2-2-abstraction-of-computation-in-tensor-expression-te"><a href="#_2-2-abstraction-of-computation-in-tensor-expression-te" class="header-anchor">#</a> 2.2 Abstraction of Computation in Tensor Expression (TE)</h3> <p>TE provides a declarative way to express computation. This includes operations like tiling, unrolling, and vectorizing.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Tensor Expression Build</td> <td>src/te/operation/create_primfunc.cc</td> <td>CreatePrimFunc, ComputeBody, ScheduleOps</td></tr> <tr><td>Compute Definition</td> <td>src/te/operation/compute_op.cc</td> <td>ComputeOpNode, ComputeOp</td></tr> <tr><td>Tensor Compute Intrinsics</td> <td>src/te/operation/tensorize.cc</td> <td>Tensorize, CreateIntrinBody</td></tr></tbody></table> <p>Explanation:</p> <ul><li>High-level computations are abstracted into a declarative format using ComputeOp.</li> <li>Intrinsic support for tensorization is added for specialized hardware operations.</li></ul> <h3 id="_2-3-scheduling-in-tensor-expression"><a href="#_2-3-scheduling-in-tensor-expression" class="header-anchor">#</a> 2.3 Scheduling in Tensor Expression</h3> <p>Scheduling is where TVM optimizes how computations are performed on the target device.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Tile, Unroll, Vectorize</td> <td>src/te/schedule/schedule_dataflow_rewrite.cc</td> <td>ScheduleDataFlowRewrite, Tile, Unroll, Vectorize</td></tr> <tr><td>Thread and Block Mapping</td> <td>src/te/schedule/schedule_lang.cc</td> <td>bind, split, reorder, fuse</td></tr> <tr><td>AutoScheduler Interface</td> <td>src/auto_scheduler/compute_dag.cc</td> <td>ComputeDAG, ApplySteps</td></tr> <tr><td>Lowering Schedule to TIR</td> <td>src/te/schedule/graph.cc</td> <td>ScheduleGraph, LowerSchedule</td></tr></tbody></table> <p>Explanation:</p> <ul><li>This phase defines how computations should be split into smaller tiles, mapped to hardware threads and blocks, and optimized using unrolling and vectorization.</li> <li>Tensor schedules are converted into lower-level forms through ScheduleGraph.</li></ul> <h3 id="_2-4-constructing-low-level-tensor-ir-tir"><a href="#_2-4-constructing-low-level-tensor-ir-tir" class="header-anchor">#</a> 2.4 Constructing Low-Level Tensor IR (TIR)</h3> <p>TIR represents a low-level, device-specific IR used to generate target-specific code.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>TIR Construction</td> <td>src/tir/stmt_functor.cc</td> <td>StmtFunctor, VisitStmt, MakeStmt</td></tr> <tr><td>Lowering to TIR</td> <td>src/tir/transforms/lower_tir.cc</td> <td>LowerTIR, TransformTIR</td></tr> <tr><td>Memory Planning</td> <td>src/tir/transforms/storage_rewrite.cc</td> <td>StorageRewrite, PlanMemory</td></tr> <tr><td>Device-Specific TIR</td> <td>src/target/codegen.cc</td> <td>Build, BuildIRModule</td></tr></tbody></table> <p>Explanation:</p> <ul><li>TE schedules are converted into TIR, which provides explicit control over memory accesses and device-specific optimizations.</li> <li>StorageRewrite optimizes memory allocation and reuse.</li></ul> <h3 id="_2-5-device-specific-optimizations"><a href="#_2-5-device-specific-optimizations" class="header-anchor">#</a> 2.5 Device-Specific Optimizations</h3> <p>Device-specific optimizations tailor the generated code for the target platform (e.g., CUDA).</p> <table><thead><tr><th>Transformation</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Thread/Block Mapping</td> <td>src/tir/transforms/thread_storage_sync.cc</td> <td>ThreadStorageSync</td></tr> <tr><td>Loop Partitioning</td> <td>src/tir/transforms/loop_partition.cc</td> <td>LoopPartition</td></tr> <tr><td>Device Codegen</td> <td>src/target/source/codegen_cuda.cc</td> <td>CodeGenCUDA, PrintKernel</td></tr></tbody></table> <p>High-Level Summary of the Workflow</p> <ul><li>Relay to TE:<br>
Converts high-level operations into Tensor Expression (TE) definitions using strategies (src/relay/backend/te_compiler.cc).</li> <li>Computation Abstraction:
Defines computations in TE with ComputeOp (src/te/operation/compute_op.cc).</li> <li>Scheduling:<br>
Applies optimizations like tiling, unrolling, and mapping computations to threads/blocks (src/te/schedule/schedule_lang.cc).</li> <li>Lowering to TIR:<br>
Translates the schedule into TIR, which explicitly handles device memory and control flow (src/tir/transforms/lower_tir.cc).</li> <li>Device-Specific Codegen:<br>
Emits target-specific code (e.g., CUDA) via CodeGenCUDA (src/target/source/codegen_cuda.cc).</li></ul> <hr> <h2 id="_3-code-generation"><a href="#_3-code-generation" class="header-anchor">#</a> 3. Code Generation</h2> <h3 id="_3-1-gpu-code-generation"><a href="#_3-1-gpu-code-generation" class="header-anchor">#</a> 3.1 GPU Code Generation</h3> <p>This phase translates Tensor IR (TIR) into GPU-compatible low-level code, generating CUDA kernels and API calls.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>TIR to CUDA Kernel</td> <td>src/target/source/codegen_cuda.cc</td> <td>CodeGenCUDA, GenerateKernel, PrintStmt</td></tr> <tr><td>CodeGen Base Class</td> <td>src/target/source/codegen_c.cc</td> <td>CodeGenC, PrintExpr</td></tr> <tr><td>Shared Memory Handling</td> <td>src/target/source/codegen_cuda.cc</td> <td>PrintStorageScope, PrintStorageSync</td></tr> <tr><td>Thread/Block Synchronization</td> <td>src/tir/transforms/thread_storage_sync.cc</td> <td>ThreadStorageSync</td></tr></tbody></table> <p><strong>Explanation:</strong>
CodeGenCUDA translates TIR to CUDA kernels, emitting device-side code and managing constructs like thread/block mappings, shared memory, and synchronization.
Synchronization points are inserted using PrintStorageSync.</p> <h3 id="_3-2-kernel-construction"><a href="#_3-2-kernel-construction" class="header-anchor">#</a> 3.2. Kernel Construction</h3> <p>Kernel construction involves creating CUDA device kernels and host-side launcher code to invoke them.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Kernel Emission</td> <td>src/target/source/codegen_cuda.cc</td> <td>PrintFuncBody, EmitFunction</td></tr> <tr><td>Kernel Launch Code</td> <td>src/runtime/cuda/cuda_module.cc</td> <td>CUDAWrappedFunc, LaunchKernel</td></tr> <tr><td>Kernel Metadata Management</td> <td>src/runtime/module.cc</td> <td>PackImports, ExportModule</td></tr></tbody></table> <p>Explanation:
The EmitFunction generates kernel function declarations and definitions for execution on the GPU.
Host-side kernel launchers are defined in cuda_module.cc.</p> <h3 id="_3-3-cublas-cutlass-integration"><a href="#_3-3-cublas-cutlass-integration" class="header-anchor">#</a> 3.3. cuBLAS/CUTLASS Integration</h3> <p>When using cuBLAS or CUTLASS for tensor computations (e.g., GEMM), TVM generates calls to these libraries instead of writing explicit CUDA kernels.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>cuBLAS Integration</td> <td>src/runtime/contrib/cublas/cublas.cc</td> <td>CUBLASCall, InitCUBLASHandle, GemmOp</td></tr> <tr><td>CUTLASS Integration</td> <td>src/contrib/cutlass/gen_cutlass_gemm.cc</td> <td>GenerateCutlassGemm, EmitCutlassCode</td></tr> <tr><td>External Code Generation</td> <td>src/relay/backend/contrib/cublas_codegen.cc</td> <td>CUBLASFunction, CodegenCUBLAS</td></tr></tbody></table> <p>Explanation:
cublas.cc provides wrappers for cuBLAS API calls like cublasSgemm, with TVM handling data layout transformations as needed.
CUTLASS integration uses template-based code generation in gen_cutlass_gemm.cc, emitting optimized kernels for matrix operations.</p> <h3 id="_3-4-target-specific-optimizations"><a href="#_3-4-target-specific-optimizations" class="header-anchor">#</a> 3.4. Target-Specific Optimizations</h3> <p>Target-specific optimizations fine-tune the generated CUDA code based on the GPU architecture and memory hierarchy.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Thread/Block Mapping</td> <td>src/tir/transforms/thread_storage_sync.cc</td> <td>ThreadStorageSync, OptimizeThreads</td></tr> <tr><td>Loop Partitioning</td> <td>src/tir/transforms/loop_partition.cc</td> <td>LoopPartition</td></tr> <tr><td>Memory Planning</td> <td>src/tir/transforms/storage_rewrite.cc</td> <td>StorageRewrite, PlanMemory</td></tr> <tr><td>Warp-Level Optimization</td> <td>src/tir/transforms/vectorize_loop.cc</td> <td>VectorizeLoop, Vectorizer</td></tr></tbody></table> <p>Explanation:
Thread and block mapping ensures optimal utilization of GPU threads and memory.
Loop partitioning and vectorization optimize data access patterns for warp-level efficiency.
StorageRewrite minimizes memory usage by analyzing reuse patterns and adjusting allocation.</p> <h3 id="_3-5-memory-management"><a href="#_3-5-memory-management" class="header-anchor">#</a> 3.5. Memory Management</h3> <p>Efficient memory management involves optimizing shared/global memory usage and enabling memory reuse.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Shared Memory Usage</td> <td>src/target/source/codegen_cuda.cc</td> <td>PrintStorageScope, EmitSharedMemory</td></tr> <tr><td>Memory Allocation</td> <td>src/tir/transforms/storage_rewrite.cc</td> <td>PlanMemory, ReuseMemory</td></tr> <tr><td>Memory Alignment</td> <td>src/target/source/codegen_cuda.cc</td> <td>PrintStorageAlloc</td></tr></tbody></table> <p>Explanation:
Shared memory scopes are explicitly emitted during CUDA codegen (EmitSharedMemory).
PlanMemory optimizes allocation to minimize fragmentation and overhead.</p> <h3 id="_3-6-overall-codegen-workflow"><a href="#_3-6-overall-codegen-workflow" class="header-anchor">#</a> 3.6. Overall Codegen Workflow</h3> <p>Key Stages and Their Files</p> <ul><li><p>TIR Lowering:<br>
File: src/tir/transforms/lower_tir.cc<br>
Function: LowerTIR, TransformTIR</p></li> <li><p>CUDA Kernel Emission:<br>
File: src/target/source/codegen_cuda.cc<br>
Function: EmitFunction, GenerateKernel</p></li> <li><p>cuBLAS Integration:<br>
File: src/runtime/contrib/cublas/cublas.cc<br>
Function: CUBLASCall, InitCUBLASHandle</p></li> <li><p>CUTLASS Integration:<br>
File: src/contrib/cutlass/gen_cutlass_gemm.cc<br>
Function: GenerateCutlassGemm, EmitCutlassCode</p></li> <li><p>Target-Specific Optimizations:<br>
File: src/tir/transforms/thread_storage_sync.cc<br>
Function: ThreadStorageSync, OptimizeThreads</p></li></ul></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/02.compiler/09.learn_tvm_1.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/06/29, 22:25:06</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/000008/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Writing TinyRISCV Backend</div></a> <a href="/qishao-notes/pages/000010/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Learn TPU_MLIR</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/000008/" class="prev">Writing TinyRISCV Backend</a></span> <span class="next"><a href="/qishao-notes/pages/000010/">Learn TPU_MLIR</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.a069b46c.js" defer></script><script src="/qishao-notes/assets/js/2.4d9050ab.js" defer></script><script src="/qishao-notes/assets/js/31.8f3dd2fd.js" defer></script>
  </body>
</html>
