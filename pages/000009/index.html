<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Learn TVM | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.d2b7dd9b.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.6d8a25ce.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/31.219afea4.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.c53d023c.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.1dbf4645.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.6006be8e.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.02d8395d.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.295cb0f9.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.d3aeac64.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.61cf8dbe.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.5d333160.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.de5b1de8.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.f2f35e42.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.e6d0a7ee.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.b6f7d42d.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.8ad3039f.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.aee74a99.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.6c71b3e4.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.51ed4029.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.6dd5878f.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.b5f08e44.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.71606c98.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.5332a3b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.e32c5cb8.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.98c55885.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.dc4136f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.612ebbf7.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.76b913fb.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.b033a71a.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.f9a8721f.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.57aa9999.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.2785b48b.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.33791185.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.35cdaa5c.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.ba5a7695.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.8dd9af70.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.b688542f.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.ba9d4baf.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.42f21f4f.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.78b7313e.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.49321259.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.58ec3c1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.049ba004.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.f485ff78.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.18fc9823.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.494aa23f.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.596c7d2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.c0382aef.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.a9c55c08.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.ae558e88.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.a37871e7.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.33fdfa7d.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.dd63beb3.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.84f10398.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.b14f6e3a.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.21d1843c.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.c95eff2e.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.d63a3d79.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.fdab2a0b.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.bae99c2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.89b382c5.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.ea1753be.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.046efc3c.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.f5fc0a4e.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.3cc58cc5.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.dde88fd3.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.4edc7d96.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.f311eaa5.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.526dc690.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.28e90c7e.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.8b03d117.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.6d6c79ff.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.0995d9c1.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.28e64713.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.bfe1dda8.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.9480e434.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.f0724575.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.3e2072af.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.ecef8b2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.e1d3c447.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.fe5b5324.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.f0fbf90b.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.35000e6b.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.f32429ee.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.0e73c7e2.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.19c15ad2.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.adfe2eec.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.0e13cd4c.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.021a0ef2.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.747af7e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.7d99e80d.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.0166a661.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.a776222e.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.3b4ae10c.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.5b06b360.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.bc22dca0.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.3a589a2f.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.8deb1a9c.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.83e128af.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.5116e0c3.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.6437ce99.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.1204c918.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.2481c682.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.de603d6f.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.41b3505b.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.0e986607.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.022f2c05.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.ebab277e.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.c4cdc44c.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.9d4f79b2.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.07809e54.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.268912b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.06f1629c.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.6b2d7130.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.7d0e7d9f.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.5d0d8d9e.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.8d679031.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.2852c8c2.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.0d191959.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.adccea8d.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.c845a5ab.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.042f6fe8.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.33138869.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.2e422df8.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.d5bb202f.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/000001/" class="sidebar-link">llvm flow</a></li><li><a href="/qishao-notes/pages/000002/" class="sidebar-link">Getting Started with LLVM Core Libraries Chap5 IR</a></li><li><a href="/qishao-notes/pages/000003/" class="sidebar-link">Getting Started with LLVM Core Libraries Chap6 Backend</a></li><li><a href="/qishao-notes/pages/000004/" class="sidebar-link">Learning LLVM Notes</a></li><li><a href="/qishao-notes/pages/000005/" class="sidebar-link">Add New DIY Instruction ACE to LLVM</a></li><li><a href="/qishao-notes/pages/000006/" class="sidebar-link">How does LLVM perform instruction combine</a></li><li><a href="/qishao-notes/pages/000007/" class="sidebar-link">Understand llvm with its source code</a></li><li><a href="/qishao-notes/pages/000008/" class="sidebar-link">Writing TinyRISCV Backend</a></li><li><a href="/qishao-notes/pages/000009/" aria-current="page" class="active sidebar-link">Learn TVM</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/000009/#_0-feel-the-flow-of-tvm-compilation" class="sidebar-link">0. Feel the flow of TVM compilation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-1-model-parsing-and-relay-ir-construction" class="sidebar-link">0.1 Model Parsing and Relay IR Construction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-2-high-level-optimizations-in-relay" class="sidebar-link">0.2 High-Level Optimizations in Relay</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-3-lowering-to-tensor-expression-te" class="sidebar-link">0.3. Lowering to Tensor Expression (TE)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-4-scheduling-in-te" class="sidebar-link">0.4. Scheduling in TE</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-5-lowering-to-tir" class="sidebar-link">0.5. Lowering to TIR</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-6-code-generation" class="sidebar-link">0.6. Code Generation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-7-final-compilation-and-deployment" class="sidebar-link">0.7. Final Compilation and Deployment</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/000009/#_1-model-parsing-and-relay-ir-construction" class="sidebar-link">1. Model Parsing and Relay IR Construction</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-1-graph-level-optimizations" class="sidebar-link">1.1 Graph-Level Optimizations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-2-data-layout-transformations" class="sidebar-link">1.2 Data Layout Transformations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-3-quantization-and-precision-management" class="sidebar-link">1.3 Quantization and Precision Management</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-4-automatic-differentiation" class="sidebar-link">1.4 Automatic Differentiation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-5-high-level-hardware-aware-optimizations" class="sidebar-link">1.5 High-Level Hardware-Aware Optimizations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-6-device-placement" class="sidebar-link">1.6 Device Placement</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-7-meta-pass-management" class="sidebar-link">1.7 Meta-Pass Management</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/000009/#_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir" class="sidebar-link">2 Lowering to Tensor Expression (TE), Scheduing in TE and Lowering into TIR</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-1-converting-relay-ir-to-tensor-expression-te" class="sidebar-link">2.1 Converting Relay IR to Tensor Expression (TE)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-2-abstraction-of-computation-in-tensor-expression-te" class="sidebar-link">2.2 Abstraction of Computation in Tensor Expression (TE)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-3-scheduling-in-tensor-expression" class="sidebar-link">2.3 Scheduling in Tensor Expression</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-4-constructing-low-level-tensor-ir-tir" class="sidebar-link">2.4 Constructing Low-Level Tensor IR (TIR)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-5-device-specific-optimizations" class="sidebar-link">2.5 Device-Specific Optimizations</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/000009/#_3-code-generation" class="sidebar-link">3. Code Generation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-1-gpu-code-generation" class="sidebar-link">3.1 GPU Code Generation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-2-kernel-construction" class="sidebar-link">3.2. Kernel Construction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-3-cublas-cutlass-integration" class="sidebar-link">3.3. cuBLAS/CUTLASS Integration</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-4-target-specific-optimizations" class="sidebar-link">3.4. Target-Specific Optimizations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-5-memory-management" class="sidebar-link">3.5. Memory Management</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-6-overall-codegen-workflow" class="sidebar-link">3.6. Overall Codegen Workflow</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/000010/" class="sidebar-link">Learn TPU_MLIR</a></li><li><a href="/qishao-notes/pages/000011/" class="sidebar-link">MLIR TOY Tutorial</a></li><li><a href="/qishao-notes/pages/000012/" class="sidebar-link">Auto Differentiation in Compiler</a></li><li><a href="/qishao-notes/pages/000013/" class="sidebar-link">MLIR Open Meeting Notes The Torch MLIR Project</a></li><li><a href="/qishao-notes/pages/000014/" class="sidebar-link">MLIR Compiling Flow of Conv2D</a></li><li><a href="/qishao-notes/pages/000015/" class="sidebar-link">MLIR Compiling Flow of Conv2D</a></li><li><a href="/qishao-notes/pages/000016/" class="sidebar-link">MLIR Compiling Flow of Transformer-Decoder</a></li><li><a href="/qishao-notes/pages/000017/" class="sidebar-link">MLIR Essential Concepts</a></li><li><a href="/qishao-notes/pages/000018/" class="sidebar-link">MLIR NVGPU Dialect</a></li><li><a href="/qishao-notes/pages/000019/" class="sidebar-link">MLIR Linalg Dialect</a></li><li><a href="/qishao-notes/pages/000020/" class="sidebar-link">MLIR Bufferization</a></li><li><a href="/qishao-notes/pages/000021/" class="sidebar-link">MLIR Bufferization Passes</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/compiler/#compiler" data-v-06225672>compiler</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2024-12-08</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">Learn TVM<!----></h1> <!----> <div class="theme-vdoing-content content__default"><hr> <h2 id="_0-feel-the-flow-of-tvm-compilation"><a href="#_0-feel-the-flow-of-tvm-compilation" class="header-anchor">#</a> 0. Feel the flow of TVM compilation</h2> <p><strong>Model Definition</strong></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>import tvm
from tvm import relay, te
import numpy as np

# Model parameters
batch_size, input_dim, output_dim = 32, 128, 64

# Relay model
x = relay.var(&quot;x&quot;, shape=(batch_size, input_dim), dtype=&quot;float32&quot;)
w = relay.var(&quot;w&quot;, shape=(input_dim, output_dim), dtype=&quot;float32&quot;)
y = relay.nn.dense(x, w)
model = relay.Function([x, w], y)

# Input data
x_data = np.random.rand(batch_size, input_dim).astype(&quot;float32&quot;)
w_data = np.random.rand(input_dim, output_dim).astype(&quot;float32&quot;)
params = {&quot;w&quot;: w_data}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p><strong>Relay IR</strong>
The relay.Function represents the high-level computational graph.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>print(model)

# Simplified Relay IR:
# fn (%x: Tensor[(32, 128), float32], %w: Tensor[(128, 64), float32]) {
#   nn.dense(%x, %w)
# }

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p><strong>Lowering to Tensor Expression (TE)</strong></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>with tvm.transform.PassContext(opt_level=3):
    mod, params = relay.build_module.bind_params_by_name(model, params)
    graph, lib, params = relay.build(mod, target=&quot;cuda&quot;, params=params)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>In Tensor Expression (TE), computations are represented using tensor operations:</p> <ul><li>Compute: C[i, j] = sum(A[i, k] * B[k, j] for k in range(input_dim))</li> <li>Schedule: Operations like tiling, thread binding, and vectorization are applied.</li></ul> <p><strong>Example TE for matrix multiplication:</strong></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>A = te.placeholder((batch_size, input_dim), name=&quot;A&quot;)
B = te.placeholder((input_dim, output_dim), name=&quot;B&quot;)
k = te.reduce_axis((0, input_dim), name=&quot;k&quot;)

# Compute definition
C = te.compute(
    (batch_size, output_dim),
    lambda i, j: te.sum(A[i, k] * B[k, j], axis=k),
    name=&quot;C&quot;
)

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p><strong>TIR (Tensor IR)</strong></p> <p>After applying schedules, TE is lowered to TIR. TIR is a low-level representation focusing on loops and memory hierarchy.</p> <p>Example TIR (simplified):</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>@tvm.script.ir_module
class MyModule:
    @tvm.tir.prim_func
    def main(A: tvm.tir.Buffer[(32, 128), &quot;float32&quot;],
             B: tvm.tir.Buffer[(128, 64), &quot;float32&quot;],
             C: tvm.tir.Buffer[(32, 64), &quot;float32&quot;]):
        for i in range(32):  # Outer loop for batch
            for j in range(64):  # Outer loop for output_dim
                with tvm.tir.block(&quot;C&quot;):
                    C[i, j] = 0.0
                    for k in range(128):  # Reduction loop for input_dim
                        C[i, j] += A[i, k] * B[k, j]

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p><strong>CUDA Code Generation</strong></p> <p>Finally, TIR is compiled into CUDA code:</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>print(lib.imported_modules[0].get_source())

# Simplified CUDA Code:
# __global__ void fused_dense(float* __restrict__ A, float* __restrict__ B, float* __restrict__ C) {
#   int idx = threadIdx.x + blockIdx.x * blockDim.x;
#   if (idx &lt; 2048) {  // 32 * 64 = batch_size * output_dim
#     int i = idx / 64;  // Batch index
#     int j = idx % 64;  // Output index
#     float result = 0.0;
#     for (int k = 0; k &lt; 128; ++k) {  // Reduction loop
#       result += A[i * 128 + k] * B[k * 64 + j];
#     }
#     C[idx] = result;
#   }
# }

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p><strong>Summary of Intermediate Representations</strong></p> <ol><li>Relay IR: High-level computational graph, defines operators like nn.dense.</li> <li>TE: Abstracts computation using mathematical tensor operations and supports scheduling primitives.</li> <li>TIR: Low-level, loop-based representation with explicit memory hierarchy.</li> <li>CUDA Code: GPU kernel for matrix multiplication, including thread and block mappings.</li></ol> <hr> <h3 id="_0-1-model-parsing-and-relay-ir-construction"><a href="#_0-1-model-parsing-and-relay-ir-construction" class="header-anchor">#</a> 0.1 Model Parsing and Relay IR Construction</h3> <ul><li>Input
Model in high-level frameworks like TensorFlow, PyTorch, or ONNX.</li> <li>Process:
<ul><li>TVM parses the input model and converts it into Relay IR, a hig-h-level intermediate representation.</li> <li>The Relay IR describes the computational graph with operator-level abstractions.</li></ul></li> <li>Key Functions:
relay.frontend.from_pytorch(), relay.frontend.from_onnx() in src/relay/frontend/.</li></ul> <h3 id="_0-2-high-level-optimizations-in-relay"><a href="#_0-2-high-level-optimizations-in-relay" class="header-anchor">#</a> 0.2 High-Level Optimizations in Relay</h3> <ul><li>Input: Relay IR.</li> <li>Process:
<ul><li>Optimize the Relay IR for performance and hardware compatibility through:</li> <li>Operator Fusion: Fuse adjacent operations.</li> <li>Constant Folding: Precompute static expressions.</li> <li>Layout Transformation: Adjust data layouts (e.g., NCHW → NCHWc).</li> <li>Quantization: Lower precision where applicable.</li> <li>Common Subexpression Elimination.
Finalize the optimized Relay graph.</li></ul></li> <li>Key Functions:<br>
src/relay/transforms/ for passes like fuse_ops.cc, alter_op_layout.cc.</li></ul> <h3 id="_0-3-lowering-to-tensor-expression-te"><a href="#_0-3-lowering-to-tensor-expression-te" class="header-anchor">#</a> 0.3. Lowering to Tensor Expression (TE)</h3> <ul><li>Input: Optimized Relay IR.</li> <li>Process:
<ul><li>Translate high-level Relay operators into Tensor Expressions (TE).</li> <li>TE represents computations as mathematical tensor operations and allows for:
<ul><li>Abstraction of computation patterns (e.g., matrix multiplication).</li> <li>Introduction of scheduling primitives (e.g., tiling, unrolling, vectorization).</li></ul></li></ul></li> <li>Key Functions:
<ul><li>src/relay/backend/te_compiler.cc: Bridges Relay IR and TE.</li> <li>src/te/tensor.cc: Constructs tensor expressions.</li></ul></li></ul> <h3 id="_0-4-scheduling-in-te"><a href="#_0-4-scheduling-in-te" class="header-anchor">#</a> 0.4. Scheduling in TE</h3> <ul><li>Input: Tensor Expressions.</li> <li>Process:
<ul><li>Apply scheduling primitives to improve performance:</li> <li>Tiling: Divide tensors into smaller chunks for parallelism.</li> <li>Unrolling: Optimize loops for instruction pipelining.</li> <li>Thread/Block Mapping: Map computations to GPU threads and blocks.</li> <li>Vectorization: Use SIMD instructions where applicable.</li> <li>Refines Tensor Expressions into Tensor Intermediate Representation (TIR).</li></ul></li> <li>Key Functions:
<ul><li>src/te/schedule/ for scheduling functions.</li> <li>src/te/schedule/schedule_dataflow_rewrite.cc: Handles dataflow rewrite scheduling.</li></ul></li></ul> <h3 id="_0-5-lowering-to-tir"><a href="#_0-5-lowering-to-tir" class="header-anchor">#</a> 0.5. Lowering to TIR</h3> <ul><li>Input: Tensor Expressions with schedules.</li> <li>Process:
<ul><li>Convert TE into Tensor IR (TIR), a low-level IR closer to device execution.</li> <li>Perform device-specific optimizations for CUDA (e.g., thread hierarchy mapping).</li></ul></li> <li>Key Functions:
<ul><li>src/tir/transform/ for device-specific passes like loop unrolling and thread binding.</li></ul></li></ul> <h3 id="_0-6-code-generation"><a href="#_0-6-code-generation" class="header-anchor">#</a> 0.6. Code Generation</h3> <ul><li>Input: TIR optimized for CUDA.</li> <li>Process:
<ul><li>Code Generation:Translate TIR into CUDA kernels. Use TVM's built-in CUDA code generator.</li> <li>Calling cuBLAS/cuDNN or CUTLASS:
<ul><li>For specific operations (e.g., GEMM), call external libraries.</li> <li>Determine the sequence of library calls and parameters based on operator attributes.</li></ul></li> <li>Memory Allocation: Analyze dataflow to allocate memory efficiently on GPU.</li></ul></li> <li>Key Functions:
<ul><li>CUDA Codegen:
src/target/source/codegen_cuda.cc: Generates CUDA source code.</li> <li>External Libraries:
src/runtime/contrib/cublas.cc: Integrates with cuBLAS.
src/runtime/contrib/cudnn.cc: Integrates with cuDNN.
src/contrib/cutlass/: Integrates with CUTLASS.</li></ul></li></ul> <h3 id="_0-7-final-compilation-and-deployment"><a href="#_0-7-final-compilation-and-deployment" class="header-anchor">#</a> 0.7. Final Compilation and Deployment</h3> <ul><li>Input: CUDA source code.</li> <li>Process:
<ul><li>Compile CUDA source code using NVCC or the TVM runtime.</li> <li>Deploy the compiled kernel and runtime modules.</li></ul></li> <li>Key Functions:
<ul><li>src/target/source/: Handles code generation.</li> <li>src/runtime/: Manages runtime execution and deployment.</li></ul></li></ul> <hr> <h2 id="_1-model-parsing-and-relay-ir-construction"><a href="#_1-model-parsing-and-relay-ir-construction" class="header-anchor">#</a> 1. Model Parsing and Relay IR Construction</h2> <p>In TVM, high-level optimization in the Relay IR phase includes several graph-level optimizations, data layout transformations, and other functional passes.</p> <p>These optimizations are implemented in various source files under src/relay/transform and src/relay/op directories.</p> <p>Below is a categorized list of these optimizations along with their corresponding source code files and functions:</p> <h3 id="_1-1-graph-level-optimizations"><a href="#_1-1-graph-level-optimizations" class="header-anchor">#</a> 1.1 Graph-Level Optimizations</h3> <p>Graph-level optimizations restructure or simplify the computation graph for better performance.</p> <table><thead><tr><th>Optimization	Source</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Constant Folding</td> <td>src/relay/transform/fold_constant.cc</td> <td>FoldConstant, ConstantFolder</td></tr> <tr><td>Operator Fusion</td> <td>src/relay/transform/fuse_ops.cc</td> <td>FuseOps, FuseMutator, PatternMatcher</td></tr> <tr><td>Dead Code Elimination (DCE)</td> <td>src/relay/transform/eliminate_common_subexpr.cc</td> <td>EliminateCommonSubexpr</td></tr> <tr><td>Common Subexpression Elimination</td> <td>src/relay/transform/eliminate_common_subexpr.cc</td> <td>EliminateCommonSubexpr</td></tr> <tr><td>Simplify Inference</td> <td>src/relay/transform/simplify_inference.cc</td> <td>SimplifyInference, SimplifyInferenceMutator</td></tr> <tr><td>Call Folding</td> <td>src/relay/transform/fold_call.cc</td> <td>FoldCall</td></tr> <tr><td>Inline Functions</td> <td>src/relay/transform/inline.cc</td> <td>Inline, InlineMutator</td></tr> <tr><td>Prune Unused Functions</td> <td>src/relay/transform/prune_unused_functions.cc</td> <td>PruneUnusedFunctions</td></tr></tbody></table> <h3 id="_1-2-data-layout-transformations"><a href="#_1-2-data-layout-transformations" class="header-anchor">#</a> 1.2 Data Layout Transformations</h3> <p>These optimizations adjust the layout of tensors for better memory access patterns and compatibility with target hardware.</p> <table><thead><tr><th>Transformation</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Alter Layout</td> <td>src/relay/transform/alter_op_layout.cc</td> <td>AlterOpLayout, AlterOpLayoutRewriter</td></tr> <tr><td>Convert Layout</td> <td>s	src/relay/transform/convert_layout.cc</td> <td>ConvertLayout</td></tr> <tr><td>Fold Scale Axis</td> <td>src/relay/transform/fold_scale_axis.cc</td> <td>FoldScaleAxis, ScaleAxisSimplifier</td></tr> <tr><td>Layout Optimization</td> <td>src/relay/transform/layout_rewrite.cc</td> <td>LayoutRewrite</td></tr></tbody></table> <h3 id="_1-3-quantization-and-precision-management"><a href="#_1-3-quantization-and-precision-management" class="header-anchor">#</a> 1.3 Quantization and Precision Management</h3> <p>TVM supports quantization optimizations for reduced precision operations.</p> <table><thead><tr><th>Optimization</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Quantize</td> <td>src/relay/quantize/quantize.cc</td> <td>Quantize, CreateQuantizePass</td></tr> <tr><td>Dequantize</td> <td>src/relay/quantize/dequantize.cc</td> <td>Dequantize</td></tr> <tr><td>SimplifyQuantize</td> <td>src/relay/transform/simplify_quantize.cc</td> <td>SimplifyQuantize, SimplifyQuantizeRewriter</td></tr></tbody></table> <h3 id="_1-4-automatic-differentiation"><a href="#_1-4-automatic-differentiation" class="header-anchor">#</a> 1.4 Automatic Differentiation</h3> <p>TVM includes an autodiff system for neural networks.</p> <table><thead><tr><th>Transformation</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Reverse Mode Autodiff</td> <td>src/relay/transforms/gradient.cc</td> <td>AutomaticDifferentiation, ReverseAD</td></tr></tbody></table> <h3 id="_1-5-high-level-hardware-aware-optimizations"><a href="#_1-5-high-level-hardware-aware-optimizations" class="header-anchor">#</a> 1.5 High-Level Hardware-Aware Optimizations</h3> <p>These optimizations modify operations based on the target hardware.</p> <table><thead><tr><th>Optimization</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Annotate Target</td> <td>src/relay/transform/annotate_target.cc</td> <td>AnnotateTarget</td></tr> <tr><td>Partition Graph</td> <td>src/relay/transform/partition_graph.cc</td> <td>PartitionGraph</td></tr> <tr><td>Merge Compiler Regions</td> <td>src/relay/transform/merge_compiler_regions.cc</td> <td>MergeCompilerRegions</td></tr></tbody></table> <h3 id="_1-6-device-placement"><a href="#_1-6-device-placement" class="header-anchor">#</a> 1.6 Device Placement</h3> <p>These passes assign operations to devices for heterogeneous execution.</p> <table><thead><tr><th>Transformation</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Rewrite Annotated Ops</td> <td>src/relay/transform/rewrite_annotated_ops.cc</td> <td>RewriteAnnotatedOps</td></tr> <tr><td>Device Annotation</td> <td>src/relay/transform/device_annotation.cc</td> <td>DeviceAnnotation</td></tr></tbody></table> <h3 id="_1-7-meta-pass-management"><a href="#_1-7-meta-pass-management" class="header-anchor">#</a> 1.7 Meta-Pass Management</h3> <p>Relay provides a meta-pass system to manage and sequence passes.</p> <table><thead><tr><th>Meta-Pass</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Sequential Pass Manager</td> <td>src/relay/transform/sequential.cc</td> <td>Sequential, PassManager</td></tr> <tr><td>Pass Context</td> <td>src/relay/transform/pass.cc</td> <td>PassContext, WithPassContext</td></tr></tbody></table> <hr> <h2 id="_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir"><a href="#_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir" class="header-anchor">#</a> 2 Lowering to Tensor Expression (TE), Scheduing in TE and Lowering into TIR</h2> <p>The lowering process from Relay IR to Tensor Expression (TE) and Tensor IR (TIR) in TVM involves multiple phases.</p> <p>These include converting Relay IR to TE, applying tensor computation abstractions, performing scheduling, and constructing device-specific low-level TIR.</p> <p>Here’s a detailed breakdown of the corresponding TVM source code files and functions for these stages:</p> <h3 id="_2-1-converting-relay-ir-to-tensor-expression-te"><a href="#_2-1-converting-relay-ir-to-tensor-expression-te" class="header-anchor">#</a> 2.1 Converting Relay IR to Tensor Expression (TE)</h3> <p>This phase converts high-level Relay IR into the computation abstractions provided by TE.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Relay to TE Lowering</td> <td>src/relay/backend/te_compiler.cc</td> <td>LowerToTE, CreateSchedule, ScheduleGetter</td></tr> <tr><td>Operator Strategy</td> <td>src/relay/op/strategy/generic.cc</td> <td>GenericFunc, OpStrategy</td></tr> <tr><td>Relay to TE Bridge</td> <td>src/relay/backend/te_compiler_cache.cc</td> <td>TECompiler, LowerTE</td></tr> <tr><td>Shape Function Lowering</td> <td>src/relay/backend/te_compiler.cc</td> <td>LowerShapeFunc</td></tr></tbody></table> <p>Explanation:</p> <ul><li>The Relay IR graph is analyzed, and for each operator, TVM retrieves a corresponding TE function using OpStrategy.</li> <li>TE functions define high-level operations like matrix multiplication, element-wise addition, etc.</li></ul> <h3 id="_2-2-abstraction-of-computation-in-tensor-expression-te"><a href="#_2-2-abstraction-of-computation-in-tensor-expression-te" class="header-anchor">#</a> 2.2 Abstraction of Computation in Tensor Expression (TE)</h3> <p>TE provides a declarative way to express computation. This includes operations like tiling, unrolling, and vectorizing.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Tensor Expression Build</td> <td>src/te/operation/create_primfunc.cc</td> <td>CreatePrimFunc, ComputeBody, ScheduleOps</td></tr> <tr><td>Compute Definition</td> <td>src/te/operation/compute_op.cc</td> <td>ComputeOpNode, ComputeOp</td></tr> <tr><td>Tensor Compute Intrinsics</td> <td>src/te/operation/tensorize.cc</td> <td>Tensorize, CreateIntrinBody</td></tr></tbody></table> <p>Explanation:</p> <ul><li>High-level computations are abstracted into a declarative format using ComputeOp.</li> <li>Intrinsic support for tensorization is added for specialized hardware operations.</li></ul> <h3 id="_2-3-scheduling-in-tensor-expression"><a href="#_2-3-scheduling-in-tensor-expression" class="header-anchor">#</a> 2.3 Scheduling in Tensor Expression</h3> <p>Scheduling is where TVM optimizes how computations are performed on the target device.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Tile, Unroll, Vectorize</td> <td>src/te/schedule/schedule_dataflow_rewrite.cc</td> <td>ScheduleDataFlowRewrite, Tile, Unroll, Vectorize</td></tr> <tr><td>Thread and Block Mapping</td> <td>src/te/schedule/schedule_lang.cc</td> <td>bind, split, reorder, fuse</td></tr> <tr><td>AutoScheduler Interface</td> <td>src/auto_scheduler/compute_dag.cc</td> <td>ComputeDAG, ApplySteps</td></tr> <tr><td>Lowering Schedule to TIR</td> <td>src/te/schedule/graph.cc</td> <td>ScheduleGraph, LowerSchedule</td></tr></tbody></table> <p>Explanation:</p> <ul><li>This phase defines how computations should be split into smaller tiles, mapped to hardware threads and blocks, and optimized using unrolling and vectorization.</li> <li>Tensor schedules are converted into lower-level forms through ScheduleGraph.</li></ul> <h3 id="_2-4-constructing-low-level-tensor-ir-tir"><a href="#_2-4-constructing-low-level-tensor-ir-tir" class="header-anchor">#</a> 2.4 Constructing Low-Level Tensor IR (TIR)</h3> <p>TIR represents a low-level, device-specific IR used to generate target-specific code.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>TIR Construction</td> <td>src/tir/stmt_functor.cc</td> <td>StmtFunctor, VisitStmt, MakeStmt</td></tr> <tr><td>Lowering to TIR</td> <td>src/tir/transforms/lower_tir.cc</td> <td>LowerTIR, TransformTIR</td></tr> <tr><td>Memory Planning</td> <td>src/tir/transforms/storage_rewrite.cc</td> <td>StorageRewrite, PlanMemory</td></tr> <tr><td>Device-Specific TIR</td> <td>src/target/codegen.cc</td> <td>Build, BuildIRModule</td></tr></tbody></table> <p>Explanation:</p> <ul><li>TE schedules are converted into TIR, which provides explicit control over memory accesses and device-specific optimizations.</li> <li>StorageRewrite optimizes memory allocation and reuse.</li></ul> <h3 id="_2-5-device-specific-optimizations"><a href="#_2-5-device-specific-optimizations" class="header-anchor">#</a> 2.5 Device-Specific Optimizations</h3> <p>Device-specific optimizations tailor the generated code for the target platform (e.g., CUDA).</p> <table><thead><tr><th>Transformation</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Thread/Block Mapping</td> <td>src/tir/transforms/thread_storage_sync.cc</td> <td>ThreadStorageSync</td></tr> <tr><td>Loop Partitioning</td> <td>src/tir/transforms/loop_partition.cc</td> <td>LoopPartition</td></tr> <tr><td>Device Codegen</td> <td>src/target/source/codegen_cuda.cc</td> <td>CodeGenCUDA, PrintKernel</td></tr></tbody></table> <p>High-Level Summary of the Workflow</p> <ul><li>Relay to TE:<br>
Converts high-level operations into Tensor Expression (TE) definitions using strategies (src/relay/backend/te_compiler.cc).</li> <li>Computation Abstraction:
Defines computations in TE with ComputeOp (src/te/operation/compute_op.cc).</li> <li>Scheduling:<br>
Applies optimizations like tiling, unrolling, and mapping computations to threads/blocks (src/te/schedule/schedule_lang.cc).</li> <li>Lowering to TIR:<br>
Translates the schedule into TIR, which explicitly handles device memory and control flow (src/tir/transforms/lower_tir.cc).</li> <li>Device-Specific Codegen:<br>
Emits target-specific code (e.g., CUDA) via CodeGenCUDA (src/target/source/codegen_cuda.cc).</li></ul> <hr> <h2 id="_3-code-generation"><a href="#_3-code-generation" class="header-anchor">#</a> 3. Code Generation</h2> <h3 id="_3-1-gpu-code-generation"><a href="#_3-1-gpu-code-generation" class="header-anchor">#</a> 3.1 GPU Code Generation</h3> <p>This phase translates Tensor IR (TIR) into GPU-compatible low-level code, generating CUDA kernels and API calls.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>TIR to CUDA Kernel</td> <td>src/target/source/codegen_cuda.cc</td> <td>CodeGenCUDA, GenerateKernel, PrintStmt</td></tr> <tr><td>CodeGen Base Class</td> <td>src/target/source/codegen_c.cc</td> <td>CodeGenC, PrintExpr</td></tr> <tr><td>Shared Memory Handling</td> <td>src/target/source/codegen_cuda.cc</td> <td>PrintStorageScope, PrintStorageSync</td></tr> <tr><td>Thread/Block Synchronization</td> <td>src/tir/transforms/thread_storage_sync.cc</td> <td>ThreadStorageSync</td></tr></tbody></table> <p><strong>Explanation:</strong>
CodeGenCUDA translates TIR to CUDA kernels, emitting device-side code and managing constructs like thread/block mappings, shared memory, and synchronization.
Synchronization points are inserted using PrintStorageSync.</p> <h3 id="_3-2-kernel-construction"><a href="#_3-2-kernel-construction" class="header-anchor">#</a> 3.2. Kernel Construction</h3> <p>Kernel construction involves creating CUDA device kernels and host-side launcher code to invoke them.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Kernel Emission</td> <td>src/target/source/codegen_cuda.cc</td> <td>PrintFuncBody, EmitFunction</td></tr> <tr><td>Kernel Launch Code</td> <td>src/runtime/cuda/cuda_module.cc</td> <td>CUDAWrappedFunc, LaunchKernel</td></tr> <tr><td>Kernel Metadata Management</td> <td>src/runtime/module.cc</td> <td>PackImports, ExportModule</td></tr></tbody></table> <p>Explanation:
The EmitFunction generates kernel function declarations and definitions for execution on the GPU.
Host-side kernel launchers are defined in cuda_module.cc.</p> <h3 id="_3-3-cublas-cutlass-integration"><a href="#_3-3-cublas-cutlass-integration" class="header-anchor">#</a> 3.3. cuBLAS/CUTLASS Integration</h3> <p>When using cuBLAS or CUTLASS for tensor computations (e.g., GEMM), TVM generates calls to these libraries instead of writing explicit CUDA kernels.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>cuBLAS Integration</td> <td>src/runtime/contrib/cublas/cublas.cc</td> <td>CUBLASCall, InitCUBLASHandle, GemmOp</td></tr> <tr><td>CUTLASS Integration</td> <td>src/contrib/cutlass/gen_cutlass_gemm.cc</td> <td>GenerateCutlassGemm, EmitCutlassCode</td></tr> <tr><td>External Code Generation</td> <td>src/relay/backend/contrib/cublas_codegen.cc</td> <td>CUBLASFunction, CodegenCUBLAS</td></tr></tbody></table> <p>Explanation:
cublas.cc provides wrappers for cuBLAS API calls like cublasSgemm, with TVM handling data layout transformations as needed.
CUTLASS integration uses template-based code generation in gen_cutlass_gemm.cc, emitting optimized kernels for matrix operations.</p> <h3 id="_3-4-target-specific-optimizations"><a href="#_3-4-target-specific-optimizations" class="header-anchor">#</a> 3.4. Target-Specific Optimizations</h3> <p>Target-specific optimizations fine-tune the generated CUDA code based on the GPU architecture and memory hierarchy.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Thread/Block Mapping</td> <td>src/tir/transforms/thread_storage_sync.cc</td> <td>ThreadStorageSync, OptimizeThreads</td></tr> <tr><td>Loop Partitioning</td> <td>src/tir/transforms/loop_partition.cc</td> <td>LoopPartition</td></tr> <tr><td>Memory Planning</td> <td>src/tir/transforms/storage_rewrite.cc</td> <td>StorageRewrite, PlanMemory</td></tr> <tr><td>Warp-Level Optimization</td> <td>src/tir/transforms/vectorize_loop.cc</td> <td>VectorizeLoop, Vectorizer</td></tr></tbody></table> <p>Explanation:
Thread and block mapping ensures optimal utilization of GPU threads and memory.
Loop partitioning and vectorization optimize data access patterns for warp-level efficiency.
StorageRewrite minimizes memory usage by analyzing reuse patterns and adjusting allocation.</p> <h3 id="_3-5-memory-management"><a href="#_3-5-memory-management" class="header-anchor">#</a> 3.5. Memory Management</h3> <p>Efficient memory management involves optimizing shared/global memory usage and enabling memory reuse.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Shared Memory Usage</td> <td>src/target/source/codegen_cuda.cc</td> <td>PrintStorageScope, EmitSharedMemory</td></tr> <tr><td>Memory Allocation</td> <td>src/tir/transforms/storage_rewrite.cc</td> <td>PlanMemory, ReuseMemory</td></tr> <tr><td>Memory Alignment</td> <td>src/target/source/codegen_cuda.cc</td> <td>PrintStorageAlloc</td></tr></tbody></table> <p>Explanation:
Shared memory scopes are explicitly emitted during CUDA codegen (EmitSharedMemory).
PlanMemory optimizes allocation to minimize fragmentation and overhead.</p> <h3 id="_3-6-overall-codegen-workflow"><a href="#_3-6-overall-codegen-workflow" class="header-anchor">#</a> 3.6. Overall Codegen Workflow</h3> <p>Key Stages and Their Files</p> <ul><li><p>TIR Lowering:<br>
File: src/tir/transforms/lower_tir.cc<br>
Function: LowerTIR, TransformTIR</p></li> <li><p>CUDA Kernel Emission:<br>
File: src/target/source/codegen_cuda.cc<br>
Function: EmitFunction, GenerateKernel</p></li> <li><p>cuBLAS Integration:<br>
File: src/runtime/contrib/cublas/cublas.cc<br>
Function: CUBLASCall, InitCUBLASHandle</p></li> <li><p>CUTLASS Integration:<br>
File: src/contrib/cutlass/gen_cutlass_gemm.cc<br>
Function: GenerateCutlassGemm, EmitCutlassCode</p></li> <li><p>Target-Specific Optimizations:<br>
File: src/tir/transforms/thread_storage_sync.cc<br>
Function: ThreadStorageSync, OptimizeThreads</p></li></ul></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/02.compiler/09.learn_tvm_1.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/04/01, 19:42:51</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/000008/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Writing TinyRISCV Backend</div></a> <a href="/qishao-notes/pages/000010/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Learn TPU_MLIR</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/000008/" class="prev">Writing TinyRISCV Backend</a></span> <span class="next"><a href="/qishao-notes/pages/000010/">Learn TPU_MLIR</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.d2b7dd9b.js" defer></script><script src="/qishao-notes/assets/js/2.6d8a25ce.js" defer></script><script src="/qishao-notes/assets/js/31.219afea4.js" defer></script>
  </body>
</html>
