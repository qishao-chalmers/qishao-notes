<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Learn TVM | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.bd7c0775.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.b768edde.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/31.219afea4.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.08aa09ef.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.fca6c3ed.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.40c1ee10.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.f184eb80.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.c3992f4f.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.ba5a7695.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.11910620.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.9fc7527c.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.470802ea.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.537e76f7.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.bb065338.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.c0805dc6.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.70be0798.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.9a151267.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.b08a9f3b.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.b667bd31.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.494aa23f.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.ac0f6a8b.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.dfda1a33.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.21a8bb75.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.6c83318f.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.a37871e7.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.d31e1099.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.f857f55e.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.63e48fb0.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.a2d6de5c.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.50244ff7.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.4aaa12a0.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.4fb656ab.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.a41af3bf.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.0f47cab8.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.214454b3.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.fb8d64e7.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.7bd3cb20.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.74f957d1.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.6d7767ac.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.b4073052.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.22032325.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.8cfabb38.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.6138667d.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.768f9cc0.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.a6b0d998.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.120c9484.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.241f1d91.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.f4d4bf59.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.1741ac0a.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.c8716fa1.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.239b40a9.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.f55d0c25.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.cf821457.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.3acbf519.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.c1752c32.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.2827b429.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.67ee5b21.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.4a4b7baa.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.d14c0b0d.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.1e1dc6d9.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.17271e14.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.7c1390fb.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.e88c570a.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.f714fa17.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.c6724536.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.6936f79c.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.42ff80bc.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.fd1ba165.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.407a2005.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.e6689334.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.a9920e67.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.374b8515.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.ed4f8d9d.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.d95bad55.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.cb231a96.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.ff42cd7d.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.d94ba878.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.f90e1643.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.b94bc7a4.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.0462ea96.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.64ab6777.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.26321942.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.49eca0e8.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.2f8b50cd.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.e82423bf.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.a3fd8bce.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.32bac4de.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.6b24a791.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.d1b8e683.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.5293a03f.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.b432d410.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.9dcb6cd6.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.03591c40.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.30185309.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/000001/" class="sidebar-link">llvm flow</a></li><li><a href="/qishao-notes/pages/000002/" class="sidebar-link">Getting Started with LLVM Core Libraries-Notes Chap5 IR</a></li><li><a href="/qishao-notes/pages/000003/" class="sidebar-link">Getting Started with LLVM Core Libraries-Notes Chap6 Backend</a></li><li><a href="/qishao-notes/pages/000004/" class="sidebar-link">Learning LLVM Notes 0</a></li><li><a href="/qishao-notes/pages/000005/" class="sidebar-link">Add New Instruction &quot;ACE&quot; to LLVM</a></li><li><a href="/qishao-notes/pages/000006/" class="sidebar-link">How does LLVM perform instruction combine</a></li><li><a href="/qishao-notes/pages/000007/" class="sidebar-link">Understand llvm with its source code</a></li><li><a href="/qishao-notes/pages/000008/" class="sidebar-link">Writing TinyRISCV Backend</a></li><li><a href="/qishao-notes/pages/000009/" aria-current="page" class="active sidebar-link">Learn TVM</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/000009/#_0-feel-the-flow-of-tvm-compilation" class="sidebar-link">0. Feel the flow of TVM compilation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-1-model-parsing-and-relay-ir-construction" class="sidebar-link">0.1 Model Parsing and Relay IR Construction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-2-high-level-optimizations-in-relay" class="sidebar-link">0.2 High-Level Optimizations in Relay</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-3-lowering-to-tensor-expression-te" class="sidebar-link">0.3. Lowering to Tensor Expression (TE)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-4-scheduling-in-te" class="sidebar-link">0.4. Scheduling in TE</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-5-lowering-to-tir" class="sidebar-link">0.5. Lowering to TIR</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-6-code-generation" class="sidebar-link">0.6. Code Generation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_0-7-final-compilation-and-deployment" class="sidebar-link">0.7. Final Compilation and Deployment</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/000009/#_1-model-parsing-and-relay-ir-construction" class="sidebar-link">1. Model Parsing and Relay IR Construction</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-1-graph-level-optimizations" class="sidebar-link">1.1 Graph-Level Optimizations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-2-data-layout-transformations" class="sidebar-link">1.2 Data Layout Transformations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-3-quantization-and-precision-management" class="sidebar-link">1.3 Quantization and Precision Management</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-4-automatic-differentiation" class="sidebar-link">1.4 Automatic Differentiation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-5-high-level-hardware-aware-optimizations" class="sidebar-link">1.5 High-Level Hardware-Aware Optimizations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-6-device-placement" class="sidebar-link">1.6 Device Placement</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_1-7-meta-pass-management" class="sidebar-link">1.7 Meta-Pass Management</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/000009/#_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir" class="sidebar-link">2 Lowering to Tensor Expression (TE), Scheduing in TE and Lowering into TIR</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-1-converting-relay-ir-to-tensor-expression-te" class="sidebar-link">2.1 Converting Relay IR to Tensor Expression (TE)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-2-abstraction-of-computation-in-tensor-expression-te" class="sidebar-link">2.2 Abstraction of Computation in Tensor Expression (TE)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-3-scheduling-in-tensor-expression" class="sidebar-link">2.3 Scheduling in Tensor Expression</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-4-constructing-low-level-tensor-ir-tir" class="sidebar-link">2.4 Constructing Low-Level Tensor IR (TIR)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_2-5-device-specific-optimizations" class="sidebar-link">2.5 Device-Specific Optimizations</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/000009/#_3-code-generation" class="sidebar-link">3. Code Generation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-1-gpu-code-generation" class="sidebar-link">3.1 GPU Code Generation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-2-kernel-construction" class="sidebar-link">3.2. Kernel Construction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-3-cublas-cutlass-integration" class="sidebar-link">3.3. cuBLAS/CUTLASS Integration</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-4-target-specific-optimizations" class="sidebar-link">3.4. Target-Specific Optimizations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-5-memory-management" class="sidebar-link">3.5. Memory Management</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000009/#_3-6-overall-codegen-workflow" class="sidebar-link">3.6. Overall Codegen Workflow</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/000010/" class="sidebar-link">Learn TPU_MLIR</a></li><li><a href="/qishao-notes/pages/000011/" class="sidebar-link">MLIR TOY Tutorial</a></li><li><a href="/qishao-notes/pages/000012/" class="sidebar-link">Auto Differentiation in Compiler</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/compiler/#compiler" data-v-06225672>compiler</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2024-12-08</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">Learn TVM<!----></h1> <!----> <div class="theme-vdoing-content content__default"><hr> <h2 id="_0-feel-the-flow-of-tvm-compilation"><a href="#_0-feel-the-flow-of-tvm-compilation" class="header-anchor">#</a> 0. Feel the flow of TVM compilation</h2> <p><strong>Model Definition</strong></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>import tvm
from tvm import relay, te
import numpy as np

# Model parameters
batch_size, input_dim, output_dim = 32, 128, 64

# Relay model
x = relay.var(&quot;x&quot;, shape=(batch_size, input_dim), dtype=&quot;float32&quot;)
w = relay.var(&quot;w&quot;, shape=(input_dim, output_dim), dtype=&quot;float32&quot;)
y = relay.nn.dense(x, w)
model = relay.Function([x, w], y)

# Input data
x_data = np.random.rand(batch_size, input_dim).astype(&quot;float32&quot;)
w_data = np.random.rand(input_dim, output_dim).astype(&quot;float32&quot;)
params = {&quot;w&quot;: w_data}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p><strong>Relay IR</strong>
The relay.Function represents the high-level computational graph.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>print(model)

# Simplified Relay IR:
# fn (%x: Tensor[(32, 128), float32], %w: Tensor[(128, 64), float32]) {
#   nn.dense(%x, %w)
# }

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p><strong>Lowering to Tensor Expression (TE)</strong></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>with tvm.transform.PassContext(opt_level=3):
    mod, params = relay.build_module.bind_params_by_name(model, params)
    graph, lib, params = relay.build(mod, target=&quot;cuda&quot;, params=params)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>In Tensor Expression (TE), computations are represented using tensor operations:</p> <ul><li>Compute: C[i, j] = sum(A[i, k] * B[k, j] for k in range(input_dim))</li> <li>Schedule: Operations like tiling, thread binding, and vectorization are applied.</li></ul> <p><strong>Example TE for matrix multiplication:</strong></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>A = te.placeholder((batch_size, input_dim), name=&quot;A&quot;)
B = te.placeholder((input_dim, output_dim), name=&quot;B&quot;)
k = te.reduce_axis((0, input_dim), name=&quot;k&quot;)

# Compute definition
C = te.compute(
    (batch_size, output_dim),
    lambda i, j: te.sum(A[i, k] * B[k, j], axis=k),
    name=&quot;C&quot;
)

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p><strong>TIR (Tensor IR)</strong></p> <p>After applying schedules, TE is lowered to TIR. TIR is a low-level representation focusing on loops and memory hierarchy.</p> <p>Example TIR (simplified):</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>@tvm.script.ir_module
class MyModule:
    @tvm.tir.prim_func
    def main(A: tvm.tir.Buffer[(32, 128), &quot;float32&quot;],
             B: tvm.tir.Buffer[(128, 64), &quot;float32&quot;],
             C: tvm.tir.Buffer[(32, 64), &quot;float32&quot;]):
        for i in range(32):  # Outer loop for batch
            for j in range(64):  # Outer loop for output_dim
                with tvm.tir.block(&quot;C&quot;):
                    C[i, j] = 0.0
                    for k in range(128):  # Reduction loop for input_dim
                        C[i, j] += A[i, k] * B[k, j]

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p><strong>CUDA Code Generation</strong></p> <p>Finally, TIR is compiled into CUDA code:</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>print(lib.imported_modules[0].get_source())

# Simplified CUDA Code:
# __global__ void fused_dense(float* __restrict__ A, float* __restrict__ B, float* __restrict__ C) {
#   int idx = threadIdx.x + blockIdx.x * blockDim.x;
#   if (idx &lt; 2048) {  // 32 * 64 = batch_size * output_dim
#     int i = idx / 64;  // Batch index
#     int j = idx % 64;  // Output index
#     float result = 0.0;
#     for (int k = 0; k &lt; 128; ++k) {  // Reduction loop
#       result += A[i * 128 + k] * B[k * 64 + j];
#     }
#     C[idx] = result;
#   }
# }

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p><strong>Summary of Intermediate Representations</strong></p> <ol><li>Relay IR: High-level computational graph, defines operators like nn.dense.</li> <li>TE: Abstracts computation using mathematical tensor operations and supports scheduling primitives.</li> <li>TIR: Low-level, loop-based representation with explicit memory hierarchy.</li> <li>CUDA Code: GPU kernel for matrix multiplication, including thread and block mappings.</li></ol> <hr> <h3 id="_0-1-model-parsing-and-relay-ir-construction"><a href="#_0-1-model-parsing-and-relay-ir-construction" class="header-anchor">#</a> 0.1 Model Parsing and Relay IR Construction</h3> <ul><li>Input
Model in high-level frameworks like TensorFlow, PyTorch, or ONNX.</li> <li>Process:
<ul><li>TVM parses the input model and converts it into Relay IR, a hig-h-level intermediate representation.</li> <li>The Relay IR describes the computational graph with operator-level abstractions.</li></ul></li> <li>Key Functions:
relay.frontend.from_pytorch(), relay.frontend.from_onnx() in src/relay/frontend/.</li></ul> <h3 id="_0-2-high-level-optimizations-in-relay"><a href="#_0-2-high-level-optimizations-in-relay" class="header-anchor">#</a> 0.2 High-Level Optimizations in Relay</h3> <ul><li>Input: Relay IR.</li> <li>Process:
<ul><li>Optimize the Relay IR for performance and hardware compatibility through:</li> <li>Operator Fusion: Fuse adjacent operations.</li> <li>Constant Folding: Precompute static expressions.</li> <li>Layout Transformation: Adjust data layouts (e.g., NCHW → NCHWc).</li> <li>Quantization: Lower precision where applicable.</li> <li>Common Subexpression Elimination.
Finalize the optimized Relay graph.</li></ul></li> <li>Key Functions:<br>
src/relay/transforms/ for passes like fuse_ops.cc, alter_op_layout.cc.</li></ul> <h3 id="_0-3-lowering-to-tensor-expression-te"><a href="#_0-3-lowering-to-tensor-expression-te" class="header-anchor">#</a> 0.3. Lowering to Tensor Expression (TE)</h3> <ul><li>Input: Optimized Relay IR.</li> <li>Process:
<ul><li>Translate high-level Relay operators into Tensor Expressions (TE).</li> <li>TE represents computations as mathematical tensor operations and allows for:
<ul><li>Abstraction of computation patterns (e.g., matrix multiplication).</li> <li>Introduction of scheduling primitives (e.g., tiling, unrolling, vectorization).</li></ul></li></ul></li> <li>Key Functions:
<ul><li>src/relay/backend/te_compiler.cc: Bridges Relay IR and TE.</li> <li>src/te/tensor.cc: Constructs tensor expressions.</li></ul></li></ul> <h3 id="_0-4-scheduling-in-te"><a href="#_0-4-scheduling-in-te" class="header-anchor">#</a> 0.4. Scheduling in TE</h3> <ul><li>Input: Tensor Expressions.</li> <li>Process:
<ul><li>Apply scheduling primitives to improve performance:</li> <li>Tiling: Divide tensors into smaller chunks for parallelism.</li> <li>Unrolling: Optimize loops for instruction pipelining.</li> <li>Thread/Block Mapping: Map computations to GPU threads and blocks.</li> <li>Vectorization: Use SIMD instructions where applicable.</li> <li>Refines Tensor Expressions into Tensor Intermediate Representation (TIR).</li></ul></li> <li>Key Functions:
<ul><li>src/te/schedule/ for scheduling functions.</li> <li>src/te/schedule/schedule_dataflow_rewrite.cc: Handles dataflow rewrite scheduling.</li></ul></li></ul> <h3 id="_0-5-lowering-to-tir"><a href="#_0-5-lowering-to-tir" class="header-anchor">#</a> 0.5. Lowering to TIR</h3> <ul><li>Input: Tensor Expressions with schedules.</li> <li>Process:
<ul><li>Convert TE into Tensor IR (TIR), a low-level IR closer to device execution.</li> <li>Perform device-specific optimizations for CUDA (e.g., thread hierarchy mapping).</li></ul></li> <li>Key Functions:
<ul><li>src/tir/transform/ for device-specific passes like loop unrolling and thread binding.</li></ul></li></ul> <h3 id="_0-6-code-generation"><a href="#_0-6-code-generation" class="header-anchor">#</a> 0.6. Code Generation</h3> <ul><li>Input: TIR optimized for CUDA.</li> <li>Process:
<ul><li>Code Generation:Translate TIR into CUDA kernels. Use TVM's built-in CUDA code generator.</li> <li>Calling cuBLAS/cuDNN or CUTLASS:
<ul><li>For specific operations (e.g., GEMM), call external libraries.</li> <li>Determine the sequence of library calls and parameters based on operator attributes.</li></ul></li> <li>Memory Allocation: Analyze dataflow to allocate memory efficiently on GPU.</li></ul></li> <li>Key Functions:
<ul><li>CUDA Codegen:
src/target/source/codegen_cuda.cc: Generates CUDA source code.</li> <li>External Libraries:
src/runtime/contrib/cublas.cc: Integrates with cuBLAS.
src/runtime/contrib/cudnn.cc: Integrates with cuDNN.
src/contrib/cutlass/: Integrates with CUTLASS.</li></ul></li></ul> <h3 id="_0-7-final-compilation-and-deployment"><a href="#_0-7-final-compilation-and-deployment" class="header-anchor">#</a> 0.7. Final Compilation and Deployment</h3> <ul><li>Input: CUDA source code.</li> <li>Process:
<ul><li>Compile CUDA source code using NVCC or the TVM runtime.</li> <li>Deploy the compiled kernel and runtime modules.</li></ul></li> <li>Key Functions:
<ul><li>src/target/source/: Handles code generation.</li> <li>src/runtime/: Manages runtime execution and deployment.</li></ul></li></ul> <hr> <h2 id="_1-model-parsing-and-relay-ir-construction"><a href="#_1-model-parsing-and-relay-ir-construction" class="header-anchor">#</a> 1. Model Parsing and Relay IR Construction</h2> <p>In TVM, high-level optimization in the Relay IR phase includes several graph-level optimizations, data layout transformations, and other functional passes.</p> <p>These optimizations are implemented in various source files under src/relay/transform and src/relay/op directories.</p> <p>Below is a categorized list of these optimizations along with their corresponding source code files and functions:</p> <h3 id="_1-1-graph-level-optimizations"><a href="#_1-1-graph-level-optimizations" class="header-anchor">#</a> 1.1 Graph-Level Optimizations</h3> <p>Graph-level optimizations restructure or simplify the computation graph for better performance.</p> <table><thead><tr><th>Optimization	Source</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Constant Folding</td> <td>src/relay/transform/fold_constant.cc</td> <td>FoldConstant, ConstantFolder</td></tr> <tr><td>Operator Fusion</td> <td>src/relay/transform/fuse_ops.cc</td> <td>FuseOps, FuseMutator, PatternMatcher</td></tr> <tr><td>Dead Code Elimination (DCE)</td> <td>src/relay/transform/eliminate_common_subexpr.cc</td> <td>EliminateCommonSubexpr</td></tr> <tr><td>Common Subexpression Elimination</td> <td>src/relay/transform/eliminate_common_subexpr.cc</td> <td>EliminateCommonSubexpr</td></tr> <tr><td>Simplify Inference</td> <td>src/relay/transform/simplify_inference.cc</td> <td>SimplifyInference, SimplifyInferenceMutator</td></tr> <tr><td>Call Folding</td> <td>src/relay/transform/fold_call.cc</td> <td>FoldCall</td></tr> <tr><td>Inline Functions</td> <td>src/relay/transform/inline.cc</td> <td>Inline, InlineMutator</td></tr> <tr><td>Prune Unused Functions</td> <td>src/relay/transform/prune_unused_functions.cc</td> <td>PruneUnusedFunctions</td></tr></tbody></table> <h3 id="_1-2-data-layout-transformations"><a href="#_1-2-data-layout-transformations" class="header-anchor">#</a> 1.2 Data Layout Transformations</h3> <p>These optimizations adjust the layout of tensors for better memory access patterns and compatibility with target hardware.</p> <table><thead><tr><th>Transformation</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Alter Layout</td> <td>src/relay/transform/alter_op_layout.cc</td> <td>AlterOpLayout, AlterOpLayoutRewriter</td></tr> <tr><td>Convert Layout</td> <td>s	src/relay/transform/convert_layout.cc</td> <td>ConvertLayout</td></tr> <tr><td>Fold Scale Axis</td> <td>src/relay/transform/fold_scale_axis.cc</td> <td>FoldScaleAxis, ScaleAxisSimplifier</td></tr> <tr><td>Layout Optimization</td> <td>src/relay/transform/layout_rewrite.cc</td> <td>LayoutRewrite</td></tr></tbody></table> <h3 id="_1-3-quantization-and-precision-management"><a href="#_1-3-quantization-and-precision-management" class="header-anchor">#</a> 1.3 Quantization and Precision Management</h3> <p>TVM supports quantization optimizations for reduced precision operations.</p> <table><thead><tr><th>Optimization</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Quantize</td> <td>src/relay/quantize/quantize.cc</td> <td>Quantize, CreateQuantizePass</td></tr> <tr><td>Dequantize</td> <td>src/relay/quantize/dequantize.cc</td> <td>Dequantize</td></tr> <tr><td>SimplifyQuantize</td> <td>src/relay/transform/simplify_quantize.cc</td> <td>SimplifyQuantize, SimplifyQuantizeRewriter</td></tr></tbody></table> <h3 id="_1-4-automatic-differentiation"><a href="#_1-4-automatic-differentiation" class="header-anchor">#</a> 1.4 Automatic Differentiation</h3> <p>TVM includes an autodiff system for neural networks.</p> <table><thead><tr><th>Transformation</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Reverse Mode Autodiff</td> <td>src/relay/transforms/gradient.cc</td> <td>AutomaticDifferentiation, ReverseAD</td></tr></tbody></table> <h3 id="_1-5-high-level-hardware-aware-optimizations"><a href="#_1-5-high-level-hardware-aware-optimizations" class="header-anchor">#</a> 1.5 High-Level Hardware-Aware Optimizations</h3> <p>These optimizations modify operations based on the target hardware.</p> <table><thead><tr><th>Optimization</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Annotate Target</td> <td>src/relay/transform/annotate_target.cc</td> <td>AnnotateTarget</td></tr> <tr><td>Partition Graph</td> <td>src/relay/transform/partition_graph.cc</td> <td>PartitionGraph</td></tr> <tr><td>Merge Compiler Regions</td> <td>src/relay/transform/merge_compiler_regions.cc</td> <td>MergeCompilerRegions</td></tr></tbody></table> <h3 id="_1-6-device-placement"><a href="#_1-6-device-placement" class="header-anchor">#</a> 1.6 Device Placement</h3> <p>These passes assign operations to devices for heterogeneous execution.</p> <table><thead><tr><th>Transformation</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Rewrite Annotated Ops</td> <td>src/relay/transform/rewrite_annotated_ops.cc</td> <td>RewriteAnnotatedOps</td></tr> <tr><td>Device Annotation</td> <td>src/relay/transform/device_annotation.cc</td> <td>DeviceAnnotation</td></tr></tbody></table> <h3 id="_1-7-meta-pass-management"><a href="#_1-7-meta-pass-management" class="header-anchor">#</a> 1.7 Meta-Pass Management</h3> <p>Relay provides a meta-pass system to manage and sequence passes.</p> <table><thead><tr><th>Meta-Pass</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Sequential Pass Manager</td> <td>src/relay/transform/sequential.cc</td> <td>Sequential, PassManager</td></tr> <tr><td>Pass Context</td> <td>src/relay/transform/pass.cc</td> <td>PassContext, WithPassContext</td></tr></tbody></table> <hr> <h2 id="_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir"><a href="#_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir" class="header-anchor">#</a> 2 Lowering to Tensor Expression (TE), Scheduing in TE and Lowering into TIR</h2> <p>The lowering process from Relay IR to Tensor Expression (TE) and Tensor IR (TIR) in TVM involves multiple phases.</p> <p>These include converting Relay IR to TE, applying tensor computation abstractions, performing scheduling, and constructing device-specific low-level TIR.</p> <p>Here’s a detailed breakdown of the corresponding TVM source code files and functions for these stages:</p> <h3 id="_2-1-converting-relay-ir-to-tensor-expression-te"><a href="#_2-1-converting-relay-ir-to-tensor-expression-te" class="header-anchor">#</a> 2.1 Converting Relay IR to Tensor Expression (TE)</h3> <p>This phase converts high-level Relay IR into the computation abstractions provided by TE.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Relay to TE Lowering</td> <td>src/relay/backend/te_compiler.cc</td> <td>LowerToTE, CreateSchedule, ScheduleGetter</td></tr> <tr><td>Operator Strategy</td> <td>src/relay/op/strategy/generic.cc</td> <td>GenericFunc, OpStrategy</td></tr> <tr><td>Relay to TE Bridge</td> <td>src/relay/backend/te_compiler_cache.cc</td> <td>TECompiler, LowerTE</td></tr> <tr><td>Shape Function Lowering</td> <td>src/relay/backend/te_compiler.cc</td> <td>LowerShapeFunc</td></tr></tbody></table> <p>Explanation:</p> <ul><li>The Relay IR graph is analyzed, and for each operator, TVM retrieves a corresponding TE function using OpStrategy.</li> <li>TE functions define high-level operations like matrix multiplication, element-wise addition, etc.</li></ul> <h3 id="_2-2-abstraction-of-computation-in-tensor-expression-te"><a href="#_2-2-abstraction-of-computation-in-tensor-expression-te" class="header-anchor">#</a> 2.2 Abstraction of Computation in Tensor Expression (TE)</h3> <p>TE provides a declarative way to express computation. This includes operations like tiling, unrolling, and vectorizing.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Tensor Expression Build</td> <td>src/te/operation/create_primfunc.cc</td> <td>CreatePrimFunc, ComputeBody, ScheduleOps</td></tr> <tr><td>Compute Definition</td> <td>src/te/operation/compute_op.cc</td> <td>ComputeOpNode, ComputeOp</td></tr> <tr><td>Tensor Compute Intrinsics</td> <td>src/te/operation/tensorize.cc</td> <td>Tensorize, CreateIntrinBody</td></tr></tbody></table> <p>Explanation:</p> <ul><li>High-level computations are abstracted into a declarative format using ComputeOp.</li> <li>Intrinsic support for tensorization is added for specialized hardware operations.</li></ul> <h3 id="_2-3-scheduling-in-tensor-expression"><a href="#_2-3-scheduling-in-tensor-expression" class="header-anchor">#</a> 2.3 Scheduling in Tensor Expression</h3> <p>Scheduling is where TVM optimizes how computations are performed on the target device.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Tile, Unroll, Vectorize</td> <td>src/te/schedule/schedule_dataflow_rewrite.cc</td> <td>ScheduleDataFlowRewrite, Tile, Unroll, Vectorize</td></tr> <tr><td>Thread and Block Mapping</td> <td>src/te/schedule/schedule_lang.cc</td> <td>bind, split, reorder, fuse</td></tr> <tr><td>AutoScheduler Interface</td> <td>src/auto_scheduler/compute_dag.cc</td> <td>ComputeDAG, ApplySteps</td></tr> <tr><td>Lowering Schedule to TIR</td> <td>src/te/schedule/graph.cc</td> <td>ScheduleGraph, LowerSchedule</td></tr></tbody></table> <p>Explanation:</p> <ul><li>This phase defines how computations should be split into smaller tiles, mapped to hardware threads and blocks, and optimized using unrolling and vectorization.</li> <li>Tensor schedules are converted into lower-level forms through ScheduleGraph.</li></ul> <h3 id="_2-4-constructing-low-level-tensor-ir-tir"><a href="#_2-4-constructing-low-level-tensor-ir-tir" class="header-anchor">#</a> 2.4 Constructing Low-Level Tensor IR (TIR)</h3> <p>TIR represents a low-level, device-specific IR used to generate target-specific code.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>TIR Construction</td> <td>src/tir/stmt_functor.cc</td> <td>StmtFunctor, VisitStmt, MakeStmt</td></tr> <tr><td>Lowering to TIR</td> <td>src/tir/transforms/lower_tir.cc</td> <td>LowerTIR, TransformTIR</td></tr> <tr><td>Memory Planning</td> <td>src/tir/transforms/storage_rewrite.cc</td> <td>StorageRewrite, PlanMemory</td></tr> <tr><td>Device-Specific TIR</td> <td>src/target/codegen.cc</td> <td>Build, BuildIRModule</td></tr></tbody></table> <p>Explanation:</p> <ul><li>TE schedules are converted into TIR, which provides explicit control over memory accesses and device-specific optimizations.</li> <li>StorageRewrite optimizes memory allocation and reuse.</li></ul> <h3 id="_2-5-device-specific-optimizations"><a href="#_2-5-device-specific-optimizations" class="header-anchor">#</a> 2.5 Device-Specific Optimizations</h3> <p>Device-specific optimizations tailor the generated code for the target platform (e.g., CUDA).</p> <table><thead><tr><th>Transformation</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Thread/Block Mapping</td> <td>src/tir/transforms/thread_storage_sync.cc</td> <td>ThreadStorageSync</td></tr> <tr><td>Loop Partitioning</td> <td>src/tir/transforms/loop_partition.cc</td> <td>LoopPartition</td></tr> <tr><td>Device Codegen</td> <td>src/target/source/codegen_cuda.cc</td> <td>CodeGenCUDA, PrintKernel</td></tr></tbody></table> <p>High-Level Summary of the Workflow</p> <ul><li>Relay to TE:<br>
Converts high-level operations into Tensor Expression (TE) definitions using strategies (src/relay/backend/te_compiler.cc).</li> <li>Computation Abstraction:
Defines computations in TE with ComputeOp (src/te/operation/compute_op.cc).</li> <li>Scheduling:<br>
Applies optimizations like tiling, unrolling, and mapping computations to threads/blocks (src/te/schedule/schedule_lang.cc).</li> <li>Lowering to TIR:<br>
Translates the schedule into TIR, which explicitly handles device memory and control flow (src/tir/transforms/lower_tir.cc).</li> <li>Device-Specific Codegen:<br>
Emits target-specific code (e.g., CUDA) via CodeGenCUDA (src/target/source/codegen_cuda.cc).</li></ul> <hr> <h2 id="_3-code-generation"><a href="#_3-code-generation" class="header-anchor">#</a> 3. Code Generation</h2> <h3 id="_3-1-gpu-code-generation"><a href="#_3-1-gpu-code-generation" class="header-anchor">#</a> 3.1 GPU Code Generation</h3> <p>This phase translates Tensor IR (TIR) into GPU-compatible low-level code, generating CUDA kernels and API calls.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>TIR to CUDA Kernel</td> <td>src/target/source/codegen_cuda.cc</td> <td>CodeGenCUDA, GenerateKernel, PrintStmt</td></tr> <tr><td>CodeGen Base Class</td> <td>src/target/source/codegen_c.cc</td> <td>CodeGenC, PrintExpr</td></tr> <tr><td>Shared Memory Handling</td> <td>src/target/source/codegen_cuda.cc</td> <td>PrintStorageScope, PrintStorageSync</td></tr> <tr><td>Thread/Block Synchronization</td> <td>src/tir/transforms/thread_storage_sync.cc</td> <td>ThreadStorageSync</td></tr></tbody></table> <p><strong>Explanation:</strong>
CodeGenCUDA translates TIR to CUDA kernels, emitting device-side code and managing constructs like thread/block mappings, shared memory, and synchronization.
Synchronization points are inserted using PrintStorageSync.</p> <h3 id="_3-2-kernel-construction"><a href="#_3-2-kernel-construction" class="header-anchor">#</a> 3.2. Kernel Construction</h3> <p>Kernel construction involves creating CUDA device kernels and host-side launcher code to invoke them.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Kernel Emission</td> <td>src/target/source/codegen_cuda.cc</td> <td>PrintFuncBody, EmitFunction</td></tr> <tr><td>Kernel Launch Code</td> <td>src/runtime/cuda/cuda_module.cc</td> <td>CUDAWrappedFunc, LaunchKernel</td></tr> <tr><td>Kernel Metadata Management</td> <td>src/runtime/module.cc</td> <td>PackImports, ExportModule</td></tr></tbody></table> <p>Explanation:
The EmitFunction generates kernel function declarations and definitions for execution on the GPU.
Host-side kernel launchers are defined in cuda_module.cc.</p> <h3 id="_3-3-cublas-cutlass-integration"><a href="#_3-3-cublas-cutlass-integration" class="header-anchor">#</a> 3.3. cuBLAS/CUTLASS Integration</h3> <p>When using cuBLAS or CUTLASS for tensor computations (e.g., GEMM), TVM generates calls to these libraries instead of writing explicit CUDA kernels.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>cuBLAS Integration</td> <td>src/runtime/contrib/cublas/cublas.cc</td> <td>CUBLASCall, InitCUBLASHandle, GemmOp</td></tr> <tr><td>CUTLASS Integration</td> <td>src/contrib/cutlass/gen_cutlass_gemm.cc</td> <td>GenerateCutlassGemm, EmitCutlassCode</td></tr> <tr><td>External Code Generation</td> <td>src/relay/backend/contrib/cublas_codegen.cc</td> <td>CUBLASFunction, CodegenCUBLAS</td></tr></tbody></table> <p>Explanation:
cublas.cc provides wrappers for cuBLAS API calls like cublasSgemm, with TVM handling data layout transformations as needed.
CUTLASS integration uses template-based code generation in gen_cutlass_gemm.cc, emitting optimized kernels for matrix operations.</p> <h3 id="_3-4-target-specific-optimizations"><a href="#_3-4-target-specific-optimizations" class="header-anchor">#</a> 3.4. Target-Specific Optimizations</h3> <p>Target-specific optimizations fine-tune the generated CUDA code based on the GPU architecture and memory hierarchy.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Thread/Block Mapping</td> <td>src/tir/transforms/thread_storage_sync.cc</td> <td>ThreadStorageSync, OptimizeThreads</td></tr> <tr><td>Loop Partitioning</td> <td>src/tir/transforms/loop_partition.cc</td> <td>LoopPartition</td></tr> <tr><td>Memory Planning</td> <td>src/tir/transforms/storage_rewrite.cc</td> <td>StorageRewrite, PlanMemory</td></tr> <tr><td>Warp-Level Optimization</td> <td>src/tir/transforms/vectorize_loop.cc</td> <td>VectorizeLoop, Vectorizer</td></tr></tbody></table> <p>Explanation:
Thread and block mapping ensures optimal utilization of GPU threads and memory.
Loop partitioning and vectorization optimize data access patterns for warp-level efficiency.
StorageRewrite minimizes memory usage by analyzing reuse patterns and adjusting allocation.</p> <h3 id="_3-5-memory-management"><a href="#_3-5-memory-management" class="header-anchor">#</a> 3.5. Memory Management</h3> <p>Efficient memory management involves optimizing shared/global memory usage and enabling memory reuse.</p> <table><thead><tr><th>Process</th> <th>File</th> <th>Key Functions/Classes</th></tr></thead> <tbody><tr><td>Shared Memory Usage</td> <td>src/target/source/codegen_cuda.cc</td> <td>PrintStorageScope, EmitSharedMemory</td></tr> <tr><td>Memory Allocation</td> <td>src/tir/transforms/storage_rewrite.cc</td> <td>PlanMemory, ReuseMemory</td></tr> <tr><td>Memory Alignment</td> <td>src/target/source/codegen_cuda.cc</td> <td>PrintStorageAlloc</td></tr></tbody></table> <p>Explanation:
Shared memory scopes are explicitly emitted during CUDA codegen (EmitSharedMemory).
PlanMemory optimizes allocation to minimize fragmentation and overhead.</p> <h3 id="_3-6-overall-codegen-workflow"><a href="#_3-6-overall-codegen-workflow" class="header-anchor">#</a> 3.6. Overall Codegen Workflow</h3> <p>Key Stages and Their Files</p> <ul><li><p>TIR Lowering:<br>
File: src/tir/transforms/lower_tir.cc<br>
Function: LowerTIR, TransformTIR</p></li> <li><p>CUDA Kernel Emission:<br>
File: src/target/source/codegen_cuda.cc<br>
Function: EmitFunction, GenerateKernel</p></li> <li><p>cuBLAS Integration:<br>
File: src/runtime/contrib/cublas/cublas.cc<br>
Function: CUBLASCall, InitCUBLASHandle</p></li> <li><p>CUTLASS Integration:<br>
File: src/contrib/cutlass/gen_cutlass_gemm.cc<br>
Function: GenerateCutlassGemm, EmitCutlassCode</p></li> <li><p>Target-Specific Optimizations:<br>
File: src/tir/transforms/thread_storage_sync.cc<br>
Function: ThreadStorageSync, OptimizeThreads</p></li></ul></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/02.compiler/09.learn_tvm_1.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2024/12/31, 03:39:31</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/000008/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Writing TinyRISCV Backend</div></a> <a href="/qishao-notes/pages/000010/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Learn TPU_MLIR</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/000008/" class="prev">Writing TinyRISCV Backend</a></span> <span class="next"><a href="/qishao-notes/pages/000010/">Learn TPU_MLIR</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2024
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.bd7c0775.js" defer></script><script src="/qishao-notes/assets/js/2.b768edde.js" defer></script><script src="/qishao-notes/assets/js/31.219afea4.js" defer></script>
  </body>
</html>
