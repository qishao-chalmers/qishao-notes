<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Large Language Model Paper List | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.d95801da.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.d64792be.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/37.97f068b3.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.bb9b167a.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.dba5667f.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.cd5972d9.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.5d2c13e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.4398ccf7.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.7a4b874f.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.f5ba7842.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.d76d4c59.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.f2213733.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.e5199ea8.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.e4590625.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.77fccc08.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.f92358ce.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.c8bd80e3.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.34d5b8bd.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.94570699.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.2218cfa9.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.f2d23423.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.8d539e2c.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.c4f8ad67.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.9ea0c48c.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.3528b207.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.7bce59db.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.481cf8d9.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.f8986527.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.970c3618.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.43cdc609.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.e6015528.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.f4f7fb82.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.2bdc45a0.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.7df6fc1d.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.d0950448.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.51790460.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.034f2461.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.42890ab3.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.ab1e56e4.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.fffcbbc3.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.59f4480e.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.63976d34.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.ca03f35c.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.3f1934ac.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.1f319a95.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.c1f5a851.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.25fb7ffc.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.edca9b15.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.7a107b3d.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.a1ffa421.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.214a53de.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.21fe1a6c.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.5e620b99.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.895a3f05.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.8b2d00b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.5e92cb01.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.3f25286b.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.b000f071.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.c31143c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.2dd61ca4.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.8c33dfd7.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.37fe36b3.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.832e16e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.8303dbe4.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.1a6248da.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.0bec6673.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.8115d37d.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.7797cb64.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.7e1a632c.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.86a4dc3a.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.c13258af.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.12d34e49.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.8ddcbb70.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.ce6b7758.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.1766bb9e.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.cac57c85.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.c3409789.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.46f07fc3.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.8e0705cf.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.56bdeff0.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.d67d9234.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/cc7034/" class="sidebar-link">Operand Collector</a></li><li><a href="/qishao-notes/pages/2476ae/" class="sidebar-link">GPU WARP Scheduler</a></li><li><a href="/qishao-notes/pages/14769f/" class="sidebar-link">Precision Exception</a></li><li><a href="/qishao-notes/pages/44771e/" class="sidebar-link">Unified Memory Paper List</a></li><li><a href="/qishao-notes/pages/44871e/" class="sidebar-link">TensorCore Paper List</a></li><li><a href="/qishao-notes/pages/45871e/" class="sidebar-link">Memory Behaviour Paper List</a></li><li><a href="/qishao-notes/pages/45871f/" class="sidebar-link">GPU Virtualization Paper List</a></li><li><a href="/qishao-notes/pages/458720/" aria-current="page" class="active sidebar-link">Large Language Model Paper List</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/qishao-notes/pages/458721/" class="sidebar-link">GPU Simulator</a></li><li><a href="/qishao-notes/pages/458722/" class="sidebar-link">Architectural Survey</a></li><li><a href="/qishao-notes/pages/458724/" class="sidebar-link">Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper</a></li><li><a href="/qishao-notes/pages/458725/" class="sidebar-link">Understanding GPGPU-SIM 1 How to get Instruction</a></li><li><a href="/qishao-notes/pages/458726/" class="sidebar-link">Understanding GPGPU-SIM 2 Instruction Execution</a></li><li><a href="/qishao-notes/pages/458727/" class="sidebar-link">Understanding GPGPU-SIM 3 How is the simulation started</a></li><li><a href="/qishao-notes/pages/45872/" class="sidebar-link">Understanding GPGPU-SIM 4 Microarchitecture</a></li><li><a href="/qishao-notes/pages/45874/" class="sidebar-link">Understanding GPGPU-SIM 5  Memory Interface</a></li><li><a href="/qishao-notes/pages/45873/" class="sidebar-link">Warp Related Memory Optimization</a></li><li><a href="/qishao-notes/pages/45875/" class="sidebar-link">GPU Cache Coherency</a></li><li><a href="/qishao-notes/pages/45876/" class="sidebar-link">GPU Cache &amp; Memory Hirerarchy</a></li><li><a href="/qishao-notes/pages/45877/" class="sidebar-link">GPU TLB</a></li><li><a href="/qishao-notes/pages/45878/" class="sidebar-link">GPU Page Table Walk</a></li><li><a href="/qishao-notes/pages/45879/" class="sidebar-link">GPU Cache's Papers</a></li><li><a href="/qishao-notes/pages/45880/" class="sidebar-link">GPU WARP Mangement Papers</a></li><li><a href="/qishao-notes/pages/45882/" class="sidebar-link">GPU Unified Memory Innovations</a></li><li><a href="/qishao-notes/pages/45883/" class="sidebar-link">GPU MultiTask</a></li><li><a href="/qishao-notes/pages/45884/" class="sidebar-link">GPU Training Notes</a></li><li><a href="/qishao-notes/pages/45885/" class="sidebar-link">GPU Paper with Code</a></li><li><a href="/qishao-notes/pages/45886/" class="sidebar-link">GPU Workload Characteristics</a></li><li><a href="/qishao-notes/pages/47871e/" class="sidebar-link">TO READ</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/gpu/#gpu" data-v-06225672>gpu</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2023-12-19</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">Large Language Model Paper List<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>Efficient Memory Management for Large Language Model Serving with PagedAttention [2023]</li> <li>LLM in a flash: Efficient Large Language Model Inference with Limited Memory [Apple 2023]</li> <li>[591 Year: 2021] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM <strong>Not Read</strong></li></ol> <hr> <h3 id="_1-efficient-memory-management-for-large-language-model-serving-with-pagedattention"><a href="#_1-efficient-memory-management-for-large-language-model-serving-with-pagedattention" class="header-anchor">#</a> 1. Efficient Memory Management for Large Language Model Serving with PagedAttention</h3> <p>Disscussed the GEMM in prompt and GEMV in auto regression.
In GEMV, LLM is memory bound. There is lot of fragment in KVCache.
It also quantize the memory necessity for parameter in KV Cache.
They came up the method similar to paging in OS to manage KV in KV cache, reducing the fragment.</p> <h3 id="_2-llm-in-a-flash-efficient-large-language-model-inference-with-limited-memory"><a href="#_2-llm-in-a-flash-efficient-large-language-model-inference-with-limited-memory" class="header-anchor">#</a> 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory</h3> <p>Upproject matrix and downprojection matrix:
https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/
Related paper:
Parameter-Efficient Transfer Learning for NLP
This introduce low-rank.
<img src="https://github.com/hitqshao/qishao-notes/assets/23403286/14c7f4b6-7709-48b9-9871-77296acb8e19" alt="image"></p> <p>sliding window.
<img src="https://github.com/hitqshao/qishao-notes/assets/23403286/b3d007ea-743d-445b-8236-8e1aaaea816e" alt="image"></p> <ol><li><p>high sparsity in FeedForward Layers, more than 90%
Selectively only load parameters from memory either no-zero input or predicted have non-zero output</p></li> <li><p>Minimize data transfer and maximize flash memory throughout
<strong>Window sliding</strong>: Load parameters for only the past few tokens, reusing activations from recently computed tokens. This sliding window approach reduces the number of IO requests to load weights.
<strong>Row-column bundling</strong>: We store a concatenated row and column of the up-projection and down-projection layers to read bigger contiguous chunks from flash memory. This increases throughput by reading larger chunks.</p></li> <li><p>Predict FFN sparsity and avoid loading zeroed-out parameter
to minimize the number of weights to be transferred from flash memory to DRAM.</p></li> <li><p>Static memory preallocation</p></li></ol> <p>Also a model to predict the tradeoff between loading less data and reading larger chunks</p> <p>Load only 2% of FFN layer from flash</p> <ol><li><p>Larger chunk
Although throughput growth is not linear (larger chunks take longer to transfer), the latency for the initial byte becomes a smaller fraction of the total request time, resulting in more efficient data reading.
<img src="https://github.com/hitqshao/qishao-notes/assets/23403286/de3b1b16-fc7e-4ce9-bba6-aa6ad2bccf34" alt="image"></p></li> <li><p>Load From Flash</p></li></ol> <p>2.1  inherent sparsity found in Feed-Forward Network (FFN) model</p> <p><strong>Selective Persistence Strategy</strong>
Retain the embeddings and matrices within the attention mechanism of the transformer constant.Attentions weights 1/3 of the model size.For the Feed-Forward Network (FFN) portions, only the non-sparse segments are dynamically loaded into DRAM as needed.</p> <p><strong>Anticipating ReLU Sparsity</strong><br>
Relu activation can induce 90% sparsity. Optimize preceding layer, up project by low-rank predictor to identify the zeroed elements post-ReLU.<br>
In contrast to their work, our predictor needs only the output of the current layer’s attention module and not the previous layer’s FFN module.
<img src="https://github.com/hitqshao/qishao-notes/assets/23403286/85f3c2dc-a352-4506-b4d4-08abb896e8c7" alt="image"></p> <p><strong>Neuron Data Management via Sliding Window Technique</strong>
Our approach focuses on managing neuron data by employing a Sliding Window Technique. This methodology entails maintaining neuron data only for a recent subset of input tokens in the memory.<br>
The key aspect of this technique is the <strong>selective loading of neuron data that differs between the current input token and its immediate predecessors</strong>.<br>
Frees up memory resources previously allocated to neuron data from older tokens that are no longer within the sliding window
<img src="https://github.com/hitqshao/qishao-notes/assets/23403286/10b9ef42-5aac-4826-9247-b90ad9a171e7" alt="image"></p> <p>Let sagg(k) denote the cumulative use of neuron data across a sequence of k input tokens.
This reduction in data loading is counterbalanced by the memory cost associated with storing sagg(k). In determining the size of the sliding window, the aim is to maximize it within the constraints imposed by the available memory capacity.</p> <p>2.2 Improve Transfer Throughput with Increased Chunk Sizes</p> <p><strong>Bundling Columns and Rows</strong> for upward and downward projection</p> <p><strong>Bundling Based on Co-activation</strong> fetch neuron with its cloest friend. But there is WARM-GUY problem.</p> <p>2.3 Optimized Data Management in DRAM</p> <p>When a substantial portion (approximately 25%) of the Feed-Forward Networks (FFNs) in DRAM needs to be rewritten.</p> <p>When introducing data for new neurons, reallocating the matrix and appending new matrices can lead to significant overhead due to the need for rewriting existing neurons data in DRAM. This involves the preallocation of all necessary memory and the establishment of a corresponding data structure for efficient management.
<img src="https://github.com/hitqshao/qishao-notes/assets/23403286/39b7adfe-fad8-4bd3-9be3-f084c260fcec" alt="image"></p> <hr> <p>LLM Principles</p> <h3 id="_1-a-survey-on-hallucination-in-large-language-models-principles-taxonomy-challenges-and-open-questions"><a href="#_1-a-survey-on-hallucination-in-large-language-models-principles-taxonomy-challenges-and-open-questions" class="header-anchor">#</a> 1. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions</h3> <p><img src="https://github.com/hitqshao/qishao-notes/assets/23403286/35d4a401-2b5f-43ea-8070-8d1043f9d8e2" alt="image"></p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/03.gpu/08.LLM.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2024/11/04, 00:08:47</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/45871f/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">GPU Virtualization Paper List</div></a> <a href="/qishao-notes/pages/458721/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">GPU Simulator</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/45871f/" class="prev">GPU Virtualization Paper List</a></span> <span class="next"><a href="/qishao-notes/pages/458721/">GPU Simulator</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2024
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.d95801da.js" defer></script><script src="/qishao-notes/assets/js/2.d64792be.js" defer></script><script src="/qishao-notes/assets/js/37.97f068b3.js" defer></script>
  </body>
</html>
