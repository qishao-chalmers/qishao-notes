<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>MLIR Open Meeting Notes The Torch MLIR Project | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.f36c1d02.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.6d8a25ce.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/35.464f648d.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.c53d023c.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.e2b30df6.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.cd5b5485.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.8905e2e7.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.9f10eba5.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.b501a732.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.61edaaa1.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.847ccb05.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.1d6d4d7c.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.4068280c.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.8f11dcf6.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.b6f7d42d.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.66a31a5e.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.1dfde1cd.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.94a55f00.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.ba397c21.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.fc7c01cb.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.4687544f.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.3c4f90ba.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.e4355921.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.bab4312c.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.cd609562.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.dc4136f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.bfc66fcf.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.400c3b79.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.e37f33b1.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.a811aa63.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.496cb071.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.7034ba9b.js"><link rel="prefetch" href="/qishao-notes/assets/js/126.fc7dda71.js"><link rel="prefetch" href="/qishao-notes/assets/js/127.3e9d8080.js"><link rel="prefetch" href="/qishao-notes/assets/js/128.6b84b360.js"><link rel="prefetch" href="/qishao-notes/assets/js/129.5bf9d957.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.33791185.js"><link rel="prefetch" href="/qishao-notes/assets/js/130.04f93760.js"><link rel="prefetch" href="/qishao-notes/assets/js/131.18dfad89.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.35cdaa5c.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.936b94c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.1c803b2c.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.b688542f.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.ba9d4baf.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.42f21f4f.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a7b3ac76.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.93c4077f.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.96a79aa3.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.53082807.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.9bd10ddd.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.18fc9823.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.5828e36e.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.25eb690b.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.5e5f9746.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.a9c55c08.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.a6d85e2e.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.7dcaf9ed.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.141faad8.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.317fcf55.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.7f08612e.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.63e48fb0.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.21d1843c.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.c95eff2e.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.d63a3d79.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.fdab2a0b.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.bae99c2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.89b382c5.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.ea1753be.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.046efc3c.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.f5fc0a4e.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.3cc58cc5.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.dde88fd3.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.4edc7d96.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.f311eaa5.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.526dc690.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.28e90c7e.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.8b03d117.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.6d6c79ff.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.0995d9c1.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.28e64713.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.bfe1dda8.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.9480e434.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.9a082998.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.a3b517e3.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.e6a3f4e4.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.b16ff38f.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.861f9f7a.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.2827b429.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.fe410b9d.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.f32429ee.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.6190f2a9.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.07490016.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.4dc5ddbb.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.d63f1f61.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.b6e66761.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.db27e9db.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.7d99e80d.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.51699562.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.5d95b052.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.1905d6a8.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.05ba7747.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.8f7da019.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.3a589a2f.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.8deb1a9c.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.83e128af.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.55fb1852.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.9c8ec88e.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.68538f63.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.fcb5bc6d.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.f90e1643.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.0540653b.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.47a14f22.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.f2525f33.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.1452152d.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.fbb863e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.53c6d81d.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.e4e19469.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.2d5d9b41.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.665c7088.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.e59c5ce4.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.d1b8e683.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.d729ca45.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.4d5bdf89.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.ce806d59.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.9dfed52c.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.2dcc72f2.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.2bba59c2.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.f04fc93e.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.64849ece.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.747a996c.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.99181d1e.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/000001/" class="sidebar-link">llvm flow</a></li><li><a href="/qishao-notes/pages/000002/" class="sidebar-link">Getting Started with LLVM Core Libraries Chap5 IR</a></li><li><a href="/qishao-notes/pages/000003/" class="sidebar-link">Getting Started with LLVM Core Libraries Chap6 Backend</a></li><li><a href="/qishao-notes/pages/000004/" class="sidebar-link">Learning LLVM Notes</a></li><li><a href="/qishao-notes/pages/000005/" class="sidebar-link">Add New DIY Instruction ACE to LLVM</a></li><li><a href="/qishao-notes/pages/000006/" class="sidebar-link">How does LLVM perform instruction combine</a></li><li><a href="/qishao-notes/pages/000007/" class="sidebar-link">Understand llvm with its source code</a></li><li><a href="/qishao-notes/pages/000008/" class="sidebar-link">Writing TinyRISCV Backend</a></li><li><a href="/qishao-notes/pages/000009/" class="sidebar-link">Learn TVM</a></li><li><a href="/qishao-notes/pages/000010/" class="sidebar-link">Learn TPU_MLIR</a></li><li><a href="/qishao-notes/pages/000011/" class="sidebar-link">MLIR TOY Tutorial</a></li><li><a href="/qishao-notes/pages/000012/" class="sidebar-link">Auto Differentiation in Compiler</a></li><li><a href="/qishao-notes/pages/000013/" aria-current="page" class="active sidebar-link">MLIR Open Meeting Notes The Torch MLIR Project</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/000013/#_1-mlir-open-meeting-2021-10-7-the-torch-mlir-project" class="sidebar-link">[1] MLIR Open Meeting 2021-10-7: The Torch MLIR Project</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000013/#torch-mlir-project-and-op-autogeneration" class="sidebar-link">Torch MLIR Project and Op Autogeneration</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000013/#ods-operation-definition-specification-in-torch-mlir" class="sidebar-link">ODS (Operation Definition Specification) in Torch MLIR</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/000013/#_3-torch-dialect-transformations" class="sidebar-link">3. Torch Dialect Transformations</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/000014/" class="sidebar-link">MLIR Compiling Flow of Conv2D</a></li><li><a href="/qishao-notes/pages/000015/" class="sidebar-link">MLIR Compiling Flow of Conv2D</a></li><li><a href="/qishao-notes/pages/000016/" class="sidebar-link">MLIR Compiling Flow of Transformer-Decoder</a></li><li><a href="/qishao-notes/pages/000017/" class="sidebar-link">MLIR Essential Concepts</a></li><li><a href="/qishao-notes/pages/000018/" class="sidebar-link">MLIR NVGPU Dialect</a></li><li><a href="/qishao-notes/pages/000019/" class="sidebar-link">MLIR Linalg Dialect</a></li><li><a href="/qishao-notes/pages/000020/" class="sidebar-link">MLIR Bufferization</a></li><li><a href="/qishao-notes/pages/000021/" class="sidebar-link">MLIR Bufferization Passes</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/compiler/#compiler" data-v-06225672>compiler</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-03-04</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">MLIR Open Meeting Notes The Torch MLIR Project<!----></h1> <!----> <div class="theme-vdoing-content content__default"><p><strong>Sources</strong></p> <ul><li>MLIR Open Meeting 2021-10-7: The Torch MLIR Project</li></ul> <h2 id="_1-mlir-open-meeting-2021-10-7-the-torch-mlir-project"><a href="#_1-mlir-open-meeting-2021-10-7-the-torch-mlir-project" class="header-anchor">#</a> [1] MLIR Open Meeting 2021-10-7: The Torch MLIR Project</h2> <h3 id="torch-mlir-project-and-op-autogeneration"><a href="#torch-mlir-project-and-op-autogeneration" class="header-anchor">#</a> Torch MLIR Project and Op Autogeneration</h3> <p><img src="https://github.com/user-attachments/assets/e739cce0-2bc7-4b61-a44a-a82b928010b0" alt="image"></p> <p>The Torch MLIR project aims to bridge PyTorch and MLIR ecosystems, automating op generation to facilitate efficient lowering of PyTorch programs into MLIR.</p> <h4 id="_1-op-autogeneration-process"><a href="#_1-op-autogeneration-process" class="header-anchor">#</a> <strong>1.Op Autogeneration Process</strong></h4> <ul><li>Source of Ops: Extracted from the Torch registry at runtime (rather than <em>native_functions.yaml</em>).</li> <li>Example: <em>torch.aten.relu</em></li> <li>MLIR Op Name: <em>torch.aten.relu</em></li> <li>Definition Name: <em>Torch_AtenReluOp</em></li> <li>Extracted Attributes:
<ul><li>Namespace: <em>aten</em></li> <li>Unqualified Name: <em>relu</em></li> <li>Is mutable: False</li> <li>Input argument: <em>self</em> (Tensor)</li> <li>Return type: <em>Tensor</em></li></ul></li></ul> <h4 id="_2-ods-operation-definition-specification-autogeneration"><a href="#_2-ods-operation-definition-specification-autogeneration" class="header-anchor">#</a> <strong>2.ODS (Operation Definition Specification) Autogeneration</strong></h4> <ul><li>Uses the Torch registry information to auto-generate ODS entries.</li> <li>Example ODS for <em>torch.aten.relu</em>:</li></ul> <div class="language-mlir line-numbers-mode"><pre class="language-text"><code>def Torch_AtenReluOp : Torch_Op&lt;&quot;aten.relu&quot;, [
    AllowsTypeRefinement,
    HasValueSemantics
]&gt; {
    let summary = &quot;Generated op for `aten::relu : (Tensor) -&gt; (Tensor)`&quot;;
    let arguments = (ins AnyTorchTensorType:$self);
    let results = (outs AnyTorchTensorType:$result);
    let assemblyFormat = &quot;$self attr-dict `:` type($self) `-&gt;` type($result)&quot;;
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><h4 id="_3-key-traits"><a href="#_3-key-traits" class="header-anchor">#</a> <strong>3.Key Traits:</strong></h4> <ul><li><em>AllowsTypeRefinement</em>: Supports type propagation.</li> <li><em>HasValueSemantics</em>: Indicates operations that return new tensors instead of modifying in-place.</li></ul> <h4 id="_4-handling-mutability-in-place-ops"><a href="#_4-handling-mutability-in-place-ops" class="header-anchor">#</a> <strong>4.Handling Mutability (_ In-Place Ops)</strong></h4> <ul><li>PyTorch has many in-place ops (<em>relu_</em>, <em>add_</em>).</li> <li>The Torch registry contains aliasing information that marks them as mutable.</li> <li>This allows <em>torch.aten.add_</em> to be auto-generated with correct aliasing and mutability constraints.</li></ul> <h4 id="_5-benefits-of-op-autogeneration"><a href="#_5-benefits-of-op-autogeneration" class="header-anchor">#</a> <strong>5.Benefits of Op Autogeneration</strong></h4> <ul><li>Consistency: All ops align with the PyTorch op registry.</li> <li>Minimal Maintenance: Automatically updates as PyTorch evolves.</li> <li>Correctness: Ensures accurate aliasing, mutability, and type properties.</li></ul> <h3 id="ods-operation-definition-specification-in-torch-mlir"><a href="#ods-operation-definition-specification-in-torch-mlir" class="header-anchor">#</a> ODS (Operation Definition Specification) in Torch MLIR</h3> <p>In the Torch MLIR project, ODS (Operation Definition Specification) is a crucial mechanism for defining MLIR operations in a structured way.</p> <p>It allows automated generation of boilerplate code, ensuring that operations are correctly specified, optimized, and maintainable.</p> <h4 id="_1-example-ods-definition-for-torch-aten-relu"><a href="#_1-example-ods-definition-for-torch-aten-relu" class="header-anchor">#</a> 1. Example ODS Definition for torch.aten.relu</h4> <p>From the presentation, the following ODS definition was automatically generated for the torch.aten.relu operation:</p> <div class="language-mlir line-numbers-mode"><pre class="language-text"><code>def Torch_AtenReluOp : Torch_Op&lt;&quot;aten.relu&quot;, [
    AllowsTypeRefinement,
    HasValueSemantics
]&gt; {
    let summary = &quot;Generated op for `aten::relu : (Tensor) -&gt; (Tensor)`&quot;;
    let arguments = (ins AnyTorchTensorType:$self);
    let results = (outs AnyTorchTensorType:$result);
    let assemblyFormat = &quot;$self attr-dict `:` type($self) `-&gt;` type($result)&quot;;
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><h4 id="_2-breakdown-of-ods-components"><a href="#_2-breakdown-of-ods-components" class="header-anchor">#</a> 2. Breakdown of ODS Components</h4> <p><em>A. Operation Definition Header</em></p> <div class="language-mlir line-numbers-mode"><pre class="language-text"><code>def Torch_AtenReluOp : Torch_Op&lt;&quot;aten.relu&quot;, [
    AllowsTypeRefinement,
    HasValueSemantics
]&gt; {
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><ul><li>def Torch_AtenReluOp → Defines a new operation named Torch_AtenReluOp.</li> <li>Torch_Op&lt;&quot;aten.relu&quot;, [...]&gt; → This specifies:
<ul><li>&quot;aten.relu&quot;: The op name as it appears in MLIR.</li> <li>Traits ([...]): Additional properties assigned to the operation.</li></ul></li></ul> <p><em>B. Traits (Operation Properties)</em></p> <div class="language-mlir line-numbers-mode"><pre class="language-text"><code>AllowsTypeRefinement,
HasValueSemantics
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><ul><li>AllowsTypeRefinement → Allows the operation's type to be refined in later transformations.
<ul><li>Example: If the tensor initially has an unknown dtype (unk), it can later be inferred to f32 or i32 based on usage.</li></ul></li> <li>HasValueSemantics → Indicates that the operation does not modify its inputs but instead produces a new output tensor.
<ul><li>Example: relu(x) returns a new tensor, while relu_(x) modifies x in-place.</li></ul></li></ul> <p><em>C. Summary</em></p> <div class="language-mlir line-numbers-mode"><pre class="language-text"><code>let summary = &quot;Generated op for `aten::relu : (Tensor) -&gt; (Tensor)`&quot;;
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>Provides a description of what the operation does.</li> <li>Helps with documentation and debugging.</li></ul> <p><em>D. Arguments (Inputs)</em></p> <div class="language-mlir line-numbers-mode"><pre class="language-text"><code>let arguments = (ins AnyTorchTensorType:$self);
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>(ins ...) → Specifies the input types.</li> <li>AnyTorchTensorType:$self →
<ul><li>This operation takes a single argument named $self (representing the input tensor).</li> <li>The tensor type is generalized (AnyTorchTensorType), meaning it can be any valid tensor in PyTorch.</li></ul></li></ul> <p><em>E. Results (Outputs)</em></p> <div class="language-mlir line-numbers-mode"><pre class="language-text"><code>let results = (outs AnyTorchTensorType:$result);
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>(outs ...) → Specifies the return type.</li> <li>AnyTorchTensorType:$result →
<ul><li>The output is also a tensor (same type as input).</li> <li>$result represents the output variable.</li></ul></li></ul> <p><em>F. Assembly Format</em></p> <div class="language-mlir line-numbers-mode"><pre class="language-text"><code>let assemblyFormat = &quot;$self attr-dict `:` type($self) `-&gt;` type($result)&quot;;
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>Defines how the operation should be printed in MLIR's textual format.</li></ul> <p>Example MLIR Representation:</p> <div class="language-mlir line-numbers-mode"><pre class="language-text"><code>%result = torch.aten.relu %self : !torch.tensor&lt;*xf32&gt; -&gt; !torch.tensor&lt;*xf32&gt;
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li><em>%result</em> stores the output.</li> <li><em>torch.aten.relu</em> is the op name.</li> <li><em>%self</em> is the input tensor.</li> <li>*!torch.tensor&lt;<em>xf32&gt;</em> is the type of the input tensor.</li> <li>*!torch.tensor&lt;<em>xf32&gt;</em> is the type of the output tensor.</li></ul> <h4 id="_3-ods-for-in-place-operations-e-g-relu"><a href="#_3-ods-for-in-place-operations-e-g-relu" class="header-anchor">#</a> 3. ODS for In-Place Operations (e.g., relu_)</h4> <p>For in-place operations like torch.aten.relu_, the ODS differs slightly:</p> <div class="language-mlir line-numbers-mode"><pre class="language-text"><code>def Torch_AtenRelu_InplaceOp : Torch_Op&lt;&quot;aten.relu_&quot;, [
    AllowsTypeRefinement
]&gt; {
    let summary = &quot;Generated op for `aten::relu_ : (Tensor) -&gt; (Tensor)`&quot;;
    let arguments = (ins AnyTorchTensorType:$self);
    let results = (outs AnyTorchTensorType:$self);
    let assemblyFormat = &quot;$self attr-dict `:` type($self) `-&gt;` type($self)&quot;;
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>Key Differences</p> <ul><li>No HasValueSemantics Trait
<ul><li>Since this op modifies self in-place, it does not have value semantics.</li></ul></li> <li>Results Alias Input
<ul><li>Output ($self) is the same as the input ($self), meaning no new tensor is allocated.</li></ul></li></ul> <p>Example MLIR Representation</p> <div class="language-mlir line-numbers-mode"><pre class="language-text"><code>%self = torch.aten.relu_ %self : !torch.tensor&lt;*xf32&gt; -&gt; !torch.tensor&lt;*xf32&gt;
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>Here, the input %self is updated in-place.</li></ul> <h4 id="_4-benefits-of-ods-based-op-autogeneration"><a href="#_4-benefits-of-ods-based-op-autogeneration" class="header-anchor">#</a> 4. Benefits of ODS-Based Op Autogeneration</h4> <ul><li>Automatically Syncs with PyTorch
<ul><li>Since the ops are derived from the Torch registry, they always stay up-to-date with PyTorch.</li></ul></li> <li>Eliminates Manual Specification
<ul><li>Instead of manually defining hundreds of ops, a script can generate ODS files automatically.</li></ul></li></ul> <h4 id="_5-future-enhancements"><a href="#_5-future-enhancements" class="header-anchor">#</a> 5. Future Enhancements</h4> <ul><li>More Ops → Expanding the ODS-based system to cover more PyTorch ops.</li> <li>More Traits → Additional MLIR traits like Pure for mathematical functions.</li> <li>Better Type Inference → Enhancing AllowsTypeRefinement to refine tensor shapes dynamically.</li></ul> <h4 id="_6-summary"><a href="#_6-summary" class="header-anchor">#</a> 6. Summary</h4> <table><thead><tr><th>ODS Component</th> <th>Description</th></tr></thead> <tbody><tr><td>Op Name</td> <td>torch.aten.relu (MLIR name)</td></tr> <tr><td>Base Class</td> <td>Torch_Op (inherits from PyTorch MLIR ops)</td></tr> <tr><td>Traits</td> <td>AllowsTypeRefinement, HasValueSemantics</td></tr> <tr><td>Inputs</td> <td>AnyTorchTensorType:$self</td></tr> <tr><td>Outputs</td> <td>AnyTorchTensorType:$result</td></tr> <tr><td>Assembly Format</td> <td>Defines textual representation</td></tr></tbody></table> <p>Example: MLIR Representation</p> <div class="language-mlir line-numbers-mode"><pre class="language-text"><code>%result = torch.aten.relu %self : !torch.tensor&lt;*xf32&gt; -&gt; !torch.tensor&lt;*xf32&gt;
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>This ODS-based framework automates MLIR integration for PyTorch ops, making it scalable, maintainable, and accurate.</p> <h3 id="_3-torch-dialect-transformations"><a href="#_3-torch-dialect-transformations" class="header-anchor">#</a> 3. Torch Dialect Transformations</h3> <p><img src="https://github.com/user-attachments/assets/b0a07f78-1131-42c1-a0cd-14382ae2b75b" alt="image"></p> <h4 id="_1-reduceopvariants"><a href="#_1-reduceopvariants" class="header-anchor">#</a> 1. ReduceOpVariants</h4> <p>This transformation aims to simplify the IR by reducing multiple similar operations into a smaller, canonical set of operations.</p> <p>In PyTorch and MLIR, there can be multiple variants of the same operation that differ slightly in their implementation or behavior (for example, operations that do the same thing but have in-place variants, like <em>add</em> and <em>add_</em>).</p> <p>By reducing these to a canonical form, it simplifies the IR, which can make subsequent optimizations and transformations easier and more effective.</p> <h4 id="_2-maximizevaluesemantics"><a href="#_2-maximizevaluesemantics" class="header-anchor">#</a> 2. MaximizeValueSemantics</h4> <p>In programming, &quot;value semantics&quot; refer to handling data in such a way that operations on data do not change the original data but rather return new data based on operations.</p> <p>This transformation tries to convert as much of the program as possible to use value semantics.</p> <p>This is beneficial because it can help prevent side effects and makes the program easier to reason about, debug, and optimize, since data flows can be more predictable and less intertwined.</p> <h4 id="_3-refinetypes"><a href="#_3-refinetypes" class="header-anchor">#</a> 3. RefineTypes</h4> <p>Type refinement involves propagating more precise type information throughout the program.</p> <p>This can include refining data types from broader categories (like a tensor) to more specific ones (like a tensor of floats), or adding dimensionality and shape information where possible.</p> <p>Improved type information helps with optimizations like <em>loop unrolling</em>, <em>vectorization</em>, and <em>specialized kernel generation</em>, which can significantly impact performance especially on hardware that benefits from such optimizations.</p> <h4 id="_4-globalizeobjectgraph"><a href="#_4-globalizeobjectgraph" class="header-anchor">#</a> 4. GlobalizeObjectGraph</h4> <p>TorchScript, which is used to convert PyTorch programs into a form that can be optimized and executed by MLIR, organizes code as a graph of objects (similar to the objects in object-oriented programming).</p> <p>Each object can contain methods and properties that reference other objects.</p> <p>The GlobalizeObjectGraph transformation converts this object graph into a flat list of global functions and variables</p> <p>This transformation simplifies the program structure, making it more amenable to standard compiler optimizations that operate on a global or function scope rather than having to navigate complex object interdependencies.</p> <h4 id="_5-summary-and-benefits"><a href="#_5-summary-and-benefits" class="header-anchor">#</a> 5. Summary and Benefits</h4> <p>These transformations collectively aim to streamline the processing of PyTorch models into an efficient, standardized format suitable for backend optimizations and lowering to target hardware.</p> <p>By reducing operation variants, maximizing value semantics, refining types, and globalizing the object graph, the torch dialect in MLIR can better optimize and execute PyTorch programs, potentially improving performance and reducing runtime overhead.</p> <p>These are crucial for deploying machine learning models in resource-constrained environments or where high performance is required.</p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/02.compiler/13.mlir_notes_01.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/04/15, 22:00:21</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/000012/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Auto Differentiation in Compiler</div></a> <a href="/qishao-notes/pages/000014/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">MLIR Compiling Flow of Conv2D</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/000012/" class="prev">Auto Differentiation in Compiler</a></span> <span class="next"><a href="/qishao-notes/pages/000014/">MLIR Compiling Flow of Conv2D</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.f36c1d02.js" defer></script><script src="/qishao-notes/assets/js/2.6d8a25ce.js" defer></script><script src="/qishao-notes/assets/js/35.464f648d.js" defer></script>
  </body>
</html>
