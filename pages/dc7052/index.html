<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>LLM Scaling Law | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.2b30d2b7.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.6d8a25ce.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/101.d2879d9e.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.3e10e050.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.a6846821.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.be2face5.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.e14139ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.b501a732.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.61edaaa1.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.847ccb05.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.5827ba3f.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.e5798645.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.deb9a6dc.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.b6f7d42d.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.9a3c68a2.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.533cb663.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.cb379c26.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.78f46f88.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.c908bb9f.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.916e4961.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.c1214fe6.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.d588ee7a.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.838e610e.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.905af094.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.dc4136f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.8ab1304f.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.a16bcf0f.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.afcabf66.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.8aa890c1.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.34137ec1.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.6ec2ee22.js"><link rel="prefetch" href="/qishao-notes/assets/js/126.04c3baf6.js"><link rel="prefetch" href="/qishao-notes/assets/js/127.10d86fde.js"><link rel="prefetch" href="/qishao-notes/assets/js/128.f0a20af7.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.e5eb84f1.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.bfce2e69.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.18b8bfd8.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.8dd9af70.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.0696685a.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.ba9d4baf.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.7e97f9ce.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a7b3ac76.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.c0805dc6.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.58ec3c1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.dc67cde7.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.f485ff78.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.18fc9823.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.f10809d3.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.223b71e2.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.f0a00d23.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.e1097275.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.18c7702c.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.b7a33d18.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.141faad8.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.1c45955e.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.f857f55e.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.84f10398.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.464f648d.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.fb1d1667.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.c95eff2e.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.d63a3d79.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.766a248f.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.bae99c2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.89b382c5.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.7a14ada7.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.046efc3c.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.f5fc0a4e.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.9cc4d428.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.92df1709.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.4edc7d96.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.54f2ec7e.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.4a1ed3f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.a778f1ef.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.8b03d117.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.92acbe8e.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.0995d9c1.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.28e64713.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.904b83d9.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.2afe64a2.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.9a082998.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.a3b517e3.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.e6a3f4e4.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.e1d3c447.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.fe5b5324.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.f0fbf90b.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.1780d091.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.65de7c88.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.6190f2a9.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.cad1f762.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.4dc5ddbb.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.0e13cd4c.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.aa72dbcf.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.db27e9db.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.daaaa55b.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.0166a661.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.1e07ff8f.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.acfdc18a.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.05ba7747.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.8f7da019.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.c837f3e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.3da94563.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.4bc93321.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.9480ce48.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.9c8ec88e.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.68538f63.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.5153e8b3.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.f90e1643.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.fbebb0b8.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.8c457734.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.5d9b51d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.1452152d.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.a61ee479.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.53c6d81d.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.d9268292.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.2d5d9b41.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.665c7088.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.c94ca959.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.0dfd9c33.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.c168b3cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.4e398438.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.8e749192.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.9dfed52c.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.6325628e.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.4767cd75.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.0a0aa512.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.1f728656.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.6294ae42.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.71ed026c.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Paper List</a></li><li><a href="/qishao-notes/pages/dc7043/" class="sidebar-link">Efficient LLM</a></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" class="sidebar-link">Summery of Inner Workings of LLM</a></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" class="sidebar-link">Memory Optimizations in LLM</a></li><li><a href="/qishao-notes/pages/dc7049/" class="sidebar-link">Reasoning in LLM</a></li><li><a href="/qishao-notes/pages/dc7050/" class="sidebar-link">LLM Mixed Precision &amp; Quantization &amp; Outlier</a></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" aria-current="page" class="active sidebar-link">LLM Scaling Law</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7052/#a-2025-s1-simple-test-time-scaling" class="sidebar-link">[A 2025] s1: Simple test-time scaling :+1:</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7052/#budget-forcing" class="sidebar-link">Budget Forcing</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7052/#chatgpt-summary-of-s1-simple-test-time-scaling" class="sidebar-link">Chatgpt Summary of &quot;s1: Simple Test-Time Scaling&quot;</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7052/#c13-2024-inference-scaling-laws-an-empirical-analysis-of-compute-optimal-inference-for-llm-problem-solving" class="sidebar-link">[C13 2024] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving</a></li></ul></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7059/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li><li><a href="/qishao-notes/pages/dc7060/" class="sidebar-link">LLM MOE Inference</a></li><li><a href="/qishao-notes/pages/dc7061/" class="sidebar-link">LLM Compression</a></li><li><a href="/qishao-notes/pages/dc7062/" class="sidebar-link">LLM Optimizer Optimization</a></li><li><a href="/qishao-notes/pages/dc7063/" class="sidebar-link">LLM Posttraining</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-02-03</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">LLM Scaling Law<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[A 2025] s1: Simple test-time scaling 👍</li> <li>[C13 2024] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving</li></ol> <hr> <h2 id="a-2025-s1-simple-test-time-scaling"><a href="#a-2025-s1-simple-test-time-scaling" class="header-anchor">#</a> [A 2025] s1: Simple test-time scaling 👍</h2> <ul><li><p>First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality.</p></li> <li><p>Second, we develop budget forcing to control test-time compute by forcefully terminating the model’s thinking process or lengthening it by appending “Wait” multiple times to the model’s generation when it tries to end.</p></li></ul> <p>This can lead the model to double-check its answer, often fixing incorrect reasoning steps.</p> <p>After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questionsby up to 27% (MATH and AIME24).</p> <p><img src="https://github.com/user-attachments/assets/b24f8dba-b210-4b4a-abd4-93fa1f8ac594" alt="image"></p> <p>Finetune train 26 mins on 1,000 carefully curated questions.</p> <p>(I) If the model generates more thinking tokens than a desired limit, we forcefully end the thinking process by appending an end-of-thinking token delimiter.
Ending the thinking this way makes the model transition to generating its answer.<br>
(II) If we want the model to spend more test-time compute on a problem, we suppress the generation of the end-of-thinking token delimiter and instead append “Wait” to the model’s current reasoning trace to encourage more exploration.</p> <p><img src="https://github.com/user-attachments/assets/ad80b3f2-81d0-4909-82d1-881af722529e" alt="image"></p> <h3 id="budget-forcing"><a href="#budget-forcing" class="header-anchor">#</a> Budget Forcing</h3> <p><strong>Maximum token</strong></p> <p>we enforce a maximum token count by simply appending the end-of-thinking token delimiter and “Final Answer: to early exit the thinking stage and make the model provide its current best answer.</p> <p><strong>Minimum token</strong></p> <p>we suppress the generation of the end-of-thinking token delimiter and optionally append the string “Wait” to the model’s current reasoning trace to encourage the model to reflect on its current generation.</p> <h4 id="benchmark"><a href="#benchmark" class="header-anchor">#</a> benchmark</h4> <p>(I) Conditional length-control methods, which rely on telling the model in the prompt how long it should generate for.<br>
We group them by granularity into\</p> <ul><li>Token-conditional control: We specify an upper bound of thinking tokens in the prompt;\</li> <li>Step-conditional control: We specify an upper bound of thinking steps, where each step is around 100 tokens;\</li> <li>Class-conditional control: We write two generic prompts that tell the model to either think for a short or long amount of time.</li></ul> <p>(II) Rejection sampling, which samples until a generation fits a predetermined compute budget.</p> <p><img src="https://github.com/user-attachments/assets/de618a69-a8c5-4ef0-98b2-34a8968a7340" alt="image"></p> <p>Sequential scaling via Forcing with S1 is better than Parallel scaling.</p> <ul><li>Token-conditional control fails without budget forcing, as our model cannot reliably count tokens - even when trained to do so.</li> <li>Under step-conditional control, the model generates a similar total number of tokens when given different step targets, as the model goes from few steps with many tokens per step, to many steps with few tokens in each step.
Thus, the model learns to hack its way around the compute constraint making the controllability of this method mediocre.</li> <li><strong>Class-conditional control can work</strong> - telling a model to simply think longer can increase its test-time compute and performance, which leads good scaling.</li></ul> <p>Class-conditional control can work. it controls number of tokens generated, but performance is not good.</p> <p><img src="https://github.com/user-attachments/assets/33459c85-985c-43f1-b8c9-03bf59bb4f4c" alt="image"></p> <p>Why does supervised finetuning on just 1,000 samples lead to such performance gains?<br>
We hypothesize that the model is already exposed to large amounts of reasoning data during pretraining which spans trillions of tokens.<br>
Thus, the ability to perform reasoning <em>is already present in our model</em>.<br>
Our <strong>sample-efficient finetuning stage just activates it</strong> and we scale it further at test time with budget forcing. This is similar to the &quot;Superficial Alignment Hypothesis&quot;.</p> <p><img src="https://github.com/user-attachments/assets/8472c39b-3371-4e85-8d15-532552c50df2" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/a2bdf4a4-8374-43e6-8ce8-3165486bd3c4" alt="image"></p> <h3 id="chatgpt-summary-of-s1-simple-test-time-scaling"><a href="#chatgpt-summary-of-s1-simple-test-time-scaling" class="header-anchor">#</a> Chatgpt Summary of &quot;s1: Simple Test-Time Scaling&quot;</h3> <p><strong>Overview</strong></p> <p>This paper introduces <strong>test-time scaling</strong> , a method that allows language models to <strong>improve their reasoning abilities</strong>  by using additional computation during inference.<br>
Unlike traditional approaches that rely on large-scale <strong>pretraining</strong>  and <strong>fine-tuning</strong> , the authors propose a <strong>minimalist approach</strong>  using a small dataset (1,000 examples) and a technique called <strong>budget forcing</strong>  to control test-time compute.<br>
The result is <strong>s1-32B</strong> , a model that <strong>outperforms OpenAI’s o1-preview on math reasoning tasks</strong>  with significantly fewer training resources.</p> <p><strong>Key Contributions</strong></p> <ol><li><strong>Test-Time Scaling Without Large Training Data</strong></li></ol> <ul><li>Previous work, including OpenAI’s <strong>o1 model</strong> , demonstrated that increasing <strong>test-time compute</strong>  leads to better reasoning performance.</li> <li>However, OpenAI did not disclose its methodology, leading to numerous replication attempts.</li> <li>This paper finds the <strong>simplest approach</strong>  to achieve similar results:
<ul><li><strong>Supervised fine-tuning (SFT)</strong>  on a <strong>small, high-quality dataset (s1K, 1,000 examples)</strong> .</li> <li>A new test-time technique called <strong>budget forcing</strong>  to regulate computational effort during inference.</li></ul></li></ul> <ol start="2"><li><strong>Curating a High-Quality Small Dataset (s1K)</strong></li></ol> <ul><li>The authors select <strong>1,000 reasoning questions</strong>  from an initial <strong>59K dataset</strong>  using three key principles:
<ul><li><strong>Quality:</strong>  Ensuring high-quality reasoning traces.</li> <li><strong>Difficulty:</strong>  Including challenging problems that require deep reasoning.</li> <li><strong>Diversity:</strong>  Covering different subject areas (e.g., math, physics, logic).</li></ul></li> <li>The dataset is distilled from <strong>Google’s Gemini Flash Thinking API</strong>  to generate reasoning traces.</li></ul> <ol start="3"><li><strong>Budget Forcing: A Simple Test-Time Scaling Strategy</strong></li></ol> <ul><li><strong>Budget forcing</strong>  controls how much computation the model spends per question by:
<ul><li><strong>Forcing early termination</strong>  when the model has computed enough.</li> <li><strong>Encouraging extended reasoning</strong>  by appending &quot;Wait&quot; when the model tries to stop prematurely.</li></ul></li> <li>This method allows the model to <strong>self-correct</strong>  and <strong>double-check its reasoning</strong> .</li> <li>Results show that increasing test-time compute through budget forcing <strong>improves performance up to 7% on math tasks</strong> .</li></ul> <ol start="4"><li><strong>Performance &amp; Efficiency</strong></li></ol> <ul><li><strong>s1-32B</strong>  is trained using only <strong>1,000 examples in 26 minutes</strong>  on <strong>16 NVIDIA H100 GPUs</strong> .</li> <li>Despite this minimal training, it achieves:
<ul><li><strong>+27% improvement</strong>  over OpenAI’s o1-preview on <strong>math benchmarks (MATH, AIME24)</strong> .</li> <li><strong>Near-parity with Gemini 2.0 Thinking Experimental</strong> , which has access to massive proprietary data.</li></ul></li> <li>The model is <strong>fully open-source</strong> , making it an accessible alternative to closed models.</li></ul> <p><strong>Results &amp; Ablations</strong></p> <ul><li><p><strong>Test-Time Compute Scaling Works</strong></p> <ul><li>More compute at inference improves accuracy (Figure 1 in the paper).</li> <li>Budget forcing <strong>extrapolates performance</strong>  (e.g., from <strong>50% to 57% on AIME24</strong> ).</li></ul></li> <li><p><strong>Smaller, High-Quality Data Beats Large Random Data</strong></p> <ul><li>Training on all <strong>59K</strong>  examples does not outperform using just <strong>1K carefully selected</strong>  examples.</li> <li>Random sampling or selecting long reasoning traces <strong>performs worse</strong> .</li></ul></li> <li><p><strong>Comparison With Other Scaling Methods</strong></p> <ul><li><strong>Majority Voting (Parallel Scaling):</strong>  Generates multiple responses and selects the best.</li> <li><strong>Rejection Sampling:</strong>  Generates responses until one meets a length constraint.</li> <li><strong>Sequential Scaling (Budget Forcing) is the best approach</strong>  for test-time scaling.</li></ul></li></ul> <p><strong>Conclusion &amp; Future Directions</strong></p> <ul><li><p><strong>s1-32B proves that strong reasoning abilities can be achieved with minimal data and simple test-time scaling techniques.</strong></p></li> <li><p>The findings challenge the idea that massive pretraining is necessary for reasoning—<strong>small but carefully chosen data works</strong> .</p></li> <li><p>Future research could explore:</p> <ul><li><strong>Scaling beyond current test-time compute limits.</strong></li> <li><strong>Combining reinforcement learning with budget forcing.</strong></li> <li><strong>Parallel reasoning methods like Monte Carlo Tree Search (MCTS) for further improvements.</strong></li></ul></li></ul> <p><strong>Final Thoughts</strong><br>
This paper presents a <strong>lightweight, effective approach</strong>  to reasoning with LLMs, demonstrating that <strong>sparsity in training data + smart inference techniques</strong>  can rival expensive large-scale models.
It contributes to the ongoing discussion on <strong>efficient AI scaling</strong>  and provides a strong open-source alternative to closed models like OpenAI’s <strong>o1</strong> .</p> <hr> <h2 id="c13-2024-inference-scaling-laws-an-empirical-analysis-of-compute-optimal-inference-for-llm-problem-solving"><a href="#c13-2024-inference-scaling-laws-an-empirical-analysis-of-compute-optimal-inference-for-llm-problem-solving" class="header-anchor">#</a> [C13 2024] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving</h2> <p><strong>Abstract</strong></p> <p>The paper investigates <strong>compute-optimal inference</strong>  strategies for <strong>large language models (LLMs)</strong> , focusing on the trade-offs between <strong>model size, inference computation (FLOPs), and accuracy</strong>.<br>
The study evaluates inference strategies like <strong>greedy search, majority voting, best-of-n, weighted voting, and tree search algorithms</strong>.\</p> <blockquote><p>Findings suggest that <strong>smaller models (e.g., Llemma-7B) can outperform larger models (Llemma-34B) under the same compute budget</strong>  when paired with advanced inference algorithms.</p></blockquote> <p>The authors introduce a novel tree search method, <strong>REBASE (REward BAlanced SEarch)</strong> , which achieves <strong>better cost-performance trade-offs</strong>  than <strong>sampling-based methods or Monte Carlo Tree Search (MCTS)</strong>.</p> <p><strong>Key Contributions &amp; Findings</strong></p> <p><strong>1. Understanding Inference Scaling Laws</strong></p> <ul><li>Traditional <strong>scaling laws</strong>  analyze LLM performance based on <strong>training compute</strong>  (Kaplan et al., 2020), but this paper focuses on <strong>inference compute scaling</strong> .</li> <li>The <strong>optimal model size depends on the available compute budget</strong> . Smaller models initially outperform larger models, but as compute increases, larger models become favorable.</li> <li>Real-world deployment is often <strong>compute-limited</strong> , making smaller models with <strong>effective inference techniques</strong>  more practical.</li></ul> <p><strong>2. Comparison of Inference Strategies</strong>
The paper evaluates different inference strategies:</p> <ul><li><p><strong>Sampling-based methods</strong> :</p> <ul><li><strong>Majority Voting</strong> : Selects the most frequent answer from multiple samples.</li> <li><strong>Best-of-N</strong> : Picks the highest-scoring response using a reward model.</li> <li><strong>Weighted Majority Voting</strong> : Assigns weights to answers based on reward scores.</li></ul></li> <li><p><strong>Tree search methods</strong> :</p> <ul><li><strong>Monte Carlo Tree Search (MCTS)</strong> : Often inefficient due to unfinished paths and wasted compute.</li> <li><strong>REBASE (Proposed Method)</strong> : Uses a <strong>reward model</strong>  to guide node expansion, ensuring better trade-offs between <strong>accuracy and compute</strong> .</li></ul></li></ul> <blockquote><p>Typically, increasing the compute budget leads to higher accuracy until the accuracy reaches saturation.<br>
As the compute budget increases, smaller models initially perform better than larger ones, but once the accuracy of the smaller models saturates, the larger models have favorable performance.</p></blockquote> <p><strong>3. REBASE: A New Compute-Optimal Tree Search Method</strong></p> <ul><li><strong>Key Idea</strong> : Uses a <strong>node-quality reward function</strong>  to <strong>prune bad paths early</strong> , reducing computational waste.</li> <li><strong>Advantages</strong> :
<ul><li>Achieves <strong>higher accuracy with fewer FLOPs</strong>  compared to sampling and MCTS.</li> <li><strong>Outperforms MCTS</strong> , which wastes compute on incomplete solutions.</li> <li><strong>Scales better with compute</strong> , avoiding early saturation seen in sampling.</li></ul></li></ul> <p><strong>4. Empirical Results on Math Benchmarks</strong></p> <ul><li>Evaluated on <strong>GSM8K and MATH500 datasets</strong> .</li> <li><strong>Llemma-7B (small model) with REBASE</strong>  achieves similar or better accuracy than <strong>Llemma-34B (large model) with standard voting</strong> , while using <strong>2× fewer FLOPs</strong> .</li> <li><strong>REBASE consistently outperforms MCTS</strong>  across all settings.</li></ul> <p><strong>Llemma-7B model achieves competitive accuracy to Llemma-34B model with lower compute budget.</strong></p> <p><img src="https://github.com/user-attachments/assets/01b05cf4-4767-4602-b227-8dd5af8971c9" alt="image"></p> <p><strong>Conclusions</strong></p> <ul><li><strong>Compute-optimal inference</strong>  is key for practical LLM deployment.</li> <li><strong>Smaller models with sophisticated inference strategies (e.g., REBASE) can match or surpass larger models in compute-limited scenarios</strong> .</li> <li>REBASE offers a <strong>better cost-performance trade-off</strong>  than <strong>sampling-based methods and MCTS</strong> .</li> <li>Future work should explore <strong>generalizing these findings beyond mathematical reasoning</strong> .</li></ul> <p><strong>Limitations &amp; Future Directions</strong></p> <ul><li>The study is limited to <strong>mathematical problem-solving</strong> ; applying REBASE to <strong>other domains (e.g., coding, commonsense reasoning)</strong>  is a future goal.</li> <li><strong>Exploring more efficient reward models</strong>  for better tree search guidance is another area for improvement.</li></ul> <p><strong>Final Thoughts</strong></p> <p>This paper provides <strong>practical insights into LLM inference efficiency</strong> , challenging the assumption that <strong>larger models always perform better</strong> . By optimizing inference strategies, <strong>smaller models can be as effective as large ones</strong> , making LLM deployment more cost-effective.</p> <hr></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/17.llm_scale.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/04/04, 03:35:48</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7051/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">LLM Sparsity</div></a> <a href="/qishao-notes/pages/dc7055/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">LLM Attention</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7051/" class="prev">LLM Sparsity</a></span> <span class="next"><a href="/qishao-notes/pages/dc7055/">LLM Attention</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.2b30d2b7.js" defer></script><script src="/qishao-notes/assets/js/2.6d8a25ce.js" defer></script><script src="/qishao-notes/assets/js/101.d2879d9e.js" defer></script>
  </body>
</html>
