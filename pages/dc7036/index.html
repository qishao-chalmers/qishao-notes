<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>LLM Hardware Optimization | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.b33200b5.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.0833fe67.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/93.8fbec7e3.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.2e68d5ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.d92bd0c1.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.0c5c6ff7.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.eb596a88.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.96b000ef.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.fc6aeecb.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.3aec5d59.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.ce782cb6.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.c3ee9fa7.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.4ff73dfb.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.c1cee188.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.4fcd5343.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.3f92ec5c.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.a76476bd.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.f07b4077.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.721a3dd1.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.26fe7c1a.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.78afa114.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.536d72ab.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.72a80510.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.a8868c6d.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.2e58ec8c.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.cc100129.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.059e6a3d.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.c6216c45.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.f2125c90.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.063aa489.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.95ef1bfe.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.ecbceccd.js"><link rel="prefetch" href="/qishao-notes/assets/js/126.76a64eae.js"><link rel="prefetch" href="/qishao-notes/assets/js/127.270d3247.js"><link rel="prefetch" href="/qishao-notes/assets/js/128.771e22cf.js"><link rel="prefetch" href="/qishao-notes/assets/js/129.d74a104e.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.7de2e04b.js"><link rel="prefetch" href="/qishao-notes/assets/js/130.24dcbe45.js"><link rel="prefetch" href="/qishao-notes/assets/js/131.9c501be5.js"><link rel="prefetch" href="/qishao-notes/assets/js/132.c1aa2bf4.js"><link rel="prefetch" href="/qishao-notes/assets/js/133.a5456ae6.js"><link rel="prefetch" href="/qishao-notes/assets/js/134.d5220ad7.js"><link rel="prefetch" href="/qishao-notes/assets/js/135.019f9567.js"><link rel="prefetch" href="/qishao-notes/assets/js/136.4febab4e.js"><link rel="prefetch" href="/qishao-notes/assets/js/137.2bba78f8.js"><link rel="prefetch" href="/qishao-notes/assets/js/138.e28a4740.js"><link rel="prefetch" href="/qishao-notes/assets/js/139.0ab396f7.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.6b2cda69.js"><link rel="prefetch" href="/qishao-notes/assets/js/140.a6a9b30f.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.e89652e9.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.383840b9.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.63fdd997.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.bc4a5b1d.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.a2c2ac4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a4b6fb1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.8a5b62d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.4d84fdc5.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.acc5896d.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.f4835553.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.420a689a.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.ab4d38ce.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.ae348b88.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.781915d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.9921982f.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.8e39e8fa.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.93526d40.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.aa6fd9d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.96306ad4.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.7f37f31a.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.54012013.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.01d6cf82.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.c355ce28.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.6cb48c78.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.4cb8fc1b.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.60ab6920.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.2dcb8de5.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.768b44d0.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.a14373c1.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.45621e3e.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.ba787586.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.794ee8c0.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.acbac3e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.5ad3e7da.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.b3899d95.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.74d6845e.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.29e3d935.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.9d58ea7e.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.4a3eef72.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.a48f2051.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.81d0948c.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.90e62cd9.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.b73d61f1.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.5a06a2de.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.5055422e.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.39e17f91.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.d8968a5e.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.86284e6f.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.dd572b19.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.b0034962.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.f1418ea1.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.f1be6ae5.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.2e244e55.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.eeb3f036.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.e90a0d3a.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.84550fae.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.4bf88b63.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.5fa6476c.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.a14cc445.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.de46b925.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.bcd1d302.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.abfe714d.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.d2933de4.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.73c3f331.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.e1314a60.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.5e45ad5d.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.61dc6a76.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.74894327.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.b6869be2.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.391699b2.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.bec47591.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.d0c97938.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.f8bb4730.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.0b3d8ce6.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.8f1b72e0.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.445145eb.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.e0e0b007.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.afb99f3f.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.a6358d8a.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.06e3999d.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.e1509629.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.b907943c.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.70d99cd1.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.609a566c.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.1d3e26b2.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.6dfd422f.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.c69c76c8.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.7d7adee4.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.58afe762.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.752f10e9.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.79c8074d.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" aria-current="page" class="active sidebar-link">LLM Hardware Optimization</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Paper List</a></li><li><a href="/qishao-notes/pages/dc7043/" class="sidebar-link">Efficient LLM</a></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" class="sidebar-link">Summery of Inner Workings of LLM</a></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" class="sidebar-link">Memory Optimizations in LLM</a></li><li><a href="/qishao-notes/pages/dc7049/" class="sidebar-link">Reasoning in LLM</a></li><li><a href="/qishao-notes/pages/dc7050/" class="sidebar-link">LLM Mixed Precision &amp; Quantization &amp; Outlier</a></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" class="sidebar-link">LLM Scaling Law</a></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7059/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li><li><a href="/qishao-notes/pages/dc7060/" class="sidebar-link">LLM MOE Inference</a></li><li><a href="/qishao-notes/pages/dc7061/" class="sidebar-link">LLM Compression</a></li><li><a href="/qishao-notes/pages/dc7062/" class="sidebar-link">LLM Optimizer Optimization</a></li><li><a href="/qishao-notes/pages/dc7063/" class="sidebar-link">LLM Posttraining</a></li><li><a href="/qishao-notes/pages/dc7064/" class="sidebar-link">LLM MICRO - ISCA - HPCA</a></li><li><a href="/qishao-notes/pages/dc7066/" class="sidebar-link">LLM Prefilling &amp; Decoding Split</a></li><li><a href="/qishao-notes/pages/dc7067/" class="sidebar-link">Thinking of LLM Prefilling &amp; Decoding Split</a></li><li><a href="/qishao-notes/pages/dc7068/" class="sidebar-link">From Attention Sink to Massive Activation</a></li><li><a href="/qishao-notes/pages/dc7069/" class="sidebar-link">LLM CPU &amp; GPU Workloads</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2024-01-02</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">LLM Hardware Optimization<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]</li> <li>TurboTransformers: An Efficient GPU Serving System For Transformer Models [82]</li> <li>Improving the Efficiency of Transformers for Resource-Constrained Devices [8]</li> <li>Bag of Tricks for Optimizing Transformer Efficiency [5]</li> <li>Making Transformer inference faster on GPUs[Blog]</li> <li>Energy-efficient Inference Service of Transformer-based Deep Learning Models on GPUs [4]</li> <li>Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs [TACO 2023 Ref 2]</li> <li>hugging face https://huggingface.co/docs/transformers/performance</li> <li>[C17 Y2024 ASPLOS] AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference</li></ol> <hr> <h3 id="_1-hat-hardware-aware-transformers-for-efficient-natural-language-processing-mit-247"><a href="#_1-hat-hardware-aware-transformers-for-efficient-natural-language-processing-mit-247" class="header-anchor">#</a> 1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]</h3> <p>👍 👍 👍 👍</p> <h3 id="_4-making-transformer-inference-faster-on-gpus-blog"><a href="#_4-making-transformer-inference-faster-on-gpus-blog" class="header-anchor">#</a> 4. Making Transformer inference faster on GPUs[Blog]</h3> <p>https://dev-discuss.pytorch.org/t/making-transformer-inference-faster-on-gpus/190</p> <h3 id="_9-c17-y2024-asplos-attacc-unleashing-the-power-of-pim-for-batched-transformer-based-generative-model-inference"><a href="#_9-c17-y2024-asplos-attacc-unleashing-the-power-of-pim-for-batched-transformer-based-generative-model-inference" class="header-anchor">#</a> 9. [C17 Y2024 ASPLOS] AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference</h3> <p>I dont understand how PIM works, but the discussion about compute-efficiency and batching in LLM inference is solid.</p> <p>Batching to process multiple input sequences together can be a potential solution to inference throughput, but it is effective only for the FC layer, not the attention layer.</p> <p>Specifically, batching can improve the utilization of the system’s compute and memory resources for the FC layer by reusing the weight matrices and increasing arithmetic intensity, quantified by the Operation per Byte (Op/B).</p> <p>However, the Key and Value (KV) matrices of the attention layer are unique per (inference) request.
That is, batching can neither reuse the KV matrices nor improve the throughput of processing the attention layer.</p> <p>Moreover, the attention layer even limits the maximum batch size and impacts the FC layer throughput due to two
critical constraints: memory capacity and service level objective (SLO).</p> <p>(1) The memory capacity required to store KV matrices can be prohibitively high.
(2) Even if the memory capacity constraint is resolved, the SLO requirement becomes another limiting factor. As batching does not improve the throughput of the attention layer (§3.2), a larger batch leads to a longer processing time and thus violation of a given SLO constraint.</p> <p><img src="https://github.com/user-attachments/assets/81d7ee21-3fdd-42f6-bc5d-4697c94051c2" alt="image"></p> <p>The Gen (Prefilling) stages typically overwhelm the Sum (Decoding) stage in execution time due to their sequential nature of reading the entire pre-trained weights per generated (output) token.</p> <p><img src="https://github.com/user-attachments/assets/9aff3457-1a1b-4988-baa0-8933bee358dd" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/bbc697c4-efec-4195-b47b-941f5c7f2557" alt="image"></p> <p>As batching does not improve the throughput of the attention layer, the execution time for processing a
batch increases with the batch size.</p> <p>That is, when the SLO is fixed, the maximum batch size is limited due to the attention layer.</p> <p>Further, the attention layer has a low arithmetic intensity regardless of batch size (see Figure 3).</p> <p>The primary operation of the attention layer in the Gen stage is the GEMV of the score and context operations, exhibiting a low Op/B (∼1).</p> <p>Unlike the FC layer, the attention layer still has memoryintensive GEMV operations even after batching because each request uses unique KV matrices,</p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/02.LLM_HW_Opt.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/05/30, 17:24:46</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7035/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">how LLM works</div></a> <a href="/qishao-notes/pages/dc7037/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">How to run llama.cpp with gem5</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7035/" class="prev">how LLM works</a></span> <span class="next"><a href="/qishao-notes/pages/dc7037/">How to run llama.cpp with gem5</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.b33200b5.js" defer></script><script src="/qishao-notes/assets/js/2.0833fe67.js" defer></script><script src="/qishao-notes/assets/js/93.8fbec7e3.js" defer></script>
  </body>
</html>
