<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>GPU TLB | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.8b48fe63.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.75973713.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/49.2401593e.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.ff60f5f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.35356818.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.97db4364.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.117c9063.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.d32097b1.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.74839805.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.273c8e9e.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.be625961.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.c1ff1604.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.aa8ac1ae.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.535c58a2.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.deb54285.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.786d6446.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.cb104322.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.bea9a94a.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.08b2657a.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.eaff0988.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.e8d36ebd.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.07f576d2.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.55f55016.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.4719db0d.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.a830cfc8.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.0c998a26.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.8bbccd2b.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.19f8db5c.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.ffcf141e.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.675b1e40.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.0db28e45.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.bf2e5910.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.2817b3b0.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.3132cade.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.424759af.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.a6a3704a.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.533a2027.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.6032efe5.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.bae3965b.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.5d199f6c.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.9425ffca.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.c5f83261.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.fef1a5fe.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.e13524b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.1d4e5a23.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.6c60f957.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.65d968ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.e9875eea.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.bda1b0f5.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.aac58ba7.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.f6f39cd7.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.194bc337.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.8727748d.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.74ee284b.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.1488e0f5.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.938d7909.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.7eb37d38.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.4db5b0ff.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.01a108d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.69aaec14.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.c7ed7a72.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.a9f03922.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.fd797f91.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.98299dad.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.346beed8.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.e1ff15b1.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.bd09a0eb.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.3b7f275c.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.b2675e1d.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.96619bee.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.b4120698.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.316deedd.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.3863430d.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="ÁõÆÂΩï" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">llm</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="/qishao-notes/message-board/" class="nav-link">BBS</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">llm</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="/qishao-notes/message-board/" class="nav-link">BBS</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/cc7034/" class="sidebar-link">Operand Collector</a></li><li><a href="/qishao-notes/pages/2476ae/" class="sidebar-link">GPU WARP Scheduler</a></li><li><a href="/qishao-notes/pages/14769f/" class="sidebar-link">Precision Exception</a></li><li><a href="/qishao-notes/pages/44771e/" class="sidebar-link">Unified Memory Paper List</a></li><li><a href="/qishao-notes/pages/44871e/" class="sidebar-link">TensorCore Paper List</a></li><li><a href="/qishao-notes/pages/45871e/" class="sidebar-link">Memory Behaviour Paper List</a></li><li><a href="/qishao-notes/pages/45871f/" class="sidebar-link">GPU Virtualization Paper List</a></li><li><a href="/qishao-notes/pages/458720/" class="sidebar-link">Large Language Model Paper List</a></li><li><a href="/qishao-notes/pages/458721/" class="sidebar-link">GPU Simulator</a></li><li><a href="/qishao-notes/pages/458722/" class="sidebar-link">Architectural Survey</a></li><li><a href="/qishao-notes/pages/458724/" class="sidebar-link">Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper</a></li><li><a href="/qishao-notes/pages/458725/" class="sidebar-link">Understanding GPGPU-SIM &amp; GPGPU-SIM UVM_SMART (1)</a></li><li><a href="/qishao-notes/pages/458726/" class="sidebar-link">Understanding GPGPU-SIM &amp; GPGPU-SIM UVM_SMART (2)</a></li><li><a href="/qishao-notes/pages/458727/" class="sidebar-link">Understanding GPGPU-SIM &amp; GPGPU-SIM UVM_SMART (3)</a></li><li><a href="/qishao-notes/pages/45872/" class="sidebar-link">Understanding GPGPU-SIM &amp; GPGPU-SIM UVM_SMART (4)</a></li><li><a href="/qishao-notes/pages/45874/" class="sidebar-link">Understanding GPGPU-SIM &amp; GPGPU-SIM UVM_SMART (5)</a></li><li><a href="/qishao-notes/pages/45873/" class="sidebar-link">Warp Related Memory Optimization</a></li><li><a href="/qishao-notes/pages/45875/" class="sidebar-link">GPU Cache Coherency</a></li><li><a href="/qishao-notes/pages/45876/" class="sidebar-link">GPU Cache &amp; Memory Hirerarchy</a></li><li><a href="/qishao-notes/pages/45877/" aria-current="page" class="active sidebar-link">GPU TLB</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/45877/#please-notice-that-in-the-above-picture-for-each-subpage-it-has-a-physical-address-thus-it-is-not-physically-consecutive-physical-page-address-could-be-randomly-located" class="sidebar-link">Please Notice that in the above picture, for each subpage, it has a physical address. Thus it is not physically consecutive. Physical page address could be randomly located.</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45877/#_11-mask-redesigning-the-gpu-memory-hierarchy-to-support-multi-application-concurrency" class="sidebar-link">11. MASK: Redesigning the GPU Memory Hierarchy to Support Multi-Application Concurrency</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/45878/" class="sidebar-link">GPU Page Table Walk</a></li><li><a href="/qishao-notes/pages/45879/" class="sidebar-link">GPU Cache's Papers</a></li><li><a href="/qishao-notes/pages/45880/" class="sidebar-link">GPU WARP Mangement Papers</a></li><li><a href="/qishao-notes/pages/45882/" class="sidebar-link">GPU Unified Memory Innovations</a></li><li><a href="/qishao-notes/pages/47871e/" class="sidebar-link">TO READ</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="È¶ñÈ°µ" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/gpu/#gpu" data-v-06225672>gpu</a></li></ul> <div class="info" data-v-06225672><div title="‰ΩúËÄÖ" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="‰ΩúËÄÖ" class="beLink" data-v-06225672>hitqishao</a></div> <div title="ÂàõÂª∫Êó∂Èó¥" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2024-08-26</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">ÁõÆÂΩï</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">GPU TLB<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[90] Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring</li> <li>[6] SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs</li> <li>[117] Observations and Opportunities in Architecting Shared Virtual Memory for Heterogeneous Systems üëç üëç üëç üëç üë¥</li> <li>[2023] TunneLs for Bootlegging: Fully Reverse-Engineering GPU TLBs  for Challenging Isolation Guarantees of NVIDIA MIG üëç üëç üëç</li> <li>[31] Big data causing big (TLB) problems: taming random memory accesses on the GPU üëç üëç üëç</li> <li>[248] Dissecting GPU Memory Hierarchy through Microbenchmarking</li> <li>[2023 HPCA] Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding</li> <li>[2020 PACT] Enhancing Address Translations in Throughput Processors via Compression üåö</li> <li>[2024 MICRO] Improving Multi-Instance GPU Efficiency via Sub-Entry Sharing TLB Design</li> <li>[137 MICRO] Mosaic: A GPU Memory Manager with Application-Transparent Support for Multiple Page Sizes</li> <li>[109 ASPLOS] MASK: Redesigning the GPU Memory Hierarchy to Support Multi-Application Concurrency</li></ol> <hr> <p><img src="https://github.com/user-attachments/assets/e013760d-117e-474c-ac32-69361ead08f0" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/4659c5a5-6a99-4dda-acf2-5768fd5d28ff" alt="image"></p> <h3 id="_1-dissecting-the-nvidia-volta-gpu-architecture-via-microbenchmaring"><a href="#_1-dissecting-the-nvidia-volta-gpu-architecture-via-microbenchmaring" class="header-anchor">#</a> 1. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring</h3> <p>On Volta and on all other architectures we examined:</p> <ul><li>the L1 data cache is indexed by virtual addresses;</li> <li>the L2 data cache is indexed by physical addresses</li></ul> <hr> <h3 id="_2-snakebyte-a-tlb-design-with-adaptive-and-recursive-page-merging-in-gpus"><a href="#_2-snakebyte-a-tlb-design-with-adaptive-and-recursive-page-merging-in-gpus" class="header-anchor">#</a> 2. SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs</h3> <h4 id="idea"><a href="#idea" class="header-anchor">#</a> Idea</h4> <p>SnakeByte allows multiple equal-sized pages coalescing into a page table entry (PTE).<br>
It records the validity of pages to be merged using a bit vector, and few bits are annexed to indicate the size of merged pages.</p> <h4 id="tlb-ptw-gmmu"><a href="#tlb-ptw-gmmu" class="header-anchor">#</a> TLB &amp; PTW &amp; GMMU</h4> <p>Departing from conventional paging schemes of CPUs that heavily rely on operating systems, hardware-based GPU memory management units (GMMUs) are essential to effectively separate device memory management from host
CPUs.<br>
Otherwise, GPUs require the frequent intervention of OS to handle page table walks (PTWs) and TLB misses, which significantly penalize the GPU performance.</p> <p>Observations:</p> <ul><li>GPU workloads demand a large number of TLB entries (e.g., 32K to 256K entries) to handle sizable working sets, but conventional TLBs cannot provide sufficient coverage.</li> <li>GPU workloads have variable ranges of page contiguity.</li></ul> <p><img src="https://github.com/user-attachments/assets/ca8c2089-866b-4c16-a853-3a0f2fc792bc" alt="image"></p> <h5 id="paper-idea"><a href="#paper-idea" class="header-anchor">#</a> Paper Idea</h5> <p>If contiguity exists, valid bits are accordingly set in the bit vector. When all pages in the page group are allocated with contiguity (i.e., all valid bits set), the first PTE of the page group called base PTE is promoted to be further coalesced into a larger page group.</p> <h5 id="address-translation"><a href="#address-translation" class="header-anchor">#</a> Address Translation</h5> <p>An L1 TLB is private to a streaming multiprocessor (SM), and an L2 TLB is shared among SMs [41], [42].<br>
On a last-level TLB miss, a request is sent to a centralized GMMU [18], [41], [42] to walk through page tables, and the GMMU concurrently handles multiple PTW requests (e.g., 8-16 PTWs).<br>
To amortize the latency cost of PTWs, GPUs employ page walk caches that store recently used translations at different levels of page tables.<br>
Importantly, the GMMU execution has to be independent of host-side operations unlike the conventional paging schemes of CPUs that heavily rely on operating systems. <br>
Otherwise, GPUs involve frequent OS interventions, which significantly penalize the GPU performance [44], [54].</p> <p>This observation is the primary motivation of SnakeByte that can flexibly manage variable-sized page groups and maximize TLB reach.</p> <p><img src="https://github.com/user-attachments/assets/28e7240f-6b4a-4832-996b-70450bbef038" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/0661a5c0-910f-4f72-b1df-368ecf94e376" alt="image"></p> <p>When eight 4KB pages are allocated with contiguity, the page group is promoted to be coalesced into the next level of page group.</p> <p><strong>At the new page allocation, SnakeByte checks the contiguity of the new PTE with others in the page group.</strong></p> <h5 id="simulation"><a href="#simulation" class="header-anchor">#</a> Simulation</h5> <ul><li>By recursively coalescing PTEs, SnakeByte inevitably loses fine-grained controls on the A/D bits for individual pages.<br>
SnakeByte adds 8-bit access and dirty fields to a TLB entry to trace A/D states within a page group.</li> <li>GPUs have long shootdown delays (4.2us).</li> <li>The TLB hierarchy consists of a private L1 TLB per SM, a shared L2 TLB, and miss status holding registers (MSHRs).<br>
An MSHR in an L1 TLB merges up to 16 misses.</li> <li>16 page table walkers can concurrently access four-level page tables, and a page walk cache per page table level stores up to 16 recently used translations.</li> <li>When a new page is allocated, a sequential page prefetcher allocates 16 consecutive pages (total 64KB) at a time.</li> <li>To analyze the effect of page migration latency [9], [55], we add a 20us latency overhead for each 4KB page fault [55] with 8.48GB/s bandwidth for a 64KB prefetcher [18].</li></ul> <hr> <h3 id="_4-tunnels-for-bootlegging-fully-reverse-engineering-gpu-tlbs-for-challenging-isolation-guarantees-of-nvidia-mig"><a href="#_4-tunnels-for-bootlegging-fully-reverse-engineering-gpu-tlbs-for-challenging-isolation-guarantees-of-nvidia-mig" class="header-anchor">#</a> 4. TunneLs for Bootlegging: Fully Reverse-Engineering GPU TLBs  for Challenging Isolation Guarantees of NVIDIA MIG</h3> <p>However, we surprisingly find that MIG does not partitation the last-level TLB, which is shared by all the compute units in a GPU.</p> <h4 id="uvm-managed-pages"><a href="#uvm-managed-pages" class="header-anchor">#</a> UVM-Managed Pages</h4> <p>A module in the NVIDIA driver is in charge of allocating UVM-managed pages.<br>
The cudaMallocManaged() function registers a virtual address subspace for UVM use.<br>
The UVM module allocates pages when the GPU accesses addresses in the registered address space.<br>
Although three page sizes are supported (see Figure 1), we find that UVM actually only <strong>allocates 64KB and 2MB pages</strong>.</p> <p>UVM starts with allocating 64KB pages, but it will merge the 64KB pages within a 2MB page into the 2MB page if the residency reaches certain conditions.<br>
For example, if the first 17 or more 64KB pages in a 2MB page are present on GPU, the page table entries for these 64KB pages will be purged and replaced with a 2MB entry;<br>
but if just the first 16 64KB pages are used, the merging operation will not be triggered.<br>
We find that some other residency patterns with less than 17 pages can also trigger the merging.<br>
For instance, if every other 64KB page is used (i.e., 16 ones as there are 32 64KB pages in a 2MB page), the merging will also happen.</p> <p><img src="https://github.com/user-attachments/assets/45e7eaca-8a31-447a-a6f8-7520e28f2109" alt="image"></p> <h4 id="tlb-sub-entries"><a href="#tlb-sub-entries" class="header-anchor">#</a> TLB Sub-Entries</h4> <p>In [26], Nayak et al. claim that NVIDIA GPUs enforce TLB coalescing, which combines 16 address translations to occupy just one TLB entry if the corresponding virtual page numbers are consecutive and the mapped physical page frames are also contiguous.<br>
However, <em>the results of our experiments do not agree with this claim</em>.</p> <p>We notice that address translations reside in one L2-uTLB or L3-uTLB entry as long as the virtual base addresses of the corresponding pages
are:</p> <ul><li>within the same 1MB-aligned address range if the pages are 64KB</li> <li>within the same 32MB-aligned range if the pages are 2MB.</li></ul> <p>This observation disproves the existence of dynamically coalescing TLB entries and explains why we separate the base addresses by 0x100000 (i.e., 1MB) and 0x2000000 (i.e., 32MB) when using sequences of 64KB and 2MB pages respectively to perform the above-mentioned tasks.</p> <p><strong>Instead of TLB coalescing, we conjecture that there are 16 sub-entries in one L2-uTLB or L3-uTLB entry</strong>,<br>
and they have a one-to-one mapping relationship with the address translations for 16 pages of size 64KB or 2MB located in the same 1MB- or 32MB-aligned range.<br>
If any sub-entry encounters an eviction, the rest of them are also invalidated.<br> <strong>Interestingly, we find that the entries of L1-iTLB and L1-dTLB do not have such sub-entries.</strong></p> <p><em>Inclusivity and Exclusivity.</em><br>
We find that the L2-uTLB is neither inclusive nor exclusive in all the inspected GPUs. The same is also true for the L3-uTLB.</p> <p><em>Reinsertion.</em>
We find that an L2-uTLB hit is reinserted into the L1 and an L3-uTLB hit is also reinserted into the L2 and L1.</p> <p><img src="https://github.com/user-attachments/assets/7a67a196-7a7e-4e10-9fb3-4e1a55768aa4" alt="image"></p> <h3 id="observations"><a href="#observations" class="header-anchor">#</a> Observations</h3> <p>We observe that the execution of the infinite loop on all the tested GPUs is not affected after modifying the address translation for the code page, which implies that (at least) the <em>L1 TLB of a GPU is split into an L1-iTLB and an L1-dTLB.</em></p> <p><img src="https://github.com/user-attachments/assets/d3f79741-1f3f-45f5-915c-95fcce02c021" alt="image"> <em>From [6]</em></p> <p>we can infer that the L1-iTLB of these GPUs has 16 entries and is fully-associative (otherwise, the smallest ùëÅ evicting the target address translation should differ from 16 occasionally).</p> <p>Exchanging the above roles played by code and data pages, we can learn that the L1-dTLB of all these GPUs also has 16 entries and is fully-associative.</p> <p>We find that the L1-iTLB and the L1-dTLB are private to each SM in Turing GPUs (e.g., RTX 2080), but they are shared between the two SMs of each TPC in Ampere GPUs (e.g., RTX 3080 and A100).</p> <p>This observation indicates that at least two levels of unified TLBs, which we call L2-uTLB and L3-uTLB.</p> <p>L2-uTLB is 8-way set-associative in all the tested GPUs.</p> <p>These observations lead to the conclusion that the L3-uTLB in MIG-supported GPUs is still 8-way set-associative and it is (physically or just logically) split into two slices; and each slice has an 8-entry victim buffer shared by all the TLB sets in the slice.</p> <p>L3-uTLB is shared by all the SMs.</p> <h3 id="_5-big-data-causing-big-tlb-problems-taming-random-memory-accesses-on-the-gpu"><a href="#_5-big-data-causing-big-tlb-problems-taming-random-memory-accesses-on-the-gpu" class="header-anchor">#</a> 5. Big data causing big (TLB) problems: taming random memory accesses on the GPU</h3> <p>If the data accesses are irregular, like hash table accesses or random sampling, the GPU performance can suffer.<br>
Especially when scaling such accesses beyond 2GB of data, a performance decrease of an order of magnitude is encountered.<br>
This is paper analyzes the source of the slowdown through extensive micro-benchmarking, attributing the root cause to the Translation Lookaside Buffer (TLB).</p> <h4 id="introduction"><a href="#introduction" class="header-anchor">#</a> Introduction</h4> <p>GPU data larger than 2GB, which, in some cases, may result in a ‚âà13.3x runtime decrease.<br>
we identified the Translation Lookaside Buffer (TLB) as the  source of this slowdown, where TLB misses cost hundreds of cycles per memory access.</p> <ul><li>NVIDIA Kepler [15]</li> <li>NVIDIA Pascal [16]</li></ul> <p>the P100 shows a significantly better performance than the K80, as it has a newer hardware architecture.<br>
However, the slowdown for memory accesses &gt;2GB is still significant with factors of 4.3x for random sampling and 3.3x for grouping.</p> <h4 id="benchmark"><a href="#benchmark" class="header-anchor">#</a> Benchmark</h4> <h5 id="virtual-memory"><a href="#virtual-memory" class="header-anchor">#</a> Virtual Memory</h5> <p>The reasons why GPU use virtual address<br>
(1) Isolation: The indirection controls a program‚Äôs memory accesses and, thus, keeps it from disallowed memory accesses to internal
device data or to data of other applications using the same GPU.<br>
(2) Fragmentation: Memory fragmentation can be hidden with virtual pages, allowing a large consecutive region of virtual memory to be scattered across many positions in physical memory.<br>
This can also increase memory bandwidth if physical memory is scattered to multiple memory chips, which then can be accessed in parallel.</p> <h5 id="benchmark-2"><a href="#benchmark-2" class="header-anchor">#</a> Benchmark</h5> <p><em>pointer chasing with stride distance</em></p> <p><img src="https://github.com/user-attachments/assets/ffc46301-1aa9-43b0-814d-f93332aba085" alt="image"></p> <p>Every stride size smaller than the page size behaves like (1/2)*X: showing lower cycle counts but experiences the first TLB miss at the same position.</p> <h4 id="observation"><a href="#observation" class="header-anchor">#</a> Observation</h4> <p><img src="https://github.com/user-attachments/assets/00839b1e-4e68-44a9-9202-94819117e199" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/3dd1f116-73ef-4947-8689-4b600cfb2f70" alt="image"></p> <h5 id="summary"><a href="#summary" class="header-anchor">#</a> Summary</h5> <p>(1) We found three levels of TLBs for the K80 and two levels for the P100.<br>
(2) For both GPUs, the different TLB levels apparently use different page sizes, where the L1 TLB uses a small page size and the L2/L3 TLB use a 16x larger page size.<br>
(3) Compared to K80, the P100 always has 16x larger pages.<br>
(4) For data larger than 2GB, the K80 has a total delay of 241 cycles, while the P100 only has a 119 cycle delay.</p> <h4 id="plausibility-and-validation"><a href="#plausibility-and-validation" class="header-anchor">#</a> Plausibility and Validation</h4> <ul><li>First, the sizes of the L1 TLB (16 entries) and L2 TLB (65 entries) for Kepler GPUs (K80).</li> <li>We can confirm this for the L1 TLB, while the K80 already uses 2MB pages for the L2 and L3 TLB (as shown by [10]).</li> <li>Third, every TPC has its own L1 TLB and every GPC has its own L2 TLB, while the L3 TLB is shared for all SMs.</li> <li>Fourth, we can see a significant performance drop in our investigated database operations when we access more data than ‚âà2GB.<br>
Even with different page sizes for both GPUs, we can pinpoint the problem to the L3 TLB on the K80 and the L2 TLB on the P100.<br>
We can even identify the L2 TLB boundary on the K80, where performance problems start at ‚âà130MB.</li> <li>Fifth, in [6], the performance of a grouping operator on Kepler GPUs was improved by reducing the number of threads to &lt;1000
instead of multiple thousands for data accesses beyond 2GB.<br>
With our results, we can explain that this is benefinicial because each thread can load one page translation in the L3 TLB (1032 entries).<br>
The page translations stay in the TLB.</li></ul> <h4 id="argument-for-unconventional-properties"><a href="#argument-for-unconventional-properties" class="header-anchor">#</a> Argument for Unconventional Properties</h4> <p>two unconventional results:<br>
(1) TLB entry numbers not being the power of two<br>
(2) different page sizes for different TLB levels.</p> <p>We evaluated the allocation size and found that the smaller page size is always used for allocations (128KB on K80, 2MB on P100).<br>
One possible explanation for the apparently larger page sizes in the L2/L3 TLB could be a <strong>static pre-fetching algorithm</strong>, which always loads 16 contiguous pages when a TLB miss occurs.<br>
This would result in one TLB miss and 15 TLB hits, when using the small page size as traversal stride.</p> <h3 id="_6-trans-fw-short-circuiting-page-table-walk-in-multi-gpu-systems-via-remote-forwarding"><a href="#_6-trans-fw-short-circuiting-page-table-walk-in-multi-gpu-systems-via-remote-forwarding" class="header-anchor">#</a> 6. Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding</h3> <p><img src="https://github.com/user-attachments/assets/d21d7b38-670b-4bb9-8a93-a03ad1a90f64" alt="image"></p> <h4 id="address-translation-flow"><a href="#address-translation-flow" class="header-anchor">#</a> Address Translation Flow</h4> <ol><li>The memory requests generated by the same wavefront are first coalesced by the GPU memory coalescing unit.</li> <li>the L1 data cache and the L1 TLB perform lookups in parallel in a virtually indexed physically tagged (VIPT) TLB-cache design.</li> <li>Upon L1 TLB misses, the L1 Miss Status Holding Register (MSHR) is first checked to filter out repetitive requests, and the outstanding requests are forwarded to the L2 TLB for lookup.</li> <li>Translations that miss in the L2 TLB and L2 MSHR are sent to the local PT-walk in the GMMU.</li></ol> <p>Because there is limited number of PT-walk threads, L2 TLB misses may not be served immediately.\</p> <ol><li>these translation requests will be stored in the PW-queue and wait for available PT-walk threads.</li> <li>During the page table walking, the translation is first checked in the PW-cache;</li> <li>if it misses the PW-cache, the GPU local page table is accessed, which can be expensive and involves multiple memory accesses ( 5 ).</li> <li>If the page walk fails, a far fault is propagated to the GMMU and kept in a structure called GPU Fault Buffer [6], [7].</li> <li>Each time a far fault arises, the GMMU sends an alert to the UMV-driver.</li> <li>Upon the receipt of a <strong>far fault</strong>, the UVM driver fetches the fault information and caches them on the host side.</li> <li>The cached page faults are processed in batch granularity (the batch size is 256 [53]).</li> <li>Per batch, the UVM-driver initiates threads to perform page table walks using the centralized page table, initiates data transfer, and updates the GPU local page tables [7].</li> <li>The translation request is replayed after the far fault is resolved.</li></ol> <h4 id="hardware-handled-far-faults"><a href="#hardware-handled-far-faults" class="header-anchor">#</a> Hardware Handled Far Faults</h4> <p>This is the flow of hardware-acclerated Far faults handling</p> <ol><li>When a far fault is generated, it is sent to the host and then handled by host MMU.</li> <li>Specifically, upon receiving a translation request, the host MMU first performs a host MMU TLB lookup.</li> <li>If the translation misses in the TLB, the request waits in the host MMU PW-queue for PT-walk</li> <li>The PT-walk process in the host MMU is similar to the GPU local PT-walk, including host MMU PW-cache lookup</li> <li>host MMU PT-walk for PW-cache misses</li> <li>host MMU TLB update</li></ol> <p><strong>the address translation overhead of software is 4.5√ó higher than that of hardware in 32 GPUs.</strong></p> <p><em>three</em> major latencies in the baseline address translation:</p> <ol><li>waiting time for available PT-walk threads in the PW-queue</li> <li>additionally memory accesses after PW-cache misses</li> <li>handling far faults caused by page sharing among multiple GPUs</li></ol> <hr> <h3 id="_9-improving-multi-instance-gpu-efficiency-via-sub-entry-sharing-tlb-design"><a href="#_9-improving-multi-instance-gpu-efficiency-via-sub-entry-sharing-tlb-design" class="header-anchor">#</a> 9. Improving Multi-Instance GPU Efficiency via Sub-Entry Sharing TLB Design</h3> <p>Specifically, in the L2 and L3 TLBs, an entry comprises 16 sub-entries, each directly corresponding to the address translation of 16 sequential
64 KB pages within a contiguous 1 MB-aligned segment.</p> <p><em>By compressing multiple translations into a single TLB entry, the TLB can manage more data with fewer entries</em>, thereby reducing hardware overhead, while improving TLB efficiency and boosting overall performance.</p> <p>these GPUs organize their L2 and L3 TLB entries differently to increase TLB reach [60].<br>
Specifically, each of these entries contains 16 sub-entries, which directly map to the address translations for 16 pages.<br>
These pages can be either 64 KB or 2 MB in size, and all of them fall within an aligned range of either 1 MB or 32 MB in size, respectively.<br>
That means each sub-entry in a TLB entry has a one-to-one relationship with a single page.<br>
Note that, in the sub-entry setting, if any TLB entry is evicted, all the 16 sub-entries associated with that TLB entry are zeroed.</p> <p><img src="https://github.com/user-attachments/assets/e2326fd1-7dab-48e8-9f65-c5b4355adfc7" alt="image"></p> <h2 id="please-notice-that-in-the-above-picture-for-each-subpage-it-has-a-physical-address-thus-it-is-not-physically-consecutive-physical-page-address-could-be-randomly-located"><a href="#please-notice-that-in-the-above-picture-for-each-subpage-it-has-a-physical-address-thus-it-is-not-physically-consecutive-physical-page-address-could-be-randomly-located" class="header-anchor">#</a> <strong>Please Notice that in the above picture, for each subpage, it has a physical address. Thus it is not physically consecutive. Physical page address could be randomly located.</strong></h2> <h3 id="_11-mask-redesigning-the-gpu-memory-hierarchy-to-support-multi-application-concurrency"><a href="#_11-mask-redesigning-the-gpu-memory-hierarchy-to-support-multi-application-concurrency" class="header-anchor">#</a> 11. MASK: Redesigning the GPU Memory Hierarchy to Support Multi-Application Concurrency</h3> <h4 id="main-idea"><a href="#main-idea" class="header-anchor">#</a> Main Idea</h4> <p>Contention of shard TLB leads to frequent misses in the shared translation lookaside buffer (TLB), where a single miss can induce long-latency stalls for hundreds of threads.<br>
As a result, the GPU often cannot schedule enough threads to successfully hide the stalls, which diminishes system throughput and becomes a
first-order performance concern.</p> <h4 id="contributions"><a href="#contributions" class="header-anchor">#</a> Contributions</h4> <p>(1) a token-based technique to reduce TLB contention<br>
(2) a bypassing mechanism to improve the effectiveness of cached address translations<br>
(3) an application-aware memory scheduling scheme to reduce the interference between address translation and data requests.</p> <p><img src="https://github.com/user-attachments/assets/f5dc0080-a485-4548-b6fd-b730dd0070e1" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/ea172f22-625c-4eb4-8427-ea2119aeaccc" alt="image"></p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/03.gpu/20.gpu_tlb.md" target="_blank" rel="noopener noreferrer">Â∏ÆÂä©Êàë‰ª¨ÊîπÂñÑÊ≠§È°µÈù¢</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">‰∏äÊ¨°Êõ¥Êñ∞:</span> <span class="time">2024/09/08, 00:44:05</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/45876/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">GPU Cache &amp; Memory Hirerarchy</div></a> <a href="/qishao-notes/pages/45878/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">GPU Page Table Walk</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ‚Üê
        <a href="/qishao-notes/pages/45876/" class="prev">GPU Cache &amp; Memory Hirerarchy</a></span> <span class="next"><a href="/qishao-notes/pages/45878/">GPU Page Table Walk</a>‚Üí
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="ÂèëÈÇÆ‰ª∂" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="Êú¨Á´ô‰∏ªÈ¢ò">Vdoing</a> 
    | Copyright ¬© 2022-2024
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="ËøîÂõûÈ°∂ÈÉ®" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="ÂéªËØÑËÆ∫" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="‰∏ªÈ¢òÊ®°Âºè" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          Ë∑üÈöèÁ≥ªÁªü
        </li><li class="iconfont icon-rijianmoshi">
          ÊµÖËâ≤Ê®°Âºè
        </li><li class="iconfont icon-yejianmoshi">
          Ê∑±Ëâ≤Ê®°Âºè
        </li><li class="iconfont icon-yuedu">
          ÈòÖËØªÊ®°Âºè
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.8b48fe63.js" defer></script><script src="/qishao-notes/assets/js/2.75973713.js" defer></script><script src="/qishao-notes/assets/js/49.2401593e.js" defer></script>
  </body>
</html>
