<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>LLM Mixed Precision &amp; Quantization &amp; Outlier | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.f8e7e83c.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.0833fe67.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/99.3f361b43.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.2e68d5ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.efe4e18e.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.9e8143a0.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.6bc1dc1a.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.752dc056.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.fb36554a.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.c328df02.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.30fdcc2e.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.f0d436e4.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.9462122c.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.8add2597.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.8a8e0de2.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.7181140a.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.888f2fe3.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.c72d74fe.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.de852cdc.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.d6d41bfc.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.6a8ed08a.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.e0bf79a5.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.d217c2fa.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.80d6079b.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.3a815ecd.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.8535d463.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.d3f8dc72.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.058fb67a.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.b55e85af.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.50011498.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.496a473d.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.2742eaf0.js"><link rel="prefetch" href="/qishao-notes/assets/js/126.e94356a3.js"><link rel="prefetch" href="/qishao-notes/assets/js/127.a5bab308.js"><link rel="prefetch" href="/qishao-notes/assets/js/128.ff0f0892.js"><link rel="prefetch" href="/qishao-notes/assets/js/129.ec0231d7.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.7de2e04b.js"><link rel="prefetch" href="/qishao-notes/assets/js/130.8bd9606a.js"><link rel="prefetch" href="/qishao-notes/assets/js/131.c8b4b644.js"><link rel="prefetch" href="/qishao-notes/assets/js/132.369ce23a.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.c969a0b3.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.d843f1a9.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.05da202f.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.63fdd997.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.bc4a5b1d.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.a2c2ac4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a4b6fb1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.8a5b62d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.3768bbaf.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.ef51aa77.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.f4835553.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.54245a6c.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.ab4d38ce.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.ae348b88.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.781915d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.9921982f.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.b67a85e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.93526d40.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.aa6fd9d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.96306ad4.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.f2770d51.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.5e48ab77.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.01d6cf82.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.c355ce28.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.6cb48c78.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.9f9585d9.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.7f09c090.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.2dcb8de5.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.5f352ae8.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.4afd8139.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.45621e3e.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.9a39b8df.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.28d1accb.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.acbac3e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.5ad3e7da.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.b3899d95.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.74d6845e.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.14c8f20a.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.9d58ea7e.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.1f9658dd.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.a48f2051.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.a8217417.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.f3dbccbd.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.92826847.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.5a06a2de.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.c1f016c9.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.ea5b377a.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.a1fbdc11.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.86284e6f.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.bca7e213.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.c17451bf.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.f1418ea1.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.f1be6ae5.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.b9c3a1f9.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.163b25e0.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.af5651e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.7bd2530e.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.1a157136.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.b7cb5686.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.f45db467.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.d5bd83ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.c6451aec.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.358dd4b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.070bd87d.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.b4b3144f.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.0dddc9b7.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.df00923d.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.40588fe2.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.250e3b12.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.1ac71d6e.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.4779a8c7.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.bec47591.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.994852f8.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.f990a16a.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.e760ae06.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.8d6ffbf5.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.cc7ba318.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.7a21298c.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.a5186a8a.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.e725c0e8.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.8ec4ceb0.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.48a79619.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.a15ac68d.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.b8cf85cd.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.0d5c4fef.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.5a1e5fd5.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.7a8c0983.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.4fa07916.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.96018905.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.ad1b24f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.51ab3a40.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.e22a65c2.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Paper List</a></li><li><a href="/qishao-notes/pages/dc7043/" class="sidebar-link">Efficient LLM</a></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" class="sidebar-link">Summery of Inner Workings of LLM</a></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" class="sidebar-link">Memory Optimizations in LLM</a></li><li><a href="/qishao-notes/pages/dc7049/" class="sidebar-link">Reasoning in LLM</a></li><li><a href="/qishao-notes/pages/dc7050/" aria-current="page" class="active sidebar-link">LLM Mixed Precision &amp; Quantization &amp; Outlier</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization" class="sidebar-link">1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#problems" class="sidebar-link">Problems</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#solutions" class="sidebar-link">Solutions</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_2-43-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models" class="sidebar-link">2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-takeaways-in-three-sentences" class="sidebar-link">Key Takeaways in Three Sentences</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#abstract" class="sidebar-link">Abstract</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#introduction" class="sidebar-link">Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#background-and-related-works" class="sidebar-link">Background and Related Works</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#comparative-analysis-of-int-and-fp-formats" class="sidebar-link">Comparative Analysis of INT and FP Formats</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#exploiting-int-and-fp-complementarity" class="sidebar-link">Exploiting INT and FP Complementarity</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#conclusion" class="sidebar-link">Conclusion</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_5-26-fp8-lm-training-fp8-large-language-models" class="sidebar-link">5. [26] FP8-LM: Training FP8 Large Language Models</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#abstract-2" class="sidebar-link">Abstract</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction" class="sidebar-link">1. Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-fp8-llm-training" class="sidebar-link">2. FP8 LLM Training</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-experimentation" class="sidebar-link">3. Experimentation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-related-work" class="sidebar-link">4. Related Work</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_5-conclusion" class="sidebar-link">5. Conclusion</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-summary-in-3-sentences" class="sidebar-link">Key Summary in 3 Sentences*</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_6-139-fp8-formats-for-deep-learning" class="sidebar-link">6. [139] FP8 Formats for Deep Learning</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction-2" class="sidebar-link">1. Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-contributions" class="sidebar-link">Key contributions:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-aspects-of-fp8-usage-in-deep-learning" class="sidebar-link">2. Aspects of FP8 Usage in Deep Learning</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-fp8-binary-interchange-format" class="sidebar-link">3. FP8 Binary Interchange Format</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-empirical-results" class="sidebar-link">4. Empirical Results</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_5-conclusions" class="sidebar-link">5. Conclusions</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-takeaways-in-three-sentences-2" class="sidebar-link">Key Takeaways in Three Sentences</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_8-34-stable-and-low-precision-training-for-large-scale-vision-language-models" class="sidebar-link">8. [34] Stable and low-precision training for large-scale vision-language models</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction-and-motivation" class="sidebar-link">1. Introduction and Motivation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-eight-bit-8-bit-training" class="sidebar-link">2. Eight-Bit (8-bit) Training</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-stability-of-training" class="sidebar-link">3. Stability of Training</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-conclusion-and-future-directions" class="sidebar-link">4. Conclusion and Future Directions</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#three-sentence-core-summary" class="sidebar-link">Three-Sentence Core Summary</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_9-y2025-paretoq-scaling-laws-in-extremely-low-bit-llm-quantization" class="sidebar-link">9. [Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#abstract-3" class="sidebar-link">Abstract</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction-3" class="sidebar-link">1. Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-questions" class="sidebar-link">Key Questions:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#paretoq-approach" class="sidebar-link">ParetoQ Approach:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-a-better-qat-scheduling-strategy-for-extreme-low-bit-llms" class="sidebar-link">2. A Better QAT Scheduling Strategy for Extreme Low-Bit LLMs</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-a-hitchhiker-s-guide-to-quantization-method-choices" class="sidebar-link">3. A Hitchhiker’s Guide to Quantization Method Choices</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-pareto-optimality-of-extremely-low-bit-llms" class="sidebar-link">4. Pareto-Optimality of Extremely Low-Bit LLMs</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_6-related-work" class="sidebar-link">6. Related Work</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_10-47-microscaling-data-formats-for-deep-learning" class="sidebar-link">10. [47] Microscaling Data Formats for Deep Learning</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#introduction-2" class="sidebar-link">Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#microscaling-mx-data-formats" class="sidebar-link">Microscaling (MX) Data Formats</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#concrete-mx-formats" class="sidebar-link">Concrete MX Formats</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#scalar-float-to-mx-format-conversion" class="sidebar-link">Scalar Float to MX Format Conversion</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#compute-flow-and-training-pipeline" class="sidebar-link">Compute Flow and Training Pipeline</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#conclusion-2" class="sidebar-link">Conclusion</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-findings" class="sidebar-link">Key findings:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#three-sentence-summary" class="sidebar-link">Three-Sentence Summary</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_14-33-spinquant-llm-quantization-with-learned-rotations" class="sidebar-link">14. [33] SpinQuant: LLM quantization with learned rotations</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#abstract-introduction" class="sidebar-link">Abstract &amp; Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#motivation-outlier-reduction" class="sidebar-link">Motivation &amp; Outlier Reduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#methodology" class="sidebar-link">Methodology</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#experiments-results" class="sidebar-link">Experiments &amp; Results</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-findings-2" class="sidebar-link">Key findings:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#conclusions" class="sidebar-link">Conclusions</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#three-sentence-summary-2" class="sidebar-link">Three-Sentence Summary</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_17-35-fp8-versus-int8-for-efficient-deep-learning-inference" class="sidebar-link">17. [35] FP8 versus INT8 for efficient deep learning inference</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction-and-motivation-2" class="sidebar-link">1. Introduction and Motivation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-hardware-considerations" class="sidebar-link">2. Hardware Considerations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-accuracy-comparison" class="sidebar-link">3. Accuracy Comparison</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-transformer-networks" class="sidebar-link">4. Transformer Networks</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_5-comparison-to-other-work" class="sidebar-link">5. Comparison to Other Work</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_6-fp8-to-int8-conversion" class="sidebar-link">6. FP8 to INT8 Conversion</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_7-int-quantization-paradigm" class="sidebar-link">7. INT Quantization Paradigm</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_8-conclusion" class="sidebar-link">8. Conclusion</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-takeaways" class="sidebar-link">Key Takeaways:</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_16" class="sidebar-link">16.</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_17-434-integer-quantization-for-deep-learning-inference-principles-and-empirical-evaluation" class="sidebar-link">17. [434] Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-highlights" class="sidebar-link">Key Highlights:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#quantization-methods" class="sidebar-link">Quantization Methods</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#post-training-quantization-ptq-vs-quantization-aware-training-qat" class="sidebar-link">Post Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#calibration-methods" class="sidebar-link">Calibration Methods</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#optimizations-to-reduce-accuracy-loss" class="sidebar-link">Optimizations to Reduce Accuracy Loss</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#recommended-workflow" class="sidebar-link">Recommended Workflow</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#final-3-sentence-summary" class="sidebar-link">Final 3-Sentence Summary:</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_20-73-fp8-quantization-the-power-of-the-exponent" class="sidebar-link">20.[73] FP8 Quantization: The Power of the Exponent</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-takeaways-2" class="sidebar-link">Key Takeaways:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#three-sentence-summary-3" class="sidebar-link">Three-Sentence Summary</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_18-y2024-integer-scale-a-free-lunch-for-faster-fine-grained-quantization-of-llms" class="sidebar-link">18. [Y2024]Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#overview" class="sidebar-link">Overview</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-motivation" class="sidebar-link">Key Motivation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#proposed-method-integer-scale" class="sidebar-link">Proposed Method: Integer Scale</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#results" class="sidebar-link">Results</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#practical-takeaways" class="sidebar-link">Practical Takeaways</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-parts-of-the-paper-in-3-sentences" class="sidebar-link">Key Parts of the Paper in 3 Sentences</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_19-121-training-high-performance-and-large-scale-deep-neural-networks-with-full-8-bit-integers" class="sidebar-link">19. [121] Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction-4" class="sidebar-link">1. Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-related-work" class="sidebar-link">2. Related Work</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-wageubn-framework" class="sidebar-link">3. WAGEUBN Framework</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-results" class="sidebar-link">4. Results</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_5-analysis" class="sidebar-link">5. Analysis</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_6-conclusion-wageubn-is-the-first-complete-int8-quantization-framework-for-training-large-scale-dnns-it-achieves" class="sidebar-link">6. Conclusion WAGEUBN is the first complete INT8 quantization framework  for training large-scale DNNs. It achieves:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#three-sentence-summary-4" class="sidebar-link">Three-Sentence Summary</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_20-381-i-bert-integer-only-bert-quantization" class="sidebar-link">20. [381] I-BERT: Integer-only BERT Quantization</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#challenges-addressed" class="sidebar-link">Challenges Addressed</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#solution-i-bert-approach" class="sidebar-link">Solution - I-BERT Approach:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#results-and-impact" class="sidebar-link">Results and Impact:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-takeaways-in-3-sentences" class="sidebar-link">Key Takeaways in 3 Sentences</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_21-904-llm-int8-8-bit-matrix-multiplication-for-transformers-at-scale" class="sidebar-link">21. [904] LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-findings-include" class="sidebar-link">Key findings include:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#llm-int8-solution" class="sidebar-link">LLM.int8 solution</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#three-sentence-key-takeaways" class="sidebar-link">Three-Sentence Key Takeaways</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_22-637-training-deep-neural-networks-with-8-bit-floating-point-numbers" class="sidebar-link">22.[637] Training Deep Neural Networks with 8-bit Floating Point Numbers</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction-5" class="sidebar-link">1. Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-challenges-in-low-precision-training" class="sidebar-link">2. Challenges in Low-Precision Training</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-proposed-solutions" class="sidebar-link">3. Proposed Solutions</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-experimental-results" class="sidebar-link">4. Experimental Results</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_5-discussion" class="sidebar-link">5. Discussion</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_6-conclusion" class="sidebar-link">6. Conclusion</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-takeaways-in-3-sentences-2" class="sidebar-link">Key Takeaways in 3 Sentences</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_270-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks" class="sidebar-link">[270] Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_28-read-y2025-nvidia-coat-compressing-optimizer-states-and-activation-for-memory-efficient-fp8-training" class="sidebar-link">28. [Read Y2025 NVIDIA] Coat: Compressing Optimizer States and Activation for Memory-Efficient FP8 Training  :+1:  :+1:  :+1:  :+1:  :+1:</a></li></ul></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" class="sidebar-link">LLM Scaling Law</a></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7059/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li><li><a href="/qishao-notes/pages/dc7060/" class="sidebar-link">LLM MOE Inference</a></li><li><a href="/qishao-notes/pages/dc7061/" class="sidebar-link">LLM Compression</a></li><li><a href="/qishao-notes/pages/dc7062/" class="sidebar-link">LLM Optimizer Optimization</a></li><li><a href="/qishao-notes/pages/dc7063/" class="sidebar-link">LLM Posttraining</a></li><li><a href="/qishao-notes/pages/dc7064/" class="sidebar-link">LLM MICRO - ISCA - HPCA</a></li><li><a href="/qishao-notes/pages/dc7066/" class="sidebar-link">LLM Prefilling &amp; Decoding Split</a></li><li><a href="/qishao-notes/pages/dc7067/" class="sidebar-link">Thinking of LLM Prefilling &amp; Decoding Split</a></li><li><a href="/qishao-notes/pages/dc7068/" class="sidebar-link">From Attention Sink to Massive Activation</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-03-20</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">LLM Mixed Precision &amp; Quantization &amp; Outlier<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[UnRead 2] A Comprehensive Study on Quantization Techniques for Large Language Models :+1：</li> <li>[UnRead 11] A Comprehensive Evaluation of Quantization Strategies for Large Language Models 👍 👍</li> <li>[139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization</li> <li>[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</li> <li>[26] FP8-LM: Training FP8 Large Language Models 👍 👍 👍 from Microsoft <br> <em>This is discussed that first order in adam could be FP8, but second order in adam should be FP16. and the weight should be reserved a copy of FP32 full-precision or FP16 with tensor scaling..</em></li> <li>[139] FP8 Formats for Deep Learning</li> <li>[41] With Shared Microexponents, A Little Shifting Goes a Long Way 👍 from Meta, Microsoft 👍</li> <li>[34] Stable and low-precision training for large-scale vision-language models 👍 <br> <em>mentioned in Deepseek paper mixed precision training section. Not read yet.</em></li> <li>[Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization</li> <li>[47] Microscaling Data Formats for Deep Learning</li> <li>[Unread 2 Y2024] To FP8 and Back Again: Quantifying the Effects of Reducing Precision on LLM Training Stability</li> <li>[145] Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model</li> <li>[Unread 27]Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</li> <li>[45] PB-LLM: Partially Binarized Large Language Models</li> <li>[55] QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models</li> <li>[33] SpinQuant: LLM quantization with learned rotations</li> <li>[35] FP8 versus INT8 for efficient deep learning inference</li> <li>[Y2024]Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs</li> <li>[434] Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation</li> <li>[73] FP8 Quantization: The Power of the Exponent</li> <li>[UnRead 121] Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers</li> <li>[381] I-BERT: Integer-only BERT Quantization</li> <li>[637] Training Deep Neural Networks with 8-bit Floating Point Numbers</li> <li>[904] LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</li> <li>[UnRead 2568] DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients 👍</li> <li>[239] Model Accuracy and Runtime Tradeoff in Distributed Deep Learning: A Systematic Study 👍</li> <li>[270] Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks <br> <em>This is the paper where E5M2 and E4M3</em></li> <li>[Read Y2025 NVIDIA] Coat: Compressing Optimizer States and Activation for Memory-Efficient FP8 Training  👍  👍  👍  👍  👍 <br> <em>Source code: https://github.com/NVlabs/COAT</em></li></ol> <p><strong>Outlier</strong></p> <p>1.[106] Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling</p> <hr> <h2 id="_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization"><a href="#_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization" class="header-anchor">#</a> 1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization</h2> <h3 id="problems"><a href="#problems" class="header-anchor">#</a> Problems</h3> <p>The problems encountered when attempting to quantize transformer models like BERT, as highlighted in the paper:</p> <ul><li>High Dynamic Range of Activations
<ul><li>The authors observed that the activation tensors in transformer models, particularly in the residual connections, exhibit a very high dynamic range.</li> <li>This means that the values within these tensors vary significantly in magnitude.</li> <li>Quantization, especially when using low-bit fixed-point formats (like 8-bit integer), struggles to accurately represent such a wide range of values.</li> <li>Trying to capture both very small and very large values within the limited range of a low-bit format inevitably leads to significant quantization errors.</li></ul></li> <li>Presence of Structured Outliers
<ul><li>The authors identified the presence of structured outliers within the activation tensors, particularly in the residual connections after the feed-forward network (FFN).</li> <li>These outliers are not random; <strong>they appear to be correlated with specific input tokens and embedding dimensions</strong>.</li> <li>Further analysis revealed that <strong>these outliers influence the attention mechanism in the subsequent layers, encouraging the model to attend to specific tokens like the separator token ([SEP])</strong>.</li></ul></li></ul> <blockquote><p>In BERT-like models, an intermediate hidden activation tensor x has a shape (B, T, d), where B is the batch size, T is the sequence length, and d is the number of embedding dimensions (d = 768 for BERT-base, Devlin et al. 2019).
In the following figure, you could tell the x-axis is in dimension of 768.</p></blockquote> <p><img src="https://github.com/user-attachments/assets/2d6e801d-8e6a-4ebd-8237-a67bb5b7ca95" alt="image"></p> <ul><li>While this attention behavior might be beneficial for the model's performance, the outliers that cause it also create challenges for quantization.</li> <li>Sensitivity to Quantization Noise
<ul><li>Different parts of the transformer model exhibit varying sensitivities to the noise introduced by quantization.</li> <li>Some components, like the residual connections and certain attention heads, are particularly sensitive, and quantizing them aggressively can lead to a significant drop in accuracy.</li> <li>This sensitivity necessitates careful consideration of which parts of the model to quantize and at what bit-width.</li></ul></li></ul> <p><img src="https://github.com/user-attachments/assets/520b9b0c-c112-4c49-92a1-03842f41b92d" alt="image"></p> <h3 id="solutions"><a href="#solutions" class="header-anchor">#</a> Solutions</h3> <p>solutions proposed in the paper:</p> <ul><li><p>Mixed-precision PTQ</p> <ul><li>The authors observed that different parts of the BERT model have varying sensitivities to quantization noise.</li> <li>To address this, they proposed using a higher <strong>bit-width (16-bit) for the more sensitive activation tensors</strong>, <strong>particularly the residual sum after the feed-forward network (FFN)</strong>.</li> <li>This higher bit-width allows for more accurate representation of both the FFN's input and output, minimizing potential errors.</li> <li>Additionally, they explored <strong>using low-bit (2-4) quantization for weights and token embeddings</strong>, which can significantly reduce model size without much accuracy loss.</li></ul></li> <li><p>Per-embedding-group PTQ</p> <ul><li>The authors identified that outliers in the activation tensors <strong>primarily reside in a few specific embedding dimensions</strong>.</li> <li>To address this, they proposed a novel per-embedding-group (PEG) quantization scheme, where distinct quantization parameters are used for different embedding dimensions or groups of dimensions.</li> <li>This method involves <strong>splitting the activation tensor into groups along the embedding dimension and applying separate quantization parameters to each group</strong>.</li> <li>To optimize this process, they introduced a range-based permutation step to ensure that all outliers are grouped together.</li> <li>This approach effectively handles outliers without significantly increasing computational overhead.</li></ul></li> <li><p>Quantization-aware training (QAT)</p> <ul><li>The authors also explored QAT, where the model is trained with simulated quantization operations.</li> <li>This allows the model to adapt to the quantization noise, leading to improved performance compared to post-training quantization.</li> <li>During QAT, they used <strong>learnable ranges for both weights and activations</strong>, further enhancing the model's adaptability to quantization.</li></ul></li></ul> <hr> <h2 id="_2-43-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models"><a href="#_2-43-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models" class="header-anchor">#</a> 2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</h2> <h3 id="key-takeaways-in-three-sentences"><a href="#key-takeaways-in-three-sentences" class="header-anchor">#</a> <strong>Key Takeaways in Three Sentences</strong></h3> <ol><li>The study demonstrates that <strong>low-bit floating-point formats, particularly FP8, provide superior quantization accuracy for LLMs compared to INT8</strong> , with comparable hardware efficiency at 8-bit precision.</li> <li>The <strong>Mixture of Formats Quantization (MoFQ) approach optimally selects between INT and FP per layer</strong> , improving accuracy without increasing computational overhead.</li> <li>MoFQ achieves <strong>state-of-the-art results in both W4-only and W8A8 quantization</strong> , outperforming existing methods like GPTQ, AWQ, LLM.int8(), and SmoothQuant while maintaining <strong>efficient inference speed</strong> .</li></ol> <h3 id="abstract"><a href="#abstract" class="header-anchor">#</a> <strong>Abstract</strong></h3> <p>The study finds that optimal quantization formats vary across layers in LLMs, leading to the <strong>Mixture of Formats Quantization (MoFQ)</strong>  approach, which selects the best format per layer.<br>
MoFQ achieves superior or comparable performance to current quantization methods for weight-only (W-only) and weight-activation (WA) quantization without additional hardware overhead.</p> <h3 id="introduction"><a href="#introduction" class="header-anchor">#</a> <strong>Introduction</strong></h3> <p>Quantization minimizes LLMs' size and inference costs, with prior work focusing on low-bit integer formats.<br>
However, as LLMs grow, integer quantization becomes less effective, requiring optimizations or alternatives. Low-bit floating-point formats have emerged as viable alternatives, with FP8 already supported in NVIDIA’s H100 GPUs.</p> <p>The study:</p> <ol><li>Compares INT and FP formats in terms of hardware efficiency and quantization error.</li> <li>Proposes <strong>Mixture of Formats Quantization (MoFQ)</strong> , selecting the best format per layer.</li> <li>Implements an inference system for W-only quantization, maintaining performance parity with INT-based systems.</li></ol> <h3 id="background-and-related-works"><a href="#background-and-related-works" class="header-anchor">#</a> <strong>Background and Related Works</strong></h3> <p><strong>Integer vs. Floating-Point Formats</strong></p> <ul><li><strong>Integer (INT)</strong> : Uniformly distributed values.</li> <li><strong>Floating-Point (FP)</strong> : Non-uniform distribution, allowing higher precision for small values but reduced precision for large values.</li> <li><strong>Hardware efficiency</strong> : FP operations typically cost more than INT, but at 8-bit, FP8 and INT8 MAC (Multiply-Accumulate) units have nearly identical area costs.</li></ul> <p><strong>Post-Training Quantization (PTQ) for LLMs</strong></p> <p>Two main PTQ strategies:</p> <ol><li><strong>Weight-Only (W-only) Quantization:</strong>  Applies to weights only, e.g., W4A16.</li> <li><strong>Weight-Activation (WA) Quantization:</strong>  Quantizes both weights and activations, e.g., W8A8.</li></ol> <p>State-of-the-art (SOTA) methods:</p> <ul><li><strong>LLM.int8()</strong> : Uses mixed precision (INT8+FP16).</li> <li><strong>SmoothQuant</strong> : Redistributes quantization difficulty from activations to weights.</li> <li><strong>GPTQ &amp; AWQ</strong> : Use second-order information and pre-scaling techniques to improve quantization.</li></ul> <h3 id="comparative-analysis-of-int-and-fp-formats"><a href="#comparative-analysis-of-int-and-fp-formats" class="header-anchor">#</a> <strong>Comparative Analysis of INT and FP Formats</strong></h3> <p><strong>A. Hardware Cost of INT vs. FP MAC Units</strong></p> <ul><li><strong>At 8-bit precision, FP8 and INT8 MACs require nearly the same hardware area</strong> , aligning with H100 GPU capabilities.</li></ul> <p><strong>B. Quantization Error Comparison</strong></p> <ol><li><strong>4-bit Weight-Only (W4) Quantization</strong>  (LLaMA-65B model):</li></ol> <ul><li>🔥 Some layers perform better with INT4, while others favor FP4, indicating <strong>layer-dependent format preference</strong> .</li></ul> <p><img src="https://github.com/user-attachments/assets/97be028e-fdb3-4883-a92e-b784d1ff1f87" alt="image"></p> <ol start="2"><li><strong>8-bit Weight-Activation (W8A8) Quantization</strong> :</li></ol> <ul><li>🔥<strong>Weights</strong> : INT8 generally has lower quantization error.</li> <li>🔥<strong>Activations</strong> : FP8 shows <strong>better robustness</strong>  for dynamic activation tensors.</li> <li>Best choice: INT8 for weights, FP8 for activations—but hardware constraints necessitate using the same format per layer.</li></ul> <p><img src="https://github.com/user-attachments/assets/7af41adf-5821-4c0b-8599-1385c1982c87" alt="image"></p> <h3 id="exploiting-int-and-fp-complementarity"><a href="#exploiting-int-and-fp-complementarity" class="header-anchor">#</a> <strong>Exploiting INT and FP Complementarity</strong></h3> <p><strong>A. Improved Low-Bit FP4 Format</strong></p> <ul><li>IEEE floating-point format reserves exponent values for NaN and Inf.</li> <li><strong>Reallocating NaN &amp; Inf to normalized numbers improves FP4 precision</strong>  by 35%.</li></ul> <p><strong>B. Mixture of Formats Quantization (MoFQ)</strong></p> <ul><li>Selects the best quantization format (INT or FP) <strong>per layer</strong>  based on quantization error.</li> <li>Works for both <strong>W-only and WA quantization</strong> .</li> <li><strong>Algorithm</strong> : Iterates through layers, computes quantization error for INT and FP, and selects the lower-error format.</li></ul> <p><strong>C. Low-Bit W-Only Inference System</strong></p> <ul><li><strong>INT4 and FP4 require conversion to FP16 before computation</strong>  due to FP16 activations.</li> <li><strong>W8A8 quantization</strong> : FP16 activations are converted to FP8 or INT8 based on next-layer format selection.</li> <li><strong>No additional hardware overhead for FP-based or MoFQ-based inference</strong>  compared to INT-based quantization.</li></ul> <p><img src="https://github.com/user-attachments/assets/7d5e0e92-deda-4e40-8775-b428fff2ca0c" alt="image"></p> <h3 id="conclusion"><a href="#conclusion" class="header-anchor">#</a> <strong>Conclusion</strong></h3> <ul><li><strong>Comparative study</strong> : INT and FP formats have complementary strengths.</li> <li><strong>Key finding</strong> : <strong>FP8 and INT8 MAC units have similar hardware costs at low-bit quantization</strong> .</li> <li><strong>MoFQ method</strong> :
<ul><li>Selects the best quantization format <strong>per layer</strong> .</li> <li><strong>Achieves state-of-the-art accuracy</strong>  in W4-only and W8A8 quantization.</li> <li><strong>No additional inference latency or hardware overhead</strong> .</li></ul></li></ul> <hr> <h2 id="_5-26-fp8-lm-training-fp8-large-language-models"><a href="#_5-26-fp8-lm-training-fp8-large-language-models" class="header-anchor">#</a> 5. [26] FP8-LM: Training FP8 Large Language Models</h2> <h3 id="abstract-2"><a href="#abstract-2" class="header-anchor">#</a> <strong>Abstract</strong></h3> <p>The paper explores <strong>FP8 low-bit data formats</strong>  for training large language models (LLMs), significantly reducing <strong>memory usage and computation costs</strong>  while maintaining accuracy.</p> <p>The authors introduce an <strong>FP8 automatic mixed-precision training framework</strong>  with three levels of FP8 utilization, improving mixed-precision and distributed parallel training.</p> <p><strong>Key results</strong>  show that training the <strong>GPT-175B model on an H100 GPU platform</strong>  using FP8:</p> <ul><li><strong>Reduces memory usage by 39%</strong></li> <li><strong>Speeds up training by 75% compared to BF16 (Megatron-LM)</strong></li> <li><strong>Outperforms Nvidia Transformer Engine by 37%</strong></li></ul> <p>The <strong>FP8 training methodology is generalizable</strong>  to fine-tuning, instruction tuning, and reinforcement learning with human feedback (RLHF). The framework is <strong>open-sourced</strong>  at <a href="https://github.com/Azure/MS-AMP" target="_blank" rel="noopener noreferrer">aka.ms/MS.AMP<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> .</p> <h3 id="_1-introduction"><a href="#_1-introduction" class="header-anchor">#</a> <strong>1. Introduction</strong></h3> <p>LLMs have demonstrated exceptional performance in various domains but are extremely expensive to train.<br>
The cost of training models like <strong>GPT-3 (175B) or PaLM (540B)</strong>  is enormous, requiring <strong>thousands of GPUs or TPUs</strong> .<br>
Low-precision training is a <strong>promising solution</strong>  as it:</p> <ul><li><strong>Increases speed</strong></li> <li><strong>Reduces memory usage</strong></li> <li><strong>Minimizes communication overhead</strong></li></ul> <p>Most existing frameworks, such as <strong>Megatron-LM, MetaSeq, and Colossal-AI</strong> , use <strong>FP32, FP16, or BF16 mixed-precision training</strong> , but <strong>FP8 offers significant efficiency gains</strong> :</p> <ul><li><strong>2× speed-up</strong></li> <li><strong>50%-75% memory and communication savings</strong></li></ul> <h4 id="challenges-of-fp8-training"><a href="#challenges-of-fp8-training" class="header-anchor">#</a> <strong>Challenges of FP8 Training</strong></h4> <ol><li><strong>Data underflow/overflow issues</strong>  due to FP8’s limited dynamic range.</li> <li><strong>Numerical instabilities and divergence</strong>  during training.</li></ol> <h4 id="proposed-fp8-mixed-precision-framework"><a href="#proposed-fp8-mixed-precision-framework" class="header-anchor">#</a> <strong>Proposed FP8 Mixed-Precision Framework</strong></h4> <ul><li>Introduces <strong>three levels of FP8 utilization</strong>  (gradients, optimizer states, and distributed learning).</li> <li>Uses <strong>precision decoupling</strong>  and <strong>automatic scaling</strong>  to mitigate numerical instability.</li> <li>Achieves <strong>29%-39% memory savings</strong>  and <strong>63%-65% communication cost reductions</strong> .</li></ul> <blockquote><p>The resulting FP8 mixed-precision networks are more efficient than their pure FP16 counterparts, but a network that is in full INT8 is expected to be significantly more efficient yet.</p></blockquote> <h3 id="_2-fp8-llm-training"><a href="#_2-fp8-llm-training" class="header-anchor">#</a> <strong>2. FP8 LLM Training</strong></h3> <h4 id="_2-1-fp8-gradient-and-all-reduce-communication"><a href="#_2-1-fp8-gradient-and-all-reduce-communication" class="header-anchor">#</a> <strong>2.1 FP8 Gradient and All-Reduce Communication</strong></h4> <ul><li>Traditional mixed-precision training uses <strong>FP16/FP32 for gradients</strong> , leading to high communication costs.</li> <li>Applying <strong>FP8 directly to gradients</strong>  results in <strong>loss of accuracy</strong>  due to underflow/overflow.</li> <li>The paper proposes an <strong>automatic scaling technique</strong>  to adapt scaling factors dynamically, preventing numerical instability.</li></ul> <h4 id="_2-2-fp8-optimizer"><a href="#_2-2-fp8-optimizer" class="header-anchor">#</a> <strong>2.2 FP8 Optimizer</strong></h4> <ul><li>The <strong>Adam optimizer</strong>  typically consumes <strong>16 bytes per parameter</strong>  due to high-precision storage of gradients and optimizer states.</li> <li>The proposed <strong>FP8 optimizer</strong>  stores:
<ul><li>FP8 first-order moment</li> <li>FP16 master weights (with tensor scaling)</li> <li>FP16 second-order moment</li></ul></li> <li>This reduces <strong>memory consumption from 16 bytes to 6 bytes per parameter</strong>  (2.6× savings).</li></ul> <blockquote><p>My main takeaway is that direction of gradient matters, instead of magnitude.</p></blockquote> <p><img src="https://github.com/user-attachments/assets/aa8bdd9a-c5ea-4910-bfdf-836eca32f9b5" alt="image"></p> <ol><li>FP8 master weight induces performance degradation (see the #2a vs. #3 lines in Fig. 8), while FP16 can maintain accuracy as FP32 (see #2a vs. #0 and #1) but requiring using tensor scaling.
It reveals that the master weight is precision-sensitive.
This can be attributed to <strong>the master weight’s</strong> role in updating weights, which tend to exhibit small magnitudes, necessitating high precision to maintain accuracy.</li> <li>The training loss of <strong>BF16</strong> master weight is <strong>slightly higher</strong> than that of FP16 with a scaling factor because BF16 has fewer mantissa bits, resulting in lower precision (see #2a vs. #2b).</li> <li>❗ The second-order gradient moment is more precision-sensitive than the first-order one, because the ❗square calculation is easy to cause underflow and leads to accuracy degradation.
Utilizing FP8 for the second-order gradient moment can lead to divergent training loss (see the #4 dot in Fig. 8).</li></ol> <p>Please Notice that FP8 #4 is diverged, not shown in the figure.</p> <p><img src="https://github.com/user-attachments/assets/127e6274-629c-4f40-81f7-57cbb1ce9d98" alt="image"></p> <h4 id="_2-3-fp8-distributed-parallel-training"><a href="#_2-3-fp8-distributed-parallel-training" class="header-anchor">#</a> <strong>2.3 FP8 Distributed Parallel Training</strong></h4> <ul><li><strong>Tensor Parallelism</strong> : Uses <strong>FP8 for weight and activation tensors</strong> , reducing compute and communication overhead.</li> <li><strong>Sequence Parallelism</strong> : Converts activation tensors to <strong>FP8 before communication</strong> , reducing costs.</li> <li><strong>ZeRO (Zero Redundancy Optimizer) Support</strong> : Distributes <strong>full tensors</strong>  across devices while preserving <strong>FP8 scaling factors</strong> .</li></ul> <h3 id="_3-experimentation"><a href="#_3-experimentation" class="header-anchor">#</a> <strong>3. Experimentation</strong></h3> <h4 id="_3-1-experimental-setup"><a href="#_3-1-experimental-setup" class="header-anchor">#</a> <strong>3.1 Experimental Setup</strong></h4> <ul><li><strong>Training Dataset</strong> : Collected from <strong>CommonCrawl, The Pile, C4, OpenWebText, Wikipedia, RedPajama</strong> , and other curated sources.</li> <li><strong>Model Configuration</strong> : Uses a <strong>decoder-only Transformer</strong>  architecture (like GPT-3), with <strong>RoPE embeddings and Flash Attention</strong> .</li></ul> <h4 id="_3-2-main-results"><a href="#_3-2-main-results" class="header-anchor">#</a> <strong>3.2 Main Results</strong></h4> <h5 id="model-performance"><a href="#model-performance" class="header-anchor">#</a> <strong>Model Performance</strong></h5> <ul><li><strong>Loss curves of FP8 models match BF16 models</strong> , confirming <strong>accuracy preservation</strong></li> <li><strong>Zero-shot evaluations</strong>  on <strong>Lambada, HellaSwag, BoolQ, PIQA, COPA</strong>  show <strong>comparable performance between FP8 and BF16</strong> .</li> <li><strong>Fine-tuning (SFT &amp; RLHF)</strong> : FP8 achieves:
<ul><li>27% faster fine-tuning</li> <li>32% reduction in model weight memory</li> <li>62% optimizer state memory savings</li></ul></li></ul> <p><strong>System Performance</strong></p> <ul><li><strong>Memory reduction</strong> : FP8 achieves <strong>28%-39% lower memory usage</strong>  than BF16.</li> <li><strong>Training speed improvement</strong> :
<ul><li>75% faster training for GPT-175B</li> <li>37% faster than Nvidia Transformer Engine</li></ul></li> <li><strong>Communication efficiency</strong> :
<ul><li>63%-65% reduction in weight gradient communication</li> <li>34% lower activation-related communication costs</li></ul></li></ul> <h4 id="_3-3-ablation-study"><a href="#_3-3-ablation-study" class="header-anchor">#</a> <strong>3.3 Ablation Study</strong></h4> <ul><li><strong>Gradient Scaling</strong> : <strong>Automatic scaling</strong>  reduces <strong>underflow/overflow errors</strong> , improving training stability.</li> <li><strong>Optimizer Precision</strong> :
<ul><li>FP16 master weights outperform FP8 master weights in accuracy preservation.</li> <li>FP8 first-order gradient moment is viable, but FP8 second-order moment leads to divergence.</li></ul></li> <li><strong>Parallelism Optimization</strong> :
<ul><li><strong>FP8 sequence and tensor parallelism</strong>  reduce communication costs by <strong>34%</strong> .</li> <li><strong>FP8 ZeRO</strong>  maintains a balanced GPU memory load while saving memory.</li></ul></li></ul> <h3 id="_4-related-work"><a href="#_4-related-work" class="header-anchor">#</a> <strong>4. Related Work</strong></h3> <ul><li><strong>Mixed-Precision Training</strong> : Prior work focused on <strong>FP16/BF16</strong> , but <strong>FP8 remains underexplored</strong> .</li> <li><strong>Low-Precision LLM Training</strong> :
<ul><li><strong>OPT, Bloom, Gopher, Chinchilla</strong>  used <strong>BF16</strong>  for better numerical stability.</li> <li>FP8 support was limited before Nvidia Hopper GPUs.</li> <li>This work provides the <strong>first systematic FP8 training framework</strong>  for <strong>pre-training and fine-tuning LLMs</strong> .</li></ul></li></ul> <h3 id="_5-conclusion"><a href="#_5-conclusion" class="header-anchor">#</a> <strong>5. Conclusion</strong></h3> <ul><li>Introduces a <strong>new FP8 mixed-precision training framework</strong>  with <strong>automatic scaling</strong>  and <strong>precision decoupling</strong> .</li> <li>Achieves <strong>significant reductions in memory, compute, and communication costs</strong> .</li> <li><strong>Maintains model accuracy</strong>  across <strong>GPT models from 125M to 175B parameters</strong> .</li> <li>Demonstrates <strong>versatility</strong>  in pre-training, instruction tuning, and RLHF.</li> <li><strong>Future work</strong>  includes scaling to even larger models, training multi-modal models, and deploying FP8 LLMs on edge devices.</li></ul> <h3 id="key-summary-in-3-sentences"><a href="#key-summary-in-3-sentences" class="header-anchor">#</a> <em>Key Summary in 3 Sentences</em>*</h3> <p>This paper introduces an <strong>FP8 mixed-precision training framework</strong>  that reduces memory consumption by <strong>39%</strong> , speeds up training by <strong>75%</strong> , and <strong>outperforms Nvidia Transformer Engine by 37%</strong>  while maintaining LLM accuracy.<br>
The framework uses <strong>automatic scaling and precision decoupling</strong>  to stabilize training, supports <strong>FP8 optimizers and distributed training</strong> , and generalizes to <strong>fine-tuning and reinforcement learning with human feedback (RLHF)</strong> .<br>
These findings establish <strong>FP8 as the next-generation precision format for training LLMs</strong> , significantly lowering costs while preserving model performance.</p> <hr> <h2 id="_6-139-fp8-formats-for-deep-learning"><a href="#_6-139-fp8-formats-for-deep-learning" class="header-anchor">#</a> 6. [139] FP8 Formats for Deep Learning</h2> <h3 id="_1-introduction-2"><a href="#_1-introduction-2" class="header-anchor">#</a> <strong>1. Introduction</strong></h3> <p>Deep learning models require increasing computational resources, necessitating lower-precision formats for efficiency.</p> <p>FP8 is a <strong>natural evolution</strong>  from FP16 and BF16, reducing <strong>compute and memory costs</strong>  while maintaining <strong>accuracy comparable to FP16</strong> .</p> <h3 id="key-contributions"><a href="#key-contributions" class="header-anchor">#</a> Key contributions:</h3> <ul><li><p><strong>Two FP8 formats:</strong></p> <ul><li><strong>E4M3</strong> : 4-bit exponent, 3-bit mantissa (for weights and activations).</li> <li><strong>E5M2</strong> : 5-bit exponent, 2-bit mantissa (for gradients).</li></ul></li> <li><p><strong>Training and inference in FP8</strong>  match FP16/BF16 accuracy across CNNs, RNNs, and Transformers.</p></li> <li><p><strong>Post-training quantization (PTQ)</strong>  using FP8 <strong>outperforms int8</strong>  while preserving model accuracy.</p></li></ul> <h3 id="_2-aspects-of-fp8-usage-in-deep-learning"><a href="#_2-aspects-of-fp8-usage-in-deep-learning" class="header-anchor">#</a> <strong>2. Aspects of FP8 Usage in Deep Learning</strong></h3> <ul><li><strong>FP8 computations</strong>  will be performed in <strong>higher precision (FP16/FP32)</strong> , with final results cast back to FP8.</li> <li><strong>Scaling factors</strong>  are applied to <strong>optimize FP8 precision</strong> , similar to <strong>loss-scaling in FP16 mixed precision</strong> .</li> <li><strong>Handling of special values (NaNs, Infs) is modified in E4M3</strong>  to increase dynamic range.</li></ul> <h3 id="_3-fp8-binary-interchange-format"><a href="#_3-fp8-binary-interchange-format" class="header-anchor">#</a> <strong>3. FP8 Binary Interchange Format</strong></h3> <p><img src="https://github.com/user-attachments/assets/dc054ba5-6892-4c63-888e-ef24c9789d8a" alt="image"></p> <p>FP8 includes <strong>two encodings</strong> :</p> <ul><li><p><strong>E4M3</strong> :</p> <ul><li><strong>Used for weights and activations</strong> .</li> <li><strong>No representation for infinities</strong>  (max value: <strong>448</strong> ).</li> <li><strong>Single NaN representation to extend range</strong> .</li></ul></li> <li><p><strong>E5M2</strong> :</p> <ul><li><strong>Used for gradients</strong> .</li> <li><strong>Standard IEEE-like format</strong> , supporting <strong>NaNs and infinities</strong> .</li> <li>Larger range (up to <strong>57,344</strong> ).</li></ul></li></ul> <p><img src="https://github.com/user-attachments/assets/ac2dbf07-fd59-4902-a55b-0bb4b0385408" alt="image"></p> <h4 id="_3-1-special-value-representations"><a href="#_3-1-special-value-representations" class="header-anchor">#</a> <strong>3.1 Special Value Representations</strong></h4> <ul><li><strong>E4M3 removes infinities</strong>  and limits NaNs to a <strong>single pattern</strong> , extending its <strong>dynamic range</strong> .</li> <li><strong>E5M2 follows IEEE-754</strong> , allowing <strong>straightforward conversion from FP16</strong> .</li></ul> <h4 id="_3-2-exponent-bias"><a href="#_3-2-exponent-bias" class="header-anchor">#</a> <strong>3.2 Exponent Bias</strong></h4> <ul><li><strong>E4M3 bias = 7, E5M2 bias = 15</strong>  (matching IEEE-style representation).</li> <li>Some models require <strong>per-tensor scaling</strong>  rather than a fixed exponent bias (Figure 2).</li></ul> <h3 id="_4-empirical-results"><a href="#_4-empirical-results" class="header-anchor">#</a> <strong>4. Empirical Results</strong></h3> <h4 id="_4-1-training"><a href="#_4-1-training" class="header-anchor">#</a> <strong>4.1 Training</strong></h4> <ul><li>FP8 training achieves <strong>accuracy comparable to FP16/BF16</strong>  across CNNs, RNNs, and Transformers.</li> <li><strong>Image Classification</strong> :
<ul><li>FP8 accuracy is <strong>within statistical variation</strong>  of FP16 for most CNNs (ResNet, MobileNet, VGG, etc.).</li></ul></li> <li><strong>Language Translation</strong> :
<ul><li>FP8 BLEU scores <strong>match FP16</strong>  for Transformer and GNMT models.</li></ul></li> <li><strong>NLP Models (Table 4, Figure 1)</strong> :
<ul><li>GPT models (126M to 175B parameters) trained in FP8 <strong>match FP16 in perplexity</strong> .</li></ul></li></ul> <h4 id="_4-2-inference"><a href="#_4-2-inference" class="header-anchor">#</a> <strong>4.2 Inference</strong></h4> <ul><li><strong>FP8 post-training quantization (PTQ) outperforms int8</strong> , retaining <strong>full precision accuracy</strong>  for:
<ul><li>BERT (F1 score on SQuAD).</li> <li>GPT-3 (perplexity on Wikitext103).</li></ul></li> <li><strong>FP8-trained models require no additional quantization steps</strong> , simplifying deployment.</li></ul> <h4 id="_4-3-per-tensor-scaling"><a href="#_4-3-per-tensor-scaling" class="header-anchor">#</a> <strong>4.3 Per-Tensor Scaling</strong></h4> <ul><li><strong>Fixed exponent bias fails</strong>  when additional tensors (e.g., residuals) are stored in FP8.</li> <li><strong>Per-tensor scaling maintains accuracy</strong> , making FP8 viable for <strong>expanded use beyond GEMMs</strong> .</li></ul> <h3 id="_5-conclusions"><a href="#_5-conclusions" class="header-anchor">#</a> <strong>5. Conclusions</strong></h3> <ul><li><strong>FP8 formats (E4M3, E5M2) efficiently reduce training and inference costs while maintaining accuracy</strong> .</li> <li><strong>FP8 training is on par with FP16/BF16</strong> , without hyperparameter changes.</li> <li><strong>FP8 simplifies inference</strong>  by eliminating the need for quantization-aware training (QAT) required for int8.</li> <li><strong>Future work</strong> : Expanding FP8 usage to <strong>more tensor types and operations</strong>  beyond matrix multiplications.</li></ul> <h3 id="key-takeaways-in-three-sentences-2"><a href="#key-takeaways-in-three-sentences-2" class="header-anchor">#</a> <strong>Key Takeaways in Three Sentences</strong></h3> <p>FP8 formats (E4M3 for weights/activations, E5M2 for gradients) <strong>significantly reduce computation and memory overhead</strong>  while maintaining <strong>accuracy equivalent to FP16/BF16</strong>  across CNNs, RNNs, and Transformer models.</p> <p><strong>Post-training quantization (PTQ) with FP8 outperforms int8</strong> , allowing for <strong>simpler and more effective deployment</strong>  of trained models. The study <strong>validates FP8 training up to 175B parameters</strong> , proving its scalability for large-scale deep learning applications.</p> <hr> <h2 id="_8-34-stable-and-low-precision-training-for-large-scale-vision-language-models"><a href="#_8-34-stable-and-low-precision-training-for-large-scale-vision-language-models" class="header-anchor">#</a> 8. [34] Stable and low-precision training for large-scale vision-language models</h2> <h3 id="_1-introduction-and-motivation"><a href="#_1-introduction-and-motivation" class="header-anchor">#</a> 1. Introduction and Motivation</h3> <p>The paper addresses two crucial bottlenecks in large-scale vision-language model training:</p> <ul><li>Speed – how to train massive models efficiently despite ballooning compute costs.</li> <li>Stability – how to avoid “loss spikes” that can degrade or derail training.</li></ul> <p>They target contrastive language-image pre-training (CLIP) models, which fuse image and text encoders to learn aligned representations.</p> <p>These models often involve hundreds of millions to billions of parameters and require large-scale data (like LAION).</p> <p>By improving efficiency (low-precision arithmetic) and stability (modified optimizer), the authors hope to sustain further scaling of multimodal architectures.</p> <h3 id="_2-eight-bit-8-bit-training"><a href="#_2-eight-bit-8-bit-training" class="header-anchor">#</a> 2. Eight-Bit (8-bit) Training</h3> <h4 id="_2-1-preliminaries-related-work"><a href="#_2-1-preliminaries-related-work" class="header-anchor">#</a> 2.1 Preliminaries &amp; Related Work</h4> <p>16-bit operations (float16/bfloat16) are currently standard for large-scale training.</p> <p>bfloat16 has broader exponent range than float16, making it more robust at scale, but native hardware support for bfloat16 exists mainly on TPUs or newer GPUs.</p> <p>More recently, hardware support for int8 and float8 is emerging (NVIDIA Ampere has int8, Hopper adds float8), enabling potential speedups over 16-bit if quantization can be made accurate and stable.</p> <p>However, at the 1B-parameter scale of CLIP ViT-Huge, quantization can introduce noise that leads to sizable accuracy drops unless carefully managed.</p> <p><img src="https://github.com/user-attachments/assets/40b8d6df-33f0-4fb1-9c99-c86c51ba6c75" alt="image"></p> <h4 id="_2-2-the-switchback-method"><a href="#_2-2-the-switchback-method" class="header-anchor">#</a> 2.2 The SwitchBack Method</h4> <ol><li><strong>Core Idea</strong>: In a standard linear layer, three matrix multiplications occur: one in the forward pass and two in backprop (input-grad and weight-grad).</li></ol> <p>SwitchBack executes only the forward pass and input-grad steps in 8-bit, while reverting the weight-grad step to higher precision (16-bit).</p> <ol start="2"><li>Why This Matters:</li></ol> <ul><li>The weight-grad multiply has an extremely large inner dimension (batch size × sequence length), making it more prone to quantization noise.</li> <li>By keeping that specific multiplication in 16-bit, SwitchBack avoids excessive noise accumulation.</li></ul> <ol start="3"><li>Implementation Details:</li></ol> <ul><li>Uses row-wise quantization for activation/gradient matrices and tensor-wise quantization for weights.</li> <li>Relies on triton kernels for fused “quantize + transpose” in the backward pass, minimizing overhead from transposing or storing scaled values.</li> <li>Extends easily to float8 (fp8) by simulating exact fp8 values with 16-bit computations.</li></ul> <ol start="4"><li>Experimental Setup:</li></ol> <ul><li>Trains multiple CLIP ViT variants (Base, Large, Huge) on LAION-2B with a short schedule (20k iterations, patch dropout 0.5).</li> <li>Evaluates zero-shot ImageNet accuracy using standard CLIP prompts.</li></ul> <p><img src="https://github.com/user-attachments/assets/c23e270a-0096-4184-af1c-57263d4766e5" alt="image"></p> <ol start="5"><li>Results &amp; Speedups:</li></ol> <ul><li>Accuracy: For CLIP ViT-Huge (∼1B params), SwitchBack stays within 0.1% of the bfloat16 baseline, whereas a simpler baseline (LLM.int8()) underperforms by nearly 6%.</li> <li>Speed: Overall training speeds up by 13–25%. Profiling shows int8 multiplications run ~2× faster than fp16, and quantization overhead is small (&lt;25% of total time in these matmuls).</li></ul> <h4 id="_2-3-float8-via-reduced-feature-magnitudes"><a href="#_2-3-float8-via-reduced-feature-magnitudes" class="header-anchor">#</a> 2.3 Float8 via Reduced Feature Magnitudes</h4> <ul><li>While SwitchBack also supports float8 (fp8), the authors observe that a purely tensor-wise fp8 baseline diverges at large scale.</li> <li>Key Intervention: Zero-initialized layer-scale – each transformer block’s residual path is initially scaled to 0, then learned. This keeps features from growing too large, which can disrupt fp8’s narrower numeric range.</li></ul> <p><strong>Zero-initialized layer-scale</strong></p> <p><img src="https://github.com/user-attachments/assets/e8ff88ac-70ad-49ed-b994-2c50ca1cf929" alt="image"></p> <ul><li>With zero-init layer-scale, ViT-Large trains successfully in simulated fp8 (still a small gap from bfloat16, but no divergence).</li></ul> <h3 id="_3-stability-of-training"><a href="#_3-stability-of-training" class="header-anchor">#</a> 3. Stability of Training</h3> <h4 id="_3-1-problem-loss-spikes"><a href="#_3-1-problem-loss-spikes" class="header-anchor">#</a> 3.1 Problem: Loss Spikes</h4> <ul><li>Training very large CLIP models can suffer from fast, sporadic spikes in loss—episodes that slow or degrade convergence.</li> <li>Prior methods like gradient clipping or large warmup sometimes help, but for CLIP ViT-Huge, the authors pinpoint a new mechanism: the AdamW second-moment estimator for the patch embedding layer can become out-of-date.</li></ul> <h4 id="_3-2-observations-about-loss-spikes"><a href="#_3-2-observations-about-loss-spikes" class="header-anchor">#</a> 3.2 Observations About Loss Spikes</h4> <p><strong>Loss spike</strong></p> <p><img src="https://github.com/user-attachments/assets/8364d5cb-5296-47bb-9014-9c4d2235a3f6" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/7bf70d6c-aa7a-44fa-93e6-11e220f415c3" alt="image"></p> <ul><li>More Spikes at Larger Scale: Scaling model size, batch size, or learning rate all exacerbate these spikes.</li> <li>AdamW β2 Influence: Reducing β2 from its typical 0.999 helps avoid some spikes but can hamper overall performance.</li> <li><strong>Root Cause</strong>: ❗ When gradients suddenly increase for the early patch-embedding parameters, AdamW’s exponentially moving average of squared gradients (the second-moment) underestimates the true magnitude, resulting in an oversized update that triggers a transient loss explosion.</li></ul> <p><strong>Stuck-in-the-past sceneario</strong>: historicall gradient magnitude have been historically samll, thus, some parameter is larger for those parameters.
Suddenly, if those parameter receives a large gradient signal, the update can be <strong>catastrophycally</strong> big.</p> <h4 id="_3-3-stableadamw-optimizer"><a href="#_3-3-stableadamw-optimizer" class="header-anchor">#</a> 3.3 StableAdamW Optimizer</h4> <ul><li>Based on ideas from AdaFactor, the authors propose update clipping:
<ul><li>Compute an “RMS ratio” = √(E[g² / EMA(g²)]) per layer.</li> <li>If RMS ratio &gt; 1, automatically downscale the layer’s effective learning rate for that iteration.</li></ul></li> <li>StableAdamW = AdamW + update clipping. It successfully removes or mitigates these spikes better than gradient norm clipping, sustaining higher accuracy at large scale.</li></ul> <p><img src="https://github.com/user-attachments/assets/fffb86c1-4f81-442f-9d8a-3a79def24748" alt="image"></p> <h4 id="_3-4-connection-to-low-precision-training"><a href="#_3-4-connection-to-low-precision-training" class="header-anchor">#</a> 3.4 Connection to Low-Precision Training</h4> <ul><li>Loss spikes can produce extremely large activations and gradients, risking numerical issues (Inf/NaN) when using narrower floating-point types.</li> <li>The authors show that stabilizing these early-layer explosions goes hand in hand with ensuring safe low-precision training.</li> <li>They also adopt a specialized fp16 “loss scalar” strategy (per-layer updates, fixed scaling) to keep ephemeral spikes in one layer from knocking down the entire network’s scalar.</li></ul> <h3 id="_4-conclusion-and-future-directions"><a href="#_4-conclusion-and-future-directions" class="header-anchor">#</a> 4. Conclusion and Future Directions</h3> <ul><li>SwitchBack plus StableAdamW demonstrates both faster (13–25% speedups) and steadier (fewer loss spikes) training for large CLIP ViTs, outperforming or matching bfloat16 baselines.</li> <li>The authors note limitations: they simulate float8 (rather than use actual fp8 hardware), do not fully explore alternate initialization or width-scaling schemes, and their training runs are shortened for cost reasons. Nonetheless, the methods and open-source code (including Triton kernels) lay groundwork for future improvement in quantizing and stabilizing very large multimodal models.</li></ul> <h3 id="three-sentence-core-summary"><a href="#three-sentence-core-summary" class="header-anchor">#</a> Three-Sentence Core Summary</h3> <p>They propose SwitchBack, an 8-bit training approach that leaves weight-gradient computations in higher precision, yielding 13–25% speed improvements over standard 16-bit training while closely matching accuracy.</p> <p>They further show that loss spikes in large CLIP models stem from outdated second-moment estimations in AdamW, and propose StableAdamW (AdamW plus adaptive update clipping) as a fix.</p> <p>By combining these, the paper demonstrates fast, stable, and memory-efficient low-precision training on billion-parameter vision-language models.</p> <hr> <h2 id="_9-y2025-paretoq-scaling-laws-in-extremely-low-bit-llm-quantization"><a href="#_9-y2025-paretoq-scaling-laws-in-extremely-low-bit-llm-quantization" class="header-anchor">#</a> 9. [Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization</h2> <h3 id="abstract-3"><a href="#abstract-3" class="header-anchor">#</a> <strong>Abstract</strong></h3> <p>The study introduces <strong>ParetoQ</strong> , a unified framework for evaluating extremely low-bit quantization (1-bit to 4-bit) in large language models (LLMs).<br>
It identifies a <strong>learning transition</strong>  between 2-bit and 3-bit models, with 3-bit and higher retaining pre-trained distributions, while 2-bit and below undergo drastic representation changes.<br>
By optimizing training strategies and quantization functions, <strong>ParetoQ achieves state-of-the-art (SOTA) performance</strong>  across multiple bit-widths.<br>
Notably, a <strong>ternary (1.58-bit) 600M model surpasses a previous 3B ternary model</strong> , using only one-fifth of the parameters.<br>
The study finds that <strong>2-bit quantization offers a superior balance between accuracy, model size, and hardware efficiency</strong>  compared to 4-bit and binary quantization.</p> <h3 id="_1-introduction-3"><a href="#_1-introduction-3" class="header-anchor">#</a> <strong>1. Introduction</strong></h3> <p>As models scale, lower-precision computation is gaining traction due to <strong>memory savings and computational efficiency</strong> .<br>
Prior studies suggest conflicting optimal quantization levels (1.58-bit, 2-bit, 4-bit, etc.), but <strong>no unified framework existed</strong>  to systematically compare their effectiveness.</p> <h3 id="key-questions"><a href="#key-questions" class="header-anchor">#</a> <strong>Key Questions:</strong></h3> <ul><li>What is the optimal trade-off between bit-width and model size?</li> <li>How does quantization impact <strong>scaling laws</strong>  in low-bit settings?</li> <li>What training strategies and quantization functions yield <strong>Pareto-optimal results</strong> ?</li></ul> <h3 id="paretoq-approach"><a href="#paretoq-approach" class="header-anchor">#</a> <strong>ParetoQ Approach:</strong></h3> <ul><li>Incorporates <strong>five key dimensions</strong> : model size (<strong>N</strong> ), token count (<strong>D</strong> ), quantization precision (<strong>P</strong> ), training strategy (<strong>S</strong> ), and quantization function (<strong>F</strong> ).</li> <li>Identifies <strong>bit-specific training schemes and quantization functions</strong> .</li> <li>Establishes that <strong>binary quantization significantly degrades accuracy</strong> , while <strong>ternary (1.58-bit), 2-bit, and 3-bit quantization match or exceed 4-bit performance</strong> .</li></ul> <h3 id="_2-a-better-qat-scheduling-strategy-for-extreme-low-bit-llms"><a href="#_2-a-better-qat-scheduling-strategy-for-extreme-low-bit-llms" class="header-anchor">#</a> <strong>2. A Better QAT Scheduling Strategy for Extreme Low-Bit LLMs</strong></h3> <h4 id="_2-1-training-budget-allocation"><a href="#_2-1-training-budget-allocation" class="header-anchor">#</a> <strong>2.1 Training Budget Allocation</strong></h4> <ul><li><strong>Post-Training Quantization (PTQ)</strong>  is easier to implement but <strong>performs poorly</strong>  below 4-bit.</li> <li><strong>Quantization-Aware Training (QAT)</strong>  integrates quantization during training, <strong>improving low-bit performance</strong> .</li> <li><strong>Optimal budget split:</strong> <strong>90% full-precision training, 10% QAT fine-tuning</strong> .</li> <li><strong>Finding-1:</strong> <strong>QAT fine-tuning outperforms both PTQ and QAT from scratch</strong> , achieving the best trade-off between accuracy and efficiency.</li></ul> <h4 id="_2-2-fine-tuning-characteristics"><a href="#_2-2-fine-tuning-characteristics" class="header-anchor">#</a> <strong>2.2 Fine-tuning Characteristics</strong></h4> <ul><li>Fine-tuning <strong>improves accuracy across all bit-widths</strong> , including binary and ternary models.</li> <li><strong>Lower-bit models (≤2-bit) require more fine-tuning (30B tokens), while 3-bit and 4-bit saturate at 10B tokens</strong> .</li> <li><strong>Finding-2:</strong> <strong>Bit-width transition effect:</strong> <ul><li><strong>3-bit &amp; 4-bit recover near full precision with fine-tuning</strong> .</li> <li><strong>1-bit to 2-bit undergo substantial weight transformations</strong> , requiring more tokens.</li> <li>QAT serves as &quot;compensation&quot; for 3-bit+ but &quot;reconstruction&quot; for ≤2-bit models.</li></ul></li></ul> <h3 id="_3-a-hitchhiker-s-guide-to-quantization-method-choices"><a href="#_3-a-hitchhiker-s-guide-to-quantization-method-choices" class="header-anchor">#</a> <strong>3. A Hitchhiker’s Guide to Quantization Method Choices</strong></h3> <h4 id="_3-1-trade-offs-in-low-bit-quantization"><a href="#_3-1-trade-offs-in-low-bit-quantization" class="header-anchor">#</a> <strong>3.1 Trade-offs in Low-bit Quantization</strong></h4> <ol><li><strong>Range Clipping</strong> : Lower-bit quantization suffers from outlier effects, requiring range clipping or <strong>learnable scales</strong> .</li> <li><strong>Quantization Grids</strong> :</li></ol> <ul><li>Binary &amp; Ternary require balanced levels.</li> <li>2-bit prefers symmetric distribution*.</li> <li>3-bit and 4-bit benefit from including &quot;0&quot; in quantization levels.</li></ul> <h4 id="_3-2-introducing-paretoq"><a href="#_3-2-introducing-paretoq" class="header-anchor">#</a> <strong>3.2 Introducing ParetoQ</strong></h4> <ul><li><strong>Combines the best quantization functions per bit-width</strong> :
<ul><li><strong>1-bit</strong> : Elastic Binarization.</li> <li><strong>1.58-bit, 2-bit</strong> : <strong>Stretched Elastic Quant (SEQ)</strong> .</li> <li><strong>3-bit, 4-bit</strong> : <strong>Learned Step Size Quantization (LSQ)</strong> .</li></ul></li> <li><strong>Finding-3:</strong>  No single best function for all bit-widths.
<strong>Learnable range settings outperform fixed statistical methods</strong> , especially for <strong>sub-4-bit quantization</strong> .</li></ul> <h3 id="_4-pareto-optimality-of-extremely-low-bit-llms"><a href="#_4-pareto-optimality-of-extremely-low-bit-llms" class="header-anchor">#</a> <strong>4. Pareto-Optimality of Extremely Low-Bit LLMs</strong></h3> <h4 id="_4-1-accuracy-compression-trade-off"><a href="#_4-1-accuracy-compression-trade-off" class="header-anchor">#</a> <strong>4.1 Accuracy-Compression Trade-off</strong></h4> <ul><li>Ternary (1.58-bit), 2-bit, and 3-bit outperform 4-bit in accuracy-size efficiency.</li> <li>2-bit and ternary quantization sit on the Pareto frontier.</li></ul> <h4 id="_4-2-hardware-constraints"><a href="#_4-2-hardware-constraints" class="header-anchor">#</a> <strong>4.2 Hardware Constraints</strong></h4> <ul><li>Ternary (1.58-bit) appears efficient but is difficult to implement due to indexing overhead.</li> <li>2-bit is more hardware-friendly**  due to **simpler storage and arithmetic operations.</li></ul> <h4 id="_4-3-accuracy-speed-trade-off"><a href="#_4-3-accuracy-speed-trade-off" class="header-anchor">#</a> <strong>4.3 Accuracy-Speed Trade-off</strong></h4> <ul><li>2-bit achieves higher speed at the same accuracy as 4-bit.</li> <li>2-bit kernels are significantly faster than 4-bit kernels in large matrix multiplications.</li></ul> <h3 id="_6-related-work"><a href="#_6-related-work" class="header-anchor">#</a> <strong>6. Related Work</strong></h3> <ul><li><strong>Early quantization research</strong>  focused on <strong>8-bit and 4-bit LLMs</strong> .</li> <li>Recent <strong>sub-4-bit research</strong>  explored <strong>ternary, 2-bit, and 1-bit models</strong> , but lacked a <strong>unified comparison framework</strong> .</li> <li><strong>ParetoQ is the first study to systematically compare sub-4-bit quantization schemes</strong> .</li></ul> <p><strong>7. Conclusions</strong></p> <ul><li>ParetoQ unifies training and quantization schemes across five bit-widths.</li> <li>Ternary (1.58-bit), 2-bit, and 3-bit quantization outperform 4-bit in the accuracy-size trade-off.</li> <li>2-bit is the most practical choice due to its hardware efficiency.</li> <li>First framework that ensures fair comparisons across different quantization methods.</li></ul> <p><strong>Key Takeaways (3 Sentences)</strong></p> <ol><li>ParetoQ introduces a unified framework for extreme low-bit quantization, identifying 2-bit as the most efficient trade-off between accuracy, memory, and computational speed.</li> <li>A clear transition exists between 2-bit and 3-bit models, where lower-bit quantization requires substantial fine-tuning to compensate for drastic weight transformations.</li> <li>With its optimized quantization functions and training schemes, ParetoQ achieves state-of-the-art performance across all bit-widths, surpassing previous specialized methods.</li></ol> <hr> <h2 id="_10-47-microscaling-data-formats-for-deep-learning"><a href="#_10-47-microscaling-data-formats-for-deep-learning" class="header-anchor">#</a> 10. [47] Microscaling Data Formats for Deep Learning</h2> <p><img src="https://github.com/user-attachments/assets/e1533c6c-54ca-47f8-946a-2d6fe7d08aef" alt="image"></p> <h3 id="introduction-2"><a href="#introduction-2" class="header-anchor">#</a> <strong>Introduction</strong></h3> <p>The rapid advancement of deep learning models has led to increased computational and storage costs.<br>
One approach to mitigating these costs is reducing bit-width precision in data formats, moving beyond traditional <strong>FP32</strong>  to lower-bit formats such as <strong>FP16, BFloat16, FP8, and INT8</strong>.<br>
However, per-tensor scaling in low-bit-width formats struggles with dynamic range limitations.<br> <strong>Microscaling (MX) data formats</strong>  introduce <strong>per-block scaling factors</strong>  to enhance efficiency, maintain accuracy, and ease deployment in AI hardware.</p> <h3 id="microscaling-mx-data-formats"><a href="#microscaling-mx-data-formats" class="header-anchor">#</a> <strong>Microscaling (MX) Data Formats</strong></h3> <p>MX formats encode numerical values in <strong>fixed-size blocks</strong> , where each block consists of:</p> <ul><li>A <strong>shared scaling factor (X)</strong></li> <li>Multiple <strong>narrow bit-width elements (Pi)</strong></li></ul> <p>This approach <strong>extends the dynamic range</strong>  beyond what per-tensor scaling allows, making sub-8-bit computations feasible.<br>
MX formats are <strong>hardware-efficient</strong>  while minimizing accuracy loss and <strong>ensuring seamless adoption in existing AI frameworks</strong>.</p> <h3 id="concrete-mx-formats"><a href="#concrete-mx-formats" class="header-anchor">#</a> <strong>Concrete MX Formats</strong></h3> <p>MX formats are categorized based on <strong>block size, scale format, and element bit-width</strong>.</p> <table><thead><tr><th>Format</th> <th>Block Size</th> <th>Scale Data Format</th> <th>Scale Bits</th> <th>Element Format</th> <th>Element Bit-width</th></tr></thead> <tbody><tr><td>MXFP8</td> <td>32</td> <td>E8M0</td> <td>8</td> <td>FP8 (E4M3/E5M2)</td> <td>8</td></tr> <tr><td>MXFP6</td> <td>32</td> <td>E8M0</td> <td>8</td> <td>FP6 (E2M3/E3M2)</td> <td>6</td></tr> <tr><td>MXFP4</td> <td>32</td> <td>E8M0</td> <td>8</td> <td>FP4 (E2M1)</td> <td>4</td></tr> <tr><td>MXINT8</td> <td>32</td> <td>E8M0</td> <td>8</td> <td>INT8</td> <td>8</td></tr></tbody></table> <h3 id="scalar-float-to-mx-format-conversion"><a href="#scalar-float-to-mx-format-conversion" class="header-anchor">#</a> <strong>Scalar Float to MX Format Conversion</strong></h3> <p>To convert floating-point data to an MX format, the <strong>shared scaling factor (X)</strong>  is computed based on the largest absolute value in a block.<br>
Each element is then <strong>normalized using X and quantized</strong>  to the desired format. The conversion follows a <strong>quantization algorithm</strong>  that:</p> <ol><li><strong>Determines the scaling exponent</strong>  from the maximum value in the block.</li> <li><strong>Computes X as a power of two</strong> .</li> <li><strong>Quantizes elements (Pi) based on X</strong> .</li></ol> <h3 id="compute-flow-and-training-pipeline"><a href="#compute-flow-and-training-pipeline" class="header-anchor">#</a> <strong>Compute Flow and Training Pipeline</strong></h3> <p>For deep learning workloads, <strong>dot-product operations (e.g., matrix multiplication, convolutions) are performed in MX formats</strong> , while <strong>non-dot operations (e.g., activations, normalization, residual add) remain in higher-precision formats like FP32 or BFloat16</strong> .<br>
Training involves keeping a <strong>master FP32 copy of weights</strong>  while performing compute-intensive operations in MX formats.<br> <strong>Quantization-aware fine-tuning</strong>  is often required for best accuracy, particularly for lower-bit formats like MXFP6 and MXFP4.</p> <p><strong>Experimental Results</strong> <strong>Inference</strong> MX data formats were tested across <strong>language models, vision transformers, speech recognition, and recommendation models</strong>.</p> <p><strong>Direct-Cast Inference (No Fine-Tuning)</strong></p> <ul><li><strong>MXINT8 performs nearly identically to FP32</strong>  across all tasks, making it a <strong>drop-in replacement</strong> .</li> <li><strong>MXFP8 and MXFP6 maintain good accuracy</strong> , but MXFP6 requires fine-tuning for best results.</li> <li><strong>MXFP4 suffers from significant accuracy loss</strong> , especially in complex models.</li></ul> <p><strong>Post-Training Quantization (PTQ) with Error Diffusion</strong></p> <ul><li>Error diffusion (similar to <strong>GPFQ-based post-training quantization</strong> ) helps recover accuracy.</li> <li><strong>MXFP6 achieves results close to FP32</strong>  after error diffusion.</li> <li><strong>MXFP4 remains significantly worse than FP32, limiting its practical use in inference</strong> .</li></ul> <p><strong>Finetuned Inference</strong></p> <ul><li><strong>MXFP6 achieves FP32-level accuracy after fine-tuning</strong> .</li> <li><strong>MXFP4 improves slightly but still lags behind</strong> .</li> <li>MXINT8 continues to serve as the most effective <strong>low-friction alternative to FP32</strong> .</li></ul> <p><strong>Generative Model Inference (GPT-3, LLaMA)</strong></p> <ul><li><strong>MXINT8 closely matches FP32 performance</strong>  on GPT-3 and LLaMA.</li> <li><strong>MXFP6 and MXFP8 perform well in most tasks</strong> , but some degradation is observed in complex benchmarks.</li> <li><strong>MXFP4 shows noticeable loss</strong> , especially in <strong>zero-shot settings</strong> .</li></ul> <p><strong>Training with MX Formats</strong>
For the <strong>first time, MX formats enable sub-8-bit training of large-scale transformers</strong>  with minimal accuracy loss.</p> <p><strong>Training with MXFP6</strong></p> <ul><li><strong>MXFP6 (E3M2) trains models with no accuracy drop compared to FP32</strong> .</li> <li>This represents the <strong>first demonstration of 6-bit training for large transformer models</strong>  without modifications to the training recipe.</li> <li><strong>Hyperparameters remain unchanged from FP32 training</strong>, making MXFP6 a practical choice.</li></ul> <p><strong>Training with MXFP4 + MXFP6</strong></p> <ul><li><strong>MXFP4 weights combined with MXFP6 activations/gradients</strong>  yield slightly worse performance but remain viable for training.</li> <li><strong>Loss curves show only a minor increase in training loss</strong> , proving feasibility.</li></ul> <h3 id="conclusion-2"><a href="#conclusion-2" class="header-anchor">#</a> <strong>Conclusion</strong></h3> <p>Microscaling (MX) data formats introduce <strong>per-block scaling</strong>  to <strong>reduce bit-width</strong>  while maintaining <strong>high accuracy, hardware efficiency, and seamless integration</strong> .</p> <h3 id="key-findings"><a href="#key-findings" class="header-anchor">#</a> Key findings:</h3> <ol><li>MXINT8 is an effective drop-in replacement for FP32 inference.</li> <li>MXFP6 enables sub-8-bit inference and training with minimal accuracy loss.</li> <li>MXFP4 combined with MXFP6 remains viable for training but suffers in inference.</li> <li>First-ever demonstration of training large generative models using 6-bit weights, activations, and gradients** .
MX formats offer a compelling path toward <strong>lower precision deep learning</strong>  without sacrificing model quality.</li></ol> <p><img src="https://github.com/user-attachments/assets/ca5c4914-cbf2-4cc2-8d80-eb423ef1aa10" alt="image"></p> <h3 id="three-sentence-summary"><a href="#three-sentence-summary" class="header-anchor">#</a> <strong>Three-Sentence Summary</strong></h3> <p>Microscaling (MX) data formats introduce <strong>per-block scaling factors</strong> , improving the efficiency and accuracy of sub-8-bit deep learning computations.</p> <p><strong>MXINT8 serves as a near-lossless drop-in replacement for FP32 inference</strong> , while <strong>MXFP6 enables large-scale deep learning models to be trained with sub-8-bit precision without altering training recipes</strong> .</p> <p>The results demonstrate that <strong>MX formats significantly reduce computational and storage costs while maintaining model performance</strong> , making them a strong alternative to traditional floating-point formats.</p> <hr> <h2 id="_14-33-spinquant-llm-quantization-with-learned-rotations"><a href="#_14-33-spinquant-llm-quantization-with-learned-rotations" class="header-anchor">#</a> 14. [33] SpinQuant: LLM quantization with learned rotations</h2> <h3 id="abstract-introduction"><a href="#abstract-introduction" class="header-anchor">#</a> Abstract &amp; Introduction</h3> <p>SpinQuant is a novel method for post-training quantization (PTQ) of Large Language Models (LLMs) that uses learned rotation matrices to enhance quantization accuracy while maintaining full-precision outputs.</p> <p><strong>Traditional PTQ struggles with outliers, which widen quantization ranges and reduce effective bit usage. Instead of relying on random rotations, SpinQuant learns rotation matrices that optimize quantization performance.</strong></p> <p>The method significantly reduces accuracy gaps in 4-bit quantization across weights, activations, and KV-cache, outperforming previous quantization methods like LLM-QAT, SmoothQuant, and QuaRot.</p> <p>Specifically, SpinQuant reduces the zero-shot reasoning accuracy gap on LLaMA-2 7B from 12.1 to 1.6 points, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points.</p> <h3 id="motivation-outlier-reduction"><a href="#motivation-outlier-reduction" class="header-anchor">#</a> Motivation &amp; Outlier Reduction</h3> <p>Quantization reduces memory and computation costs but suffers from outliers, which stretch the value distribution.</p> <p>Existing methods like mixed-precision quantization and weight-activation trade-offs attempt to mitigate this but remain suboptimal.</p> <p><strong>SpinQuant rotates activation and weight matrices to remove outliers, making them more Gaussian-like, thus improving quantization efficiency.</strong></p> <p>The paper demonstrates that some random rotations lead to better results than others, with a 13-point accuracy difference in zero-shot reasoning tasks.</p> <h3 id="methodology"><a href="#methodology" class="header-anchor">#</a> Methodology</h3> <p><img src="https://github.com/user-attachments/assets/5906ef52-5f44-4783-a131-2e5ea9eb26c6" alt="image"></p> <p>The method optimizes rotation matrices using Cayley SGD, which maintains orthonormality while minimizing the quantized network loss.</p> <p>This process enhances quantization robustness, significantly outperforming random rotations.</p> <p><img src="https://github.com/user-attachments/assets/421f725d-9051-49e6-b640-4db5b51ded7f" alt="image"></p> <h3 id="experiments-results"><a href="#experiments-results" class="header-anchor">#</a> Experiments &amp; Results</h3> <p>Experiments were conducted on LLaMA-2 (7B/13B/70B), LLaMA-3 (1B/3B/8B), and Mistral-7B, evaluated across zero-shot reasoning tasks and perplexity on WikiText2.</p> <h3 id="key-findings-2"><a href="#key-findings-2" class="header-anchor">#</a> Key findings:</h3> <p>SpinQuantₙₒₕₐₑd effectively closes the quantization gap in W4A8KV8 settings, making it comparable to QuIP# and OmniQuant.</p> <p>SpinQuantₕₐₑd achieves 64.0 average accuracy in W4A4KV4, reducing the accuracy gap to full precision to 2.9 points (LLM-QAT had a 22-point gap).</p> <p>Against QuaRot, SpinQuant improves accuracy by up to 28.6 points in extreme quantization settings.</p> <p>Speed analysis shows that SpinQuantₕₐₑd incurs only an 8% latency overhead, making it a practical approach.</p> <h3 id="conclusions"><a href="#conclusions" class="header-anchor">#</a> Conclusions</h3> <p>SpinQuant is the first PTQ method to optimize learned rotation matrices, offering:</p> <ul><li>Significant improvements over existing quantization techniques by reducing outliers.</li> <li>State-of-the-art performance in 4-bit quantization settings.</li> <li>Compatibility with advanced weight quantization methods like GPTQ.</li></ul> <p>Its learned rotations make quantized LLMs more robust, reducing the performance gap to full-precision models, making low-bit LLM inference practical.</p> <h3 id="three-sentence-summary-2"><a href="#three-sentence-summary-2" class="header-anchor">#</a> Three-Sentence Summary</h3> <p>SpinQuant introduces learned rotation matrices to mitigate outliers in weight and activation distributions, improving LLM quantization efficiency.</p> <p>Using Cayley SGD optimization, it significantly reduces the accuracy gap in 4-bit quantization, outperforming SmoothQuant, LLM-QAT, and QuaRot.</p> <p>Experiments on LLaMA-2, LLaMA-3, and Mistral-7B show state-of-the-art quantization performance, with SpinQuant narrowing the accuracy gap to full precision by up to 45.1% relative to QuaRot.</p> <hr> <h2 id="_17-35-fp8-versus-int8-for-efficient-deep-learning-inference"><a href="#_17-35-fp8-versus-int8-for-efficient-deep-learning-inference" class="header-anchor">#</a> 17. [35] FP8 versus INT8 for efficient deep learning inference</h2> <p>This paperexplores the efficiency and accuracy of using FP8 (8-bit floating point) and INT8 (8-bit integer) formats for deep learning inference, particularly on edge devices.</p> <p>The key points and findings of the paper are summarized as follows:</p> <h3 id="_1-introduction-and-motivation-2"><a href="#_1-introduction-and-motivation-2" class="header-anchor">#</a> 1. Introduction and Motivation</h3> <p>The paper discusses the growing interest in using FP8 for neural network training, especially with Nvidia's introduction of FP8 in their Hopper architecture GPUs.</p> <p>While FP8 is being considered for training, the paper focuses on its implications for efficient inference on edge devices, where INT8 is commonly used due to its efficiency.</p> <p>The authors question whether training networks in FP8 and deploying them in the same format could bypass the need for quantization, which is currently required when converting FP32/FP16 models to INT8.</p> <h3 id="_2-hardware-considerations"><a href="#_2-hardware-considerations" class="header-anchor">#</a> 2. Hardware Considerations</h3> <p>The paper argues that FP8 is less efficient than INT8 in terms of hardware area and energy consumption. FP8 requires 50% more area and energy compared to INT8, making it less suitable for efficient inference.</p> <p>Floating-point operations are inherently more complex and costly in hardware compared to integer operations, especially when considering the need for floating-point accumulators.</p> <p>The authors highlight that FP8 implementations often involve mixed precision (e.g., FP16 for activations), which can lead to inefficiencies, particularly in networks with large activation tensors.</p> <h3 id="_3-accuracy-comparison"><a href="#_3-accuracy-comparison" class="header-anchor">#</a> 3. Accuracy Comparison</h3> <p>The paper provides a theoretical and empirical comparison of FP8 and INT8 formats in terms of network accuracy.</p> <p>The key difference between FP8 and INT8 lies in their ability to handle outliers. FP8, with its exponent bits, can better represent outliers, while INT8 is more efficient for well-behaved, Gaussian-like distributions.</p> <blockquote><p>the only significant difference between the two formats is in their ability to capture outliers.</p></blockquote> <p>❗ In post-training quantization (PTQ), INT8 generally performs better for networks without significant outliers, while FP8-E4 (4 exponent bits) is better for networks with outliers, such as <strong>ransformers</strong>.</p> <p>❗ In quantization-aware training (QAT), INT8 often outperforms FP8, as training can reduce the impact of outliers, making INT8 more accurate and efficient.</p> <h3 id="_4-transformer-networks"><a href="#_4-transformer-networks" class="header-anchor">#</a> 4. Transformer Networks</h3> <p>Transformer networks, particularly BERT, <strong>exhibit significant outliers in certain layers, making FP8-E4 more accurate in PTQ settings.</strong></p> <p>However, the paper argues that these outlier issues can be mitigated with techniques like mixed precision (W8A16) or quantization-aware training, allowing INT8 to achieve similar accuracy without the hardware inefficiencies of FP8.</p> <p><strong>FP8-E4 vs FP8-E5</strong></p> <p>❗ It’s all about the outliers. If a distribution has very significant outliers, the FP8-E4/FP8-E5 format is more accurate.</p> <p>For well-behaved networks without many outliers, the INT8 format is significantly more accurate in the PTQ setting than FP8-E4 and FP8-E5.</p> <p>We also see that FP8-E5 is never the best format for inference; even for the transformer layers with significant outliers, the FP8-E4 format is better.</p> <p>For some computer vision networks, INT8 is better; for some networks with significant outliers, the FP8-E4/FP8-E5 formats are better.</p> <p>Purely taking these results into account, the FP8-E4 format looks comparatively worse than FP8-E2 and FP8-E3.</p> <p>Combining these findings with the hardware implementation costs, the FP8-E4 format itself looks like it is a
worse choice than its lower-exponent bit brethren, which are both cheaper hardware-wise and more accurate.</p> <p>If anything, the <strong>FP8-E3 format</strong> stands out positively in this accuracy analysis compared to
other FP formats.</p> <p><strong>However, Deepseek FP8 use E4M4 for both forward and backwards computation.</strong></p> <p><strong>QAT versus PTQ</strong></p> <p><img src="https://github.com/user-attachments/assets/3bc7ef7b-959d-4869-ac98-855451453709" alt="image"></p> <p>One surprising trend is that the INT8 results improve more than their PTQ baseline than their FP8 counterparts.</p> <p>There is a good reason for this; again, it’s about the outliers.</p> <p>When performing QAT, outliers are clipped and do not receive a gradient.</p> <p>The clipped weights/activations then tend towards the clipping threshold due to regularization.</p> <p>But most importantly, with the outliers clipped, the network learns weights that still perform well despite the outliers being removed.</p> <p>when training the quantization parameters with a method like LSQ (Esser et al. (2020)), the network can learn to make the ranges smaller so as to find a better trade-off between the clipping and quantization errors.</p> <p>The smaller the range, the more accurate your quantized representation will be.</p> <p>This is especially the case for INT8, where the sensitivity to the quantization ranges is much larger than the floating-point formats with more exponent bits that are naturally more resistant to outliers.</p> <p>This way, the INT8 format benefits significantly more from QAT than the FP formats.</p> <p><strong>QAT</strong></p> <p>Numerical representation chosen for training (e.g., FP8-E4) does not dictate how the distributions of weights and activations form.</p> <p>Instead, these distributions are primarily shaped by the overall training process, including factors like regularization, optimizer settings, and initialization.</p> <p><strong>QAT Transformer</strong></p> <p>There are significant outliers in the summation going into the layer-norm in some of the fully connected modules,  especially in the final layers of the network.</p> <p>These outliers force the attention mechanism in the next layer to pay attention to some meaningless tokens – like <strong>sentence separator tokens, periods, or commas</strong> – that occur in the text, causing that specific token to not update significantly.</p> <p>Simply clipping these outlier reduces accuracy significantly.</p> <p>Only a few layers are the best in FP8-E4. The other layers find a lower MSE error with the FP8-E2 and FP8-E3 formats.</p> <p>In the most naive PTQ setting, the FP8-E4 format performs better than INT8.</p> <p><strong>Comparison to other work</strong></p> <blockquote><p><strong>The problems with the quantization of gradients put forth in the paper are well-known and have been
addressed in many works in the past. (Sun et al. (2019); Gupta et al. (2015)). Many works have shown
the necessity of stochastic rounding and proper range setting for the backward pass that alleviate
these issues and make INT8 for gradients work just as well (Sun et al. (2019).</strong>
What is this?</p></blockquote> <p>Transformer models can be executed in INT8 as well, both with PTQ and QAT.</p> <p>Finally, the only comparison with the INT8 format comes in the form of comparing transformer-based language models in the PTQ setting.</p> <p>However, as argued, these problems are easily fixable for transformer networks, making them able to execute entirely in INT8 or in mixed precision with a small number of activations in INT16.</p> <h3 id="_5-comparison-to-other-work"><a href="#_5-comparison-to-other-work" class="header-anchor">#</a> 5. Comparison to Other Work</h3> <p>The authors compare their findings with other works, such as those from Nvidia, Arm, and Intel, and Graphcore, which also explore FP8 for training.</p> <p>They find that their results are consistent with these works but provide a more comprehensive comparison between FP8 and INT8.</p> <p>The paper highlights that other works often omit critical comparisons, such as the hardware efficiency of FP8 versus INT8, and the impact of mixed precision on inference performance.</p> <p>Several works have shown that even in the INT8 PTQ setting you can get back your original accuracy with any of several possible tricks</p> <h3 id="_6-fp8-to-int8-conversion"><a href="#_6-fp8-to-int8-conversion" class="header-anchor">#</a> 6. FP8 to INT8 Conversion</h3> <p>The paper explores the feasibility of converting FP8-trained networks to INT8.</p> <p>For networks without significant outliers, the conversion is straightforward and can even improve accuracy.</p> <p><strong>For networks with outliers, such as transformers, the conversion to INT8 may degrade accuracy, but this can be mitigated with quantization-aware training.</strong></p> <h3 id="_7-int-quantization-paradigm"><a href="#_7-int-quantization-paradigm" class="header-anchor">#</a> 7. INT Quantization Paradigm</h3> <p>The authors advocate for the use of INT8 and INT4 formats for efficient inference, as they offer better hardware efficiency and accuracy for most networks.</p> <p>They present a quantization paradigm where INT16 is used for high accuracy, INT8 for most networks, and INT4 for further efficiency, especially in weight-bound networks like large language models.</p> <h3 id="_8-conclusion"><a href="#_8-conclusion" class="header-anchor">#</a> 8. Conclusion</h3> <p>The paper concludes that FP8 is not a suitable replacement for INT8 in efficient deep learning inference.</p> <p>While FP8 can handle outliers better in certain cases, the hardware inefficiencies and the availability of techniques to mitigate outlier issues in INT8 make it a less attractive option.</p> <p>The authors recommend using INT8 and INT4 formats for efficient on-device inference, as they provide the best trade-off between accuracy and efficiency.</p> <h3 id="key-takeaways"><a href="#key-takeaways" class="header-anchor">#</a> Key Takeaways:</h3> <p>FP8 is less efficient than INT8 in terms of hardware area and energy consumption.</p> <p>INT8 is more accurate for most networks, especially after quantization-aware training, which can reduce the impact of outliers.</p> <p><strong>FP8 is only beneficial in specific cases, such as transformer networks with significant outliers, but these issues can be addressed with INT8 using mixed precision or QAT.</strong></p> <p>INT4 and INT8 are recommended for efficient inference, offering a better balance of accuracy and hardware efficiency compared to FP8.</p> <p>Overall, the paper provides a comprehensive analysis of the trade-offs between FP8 and INT8, concluding that INT8 remains the superior choice for efficient deep learning inference on edge devices.</p> <hr> <h2 id="_16"><a href="#_16" class="header-anchor">#</a> 16.</h2> <hr> <h2 id="_17-434-integer-quantization-for-deep-learning-inference-principles-and-empirical-evaluation"><a href="#_17-434-integer-quantization-for-deep-learning-inference-principles-and-empirical-evaluation" class="header-anchor">#</a> 17. [434] Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation</h2> <p>This paper explores integer quantization techniques for deep learning inference, which help reduce model size and improve computational efficiency by leveraging high-throughput integer math pipelines.</p> <p>The authors provide a mathematical foundation for different quantization choices and evaluate their empirical performance across multiple deep learning models spanning vision, speech, and language domains.</p> <p>The study focuses on 8-bit integer quantization, demonstrating that accuracy loss can be minimized to within 1% of the floating-point baseline, even for challenging models like MobileNets and BERT-large.</p> <h3 id="key-highlights"><a href="#key-highlights" class="header-anchor">#</a> Key Highlights:</h3> <p>Benefits of Integer Quantization</p> <ul><li>Integer operations offer up to 16× speedup over FP32 on NVIDIA Turing GPUs.</li> <li>Smaller word sizes reduce memory bandwidth pressure and improve cache utilization.</li></ul> <h3 id="quantization-methods"><a href="#quantization-methods" class="header-anchor">#</a> Quantization Methods</h3> <ul><li>Affine Quantization: Uses a scale factor and zero-point but adds computational overhead.</li> <li>Scale Quantization: More efficient as it avoids additional computations by using only a scale factor.</li></ul> <h3 id="post-training-quantization-ptq-vs-quantization-aware-training-qat"><a href="#post-training-quantization-ptq-vs-quantization-aware-training-qat" class="header-anchor">#</a> Post Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)</h3> <ul><li>PTQ quantizes weights and activations after training but may lead to accuracy degradation for some models.</li> <li>QAT fine-tunes the network with quantization effects included, yielding better results, especially for MobileNets and Transformers.</li></ul> <p>Per-channel quantization for weights and per-tensor quantization for activations is recommended for accuracy and performance.</p> <h3 id="calibration-methods"><a href="#calibration-methods" class="header-anchor">#</a> Calibration Methods</h3> <p>Max, entropy, and percentile-based (99.9%-99.999%) calibrations are tested.</p> <p>No single calibration is best for all networks; entropy and 99.99% percentile provide optimal accuracy for most models.</p> <h3 id="optimizations-to-reduce-accuracy-loss"><a href="#optimizations-to-reduce-accuracy-loss" class="header-anchor">#</a> Optimizations to Reduce Accuracy Loss</h3> <ul><li>Partial Quantization: Leaves sensitive layers in floating point, improving accuracy while retaining performance benefits.</li> <li>Learning Quantization Parameters: Fine-tuning the quantization ranges (PACT method) slightly improves accuracy but is not always necessary for int8 quantization.</li></ul> <p><strong>ELU Clipping: For BERT, modifying the GELU activation range significantly enhances post-training quantization results.</strong></p> <h3 id="recommended-workflow"><a href="#recommended-workflow" class="header-anchor">#</a> Recommended Workflow</h3> <ul><li>Start with PTQ using max, entropy, and percentile calibrations.</li> <li>If accuracy loss is high, use Partial Quantization to leave sensitive layers unquantized.</li> <li>If further accuracy recovery is needed, perform QAT with pre-determined best calibration.</li></ul> <h3 id="final-3-sentence-summary"><a href="#final-3-sentence-summary" class="header-anchor">#</a> Final 3-Sentence Summary:</h3> <p>This paper presents a quantization workflow for deep learning inference, balancing speed and accuracy by leveraging int8 quantization with scale quantization for weights and activations.</p> <p>It demonstrates that a combination of post-training quantization, partial quantization, and quantization-aware training ensures that accuracy remains within 1% of floating-point models, even for challenging architectures like MobileNets and BERT.</p> <p>The proposed methods significantly improve inference efficiency, making integer quantization a practical approach for deploying neural networks on hardware accelerators.</p> <hr> <h2 id="_20-73-fp8-quantization-the-power-of-the-exponent"><a href="#_20-73-fp8-quantization-the-power-of-the-exponent" class="header-anchor">#</a> 20.[73] FP8 Quantization: The Power of the Exponent</h2> <p>This paper investigates FP8 quantization for neural network inference, comparing it with traditional INT8 quantization.</p> <p>The authors analyze different FP8 format configurations, focusing on the trade-off between exponent and mantissa bits.</p> <p>The study includes theoretical analysis, post-training quantization (PTQ), and quantization-aware training (QAT) across various deep learning models.</p> <p><strong>Key findings indicate that FP8 outperforms INT8 in post-training quantization, particularly for networks with activation outliers, such as Transformers.</strong></p> <p>However, in quantization-aware training, the accuracy difference between INT8 and FP8 diminishes as the network learns to adapt to the quantization scheme.</p> <h3 id="key-takeaways-2"><a href="#key-takeaways-2" class="header-anchor">#</a> Key Takeaways:</h3> <ul><li>FP8 vs. INT8: FP8 offers an additional degree of freedom through exponent bits, which helps manage outliers better than INT8, improving post-training quantization accuracy.</li> <li>Optimal Format Selection: <strong>The choice of FP8 format (e.g., 5M2E vs. 4M3E) depends on the severity of outliers in a network,</strong> with higher exponent bits benefiting models with significant activation outliers.</li> <li>QAT Reduces Differences: Quantization-aware training helps networks adapt to the quantization format, making INT8 and FP8 perform similarly in trained models.</li></ul> <h3 id="three-sentence-summary-3"><a href="#three-sentence-summary-3" class="header-anchor">#</a> Three-Sentence Summary</h3> <p>This paper explores FP8 quantization and its advantages over INT8, showing that FP8 provides better accuracy in post-training quantization due to its ability to handle activation outliers more effectively.</p> <p>Through analytical and empirical studies, the authors determine that the optimal FP8 configuration depends on the balance between exponent and mantissa bits, with higher exponent bits benefiting networks with larger outliers.</p> <p>However, in quantization-aware training, the differences between FP8 and INT8 diminish as the network learns to optimize within the quantization scheme.</p> <blockquote><p>We validated the FP8 format for many networks in a post-training quantization setting, showing that generally for neural networks the 5M2E and 4M3E FP8 format works the best, and that for networks with more outliers like transformers increasing the number of exponent bits works best.</p></blockquote> <hr> <h2 id="_18-y2024-integer-scale-a-free-lunch-for-faster-fine-grained-quantization-of-llms"><a href="#_18-y2024-integer-scale-a-free-lunch-for-faster-fine-grained-quantization-of-llms" class="header-anchor">#</a> 18. [Y2024]Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs</h2> <h3 id="overview"><a href="#overview" class="header-anchor">#</a> Overview</h3> <p>This paper deals with post-training quantization for large language models (LLMs), focusing on a technique called fine-grained quantization.
While fine-grained quantization (group-wise scales) generally retains better accuracy at lower bit-widths than coarse-grained methods, it comes with a major drawback in slower inference.
The authors identify that a large part of this overhead comes from repeatedly converting integer multiplication results into floating-point to apply each group’s “float scale.”
They therefore propose an Integer Scale approach that removes most of these costly conversions by storing and applying each group’s scale in integer form.</p> <h3 id="key-motivation"><a href="#key-motivation" class="header-anchor">#</a> Key Motivation</h3> <ol><li>Accuracy vs. Speed Trade-off:</li></ol> <ul><li>Fine-grained quantization (grouping weights or activations into small blocks) achieves higher accuracy than coarse-grained approaches, especially when pushing to lower bit-widths like W4A8 or W4A4 (weights 4-bit, activations 8 or 4-bit).</li> <li>However, the group-wise scaling factors in floating-point form introduce numerous conversions (e.g., INT32 → FP32 → multiply by FP32 scale → back to INT32), which dramatically reduce inference speed.</li></ul> <ol start="2"><li>Integer Scale as a “Free Lunch”:</li></ol> <ul><li>By transforming each group’s floating-point scale into an integer, the paper’s method avoids a lot of repetitive float conversions within the GEMM kernels.</li> <li>The authors call it a “free lunch” because it requires no additional calibration or training—just a straightforward conversion of float scales into integer form with a suitable integer “amplifier.”</li></ul> <h3 id="proposed-method-integer-scale"><a href="#proposed-method-integer-scale" class="header-anchor">#</a> Proposed Method: Integer Scale</h3> <ol><li>Integer Amplifier (α):</li></ol> <ul><li>Since the learned scales from fine-grained quantization typically lie between 0 and 1, the authors multiply them by a power-of-two factor (e.g., 2^10 = 1024) to map them into integer space without introducing big rounding errors.</li> <li>They then store these integer scales and use simple integer multiply operations inside the kernel, followed by one final float conversion for the output—rather than one float conversion per group multiplication.</li></ul> <ol start="2"><li>Modified GEMM Kernel:</li></ol> <ul><li>The new kernel accumulates partial sums at integer precision and multiplies by the integer scales on the fly.</li> <li>This leads to far fewer float conversions and yields a noticeable speedup (often well above 1.5× faster) relative to the standard float-scale approach.</li></ul> <ol start="3"><li>Stability &amp; Overflow:</li></ol> <ul><li>The paper shows that choosing an amplifier (like 2^10) is sufficient to preserve accuracy while keeping integer operations safely in INT32 range—there are no overflow issues in tested scenarios.</li> <li>The authors also show that per-layer “heuristic search” for the best amplifier can be replaced by a single fixed power-of-two amplifier (e.g., 1024), with negligible difference in accuracy.</li></ul> <p><img src="https://github.com/user-attachments/assets/57599852-ef8a-4c22-b69d-b3ed0af743e7" alt="image"></p> <h3 id="results"><a href="#results" class="header-anchor">#</a> Results</h3> <ol><li>Accuracy Retention:</li></ol> <ul><li>On tasks like LAMBADA, C4, WikiText-2, and Common Sense QA, the Integer Scale method achieves nearly the same (or slightly better) accuracy compared to float-scale baselines (GPTQ, AWQ, Omniquant) under the same fine-grained quantization setup.</li> <li>It also helps quantize more challenging models like LLaMA-3 and Mixtral 8×7B at 4-bit weights without the large drop typical of naive methods.</li></ul> <ol start="2"><li>Speed Gains:</li></ol> <ul><li>The central benefit is that Integer Scale speeds up fine-grained quantization kernels significantly.</li> <li>In many experiments, the method achieves:
<ul><li>Up to 1.85× acceleration vs. float-scale fine-grained kernels,</li> <li>Up to 1.17× faster than certain well-optimized W4A16 kernels (e.g., Marlin),</li> <li>Up to 2.13× faster than the model’s FP16 baseline (depending on batch size and model).</li></ul></li></ul> <ol start="3"><li>Comparison to Other Libraries:</li></ol> <ul><li>Compared to QServe (another W4A8 system) or Marlin’s W4A16, the Integer Scale approach often yields higher or comparable throughput, especially under typical inference batch sizes (32, 64, 128).</li></ul> <h3 id="practical-takeaways"><a href="#practical-takeaways" class="header-anchor">#</a> Practical Takeaways</h3> <ul><li><strong>Plug-and-Play for Existing Methods</strong>: Integer Scale is designed to be a drop-in replacement for float scales in fine-grained quantization. You keep the same group sizes, bit widths, etc., just multiply the group scales by a power-of-two to store them as integers.</li> <li><strong>No Extra Training</strong>: Unlike some other quantization techniques that require knowledge distillation or fine-tuning, Integer Scale only changes how scales are stored and applied. This means no additional compute overhead for calibrating or adjusting the model.</li> <li><strong>Balancing Memory- and Compute-Bound Scenarios</strong>: The paper emphasizes that as batch size grows, the gains might change (memory-bound vs. compute-bound). Still, they show consistent improvements in typical inference settings.</li></ul> <h3 id="key-parts-of-the-paper-in-3-sentences"><a href="#key-parts-of-the-paper-in-3-sentences" class="header-anchor">#</a> Key Parts of the Paper in 3 Sentences</h3> <ul><li>The authors pinpoint that existing fine-grained quantization is slowed down by many float conversions during inference.</li> <li>They propose “Integer Scale,” which transforms per-group float scales to integer scales (with a power-of-two amplifier), avoiding most of the costly conversions.</li> <li>As a result, they achieve up to 2× speed boost while retaining nearly the same quantization accuracy on a broad range of large language models.</li></ul> <hr> <h2 id="_19-121-training-high-performance-and-large-scale-deep-neural-networks-with-full-8-bit-integers"><a href="#_19-121-training-high-performance-and-large-scale-deep-neural-networks-with-full-8-bit-integers" class="header-anchor">#</a> 19. [121] Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers</h2> <h3 id="_1-introduction-4"><a href="#_1-introduction-4" class="header-anchor">#</a> <strong>1. Introduction</strong></h3> <p>Deep neural networks (DNNs) have achieved remarkable success in fields such as image processing, object detection, and natural language processing.</p> <p>However, training these models requires extensive floating-point (FP) operations, leading to high memory, compute, and energy costs.</p> <p>DNN quantization has been explored as a solution to this problem, primarily focusing on inference quantization (e.g., BWN, XNOR-Net).</p> <p>Recent advancements extend quantization to training, but existing methods still leave parts of the computation in high-precision floating-point (e.g., FP8, FP16) or do not quantize Batch Normalization (BN).</p> <p>The major challenges in achieving full quantization include:</p> <ul><li><strong>Incomplete quantization</strong> : Some parts of the model remain in floating-point, limiting memory and compute savings.</li> <li><strong>Unquantized Batch Normalization (BN)</strong> : BN is critical for training stability but is often left in floating-point.</li> <li><strong>Lack of a unified low-bit training framework</strong> : No existing method successfully trains large-scale models with only low-bit integer operations.</li></ul> <p>This work introduces <strong>WAGEUBN</strong> , a unified <strong>INT8 training framework</strong>  that quantizes all major operations, including:</p> <ul><li><strong>Weights (W)</strong></li> <li><strong>Activations (A)</strong></li> <li><strong>Gradients (G)</strong></li> <li><strong>Errors (E)</strong></li> <li><strong>Updates (U)</strong></li> <li><strong>Batch Normalization (BN)</strong></li> <li><strong>Momentum optimizer</strong></li></ul> <h3 id="_2-related-work"><a href="#_2-related-work" class="header-anchor">#</a> <strong>2. Related Work</strong></h3> <h4 id="_2-1-inference-quantization"><a href="#_2-1-inference-quantization" class="header-anchor">#</a> <strong>2.1. Inference Quantization</strong></h4> <p>Inference quantization aims to reduce the memory and compute cost of DNN inference by converting FP operations to bit-wise integer operations. Some key works include:</p> <ul><li><strong>BWN (Binary Weight Networks)</strong> : Quantizes only weights to {-1,1}.</li> <li><strong>XNOR-Net</strong> : Quantizes both weights and activations to binary values.</li> <li><strong>ADMM-based Quantization</strong> : Compresses models via alternating direction method of multipliers.</li> <li><strong>FP8/INT16-based Methods</strong> : Reduce bit-width to maintain accuracy.</li></ul> <p>However, inference quantization only focuses on the forward pass and does not address the backward pass needed for training.</p> <h4 id="_2-2-training-quantization"><a href="#_2-2-training-quantization" class="header-anchor">#</a> <strong>2.2. Training Quantization</strong></h4> <p>Training quantization extends quantization to the backward pass (gradients and updates). Key approaches include:</p> <ul><li><strong>DoReFa-Net</strong> : Uses low-bit activations, weights, and gradients but retains FP elements.</li> <li><strong>MP (Mixed Precision)</strong> : Uses FP16 for training but is not purely integer-based.</li> <li><strong>FP8 Training</strong> : Reduces training precision to FP8 but retains FP operations in BN.</li> <li><strong>QBP2</strong> : Uses 8-bit INT for weights, activations, and errors, but gradients remain FP.</li> <li><strong>WAGE</strong> : The most complete prior work, quantizing W, A, G, E, and U, but it lacks BN layers, making it unsuitable for large-scale DNNs.</li></ul> <p><img src="https://github.com/user-attachments/assets/2d177f82-6d45-424e-9801-3bf3b85c3ca8" alt="image"></p> <h3 id="_3-wageubn-framework"><a href="#_3-wageubn-framework" class="header-anchor">#</a> <strong>3. WAGEUBN Framework</strong></h3> <h4 id="_3-1-key-contributions"><a href="#_3-1-key-contributions" class="header-anchor">#</a> <strong>3.1. Key Contributions</strong></h4> <ul><li>Fully quantizes <strong>all</strong>  training data paths (W, A, G, E, U, BN, and Momentum).</li> <li>Introduces three custom quantization functions for different training components.</li> <li>Quantizes Batch Normalization (BN) for the first time.</li> <li>Applies INT8 quantization to large-scale networks like ResNet on ImageNet.</li></ul> <h4 id="_3-2-straight-through-estimator-ste"><a href="#_3-2-straight-through-estimator-ste" class="header-anchor">#</a> <strong>3.2. Straight-Through Estimator (STE)</strong></h4> <p>Quantization introduces a <strong>non-differentiability problem</strong> , making gradient updates challenging.</p> <p><strong>STE</strong>  is used to approximate gradients during backpropagation:
$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial x_q}$$</p> <p>This method allows training to proceed despite non-differentiability.</p> <h4 id="_3-3-quantization-functions"><a href="#_3-3-quantization-functions" class="header-anchor">#</a> <strong>3.3. Quantization Functions</strong></h4> <p>The framework introduces three quantization functions tailored for different data types:</p> <ol><li><strong>Direct Quantization</strong> : Used for <strong>weights, activations, and BN parameters</strong>.
$$Q(x, k) = \frac{\text{round}(x \cdot 2^{k-1})}{2^{k-1}}$$</li></ol> <p>This function approximates floating-point values to the nearest discrete integer.</p> <ol start="2"><li><strong>Constant Quantization</strong> : Used for <strong>gradients</strong>  to ensure a fixed update bit-width.
$$CQ(x) = \frac{Sd(x)}{2^{k-1}}$$</li></ol> <p>This function scales data dynamically to avoid excessive precision loss.</p> <ol start="3"><li><strong>Shift Quantization</strong> : Used for <strong>errors</strong> , which are typically small-magnitude values.
$$SQ(x, k) = R(x) \cdot \text{clip}(Q(\text{Norm}(x), k), -1, 1)$$</li></ol> <p>This function ensures errors maintain a meaningful range.</p> <h4 id="_3-4-quantized-training-steps"><a href="#_3-4-quantized-training-steps" class="header-anchor">#</a> <strong>3.4. Quantized Training Steps</strong></h4> <p>The framework quantizes the entire training process:</p> <ul><li><strong>Forward Pass:</strong> <ul><li>Quantizes inputs, applies INT8 convolutions, quantizes BN, and applies INT8 activation.</li></ul></li> <li><strong>Backward Pass:</strong> <ul><li>Uses INT8 gradients and error propagation.</li> <li>Applies INT8 momentum optimization.</li> <li>Uses fixed-point updates for weight adjustments.</li></ul></li> <li><strong>Momentum Quantization:</strong> <ul><li>Conventional optimizers like Adam/Momentum use floating-point accumulations.</li> <li>WAGEUBN constrains them to fixed-point INT8.</li></ul></li></ul> <h3 id="_4-results"><a href="#_4-results" class="header-anchor">#</a> <strong>4. Results</strong></h3> <h4 id="_4-1-accuracy-evaluation"><a href="#_4-1-accuracy-evaluation" class="header-anchor">#</a> <strong>4.1. Accuracy Evaluation</strong></h4> <p>The framework was tested on <strong>ResNet18/34/50 with ImageNet</strong>.
Two versions were evaluated:</p> <ol><li><strong>Full 8-bit INT</strong> : All computations use 8-bit integers.</li> <li><strong>16-bit E2 Variant</strong> : Uses 16-bit error gradients to improve convergence.
| Model | Vanilla FP32 | WAGEUBN (16-bit E2) | WAGEUBN (Full 8-bit) |
| --- | --- | --- | --- |
| ResNet18 | 68.70% | 67.40% | 64.79% |
| ResNet34 | 71.99% | 68.50% | 67.63% |
| ResNet50 | 74.66% | 69.07% | 67.95% |</li></ol> <ul><li><strong>Accuracy loss is minimal</strong>  (~3-5% top-1 accuracy).</li> <li><strong>16-bit E2 improves accuracy</strong>  over pure 8-bit training.</li> <li><strong>Comparable accuracy to FP8-based methods</strong>.</li></ul> <h4 id="_4-2-efficiency-gains"><a href="#_4-2-efficiency-gains" class="header-anchor">#</a> <strong>4.2. Efficiency Gains</strong></h4> <p>WAGEUBN significantly reduces hardware overhead compared to FP32:</p> <table><thead><tr><th>Precision</th> <th>Compute Speedup</th> <th>Power Reduction</th> <th>Circuit Area Reduction</th></tr></thead> <tbody><tr><td>INT8</td> <td>3× - 9×</td> <td>10× - 30×</td> <td>9× - 30×</td></tr> <tr><td>FP8</td> <td>0.73×</td> <td>0.31×</td> <td>0.4×</td></tr> <tr><td>FP16</td> <td>0.58×</td> <td>0.4×</td> <td>0.4×</td></tr></tbody></table> <ul><li><strong>Memory is reduced by 4×</strong> .</li> <li><strong>Computation is up to 9× faster</strong> .</li> <li><strong>Power usage is up to 30× lower</strong> .</li></ul> <h3 id="_5-analysis"><a href="#_5-analysis" class="header-anchor">#</a> <strong>5. Analysis</strong></h3> <ul><li><strong>Batch Size Sensitivity</strong> : WAGEUBN works best with batch sizes ≥32. Smaller batches lead to higher accuracy loss.</li> <li><strong>Error Gradient Sensitivity</strong> : The <strong>8-bit Flag QE2 method</strong>  significantly improves accuracy over simple 8-bit quantization.</li> <li><strong>Quantization Impact</strong> :
<ul><li><strong>BN and Errors are the most sensitive to precision loss</strong> .</li> <li><strong>Weights and activations are more robust to INT8 constraints</strong> .</li></ul></li></ul> <h3 id="_6-conclusion-wageubn-is-the-first-complete-int8-quantization-framework-for-training-large-scale-dnns-it-achieves"><a href="#_6-conclusion-wageubn-is-the-first-complete-int8-quantization-framework-for-training-large-scale-dnns-it-achieves" class="header-anchor">#</a> <strong>6. Conclusion</strong> WAGEUBN is the <strong>first complete INT8 quantization framework</strong>  for training large-scale DNNs. It achieves:</h3> <ul><li><strong>End-to-end INT8 training</strong>  (including BN and optimizers).</li> <li><strong>Competitive accuracy with significant hardware efficiency improvements</strong> .</li> <li><strong>Potential for online learning on energy-efficient devices</strong> .
Future work includes specialized <strong>hardware architectures</strong>  to fully exploit WAGEUBN’s benefits.</li></ul> <h3 id="three-sentence-summary-4"><a href="#three-sentence-summary-4" class="header-anchor">#</a> <strong>Three-Sentence Summary</strong></h3> <p>WAGEUBN is a <strong>fully quantized INT8 training framework</strong>  for large-scale deep learning, covering all data paths (W, A, G, E, U, BN, and Momentum).</p> <p>By introducing novel quantization functions and INT8 batch normalization, it <strong>reduces memory by 4×, accelerates computation by up to 9×, and cuts power usage by 30×</strong> , while achieving <strong>comparable accuracy to FP-based models</strong></p> <p>This work establishes a <strong>scalable and efficient approach for energy-efficient AI hardware and online learning</strong> .</p> <hr> <h2 id="_20-381-i-bert-integer-only-bert-quantization"><a href="#_20-381-i-bert-integer-only-bert-quantization" class="header-anchor">#</a> 20. [381] I-BERT: Integer-only BERT Quantization</h2> <h3 id="challenges-addressed"><a href="#challenges-addressed" class="header-anchor">#</a> Challenges Addressed</h3> <p>Inefficiency of Transformer-Based Models: BERT and RoBERTa achieve high accuracy but have high memory, latency, and power costs, making them difficult to deploy on edge devices and data centers.</p> <p>Limitations of Previous Quantization Approaches: Prior Transformer quantization methods rely on floating-point arithmetic, preventing efficient execution on integer-only hardware like ARM Cortex-M processors and Turing Tensor Cores.</p> <p>Difficulty in Handling Non-Linear Functions: Existing integer-only quantization techniques are mainly designed for CNNs with piece-wise linear functions like ReLU. Transformers use complex non-linear functions (GELU, Softmax, LayerNorm), which are hard to process using integer arithmetic without significant accuracy loss.</p> <p><img src="https://github.com/user-attachments/assets/da7b8969-c3fe-4db7-a0a0-8d1ed1210382" alt="image"></p> <h3 id="solution-i-bert-approach"><a href="#solution-i-bert-approach" class="header-anchor">#</a> Solution - I-BERT Approach:</h3> <p>Integer-Only Approximation for Non-Linear Functions:</p> <ul><li>GELU: Approximated using a second-order polynomial (i-GELU), avoiding floating-point computation while maintaining accuracy.</li> <li>Softmax: Transformed into a stable integer-friendly form using logarithm and bit-shift operations (i-exp).</li> <li>LayerNorm: Computed using an integer-only square root algorithm.</li></ul> <p>End-to-End Integer Execution:</p> <ul><li>MatMul and embeddings are computed using INT8 multiplication and INT32 accumulation.</li> <li>Non-linear operations (GELU, Softmax, LayerNorm) are applied directly to INT32 values and re-quantized to INT8.</li> <li>The entire inference process remains in integer arithmetic without dequantization.</li></ul> <h3 id="results-and-impact"><a href="#results-and-impact" class="header-anchor">#</a> Results and Impact:</h3> <ul><li>Accuracy: I-BERT achieves comparable or slightly better accuracy than FP32 models on the GLUE benchmark, with an improvement of 0.3 (Base) and 0.5 (Large) in average score.</li> <li>Efficiency: I-BERT provides 2.4× – 4.0× speedup in inference compared to FP32 on NVIDIA T4 GPUs.</li> <li>Deployment Feasibility: Eliminates floating-point dependency, making it ideal for deployment on integer-only hardware like ARM Cortex-M processors and specialized accelerators.</li></ul> <h3 id="key-takeaways-in-3-sentences"><a href="#key-takeaways-in-3-sentences" class="header-anchor">#</a> Key Takeaways in 3 Sentences</h3> <p>I-BERT introduces a novel integer-only quantization method for BERT, eliminating floating-point operations and enabling efficient deployment on integer-only hardware.</p> <p>By approximating non-linear functions like GELU, Softmax, and LayerNorm with polynomial and integer arithmetic, it maintains high accuracy while significantly improving inference speed.</p> <p>Evaluation on the GLUE benchmark and hardware tests demonstrate that I-BERT achieves up to 4× speedup while maintaining or slightly improving accuracy over FP32 models.</p> <hr> <h2 id="_21-904-llm-int8-8-bit-matrix-multiplication-for-transformers-at-scale"><a href="#_21-904-llm-int8-8-bit-matrix-multiplication-for-transformers-at-scale" class="header-anchor">#</a> 21. [904] LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</h2> <p>The paper introduces LLM.int8(), a quantization method enabling 8-bit matrix multiplication for large transformers without degrading performance.</p> <p>Traditional 8-bit quantization methods struggle with large-scale models due to systematic outlier features that disrupt quantization precision beyond 6.7B parameters.</p> <p>To overcome this, LLM.int8() combines vector-wise quantization (which assigns separate normalization constants per inner product) and mixed-precision decomposition, where <strong>outlier dimensions are computed in 16-bit</strong> while 99.9% of values remain in 8-bit.</p> <p>This approach allows large-scale transformers like OPT-175B and BLOOM to run on a single consumer GPU without accuracy loss.</p> <p><img src="https://github.com/user-attachments/assets/36c7a517-2cd5-4f69-ad74-31388c354235" alt="image"></p> <h3 id="key-findings-include"><a href="#key-findings-include" class="header-anchor">#</a> Key findings include:</h3> <ul><li>Emergent outliers: Beyond 6.7B parameters, certain feature dimensions dominate transformer attention and predictive performance, requiring higher precision.</li> <li>Quantization challenge: Existing methods fail due to these outliers, as they occupy only 0.1% of the data but significantly impact accuracy.</li></ul> <h3 id="llm-int8-solution"><a href="#llm-int8-solution" class="header-anchor">#</a> LLM.int8 solution</h3> <p>By isolating outliers in 16-bit operations while keeping most computations in 8-bit, the method retains full-precision inference while cutting memory usage by half.</p> <p>The study empirically validates that LLM.int8() maintains 16-bit accuracy across models up to 175B parameters, making LLMs more accessible and practical. The method is open-sourced and integrated with Hugging Face Transformers.</p> <h3 id="three-sentence-key-takeaways"><a href="#three-sentence-key-takeaways" class="header-anchor">#</a> Three-Sentence Key Takeaways</h3> <p>LLM.int8() enables performance-preserving 8-bit quantization for transformers up to 175B parameters by combining vector-wise quantization and mixed-precision decomposition to handle emergent large-magnitude features.</p> <p>These systematic outliers, appearing beyond 6.7B parameters, disrupt standard 8-bit quantization but can be isolated in 16-bit precision while keeping over 99.9% of computations in 8-bit, achieving a 2× memory reduction.</p> <p>This allows massive models like OPT-175B and BLOOM to run efficiently on consumer GPUs, making large-scale LLM inference more accessible.</p> <hr> <h2 id="_22-637-training-deep-neural-networks-with-8-bit-floating-point-numbers"><a href="#_22-637-training-deep-neural-networks-with-8-bit-floating-point-numbers" class="header-anchor">#</a> 22.[637] Training Deep Neural Networks with 8-bit Floating Point Numbers</h2> <h3 id="_1-introduction-5"><a href="#_1-introduction-5" class="header-anchor">#</a> 1. Introduction</h3> <p>The paper addresses the challenge of training deep neural networks (DNNs) with reduced precision floating point numbers, specifically using 8-bit floating point (FP8).</p> <p>While inference has been successfully performed with low precision (as low as 2–4 bits), training has traditionally required at least 16-bit precision due to gradient fidelity concerns.</p> <p>The paper proposes novel techniques that allow DNN training using FP8 without accuracy loss, promising 2–4× improvements in energy efficiency and throughput.</p> <h3 id="_2-challenges-in-low-precision-training"><a href="#_2-challenges-in-low-precision-training" class="header-anchor">#</a> 2. Challenges in Low-Precision Training</h3> <p>Three major challenges arise when reducing DNN training precision:</p> <ul><li>Loss of accuracy when all operands (weights, activations, errors, gradients) are quantized to 8 bits.</li> <li>Reduced accumulation precision (moving from FP32 to FP16) significantly impacts convergence.</li> <li>Weight updates in 16-bit may degrade accuracy unless managed properly.</li></ul> <p><img src="https://github.com/user-attachments/assets/90a7d951-8c63-46f1-8159-1bc21964bb6c" alt="image"></p> <h3 id="_3-proposed-solutions"><a href="#_3-proposed-solutions" class="header-anchor">#</a> 3. Proposed Solutions</h3> <p>The paper introduces several key innovations:</p> <ul><li>Custom FP8 Format: A new FP8 format (1-bit sign, 5-bit exponent, 2-bit mantissa) that effectively represents DNN parameters.</li> <li><strong>Chunk-Based Accumulation</strong>: Breaking matrix multiplications into small chunks before accumulation to prevent truncation errors.</li></ul> <p><img src="https://github.com/user-attachments/assets/ff65ed3e-1fce-4d12-b10c-263b4ebd29b1" alt="image"></p> <ul><li>Floating Point Stochastic Rounding: A rounding method that retains small numerical details to prevent loss of information.</li> <li>Mixed-Precision Computations: Using FP8 for most computations while keeping critical accumulations and weight updates in FP16.</li></ul> <h3 id="_4-experimental-results"><a href="#_4-experimental-results" class="header-anchor">#</a> 4. Experimental Results</h3> <p>The proposed FP8 training method was tested on various models, including ResNet18/50, AlexNet, and CIFAR10-CNN. Results show:</p> <ul><li>No significant accuracy loss compared to FP32.</li> <li>Memory savings: Model sizes were reduced by ~50%.</li> <li>Energy-efficient hardware implementation: A prototype chip demonstrated 2–4× efficiency gains.</li></ul> <h3 id="_5-discussion"><a href="#_5-discussion" class="header-anchor">#</a> 5. Discussion</h3> <ul><li><strong>The first and last layers of DNNs require higher precision (FP16) for better stability.</strong></li> <li>Gradient accumulation in FP16 must be carefully handled with chunk-based summation.</li> <li>Stochastic rounding outperforms nearest rounding in weight updates.</li></ul> <h3 id="_6-conclusion"><a href="#_6-conclusion" class="header-anchor">#</a> 6. Conclusion</h3> <p>The paper successfully demonstrates DNN training with FP8 while maintaining accuracy.</p> <p>The combination of chunk-based accumulation, stochastic rounding, and mixed-precision strategies opens the door for more efficient hardware training platforms.</p> <h3 id="key-takeaways-in-3-sentences-2"><a href="#key-takeaways-in-3-sentences-2" class="header-anchor">#</a> Key Takeaways in 3 Sentences</h3> <p>The paper proposes training deep neural networks using 8-bit floating point numbers by introducing a custom FP8 format, chunk-based accumulation, and stochastic rounding to prevent accuracy loss.</p> <p>Experiments across multiple models (ResNet, AlexNet) confirm that FP8 training achieves the same accuracy as FP32 while significantly reducing memory and energy costs.</p> <p>These innovations enable future hardware architectures with 2–4× improved efficiency, paving the way for practical low-precision DNN training.</p> <hr> <h2 id="_270-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks"><a href="#_270-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks" class="header-anchor">#</a> [270] Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks</h2> <p>This paper addresses the challenge of training and deploying deep neural networks (DNNs) in 8-bit precision while preserving accuracy.</p> <p>The authors note that existing 8-bit floating point (FP8) approaches (e.g., the  (1−5-2) format) can train some large-capacity networks like ResNet but struggle with compact models such as MobileNet or NLP-oriented architectures like Transformer.</p> <p><img src="https://github.com/user-attachments/assets/93af1c3c-4f98-49f0-aed5-d9c338516a5c" alt="image"></p> <p>By analyzing the distinct needs of the forward pass (weights and activations) and the backward pass (gradients), they propose a hybrid FP8 (HFP8) scheme:</p> <ul><li>Forward Pass
<ul><li>Uses the (1−4−3) format (1 sign bit, 4 exponent bits, 3 mantissa bits) with an exponent bias of 4.</li> <li>This format gives <strong>extra mantissa precision to minimize quantization noise for forward activations and weights</strong>.</li> <li><strong>The exponent bias ensures coverage for smaller values often present in weight distributions.</strong></li></ul></li> <li>Backward Pass
<ul><li>Uses (1−5−2) precision (1 sign bit, 5 exponent bits, 2 mantissa bits) for gradients and errors.</li> <li><strong>The wider exponent range captures large gradient magnitudes during backpropagation</strong>.</li> <li><strong>An auto-adjusted loss-scaling technique further prevents underflow or overflow of gradients in 8-bit.</strong></li></ul></li></ul> <p>The paper’s theoretical framework highlights how too few mantissa bits in forward activations create large quantization mismatches, which degrade training.</p> <p>Conversely, reducing exponent bits for backward gradients risks clamping large gradient ranges. By splitting the formats, HFP8 balances mantissa fidelity in the forward path and dynamic range in the backward path.</p> <blockquote><p>The underlying reason for this choice is that forward and backward passes have different optimal balances between range and precision.
While tensors in the forward pass prefer higher precision (and lower representational error), gradients in the backward pass prefer a higher dynamic range.
We describe our HFP8 training methodology where weights and activations adopt the (1-4-3) format (bias=4) while tensors used in backpropagation continue to be represented using the (1-5-2) format (in combination with loss
scaling techniques pioneered by [28]).</p></blockquote> <p><strong>Additional contributions include</strong></p> <ul><li>Post-Training FP8 Inference <br>
Directly quantizing a full-precision (FP32) model to FP8 often loses accuracy. <br>
The authors show that re-tuning Batch Normalization statistics with a small subset of unlabeled training data recovers most or all of the lost accuracy. <br>
Furthermore, depthwise convolutions (common in MobileNet-like architectures) and some layers with very small or large magnitudes may remain in slightly higher precision (FP16) to avoid accuracy drops. <br></li> <li>SoftMax Optimization <br>
For tasks like machine translation or speech recognition that rely on large final FC layers, subtracting the max logits before quantization lets the SoftMax function operate in FP8 without collapsing scores. <br></li> <li>Distributed Training <br>
The paper modifies standard ring-based all-reduce to exchange compressed 8-bit weights. <br>
A local “round-off residual” (stored in higher precision) ensures these 8-bit weight updates remain stable and converge reliably. <br>
The authors’ hardware simulations indicate that adopting HFP8 for both computation and communication can significantly reduce training time. <br></li></ul> <p>Comprehensive experiments on ImageNet (ResNet, MobileNet, DenseNet, AlexNet), WMT14 En-De (Transformer), large-scale speech (LSTM), and object detection (SSD-Lite, Mask R-CNN) confirm that HFP8 preserves baseline accuracies within a fraction of a percentage point.</p> <p>This demonstrates that 8-bit floating point is feasible not only for specialized cases but for a broad range of DNN workloads, offering substantial speedups and energy savings.</p> <p><strong>Three-Sentence Key Summary</strong></p> <ul><li>This paper proposes a hybrid 8-bit floating point (HFP8) approach that uses two different 8-bit formats to match the distinct precision requirements of forward (weights/activations) and backward (gradients/errors) passes.</li> <li>It introduces practical techniques—like Batch Normalization re-tuning, depthwise convolution in FP16, and round-off residual updates—to ensure stable training and inference in 8 bits across diverse models.</li> <li>As a result, HFP8 achieves near-baseline accuracy on tasks ranging from image classification and object detection to machine translation and speech recognition.</li></ul> <hr> <h2 id="_28-read-y2025-nvidia-coat-compressing-optimizer-states-and-activation-for-memory-efficient-fp8-training"><a href="#_28-read-y2025-nvidia-coat-compressing-optimizer-states-and-activation-for-memory-efficient-fp8-training" class="header-anchor">#</a> 28. [Read Y2025 NVIDIA] Coat: Compressing Optimizer States and Activation for Memory-Efficient FP8 Training  👍  👍  👍  👍  👍</h2> <p><em>Prior studies do not tackle the memory consumption of activations and still leave the optimizer’s second-order momentum in higher precision.</em></p> <p>COAT significantly reduces the overall memory footprint by quantizing optimizer states and activations into FP8.</p> <p>For optimizer states, we observe that <strong>FP8 format’s representation range is under-utilized when quantizing them</strong>, as illustrated in Figure 2(a).</p> <p>To address this, we introduce a novel Dynamic Range Expansion method which adjusts the distribution of optimizer states to better fit within the FP8 range, thereby minimizing quantization error.</p> <p>For activations, we propose Mixed-Granularity Activation Quantization to achieve efficient and accurate quantization. We apply fine-grained quantization to non-linear layers and apply per-tensor quantization to linear layers.</p> <p>Per-tensor quantization for matrix multiplications is more efficient and better suited for TensorCores, while fine-grained quantization helps maintain accuracy.</p> <p>These two approaches tackle high memory consumption while ensuring minimal performance degradation.</p> <p><img src="https://github.com/user-attachments/assets/cc6a4e9d-fbc3-4644-aaed-82b82a7adae3" alt="image"></p> <blockquote><p>Prior studies: FP8-LM (Peng et al., 2023) quantizes the first-order momentum to FP8 while <strong>leaving second-order momentum in FP16, which limits the overall memory savings</strong>. (Fishman et al., 2024) finds that second-order momentum is more sensitive to quantization, and proposes to quantize it using E5M2 format.</p></blockquote> <blockquote><p>In addition to quantization, there are other approaches that aim to reduce the memory footprint of the optimizer states (Shazeer &amp; Stern, 2018; Anil et al., 2019; Chen et al., 2024; Zhao et al., 2024), such as low-rank decomposition and optimizer simplification that only store the first-order momentum. These methods are orthogonal to our approach.</p></blockquote> <p>To perform optimizer state quantization, we adopt per-group quantization for both first-order and second-order momentum, following previous works (Dettmers et al., 2021; Li et al., 2024).</p> <p>Every consecutive G element forms a group (G is defined as the group size), and each group is quantized independently with its own statistics.</p> <p>Optimizer states are stored in FP8 precision, while its scaling factor is stored in BF16.</p> <p><strong>the dynamic range for first-order momentum is typically less than 1e4, and for second-order momentum, it is
usually less than 1e1—both far below the available range of FP8. That is what they mean by second-order momentum is underutilized FP8.</strong></p> <p><img src="https://github.com/user-attachments/assets/3fe1c480-39a1-4392-9af0-30fb7ab5ef36" alt="image"></p> <p>We propose to vary the quantization granularity across different layers to balance precision and efficiency in a mixed-granularity manner.</p> <p>For non-linear layers, VS-Quant (Dai et al., 2021) or PerBlock Quant (Xi et al., 2024) methods are well-suited due to their fine-grained and precise nature.</p> <p>For linear layers, we apply per-tensor quantization to maximize the performance of Tensor Cores.</p> <p>We observe that quantizing the input of layernorm across multiple token axes is detrimental to accuracy.</p> <p>As illustrated in Figure 4(a), when the number of elements that share a scaling factor is fixed, the quantization error increases significantly when quantization is performed across the token axis.</p> <p>Therefore instead of using per-block quantization with block size B × B as proposed in (Xi et al., 2024), we propose to use per-group quantization with group size 1 × G, where G = B2 to keep the granularity the same.</p> <p>This approach enhances <strong>the accuracy of non-linear layers while maintaining efficiency</strong>. Our precise FP8 precision flow is visualized in Figure 1(a), where we display the full precision flow for a Llama-style decoder layer, both forward and backward pass.</p> <p><em>Note the red color in following graph, shows <strong>BF16</strong>.</em></p> <p><img src="https://github.com/user-attachments/assets/9caf39d6-c382-4be3-8be6-9542c61dc703" alt="image"></p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/15.llm_quant.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/04/30, 18:54:43</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7049/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Reasoning in LLM</div></a> <a href="/qishao-notes/pages/dc7051/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">LLM Sparsity</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7049/" class="prev">Reasoning in LLM</a></span> <span class="next"><a href="/qishao-notes/pages/dc7051/">LLM Sparsity</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.f8e7e83c.js" defer></script><script src="/qishao-notes/assets/js/2.0833fe67.js" defer></script><script src="/qishao-notes/assets/js/99.3f361b43.js" defer></script>
  </body>
</html>
