<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>GPU Workload Characteristics | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.f2efd43d.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.0833fe67.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/73.73c3f331.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.2e68d5ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.8806abe8.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.db90e226.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.86b3ccbd.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.57b8115b.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.7f815ed9.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.2fe0ec34.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.77e48a4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.f040b312.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.9c4aa202.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.2b90331e.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.8a8e0de2.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.a5a46e61.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.a458a601.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.4e60a446.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.9aa78644.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.6dbee034.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.9a39a40f.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.78ba6a41.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.5bfdfc51.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.e08d8d99.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.8d0fb1f3.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.8535d463.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.babc130a.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.0a519c15.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.c5c23c3f.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.1bc8ea22.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.3e9bcc24.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.a2293996.js"><link rel="prefetch" href="/qishao-notes/assets/js/126.80426a6d.js"><link rel="prefetch" href="/qishao-notes/assets/js/127.b28c0cc7.js"><link rel="prefetch" href="/qishao-notes/assets/js/128.a41c1d29.js"><link rel="prefetch" href="/qishao-notes/assets/js/129.466a49bb.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.66b81125.js"><link rel="prefetch" href="/qishao-notes/assets/js/130.0722f899.js"><link rel="prefetch" href="/qishao-notes/assets/js/131.c2ef7d79.js"><link rel="prefetch" href="/qishao-notes/assets/js/132.23ded363.js"><link rel="prefetch" href="/qishao-notes/assets/js/133.355328f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/134.212ca5fc.js"><link rel="prefetch" href="/qishao-notes/assets/js/135.a20b3a39.js"><link rel="prefetch" href="/qishao-notes/assets/js/136.75d2d2e8.js"><link rel="prefetch" href="/qishao-notes/assets/js/137.89407db7.js"><link rel="prefetch" href="/qishao-notes/assets/js/138.aff4ddd8.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.343efb9f.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.e89652e9.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.383840b9.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.71442f47.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.3f69064e.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.a2c2ac4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a4b6fb1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.8a5b62d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.4d84fdc5.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.4d4cd4e1.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.88ca611d.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.ff2ed822.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.2aa82bc2.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.ae348b88.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.b0085c20.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.38e8dfbd.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.1e7f10dd.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.0858b5b0.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.aa6fd9d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.96306ad4.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.7f37f31a.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.54012013.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.01d6cf82.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.c355ce28.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.6cb48c78.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.9f9585d9.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.7f09c090.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.2dcb8de5.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.5f352ae8.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.bcc7509d.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.36f25851.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.9a39b8df.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.be5d1eda.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.a99569c7.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.86177f5e.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.3c3f4c37.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.74d6845e.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.837fe395.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.9d58ea7e.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.1d0bbfe1.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.a1fb9aec.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.7e8ad511.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.18d30bcf.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.2777a73c.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.660c4a0d.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.3e450cd0.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.cee9236c.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.d8968a5e.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.86284e6f.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.bca7e213.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.e07fb28f.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.f19094e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.f1be6ae5.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.4af7da52.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.eeb3f036.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.db55939d.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.82293b3c.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.4fb49cdd.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.539f503f.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.f45db467.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.3eecfe51.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.c6451aec.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.6df39a49.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.d2933de4.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.e1314a60.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.da6e18b2.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.999968b5.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.74894327.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.f26bc225.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.d0c64796.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.b6a1e324.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.735ba55b.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.16b90043.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.cfaa638e.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.dca50b53.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.2f5e2efd.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.46751589.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.bd07092c.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.e08feedc.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.7b318212.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.b1cc25c9.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.a15ac68d.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.bef1f685.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.889c0e19.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.c066c16a.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.474e076f.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.5c1d715f.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.c2289a4f.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.9a4b76dc.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.f88a0988.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.9101a834.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.58da1499.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/cc7034/" class="sidebar-link">Operand Collector</a></li><li><a href="/qishao-notes/pages/2476ae/" class="sidebar-link">GPU WARP Scheduler</a></li><li><a href="/qishao-notes/pages/14769f/" class="sidebar-link">Precision Exception</a></li><li><a href="/qishao-notes/pages/44771e/" class="sidebar-link">Unified Memory Paper List</a></li><li><a href="/qishao-notes/pages/44871e/" class="sidebar-link">TensorCore Paper List</a></li><li><a href="/qishao-notes/pages/45871e/" class="sidebar-link">Memory Behaviour Paper List</a></li><li><a href="/qishao-notes/pages/45871f/" class="sidebar-link">GPU Virtualization Paper List</a></li><li><a href="/qishao-notes/pages/458720/" class="sidebar-link">Large Language Model Paper List</a></li><li><a href="/qishao-notes/pages/458721/" class="sidebar-link">GPU Simulator</a></li><li><a href="/qishao-notes/pages/458722/" class="sidebar-link">Architectural Survey</a></li><li><a href="/qishao-notes/pages/458724/" class="sidebar-link">Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper</a></li><li><a href="/qishao-notes/pages/458725/" class="sidebar-link">Understanding GPGPU-SIM 1 How to get Instruction</a></li><li><a href="/qishao-notes/pages/458726/" class="sidebar-link">Understanding GPGPU-SIM 2 Instruction Execution</a></li><li><a href="/qishao-notes/pages/458727/" class="sidebar-link">Understanding GPGPU-SIM 3 How is the simulation started</a></li><li><a href="/qishao-notes/pages/45872/" class="sidebar-link">Understanding GPGPU-SIM 4 Microarchitecture</a></li><li><a href="/qishao-notes/pages/45874/" class="sidebar-link">Understanding GPGPU-SIM 5  Memory Interface</a></li><li><a href="/qishao-notes/pages/45873/" class="sidebar-link">Warp Related Memory Optimization</a></li><li><a href="/qishao-notes/pages/45875/" class="sidebar-link">GPU Cache Coherency</a></li><li><a href="/qishao-notes/pages/45876/" class="sidebar-link">GPU Cache &amp; Memory Hirerarchy</a></li><li><a href="/qishao-notes/pages/45877/" class="sidebar-link">GPU TLB</a></li><li><a href="/qishao-notes/pages/45878/" class="sidebar-link">GPU Page Table Walk</a></li><li><a href="/qishao-notes/pages/45879/" class="sidebar-link">GPU Cache's Papers</a></li><li><a href="/qishao-notes/pages/45880/" class="sidebar-link">GPU WARP Mangement Papers</a></li><li><a href="/qishao-notes/pages/45882/" class="sidebar-link">GPU Unified Memory Innovations</a></li><li><a href="/qishao-notes/pages/45883/" class="sidebar-link">GPU MultiTask</a></li><li><a href="/qishao-notes/pages/45884/" class="sidebar-link">GPU Training Notes</a></li><li><a href="/qishao-notes/pages/45885/" class="sidebar-link">GPU Paper with Code</a></li><li><a href="/qishao-notes/pages/45886/" class="sidebar-link">GPU Driver &amp; Runtime &amp; Compliation</a></li><li><a href="/qishao-notes/pages/45887/" class="sidebar-link">Accel-Sim Simulator</a></li><li><a href="/qishao-notes/pages/45889/" class="sidebar-link">Understanding GPGPU-SIM 6 Memory Space</a></li><li><a href="/qishao-notes/pages/45890/" class="sidebar-link">GPU Insturctions</a></li><li><a href="/qishao-notes/pages/45892/" class="sidebar-link">GPU GEMM</a></li><li><a href="/qishao-notes/pages/45893/" class="sidebar-link">GPU Compiler Optimization Pass</a></li><li><a href="/qishao-notes/pages/47871e/" class="sidebar-link">TO READ</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/gpu/#gpu" data-v-06225672>gpu</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2024-10-30</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">GPU Workload Characteristics<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[354] Benchmarking TPU, GPU, and CPU Platforms for Deep Learning</li> <li>[59 Year:2019] Characterizing Deep Learning Training Workloads on Alibaba-PAI</li> <li>[12] Effective Elastic Scaling of Deep Learning Workloads</li> <li>[30 Year:2024] LLM Inference Unveiled: Survey and Roofline Model Insights</li> <li>[1] Performance Modeling and Workload Analysis of Distributed Large Language Model Training and Inference</li> <li>[Blog] LLM Inference Series: 5. Dissecting model performance</li> <li>[47 Year:2024] Understanding LLMs: A Comprehensive Overview from Training to Inference</li> <li>[37 Year:2022] Reveal training performance mystery between TensorFlow and PyTorch in the single GPU environment 👍  👍  👍  👍  👍</li> <li>[46 Year:2019] Performance Characterization of DNN Training using TensorFlow and PyTorch on Modern Clusters</li> <li>[163 Year:2020] Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference</li> <li>[204 Year:2016] Fathom: Reference Workloads for Modern Deep Learning Methods</li> <li>[1: Year: 2018] µ-cuDNN Accelerating Deep Learning Frameworks with Micro-Batching</li> <li>[89 Year: 2019] Restructuring Batch Normalization to Accelerate CNN Training</li> <li>[29 Year: 2018] Characterizing the Microarchitectural Implications of a Convolutional Neural Network (CNN) Execution on GPUs</li> <li>[13 Year: 2022] cuConv: CUDA implementation of convolution for CNN inference</li> <li>[10 Year: 2018] Performance Analysis of Different Convolution Algorithms in GPU Environment</li> <li>[0 Year:2024] GPU Performance Optimization via Intergroup Cache Cooperation</li> <li>[2024] Accelerating ML Workloads using GPU Tensor Cores: The Good, the Bad, and the Ugly :+1：<em>love the name</em></li></ol> <hr> <h3 id="_1-benchmarking-tpu-gpu-and-cpu-platforms-for-deep-learning"><a href="#_1-benchmarking-tpu-gpu-and-cpu-platforms-for-deep-learning" class="header-anchor">#</a> 1. Benchmarking TPU, GPU, and CPU Platforms for Deep Learning</h3> <p>Paper from Harvard</p> <p><img src="https://github.com/user-attachments/assets/110ddbc0-1ddf-40fa-b360-9e3f589494c6" alt="image"></p> <hr> <h3 id="_2-59-year-2019-characterizing-deep-learning-training-workloads-on-alibaba-pai"><a href="#_2-59-year-2019-characterizing-deep-learning-training-workloads-on-alibaba-pai" class="header-anchor">#</a> 2 [59 Year:2019] Characterizing Deep Learning Training Workloads on Alibaba-PAI</h3> <p>This paper compares early ml, like single node, single master multi worker, nvlinkd is just introduced to Alibaba at that time.</p> <p><img src="https://github.com/user-attachments/assets/8e9eb798-a1bf-48da-aa17-d30c7e6973fc" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/ec5cdae5-2150-48aa-9626-37869115bff7" alt="image"></p> <hr> <h3 id="_10-inducing-and-exploiting-activation-sparsity-for-fast-neural-network-inference"><a href="#_10-inducing-and-exploiting-activation-sparsity-for-fast-neural-network-inference" class="header-anchor">#</a> 10. Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference</h3> <p>Sparsity across channels:</p> <p><img src="https://github.com/user-attachments/assets/d0ab98ae-adcd-438a-97fe-db8c31becb3f" alt="image"></p> <p>Sparsity across layers:
<img src="https://github.com/user-attachments/assets/68c5159a-f536-4714-aadf-0772b6c73dde" alt="image"></p> <p><em><strong>CSCC: Convolution Split Compression Calculation Algorithm for Deep Neural Network</strong></em>
Sparsity across layers:
<img src="https://github.com/user-attachments/assets/0496984d-6cad-4727-8ee7-f63023a1656c" alt="image"></p> <hr> <h3 id="_11-fathom-reference-workloads-for-modern-deep-learning-methods"><a href="#_11-fathom-reference-workloads-for-modern-deep-learning-methods" class="header-anchor">#</a> 11.Fathom: Reference Workloads for Modern Deep Learning Methods</h3> <p><img src="https://github.com/user-attachments/assets/04833dea-d804-416e-914f-d59409ab0f5e" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/4394bb14-fd24-46de-a604-f34b692fd139" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/39728745-788b-4ac1-b777-937f25b9083d" alt="image"></p> <h3 id="_12-μ-cudnn-accelerating-deep-learning-frameworks-with-micro-batching"><a href="#_12-μ-cudnn-accelerating-deep-learning-frameworks-with-micro-batching" class="header-anchor">#</a> 12. µ-cuDNN Accelerating Deep Learning Frameworks with Micro-Batching</h3> <p>Memory requirements of different matrix multiplication algorithm and their execution time.</p> <p><img src="https://github.com/user-attachments/assets/6d533da4-a5a3-46e4-b6ad-61dfa0c21d71" alt="image"></p> <h3 id="_13-restructuring-batch-normalization-to-accelerate-cnn-training"><a href="#_13-restructuring-batch-normalization-to-accelerate-cnn-training" class="header-anchor">#</a> 13. Restructuring Batch Normalization to Accelerate CNN Training</h3> <p>Batch normalization is essential in Densenet.
<img src="https://github.com/user-attachments/assets/679ff5f7-0a9d-424e-b1e7-2a9ac9d9842c" alt="image"></p> <p>The non-CONV layers of DenseNet-121 are mostly bottlenecked by the peak mainmemory bandwidth of the system we use (230.4GB/s),<br>
whereas the CONV layers underutilize the available bandwidth (only up to 120GB/s).</p> <p>non-Conv layers have less data locality and computation intensity, which makes loop blocking techniques less effective,<br>
leading to higher demand in memory bandwidth.</p> <p>Memory accesses in ReLU and BN layers mostly come from reading and writing ifmaps and ofmaps.<br>
Since BN layers have strict data dependency, cross-layer data reuse is prohibited.</p> <ul><li>In the forward pass of a BN layer, all pixels of ifmaps that belong to a mini-batch should be retrieved to get per-channel mean and variance values prior to normalizing individual pixels.</li> <li>In backpropagation, calculating the partial derivatives on γ (scaling factors) and β (shift factors) accompanies sweeping the partial derivatives on ofmaps; this should precede computing the partial derivatives on ifmaps.</li></ul> <p>Because these dependencies make data reuse distance far in BN, it is difficult to apply the data reuse techniques that were previously proposed for CONV
layers to these non-CONV layers.</p> <p>They split batch normalization into two parts:</p> <p><img src="https://github.com/user-attachments/assets/27a1b975-a383-4ecb-9d39-545ed4069388" alt="image"></p> <p><strong>what is convential kernel fusion of conv+bn about? is that fusion conv+bn totally?</strong></p> <h3 id="_14-characterizing-the-microarchitectural-implications-of-a-convolutional-neural-network-cnn-execution-on-gpus"><a href="#_14-characterizing-the-microarchitectural-implications-of-a-convolutional-neural-network-cnn-execution-on-gpus" class="header-anchor">#</a> 14. Characterizing the Microarchitectural Implications of a Convolutional Neural Network (CNN) Execution on GPUs</h3> <p>👍 👍 👍 👍 👍</p> <p>Quantization analysis of CNN on GPUs</p> <p><img src="https://github.com/user-attachments/assets/e91ea58d-414d-4d28-8209-5397b7b7dfd3" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/2526f66e-008f-44b9-aa37-21796af5c1b2" alt="image"></p> <h4 id="results-on-k40"><a href="#results-on-k40" class="header-anchor">#</a> Results on K40</h4> <h5 id="conv"><a href="#conv" class="header-anchor">#</a> Conv</h5> <ul><li>stall exec dependency: the intrinsic program characteristics of this layer</li> <li>stall not selected : the warp is not selected to run since the scheduler selects competing warps</li></ul> <p>bounded by computing</p> <h5 id="relu"><a href="#relu" class="header-anchor">#</a> relu</h5> <ul><li>stall_memory_throttle</li> <li>stall_memory_dependency
Memory bound</li></ul> <h4 id="layer-normalization-pooling-and-softmax"><a href="#layer-normalization-pooling-and-softmax" class="header-anchor">#</a> layer normalization, pooling and softmax</h4> <p>compute and memory bound<br>
when it changes to GTX1080, it is only memory-bound. GTX1080 better compute performance/</p> <h4 id="fully-connect-layer"><a href="#fully-connect-layer" class="header-anchor">#</a> fully connect layer:</h4> <p>fc6_w is somewhat special in that it is partially bounded by memory and <em>partially bounded by instruction fetch</em>.</p> <h4 id="memory-access-behaviour"><a href="#memory-access-behaviour" class="header-anchor">#</a> Memory Access Behaviour</h4> <p><img src="https://github.com/user-attachments/assets/160a0fe3-6b9b-4663-8fb1-6b242c410924" alt="image"></p> <h4 id="memory-access-behaviour-2"><a href="#memory-access-behaviour-2" class="header-anchor">#</a> Memory Access Behaviour</h4> <ul><li>layers of the linear data transformation make good use of the texture cache.
This can be explained since the computations in both the convolution and fully-connected layers exhibit a high degree of spatial locality.<br>
The texture cache is designed in such a way as to take advantage of spatial locality.</li> <li>Even the activation layer, which has <strong>no temporal locality</strong>, also makes use of the texture cache to exploit <strong>spatial locality</strong>.</li> <li>the linear data transformation layers rely heavily on shared memory and the texture cache.
As indicated in the cache hit rate figures, both the convolution and fully-connected layers possess high temporal and spatial locality, given that data accessed within a region is repeatedly accessed.<br>
As a result, there are a large number of memory transactions issued to these two memory levels, especially read requests.</li> <li>for other layers, the utilization of shared memory and texture cache is very limited.
Even for the pooling and LRN layers, the data reuse rate is very low.<br>
For the other layers, including pooling, LRN, activation, and Softmax, the number of memory transactions does not vary significantly across the memory hierarchy</li> <li>shared memory usually takes 38 cycles to read, while the texture cache takes 436-443 cycles.</li></ul> <p><img src="https://github.com/user-attachments/assets/caf5d7b9-27a6-48ac-ac92-ad84cc2eed40" alt="image"></p> <h4 id="potential-optimization"><a href="#potential-optimization" class="header-anchor">#</a> Potential Optimization</h4> <ul><li><p>convolution is compute power hungary instead of DRAM bandwidth.
Increasing the DRAM bandwidth on the GTX1080 will not benefit the CNN throughput very much.
Instead, if we increase the bandwidth of the texture cache, we should see much better performance.</p></li> <li><p>L1 cache is essentially unused in most of the layers.</p></li></ul> <p>we can enable <strong>L1 cache bypassing[27]</strong> for selected layers to avoid unnecessary data requests to the L1 cache.
When we re-run our application with the L1 cache disabled for both reads and writes, we observe a speedup in some layers for both forward and backward propagation.</p> <ul><li>apply kernel fusion[28] for the linear data transformation layers and the non-linear activation.
<img src="https://github.com/user-attachments/assets/3552f82e-7519-435b-9404-e544d92c7ecc" alt="image"></li></ul> <h3 id="_15-cuconv-cuda-implementation-of-convolution-for-cnn-inference"><a href="#_15-cuconv-cuda-implementation-of-convolution-for-cnn-inference" class="header-anchor">#</a> 15. cuConv: CUDA implementation of convolution for CNN inference</h3> <p>They design a new implementaion of convolution.</p> <p>They discucss about GEMM, Winograd and FFT and design a new two-stage convolution scheme, two-stage convolution.</p> <h3 id="_16-performance-analysis-of-different-convolution-algorithms-in-gpu-environment"><a href="#_16-performance-analysis-of-different-convolution-algorithms-in-gpu-environment" class="header-anchor">#</a> 16. Performance Analysis of Different Convolution Algorithms in GPU Environment</h3> <p>They compare the memory size and performance of FFT, gemm, Winograd.
<img src="https://github.com/user-attachments/assets/2ad332b7-d0dc-449d-a663-a85d3b920acf" alt="image"></p> <h3 id="_17-gpu-performance-optimization-via-intergroup-cache-cooperation"><a href="#_17-gpu-performance-optimization-via-intergroup-cache-cooperation" class="header-anchor">#</a> 17. GPU Performance Optimization via Intergroup Cache Cooperation</h3> <p>This paper designed cache for gpu and also analyze different level of ache behaviour on different benchmarks.\</p> <p>L1 hit ratio</p> <p><img src="https://github.com/user-attachments/assets/47ea33de-8ddb-41b9-9e97-c930ac1ac87d" alt="image"></p> <p>Duplicate data</p> <p><img src="https://github.com/user-attachments/assets/6cd548eb-4994-4a83-8239-d07898d696b7" alt="image"></p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/03.gpu/28.gpu_character.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/05/21, 23:52:46</span></div></div> <div class="page-nav-wapper"><!----> <!----></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.f2efd43d.js" defer></script><script src="/qishao-notes/assets/js/2.0833fe67.js" defer></script><script src="/qishao-notes/assets/js/73.73c3f331.js" defer></script>
  </body>
</html>
