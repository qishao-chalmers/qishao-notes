<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>VLLM Notes | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.685d6d32.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.55cd0f38.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.53666c0e.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/129.39d990be.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.446d83fa.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.3f1ed558.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.17ffc623.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.e06ee548.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.6a425931.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.ed982c91.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.ddcb7d2f.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.92b30fd3.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.4511453d.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.6b319d71.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.fde9002a.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.2552ffdb.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.a183eee6.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.129c165e.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.0b0aec54.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.83f3e6c8.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.f5f11caf.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.3fc9fea0.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.9104fc67.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.5eeb927b.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.73d54539.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.8da19415.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.5ce790ca.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.9290b729.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.cc3cc854.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.bfa94c5e.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.784952fb.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.b5ae1ec8.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.602cbbd0.js"><link rel="prefetch" href="/qishao-notes/assets/js/126.781a7e44.js"><link rel="prefetch" href="/qishao-notes/assets/js/127.fada19d1.js"><link rel="prefetch" href="/qishao-notes/assets/js/128.e5525f8d.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.250e911f.js"><link rel="prefetch" href="/qishao-notes/assets/js/130.447c8658.js"><link rel="prefetch" href="/qishao-notes/assets/js/131.a0eead09.js"><link rel="prefetch" href="/qishao-notes/assets/js/132.01bdce4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/133.25ae6d49.js"><link rel="prefetch" href="/qishao-notes/assets/js/134.e77ceb02.js"><link rel="prefetch" href="/qishao-notes/assets/js/135.10e80be9.js"><link rel="prefetch" href="/qishao-notes/assets/js/136.3d634c20.js"><link rel="prefetch" href="/qishao-notes/assets/js/137.2c401123.js"><link rel="prefetch" href="/qishao-notes/assets/js/138.e7bc05a7.js"><link rel="prefetch" href="/qishao-notes/assets/js/139.6988eb93.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.be2ee1c0.js"><link rel="prefetch" href="/qishao-notes/assets/js/140.55204db8.js"><link rel="prefetch" href="/qishao-notes/assets/js/141.aba5237f.js"><link rel="prefetch" href="/qishao-notes/assets/js/142.4a531382.js"><link rel="prefetch" href="/qishao-notes/assets/js/143.06c8fa27.js"><link rel="prefetch" href="/qishao-notes/assets/js/144.9abd2c1a.js"><link rel="prefetch" href="/qishao-notes/assets/js/145.58c6c8fb.js"><link rel="prefetch" href="/qishao-notes/assets/js/146.b35b0153.js"><link rel="prefetch" href="/qishao-notes/assets/js/147.f698f299.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.14dd04e8.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.cfc042b1.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.517d61d1.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.128c2b0e.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.e439f846.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.c8a92336.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.bc8dc9c7.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.3f92d355.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.c051858c.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.73a2c075.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.18d5d695.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.2f3cc9ee.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.4c4edb95.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.9e0d97c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.100135e7.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.0852a784.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.1e129010.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.0913dc23.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.5b5527ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.66281fc5.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.33841c37.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.3c3204eb.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.9bcf840c.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.13f0cb9e.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.43f7bc51.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.8aa6d45f.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.b35441f3.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.c9641058.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.e0f6e0a9.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.594926e3.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.4e4ba848.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.f269e52d.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.4365bb0c.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.008abbd2.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.cbbac7ec.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.aa2bff30.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.d21d110e.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.2b7b0962.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.1677be2e.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.9509318b.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.38c595e2.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.1b49527c.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.4a8a0631.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.25f53cf3.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.93750750.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.48b40588.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.81f554f1.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.28b92743.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.b23efced.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.54d1a25c.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.3a38a4c7.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.fd389573.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.796deb10.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.d5ef192c.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.4a7ea356.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.a918a9da.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.aa12798f.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.abaf1110.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.4d20a78b.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.0a20961f.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.a84d6f86.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.8518c607.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.e11d8ee7.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.6bad7981.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.364950d5.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.82c4de60.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.9b3d3ace.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.8bc22383.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.75f558a3.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.037b5e81.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.6bcd4db2.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.62c20da1.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.46b9d6ca.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.37ba1d9a.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.6ebc62a2.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.5c8fe396.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.8ac62810.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.efe09bcb.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.4ea52345.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.0979f9f9.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.fa973d57.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.bae2a1d2.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.33721688.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.fcf2a010.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.9e6b9217.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.a4d4e52e.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.8ffdeba1.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.e82bad82.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.857bf4c8.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.041a4ac2.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.94179e7e.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.44d67dad.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.685d6d32.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Paper List</a></li><li><a href="/qishao-notes/pages/dc7043/" class="sidebar-link">Efficient LLM</a></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" class="sidebar-link">Summery of Inner Workings of LLM</a></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" class="sidebar-link">Memory Optimizations in LLM</a></li><li><a href="/qishao-notes/pages/dc7049/" class="sidebar-link">Reasoning in LLM</a></li><li><a href="/qishao-notes/pages/dc7050/" class="sidebar-link">LLM Mixed Precision &amp; Quantization &amp; Outlier</a></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" class="sidebar-link">LLM Scaling Law</a></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7059/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li><li><a href="/qishao-notes/pages/dc7060/" class="sidebar-link">LLM MOE Inference</a></li><li><a href="/qishao-notes/pages/dc7061/" class="sidebar-link">LLM Compression</a></li><li><a href="/qishao-notes/pages/dc7062/" class="sidebar-link">LLM Optimizer Optimization</a></li><li><a href="/qishao-notes/pages/dc7063/" class="sidebar-link">LLM Posttraining</a></li><li><a href="/qishao-notes/pages/dc7064/" class="sidebar-link">LLM MICRO - ISCA - HPCA</a></li><li><a href="/qishao-notes/pages/dc7066/" class="sidebar-link">LLM Prefilling &amp; Decoding Split</a></li><li><a href="/qishao-notes/pages/dc7067/" class="sidebar-link">Thinking of LLM Prefilling &amp; Decoding Split</a></li><li><a href="/qishao-notes/pages/dc7068/" class="sidebar-link">From Attention Sink to Massive Activation</a></li><li><a href="/qishao-notes/pages/dc7069/" class="sidebar-link">LLM CPU &amp; GPU Workloads</a></li><li><a href="/qishao-notes/pages/dc7070/" class="sidebar-link">LLM RL Framework</a></li><li><a href="/qishao-notes/pages/dc7071/" class="sidebar-link">LLM RL Paper</a></li><li><a href="/qishao-notes/pages/dc7072/" aria-current="page" class="active sidebar-link">VLLM Notes</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7072/#_1-vllm-office-hours-distributed-inference-with-vllm-january-23-2025" class="sidebar-link">[1] vLLM Office Hours - Distributed Inference with vLLM - January 23, 2025</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7072/#summary-of-video-vllm-office-hours-distributed-inference-with-vllm" class="sidebar-link">Summary of Video: &quot;vLLM Office Hours - Distributed Inference with vLLM&quot;</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7072/#distributed-techniques" class="sidebar-link">Distributed Techniques:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7072/#key-optimization-chunk-prefill" class="sidebar-link">Key Optimization: Chunk Prefill</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7072/#_3-vllm-architecture-components-scheduler-executor-worker" class="sidebar-link">3. vLLM Architecture Components (Scheduler, Executor, Worker)</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7072/#_4-definition-of-chunk-prefill-size" class="sidebar-link">4. Definition of Chunk Prefill Size</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7072/#_5-tensor-parallelism-vs-pipeline-parallelism-comparison" class="sidebar-link">5. Tensor Parallelism vs. Pipeline Parallelism Comparison</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7072/#_6-column-and-row-parallelism-in-llms" class="sidebar-link">6. Column and Row Parallelism in LLMs</a></li></ul></li></ul></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-12-11</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">VLLM Notes<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>vLLM Office Hours - Distributed Inference with vLLM - January 23, 2025</li></ol> <hr> <h2 id="_1-vllm-office-hours-distributed-inference-with-vllm-january-23-2025"><a href="#_1-vllm-office-hours-distributed-inference-with-vllm-january-23-2025" class="header-anchor">#</a> [1] vLLM Office Hours - Distributed Inference with vLLM - January 23, 2025</h2> <p><a href="https://www.youtube.com/watch?v=LH2QZehVJoc" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h2 id="summary-of-video-vllm-office-hours-distributed-inference-with-vllm"><a href="#summary-of-video-vllm-office-hours-distributed-inference-with-vllm" class="header-anchor">#</a> Summary of Video: &quot;vLLM Office Hours - Distributed Inference with vLLM&quot;</h2> <p>The video detailed the need for distributed inference to serve large LLMs (like 600B+ parameters) that exceed single-GPU capacity.</p> <h3 id="distributed-techniques"><a href="#distributed-techniques" class="header-anchor">#</a> Distributed Techniques:</h3> <table><thead><tr><th style="text-align:left;">Technique</th> <th style="text-align:left;">Goal</th> <th style="text-align:left;">Mechanism</th> <th style="text-align:left;">Trade-offs</th></tr></thead> <tbody><tr><td style="text-align:left;"><strong>Tensor Parallelism (TP)</strong></td> <td style="text-align:left;">Fit model on a single node; improve latency and throughput.</td> <td style="text-align:left;">Shards weights <strong>horizontally</strong> across multiple GPUs (SPMD paradigm). Uses AllGather/AllReduce operations.</td> <td style="text-align:left;">High communication overhead, mitigated by high-speed interconnects (e.g., NVLink).</td></tr> <tr><td style="text-align:left;"><strong>Pipeline Parallelism (PP)</strong></td> <td style="text-align:left;">Fit model across multiple nodes.</td> <td style="text-align:left;">Shards the model <strong>by layer</strong> across multiple GPUs/nodes. Uses Send/Receive operations.</td> <td style="text-align:left;">Low communication overhead, but doesn't inherently improve latency and can suffer from GPU idle time (&quot;bubbles&quot;).</td></tr> <tr><td style="text-align:left;"><strong>Combination</strong></td> <td style="text-align:left;">PP can be used <em>across nodes</em> for low communication, and TP <em>within a node</em> for speed and memory efficiency.</td> <td style="text-align:left;"></td> <td style="text-align:left;"></td></tr></tbody></table> <h3 id="key-optimization-chunk-prefill"><a href="#key-optimization-chunk-prefill" class="header-anchor">#</a> Key Optimization: Chunk Prefill</h3> <p>Chunk Prefill is critical, especially for PP, and involves breaking large prefill operations into smaller chunks and mixing them with decode batches.</p> <ul><li><strong>Impact of Chunk Size:</strong> A small, carefully chosen size is essential for a smooth, bubble-free execution pipeline and maximum throughput.</li> <li><strong>Memory Management:</strong> It limits the memory spike from long inputs, allowing the system to safely allocate more <strong>KV Cache</strong> space, which boosts concurrency.</li></ul> <h2 id="_3-vllm-architecture-components-scheduler-executor-worker"><a href="#_3-vllm-architecture-components-scheduler-executor-worker" class="header-anchor">#</a> 3. vLLM Architecture Components (Scheduler, Executor, Worker)</h2> <p>These components coordinate to manage and execute inference requests in a distributed vLLM environment:</p> <ul><li><strong>Scheduler:</strong> Manages the flow of requests, schedules batches for processing, and receives the final results.</li> <li><strong>Executor:</strong> The primary distributed manager. It manages the Workers, handles hardware backends (Ray, multi-processing), and issues the coordinated distributed inference commands.</li> <li><strong>Worker:</strong> The execution unit associated with a specific accelerator (e.g., GPU). It performs the actual forward pass and inference computation.</li></ul> <h2 id="_4-definition-of-chunk-prefill-size"><a href="#_4-definition-of-chunk-prefill-size" class="header-anchor">#</a> 4. Definition of <code>Chunk Prefill Size</code></h2> <p>The <strong>Chunk Prefill Size</strong> parameter dictates the maximum number of tokens from a large input prompt that vLLM will process in a single step (or &quot;chunk&quot;).</p> <p>It serves to:</p> <ol><li><strong>Enforce Memory Safety:</strong> Prevents large, arbitrary inputs from causing a sudden, massive spike in activation memory, thus preventing Out-of-Memory (OOM) errors.</li> <li><strong>Amortize Cost:</strong> Smoothes the workload by breaking the long prefill into smaller, manageable pieces that can be interspersed with decode operations.</li> <li><strong>Boost Concurrency:</strong> By guaranteeing a maximum memory footprint, the system can allocate more space for the KV Cache, leading to higher overall throughput.</li></ol> <h2 id="_5-tensor-parallelism-vs-pipeline-parallelism-comparison"><a href="#_5-tensor-parallelism-vs-pipeline-parallelism-comparison" class="header-anchor">#</a> 5. Tensor Parallelism vs. Pipeline Parallelism Comparison</h2> <p>The fundamental difference lies in <strong>how</strong> the model is split:</p> <table><thead><tr><th style="text-align:left;">Feature</th> <th style="text-align:left;">Tensor Parallelism (TP)</th> <th style="text-align:left;">Pipeline Parallelism (PP) / Layer Parallelism</th></tr></thead> <tbody><tr><td style="text-align:left;"><strong>Model Split</strong></td> <td style="text-align:left;"><strong>Intra-Layer Parallelism:</strong> Splits computation/weights <em>within</em> individual layers.</td> <td style="text-align:left;"><strong>Inter-Layer Parallelism:</strong> Splits the model <strong>by layer</strong> or group of layers (stages).</td></tr> <tr><td style="text-align:left;"><strong>Communication</strong></td> <td style="text-align:left;"><strong>Frequent:</strong> Communication (All-Reduce/All-Gather) happens <em>after</em> nearly every linear layer.</td> <td style="text-align:left;"><strong>Infrequent:</strong> Communication (Send/Receive of activations) happens only at the <strong>boundaries</strong> between stages.</td></tr> <tr><td style="text-align:left;"><strong>Best Used For</strong></td> <td style="text-align:left;"><strong>Scaling within a Node</strong> (high-speed interconnects like NVLink).</td> <td style="text-align:left;"><strong>Scaling across Nodes</strong> (slower interconnects).</td></tr> <tr><td style="text-align:left;"><strong>Goal</strong></td> <td style="text-align:left;">Allow a single large layer/tensor to <strong>fit</strong> and <strong>speed up</strong> computation.</td> <td style="text-align:left;">Allow the entire model to <strong>fit</strong> by distributing its total memory footprint.</td></tr></tbody></table> <h3 id="_6-column-and-row-parallelism-in-llms"><a href="#_6-column-and-row-parallelism-in-llms" class="header-anchor">#</a> 6. Column and Row Parallelism in LLMs</h3> <p>Tensor Parallelism is implemented using two specialized patterns within the Transformer layers, especially in the Multi-Layer Perceptron (MLP) block:</p> <ol><li><strong>Column Parallelism (<code>ColumnParallelLinear</code>):</strong> <ul><li><strong>Mechanism:</strong> Splits the weight matrix <strong>by columns</strong>.</li> <li><strong>Flow:</strong> The input is multiplied by the sharded weight matrix. The output is already sharded across GPUs.</li> <li><strong>Communication:</strong> <strong>No communication</strong> is required immediately after the multiplication (e.g., after the Up-Projection layer).</li></ul></li> <li><strong>Row Parallelism (<code>RowParallelLinear</code>):</strong> <ul><li><strong>Mechanism:</strong> Splits the weight matrix <strong>by rows</strong>.</li> <li><strong>Flow:</strong> The input (which is sharded from the previous Column Parallel step) is multiplied by the sharded weight matrix.</li> <li><strong>Communication:</strong> Requires a final <strong>All-Reduce</strong> operation to sum the partial results from all GPUs and generate the correct output (e.g., after the Down-Projection layer).</li></ul></li></ol> <p>LLMs combine these: The <strong>Up-Projection</strong> uses <strong>Column Parallelism</strong> (no communication), and the subsequent <strong>Down-Projection</strong> uses <strong>Row Parallelism</strong> (single All-Reduce).</p> <p>This trick is used through all layers.</p> <p>This minimizes the required synchronization steps.</p> <img width="1421" height="349" alt="image" src="https://github.com/user-attachments/assets/b245f392-85a1-499f-90d0-ac11da9efc72"> <img width="1401" height="336" alt="image" src="https://github.com/user-attachments/assets/481808db-01fd-4bb8-98dd-e4d1c500c150"> <img width="1438" height="427" alt="image" src="https://github.com/user-attachments/assets/0c6ddc84-6d1c-4b17-b1f3-f7dd5c403953"> <p>Major takeaway:</p> <ol><li>first column parallelism, then row parallelism</li> <li>prefer tensor parallelism, gpu is parallelized but with heavy communation, this is prefered in intranodes, communicate will happen in each layer
As to pipeline parallelism, there might be bubble accross gpus. When gpu0 is done with layer0-3, it passes the intermediate result into gpu1 for layer4-7. Only result from layer3 needs to be transfered.<br>
less communication, but might introduce buffer. Used in multi node case.</li> <li>chunk size is essential. we can amortize the prefilling phase into decoding phase. So even case with long prefill, it still works well.</li></ol></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/34.vllm_notes.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/12/11, 16:23:55</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7071/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">LLM RL Paper</div></a> <!----></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7071/" class="prev">LLM RL Paper</a></span> <!----></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.55cd0f38.js" defer></script><script src="/qishao-notes/assets/js/2.53666c0e.js" defer></script><script src="/qishao-notes/assets/js/129.39d990be.js" defer></script>
  </body>
</html>
