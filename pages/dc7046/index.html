<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Summery of Inner Workings of LLM | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.c10016d1.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.0833fe67.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/99.b76f6782.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.d6069d94.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.902734ab.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.09a2e4e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.992683c3.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.af696277.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.3fc51ae4.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.5c4685e8.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.8bb8d28e.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.3f3e7378.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.78c2a71f.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.3a1c8f02.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.4fcd5343.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.6c6f8b65.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.e06c2940.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.5e01157b.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.4c05e110.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.75825c1f.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.e954bd37.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.88c87723.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.30af0b9d.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.c9dbe636.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.c655df22.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.cc100129.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.34708d2e.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.c805e1b4.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.6cce2ea2.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.2c01703f.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.39c82446.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.44e6c610.js"><link rel="prefetch" href="/qishao-notes/assets/js/126.f8ff43f8.js"><link rel="prefetch" href="/qishao-notes/assets/js/127.368492f9.js"><link rel="prefetch" href="/qishao-notes/assets/js/128.f3c523db.js"><link rel="prefetch" href="/qishao-notes/assets/js/129.e8146138.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.75d8be97.js"><link rel="prefetch" href="/qishao-notes/assets/js/130.e09685cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/131.c114de4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/132.5cb71761.js"><link rel="prefetch" href="/qishao-notes/assets/js/133.9638b7d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/134.340009e0.js"><link rel="prefetch" href="/qishao-notes/assets/js/135.07d7449e.js"><link rel="prefetch" href="/qishao-notes/assets/js/136.ecf56cf4.js"><link rel="prefetch" href="/qishao-notes/assets/js/137.c756b4b7.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.466ec3ac.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.bea831d7.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.05da202f.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.63fdd997.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.bc4a5b1d.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.a2c2ac4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a4b6fb1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.8a5b62d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.4d84fdc5.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.f78907de.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.f4835553.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.420a689a.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.2aa82bc2.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.ae348b88.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.65f9dba1.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.c8b0d700.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.3d907fd4.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.fda31cf7.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.aa6fd9d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.96306ad4.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.7f37f31a.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.54012013.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.01d6cf82.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.093d9d1a.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.0a5f6fb9.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.9f9585d9.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.60eee2cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.2dcb8de5.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.768b44d0.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.4afd8139.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.45621e3e.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.9a39b8df.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.28d1accb.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.acbac3e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.5ad3e7da.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.fce7d840.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.e19be716.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.c2547090.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.9d58ea7e.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.4a3eef72.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.a1fb9aec.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.a8217417.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.00cfb273.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.0467e711.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.545b33b8.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.63d5abb5.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.89586274.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.a1fbdc11.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.5a90503c.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.bca7e213.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.257e363f.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.cd292c5d.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.f62341ae.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.b9c3a1f9.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.163b25e0.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.af5651e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.7bd2530e.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.3ecd1c5a.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.10a96040.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.9848de59.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.3eecfe51.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.cef4957c.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.5f80ee18.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.d2933de4.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.73c3f331.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.e1314a60.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.ae6e2e24.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.e9423d0f.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.b86260cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.f26bc225.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.d0c64796.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.10049696.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.735ba55b.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.16b90043.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.d72017e1.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.e927151e.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.387e755d.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.46751589.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.352afce0.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.10b1aa96.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.b6460627.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.b48cdf4e.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.b907943c.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.c440100e.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.3d3b0253.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.c4d07b67.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.7463a61b.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.ebf130d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.0809b395.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.3092cbcc.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.a8925bae.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.1ba3852a.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Paper List</a></li><li><a href="/qishao-notes/pages/dc7043/" class="sidebar-link">Efficient LLM</a></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" aria-current="page" class="active sidebar-link">Summery of Inner Workings of LLM</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7046/#_1-attention-block" class="sidebar-link">1. Attention Block</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_1-1-attention-heads-with-interpretable-attention-weights-patterns" class="sidebar-link">1.1 Attention Heads with Interpretable Attention Weights Patterns</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_1-2-attention-heads-with-interpretable-qk-and-ov-circuits" class="sidebar-link">1.2 Attention Heads with Interpretable QK and OV Circuits</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_1-3-other-noteworthy-attention-properties" class="sidebar-link">1.3 Other Noteworthy Attention Properties</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7046/#_2-feedforward-network-block" class="sidebar-link">2. Feedforward Network Block</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_2-1-neuron-s-input-behavior" class="sidebar-link">2.1 Neuron's Input Behavior</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_2-2-neuron-s-output-behavior" class="sidebar-link">2.2 Neuron's Output Behavior</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_2-3-polysemantic-neurons" class="sidebar-link">2.3 Polysemantic Neurons</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_2-4-universality-of-neurons" class="sidebar-link">2.4 Universality of Neurons</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7046/#_3-residual-stream" class="sidebar-link">3. Residual Stream</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_3-1-information-flow-in-the-residual-stream" class="sidebar-link">3.1 Information Flow in the Residual Stream</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_3-2-outlier-dimensions" class="sidebar-link">3.2 Outlier Dimensions</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_3-3-features-in-the-residual-stream" class="sidebar-link">3.3 Features in the Residual Stream</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7046/#_4-emergent-multi-component-behavior" class="sidebar-link">4. Emergent Multi-component Behavior</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_4-1-evidence-of-multi-component-behavior" class="sidebar-link">4.1 Evidence of Multi-component Behavior</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_4-2-circuits-analysis" class="sidebar-link">4.2 Circuits Analysis</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_4-3-generality-of-circuits" class="sidebar-link">4.3 Generality of Circuits</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7046/#_5-factuality-and-hallucinations-in-model-predictions" class="sidebar-link">5. Factuality and Hallucinations in Model Predictions</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_5-1-intrinsic-views-on-hallucinatory-behavior" class="sidebar-link">5.1 Intrinsic Views on Hallucinatory Behavior</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_5-2-recall-of-factual-associations" class="sidebar-link">5.2 Recall of Factual Associations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7046/#_5-3-factuality-issues-and-model-editing" class="sidebar-link">5.3 Factuality Issues and Model Editing</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7046/#conclusion" class="sidebar-link">Conclusion</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7046/#_7-2024-interpreting-attention-layer-outputs-with-sparse-autoencoders" class="sidebar-link">[7 2024] Interpreting Attention Layer Outputs with Sparse Autoencoders</a></li></ul></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" class="sidebar-link">Memory Optimizations in LLM</a></li><li><a href="/qishao-notes/pages/dc7049/" class="sidebar-link">Reasoning in LLM</a></li><li><a href="/qishao-notes/pages/dc7050/" class="sidebar-link">LLM Mixed Precision &amp; Quantization &amp; Outlier</a></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" class="sidebar-link">LLM Scaling Law</a></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7059/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li><li><a href="/qishao-notes/pages/dc7060/" class="sidebar-link">LLM MOE Inference</a></li><li><a href="/qishao-notes/pages/dc7061/" class="sidebar-link">LLM Compression</a></li><li><a href="/qishao-notes/pages/dc7062/" class="sidebar-link">LLM Optimizer Optimization</a></li><li><a href="/qishao-notes/pages/dc7063/" class="sidebar-link">LLM Posttraining</a></li><li><a href="/qishao-notes/pages/dc7064/" class="sidebar-link">LLM MICRO - ISCA - HPCA</a></li><li><a href="/qishao-notes/pages/dc7066/" class="sidebar-link">LLM Prefilling &amp; Decoding Split</a></li><li><a href="/qishao-notes/pages/dc7067/" class="sidebar-link">Thinking of LLM Prefilling &amp; Decoding Split</a></li><li><a href="/qishao-notes/pages/dc7068/" class="sidebar-link">From Attention Sink to Massive Activation</a></li><li><a href="/qishao-notes/pages/dc7069/" class="sidebar-link">LLM CPU &amp; GPU Workloads</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-01-26</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">Summery of Inner Workings of LLM<!----></h1> <!----> <div class="theme-vdoing-content content__default"><h1 id="discovered-inner-behaviors-of-transformer-based-language-models"><a href="#discovered-inner-behaviors-of-transformer-based-language-models" class="header-anchor">#</a> Discovered Inner Behaviors of Transformer-based Language Models</h1> <p>In this blog, we will explore the <strong>discovered inner behaviors</strong> of Transformer-based language models (LMs) as outlined in the paper 👍 👍 <em>&quot;A Primer on the Inner Workings of Transformer-based Language Models&quot;</em>. The paper provides a comprehensive overview of the internal mechanisms that enable these models to perform complex language tasks. We will break down the findings into five key sections:</p> <ol><li><strong>Attention Block</strong></li> <li><strong>Feedforward Network Block</strong></li> <li><strong>Residual Stream</strong></li> <li><strong>Emergent Multi-component Behavior</strong></li> <li><strong>Factuality and Hallucinations in Model Predictions</strong></li></ol> <p>Let’s dive into each of these sections to understand how these components contribute to the overall functioning of Transformer-based LMs.</p> <hr> <h2 id="_1-attention-block"><a href="#_1-attention-block" class="header-anchor">#</a> 1. Attention Block</h2> <p>The <strong>attention mechanism</strong> is a cornerstone of Transformer models, allowing them to contextualize token representations at each layer. The attention block consists of multiple <strong>attention heads</strong>, each responsible for attending to different parts of the input sequence. The paper categorizes the behaviors of attention heads into two main groups:</p> <h3 id="_1-1-attention-heads-with-interpretable-attention-weights-patterns"><a href="#_1-1-attention-heads-with-interpretable-attention-weights-patterns" class="header-anchor">#</a> 1.1 Attention Heads with Interpretable Attention Weights Patterns</h3> <ul><li><strong>Positional Heads</strong>: These heads attend to specific positions relative to the current token, such as the previous or next token. For example, <strong>previous token heads</strong> are crucial for copying information from the previous token to the current position, which is essential for tasks like name concatenation.</li> <li><strong>Subword Joiner Heads</strong>: These heads focus on subwords that belong to the same word, helping the model understand word-level structures.</li> <li><strong>Syntactic Heads</strong>: These heads attend to tokens with specific syntactic roles, such as subjects or objects, and are crucial for understanding grammatical relationships.</li> <li><strong>Duplicate Token Heads</strong>: These heads attend to previous occurrences of the same token, which is useful for tasks like identifying repeated names in a context.</li></ul> <h3 id="_1-2-attention-heads-with-interpretable-qk-and-ov-circuits"><a href="#_1-2-attention-heads-with-interpretable-qk-and-ov-circuits" class="header-anchor">#</a> 1.2 Attention Heads with Interpretable QK and OV Circuits</h3> <ul><li><strong>Copying Heads</strong>: These heads have <strong>OV (output-value) circuits</strong> that exhibit copying behavior, meaning they can replicate information from one part of the sequence to another.</li> <li><strong>Induction Heads</strong>: These heads are responsible for completing patterns. For example, given a sequence like &quot;A B ... A&quot;, the model predicts &quot;B&quot;. This mechanism involves two heads: a <strong>previous token head</strong> that writes information into the residual stream, and an <strong>induction head</strong> that reads this information to complete the pattern.</li> <li><strong>Copy Suppression Heads</strong>: These heads reduce the logit score of a token if it appears in the context and is being confidently predicted. This mechanism improves model calibration by preventing naive copying.</li> <li><strong>Successor Heads</strong>: These heads predict the next element in an ordinal sequence (e.g., &quot;Monday&quot; → &quot;Tuesday&quot;). They rely on the output of the first <strong>feedforward network (FFN)</strong> block, which encodes a numerical structure.</li></ul> <h3 id="_1-3-other-noteworthy-attention-properties"><a href="#_1-3-other-noteworthy-attention-properties" class="header-anchor">#</a> 1.3 Other Noteworthy Attention Properties</h3> <ul><li><strong>Domain Specialization</strong>: Some heads are specialized for specific domains, such as non-English languages or coding sequences.</li> <li><strong>Attention Sinks</strong>: Certain heads attend to special tokens (e.g., BOS or punctuation) when their specialized function is not applicable. This behavior is crucial for streaming generation and model performance.</li></ul> <hr> <h2 id="_2-feedforward-network-block"><a href="#_2-feedforward-network-block" class="header-anchor">#</a> 2. Feedforward Network Block</h2> <p>The <strong>feedforward network (FFN)</strong> block is another critical component of Transformer models. It consists of two learnable weight matrices and an element-wise non-linear activation function. The FFN block has been studied extensively, with a focus on the behavior of individual neurons.</p> <h3 id="_2-1-neuron-s-input-behavior"><a href="#_2-1-neuron-s-input-behavior" class="header-anchor">#</a> 2.1 Neuron's Input Behavior</h3> <ul><li><strong>Position Ranges</strong>: Some neurons fire exclusively on specific position ranges within the input sequence.</li> <li><strong>Skill Neurons</strong>: These neurons activate based on the task of the input prompt, such as detecting whether the input is Python code or French text.</li> <li><strong>Concept-Specific Neurons</strong>: These neurons respond to specific concepts, such as grammatical features or semantic roles.</li></ul> <h3 id="_2-2-neuron-s-output-behavior"><a href="#_2-2-neuron-s-output-behavior" class="header-anchor">#</a> 2.2 Neuron's Output Behavior</h3> <ul><li><strong>Knowledge Neurons</strong>: These neurons are responsible for predicting factual information, such as the capital of a country.</li> <li><strong>Linguistically Acceptable Predictions</strong>: Some neurons ensure that the model's predictions are grammatically correct, such as predicting the correct verb number based on the subject.</li> <li><strong>Token Frequency Neurons</strong>: These neurons adjust the logits of tokens based on their frequency in the training data, shifting the output distribution towards or away from the unigram distribution.</li></ul> <h3 id="_2-3-polysemantic-neurons"><a href="#_2-3-polysemantic-neurons" class="header-anchor">#</a> 2.3 Polysemantic Neurons</h3> <ul><li><strong>N-gram Detectors</strong>: Many neurons in early layers specialize in detecting n-grams, but they often fire on a large number of unrelated n-grams, indicating <strong>polysemanticity</strong>.</li> <li><strong>Dead Neurons</strong>: Some neurons in models like OPT remain inactive (zero activation) due to the ReLU activation function.</li></ul> <h3 id="_2-4-universality-of-neurons"><a href="#_2-4-universality-of-neurons" class="header-anchor">#</a> 2.4 Universality of Neurons</h3> <ul><li><strong>Universal Neurons</strong>: Across different models, a small subset of neurons (1-5%) activate on the same inputs. These include <strong>alphabet neurons</strong>, <strong>previous token neurons</strong>, and <strong>entropy neurons</strong>, which modulate the model's uncertainty over the next token prediction.</li></ul> <hr> <h2 id="_3-residual-stream"><a href="#_3-residual-stream" class="header-anchor">#</a> 3. Residual Stream</h2> <p>The <strong>residual stream</strong> is the main communication channel in a Transformer model. It carries information from one layer to the next, with each layer adding its updates to the stream.</p> <h3 id="_3-1-information-flow-in-the-residual-stream"><a href="#_3-1-information-flow-in-the-residual-stream" class="header-anchor">#</a> 3.1 Information Flow in the Residual Stream</h3> <ul><li><strong>Direct Path</strong>: The direct path from the input embedding to the unembedding matrix mainly models bigram statistics.</li> <li><strong>Memory Management</strong>: Some components, such as attention heads and FFN neurons, remove information from the residual stream to manage memory. For example, certain neurons write vectors in the opposite direction of what they read, effectively canceling out information.</li></ul> <h3 id="_3-2-outlier-dimensions"><a href="#_3-2-outlier-dimensions" class="header-anchor">#</a> 3.2 Outlier Dimensions</h3> <ul><li><strong>Rogue Dimensions</strong>: These are dimensions in the residual stream with unusually large magnitudes. They are associated with the generation of <strong>anisotropic representations</strong>, where the residual stream states of random tokens tend to point in the same direction. Ablating these dimensions significantly decreases model performance, suggesting they encode task-specific knowledge.</li></ul> <h3 id="_3-3-features-in-the-residual-stream"><a href="#_3-3-features-in-the-residual-stream" class="header-anchor">#</a> 3.3 Features in the Residual Stream</h3> <ul><li><strong>Sparse Autoencoders (SAEs)</strong>: SAEs have been used to identify interpretable features in the residual stream. These features include <strong>local context features</strong>, <strong>partition features</strong> (which promote or suppress specific sets of tokens), and <strong>suppression features</strong> (which reduce the likelihood of certain tokens).</li></ul> <hr> <h2 id="_4-emergent-multi-component-behavior"><a href="#_4-emergent-multi-component-behavior" class="header-anchor">#</a> 4. Emergent Multi-component Behavior</h2> <p>Transformer models achieve their remarkable performance through the <strong>interaction of multiple components</strong>. The paper highlights several examples of emergent behaviors that arise from these interactions.</p> <h3 id="_4-1-evidence-of-multi-component-behavior"><a href="#_4-1-evidence-of-multi-component-behavior" class="header-anchor">#</a> 4.1 Evidence of Multi-component Behavior</h3> <ul><li><strong>Induction Mechanism</strong>: This mechanism involves two attention heads working together to complete patterns. A <strong>previous token head</strong> writes information into the residual stream, and an <strong>induction head</strong> reads this information to predict the next token.</li> <li><strong>Function Vectors</strong>: Multiple attention heads create <strong>function vectors</strong> that describe a task when given in-context examples. Intervening in the residual stream with these vectors can produce outputs that align with the encoded task.</li></ul> <h3 id="_4-2-circuits-analysis"><a href="#_4-2-circuits-analysis" class="header-anchor">#</a> 4.2 Circuits Analysis</h3> <ul><li><strong>IOI Circuit</strong>: In the <strong>Indirect Object Identification (IOI)</strong> task, the model identifies the correct name (e.g., &quot;Mary&quot;) in a sentence like &quot;When Mary and John went to the store, John gave a drink to _____&quot;. The circuit involves <strong>duplicate token heads</strong>, <strong>name mover heads</strong>, and <strong>negative mover heads</strong> (which suppress incorrect predictions).</li> <li><strong>Greater-than Circuit</strong>: In the <strong>greater-than task</strong>, the model predicts a year greater than a given year (e.g., &quot;1814&quot; → &quot;1815&quot;). The circuit involves attention heads that attend to the initial date and FFNs that compute the correct year.</li></ul> <h3 id="_4-3-generality-of-circuits"><a href="#_4-3-generality-of-circuits" class="header-anchor">#</a> 4.3 Generality of Circuits</h3> <ul><li><strong>Fine-tuning</strong>: The functionality of circuit components remains consistent after fine-tuning, suggesting that fine-tuning improves the encoding of task-relevant information rather than rearranging the circuit.</li> <li><strong>Grokking</strong>: The sudden emergence of generalization capabilities in models (known as <strong>grokking</strong>) is linked to the formation of sparse circuits that replace dense, memorizing sub-networks.</li></ul> <hr> <h2 id="_5-factuality-and-hallucinations-in-model-predictions"><a href="#_5-factuality-and-hallucinations-in-model-predictions" class="header-anchor">#</a> 5. Factuality and Hallucinations in Model Predictions</h2> <p>One of the challenges with large language models is their tendency to generate <strong>factually incorrect or nonsensical outputs</strong> (hallucinations). The paper explores the internal mechanisms behind these behaviors.</p> <h3 id="_5-1-intrinsic-views-on-hallucinatory-behavior"><a href="#_5-1-intrinsic-views-on-hallucinatory-behavior" class="header-anchor">#</a> 5.1 Intrinsic Views on Hallucinatory Behavior</h3> <ul><li><strong>Probing for Truthfulness</strong>: Probes trained on middle and last layers' representations can predict the truthfulness of model outputs.</li> <li><strong>Truthfulness Directions</strong>: Linear interventions in the direction of &quot;truthfulness&quot; can enhance the factual accuracy of model predictions.</li></ul> <h3 id="_5-2-recall-of-factual-associations"><a href="#_5-2-recall-of-factual-associations" class="header-anchor">#</a> 5.2 Recall of Factual Associations</h3> <ul><li><strong>Factual Recall Circuit</strong>: The model recalls factual information through a multi-step process. Early-middle FFNs add information about the subject to the residual stream, while later attention heads extract the correct attribute (e.g., the capital of a country).</li> <li><strong>Additive Mechanism</strong>: Attention heads' OV circuits decode attributes by combining information from the subject and relation. Some heads are <strong>subject heads</strong> (independent of the relation), <strong>relation heads</strong> (independent of the subject), and <strong>mixed heads</strong> (dependent on both).</li></ul> <h3 id="_5-3-factuality-issues-and-model-editing"><a href="#_5-3-factuality-issues-and-model-editing" class="header-anchor">#</a> 5.3 Factuality Issues and Model Editing</h3> <ul><li><strong>Model Editing</strong>: Techniques like <strong>causal interventions</strong> and <strong>knowledge neuron localization</strong> have been used to edit factual associations in models. However, these approaches face challenges like <strong>catastrophic forgetting</strong> and <strong>downstream performance loss</strong>.</li></ul> <hr> <h2 id="conclusion"><a href="#conclusion" class="header-anchor">#</a> Conclusion</h2> <p>The inner workings of Transformer-based language models are complex and multifaceted. By understanding the behaviors of individual components like attention heads, FFN neurons, and the residual stream, we can gain insights into how these models process and generate language. Moreover, the emergent behaviors that arise from the interaction of these components highlight the importance of studying models as a whole, rather than focusing on individual parts.</p> <p>As research in this area continues, we can expect to uncover even more fascinating details about how these models work, paving the way for more interpretable, reliable, and safe AI systems.</p> <hr> <p><em>This blog is based on the paper &quot;A Primer on the Inner Workings of Transformer-based Language Models&quot; by Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R. Costa-jussà.</em></p> <hr> <h2 id="_7-2024-interpreting-attention-layer-outputs-with-sparse-autoencoders"><a href="#_7-2024-interpreting-attention-layer-outputs-with-sparse-autoencoders" class="header-anchor">#</a> [7 2024] Interpreting Attention Layer Outputs with Sparse Autoencoders</h2> <p>The paper provides a layer-wise understanding of how attention outputs evolve in terms of feature utilization:</p> <ul><li>Early Layers (0-3):
<ul><li>Features:
<ul><li>Focus on simple syntactic patterns (e.g., token pairs, local relationships).</li> <li>Begin building short-range context features.</li></ul></li> <li>Utilization:
<ul><li>Early heads primarily focus on short-range interactions and token-to-token dependencies.</li></ul></li></ul></li> <li>Middle Layers (4-9):
<ul><li>Features:
<ul><li>Capture more abstract, semantic patterns (e.g., grammatical and reasoning constructs, topic tracking).</li> <li>Generate long-range context features and induction features.</li></ul></li> <li>Utilization:
<ul><li>Middle heads integrate global information and start building context-aware features (e.g., reasoning or induction patterns).</li></ul></li></ul></li> <li>Late Layers (10-11):
<ul><li>Features:
<ul><li>Refine and finalize features for specific tasks like grammatical adjustments, long-range predictions, and sequence completions.</li></ul></li> <li>Utilization:
<ul><li>Late heads mostly adjust or finalize token choices using refined long-range context features.</li></ul></li></ul></li></ul> <p><img src="https://github.com/user-attachments/assets/28f89d23-0afb-4b65-87ec-a878e3965de9" alt="image"></p> <p>The study also provides insights into how these features are distributed and utilized across layers and attention heads:</p> <p><strong>1. Long-Range Context Features</strong></p> <ul><li><p><strong>Function</strong> : Capture information that spans long distances in the text (e.g., maintaining the topic or theme of a paragraph).</p></li> <li><p><strong>Source</strong> : Typically generated in <strong>middle to late layers</strong> .</p> <ul><li><p>Middle layers begin integrating broader semantic and contextual information.</p></li> <li><p>Late layers refine the contextual understanding for tasks like logical reasoning or high-level decision-making.</p></li></ul></li> <li><p><strong>Head Specialization</strong> : Some heads specialize in aggregating context from far-away tokens, while others focus on more localized interactions.</p></li></ul> <p><strong>2. Short-Range Context Features</strong></p> <ul><li><p><strong>Function</strong> : Focus on localized relationships, such as syntactic dependencies (e.g., a word's immediate neighbors).</p></li> <li><p><strong>Source</strong> : Dominantly found in <strong>early and middle layers</strong> .</p> <ul><li><p>Early layers handle low-level syntactic features like token pairs and adjacent word dependencies.</p></li> <li><p>Some middle-layer heads expand on short-range features by integrating grammatical constructs.</p></li></ul></li> <li><p><strong>Head Specialization</strong> : Specific attention heads in early layers are responsible for capturing these short-range patterns.</p></li></ul> <p><strong>3. Induction Features</strong></p> <ul><li><p><strong>Function</strong> : Capture patterns for <strong>in-context learning</strong> , such as recognizing repeated prefixes in sequences (e.g., completing &quot;...ABC...AB&quot; with &quot;C&quot;).</p></li> <li><p><strong>Source</strong> : Strongly tied to <strong>specific induction heads</strong>  in attention layers.</p> <ul><li><p>These heads are often polysemantic but include roles specialized for induction tasks.</p></li> <li><p>Example: In GPT-2 Small, the study identifies two <strong>layer 5 heads (5.1 and 5.5)</strong>  that specialize in <strong>long-prefix</strong>  and <strong>short-prefix induction</strong> , respectively.</p></li></ul></li> <li><p><strong>Head Specialization</strong> :</p> <ul><li>Induction features are directly associated with <strong>attention heads</strong>  that attend to patterns of repeated tokens.</li></ul></li></ul></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/11.inner_working.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/05/17, 00:14:31</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7045/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Estimation of LLM</div></a> <a href="/qishao-notes/pages/dc7047/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">List of LLM Optimization Techniques</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7045/" class="prev">Estimation of LLM</a></span> <span class="next"><a href="/qishao-notes/pages/dc7047/">List of LLM Optimization Techniques</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.c10016d1.js" defer></script><script src="/qishao-notes/assets/js/2.0833fe67.js" defer></script><script src="/qishao-notes/assets/js/99.b76f6782.js" defer></script>
  </body>
</html>
