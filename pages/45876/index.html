<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>GPU Cache &amp; Memory Hirerarchy | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.f2094338.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.75973713.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/48.55f1d93a.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.ff60f5f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.35356818.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.97db4364.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.117c9063.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.d32097b1.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.74839805.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.273c8e9e.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.be625961.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.c1ff1604.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.9f7c512b.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.51f7fa25.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.c94fe2f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.786d6446.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.cb104322.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.eee2e386.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.6892530d.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.9a920fc3.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.8d8251e3.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.e91eb693.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.f587e522.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.f44cb335.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.a830cfc8.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.887d10e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.87750de9.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.19f8db5c.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.ffcf141e.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.d949676c.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.613f8a8d.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.dc096ab8.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.2817b3b0.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.3132cade.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.424759af.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.a6a3704a.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.533a2027.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.6032efe5.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.92d878f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.4efc466a.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.9425ffca.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.2fd6db52.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.9f2bbdaa.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.0980234f.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.1d4e5a23.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.7aec4caa.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.7ceac95f.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.7ba47fe9.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.6076cdb7.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.f0662b6a.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.c89d24a9.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.57f24cda.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.51b34ab2.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.33120e14.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.c9957ca1.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.938d7909.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.b3f76948.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.d99b7689.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.565651c3.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.b64c2530.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.9016cd21.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.f05a253c.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.12c3bcfd.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.376f973d.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.d36a81f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.ff752070.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.bd09a0eb.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.316deedd.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.3863430d.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="ÁõÆÂΩï" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">llm</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="/qishao-notes/message-board/" class="nav-link">BBS</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">llm</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="/qishao-notes/message-board/" class="nav-link">BBS</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/cc7034/" class="sidebar-link">Operand Collector</a></li><li><a href="/qishao-notes/pages/2476ae/" class="sidebar-link">GPU WARP Scheduler</a></li><li><a href="/qishao-notes/pages/14769f/" class="sidebar-link">Precision Exception</a></li><li><a href="/qishao-notes/pages/44771e/" class="sidebar-link">Unified Memory Paper List</a></li><li><a href="/qishao-notes/pages/44871e/" class="sidebar-link">TensorCore Paper List</a></li><li><a href="/qishao-notes/pages/45871e/" class="sidebar-link">Memory Behaviour Paper List</a></li><li><a href="/qishao-notes/pages/45871f/" class="sidebar-link">GPU Virtualization Paper List</a></li><li><a href="/qishao-notes/pages/458720/" class="sidebar-link">Large Language Model Paper List</a></li><li><a href="/qishao-notes/pages/458721/" class="sidebar-link">GPU Simulator</a></li><li><a href="/qishao-notes/pages/458722/" class="sidebar-link">Architectural Survey</a></li><li><a href="/qishao-notes/pages/458724/" class="sidebar-link">Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper</a></li><li><a href="/qishao-notes/pages/458725/" class="sidebar-link">Understanding GPGPU-SIM &amp; GPGPU-SIM UVM_SMART (1)</a></li><li><a href="/qishao-notes/pages/458726/" class="sidebar-link">Understanding GPGPU-SIM &amp; GPGPU-SIM UVM_SMART (2)</a></li><li><a href="/qishao-notes/pages/458727/" class="sidebar-link">Understanding GPGPU-SIM &amp; GPGPU-SIM UVM_SMART (3)</a></li><li><a href="/qishao-notes/pages/45872/" class="sidebar-link">Understanding GPGPU-SIM &amp; GPGPU-SIM UVM_SMART (4)</a></li><li><a href="/qishao-notes/pages/45874/" class="sidebar-link">Understanding GPGPU-SIM &amp; GPGPU-SIM UVM_SMART (5)</a></li><li><a href="/qishao-notes/pages/45873/" class="sidebar-link">Warp Related Memory Optimization</a></li><li><a href="/qishao-notes/pages/45875/" class="sidebar-link">GPU Cache Coherency</a></li><li><a href="/qishao-notes/pages/45876/" aria-current="page" class="active sidebar-link">GPU Cache &amp; Memory Hirerarchy</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/qishao-notes/pages/45877/" class="sidebar-link">GPU TLB</a></li><li><a href="/qishao-notes/pages/45878/" class="sidebar-link">gpu page table walk</a></li><li><a href="/qishao-notes/pages/47871e/" class="sidebar-link">TO READ</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="È¶ñÈ°µ" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/gpu/#gpu" data-v-06225672>gpu</a></li></ul> <div class="info" data-v-06225672><div title="‰ΩúËÄÖ" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="‰ΩúËÄÖ" class="beLink" data-v-06225672>hitqishao</a></div> <div title="ÂàõÂª∫Êó∂Èó¥" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2024-08-25</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">ÁõÆÂΩï</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">GPU Cache &amp; Memory Hirerarchy<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[248] Dissecting GPU Memory Hierarchy through Microbenchmarking</li> <li>[75] Benchmarking the Memory Hierarchy of Modern GPUs</li> <li>[18] Benchmarking the GPU memory at the warp level</li> <li>[90] Dissecting the NVidia Turing T4 GPU via Microbenchmarking</li> <li>[38] Exploring Modern GPU Memory System Design Challenges through Accurate Modeling üëç üëç üëç</li> <li>[9] OSM: Off-Chip Shared Memory for GPUs</li></ol> <hr> <h3 id="_1-dissecting-gpu-memory-hierarchy-through-microbenchmarking"><a href="#_1-dissecting-gpu-memory-hierarchy-through-microbenchmarking" class="header-anchor">#</a> 1. Dissecting GPU Memory Hierarchy through Microbenchmarking</h3> <p>A paper in 2015, profile memory in Fermi, Kepler and Maxwell</p> <p><img src="https://github.com/user-attachments/assets/683d67af-3feb-4d35-9ecf-dfeafb814c37" alt="image"></p> <h4 id="parameter"><a href="#parameter" class="header-anchor">#</a> Parameter</h4> <p><img src="https://github.com/user-attachments/assets/08215b14-4856-4d3a-8c6a-b5050f905f02" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/5daed100-0155-4fed-9358-e26681294b2a" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/60213279-226b-4a30-aa05-36271e9ac0ff" alt="image"></p> <h4 id="l1-data-cache"><a href="#l1-data-cache" class="header-anchor">#</a> L1 Data Cache</h4> <p>On the Fermi and Kepler devices, the L1 data cache and shared memory are physically implemented together.<br>
On the Maxwell devices, the L1 data cache is unified with the texture cache.</p> <p>The 16 KB L1 cache has 128 cache lines mapped onto four cache ways.<br>
For each cache way, 32 cache sets are divided into 8 major sets. Each major set contains 16 cache lines.</p> <p>The data mapping is also unconventional.<br>
The 12-13th bits in the memory address define the cache way, the 9-11th bits define the major set, and the 0-6th bits define the memory offset inside the cache line.
<img src="https://github.com/user-attachments/assets/f997bf94-4b5b-4948-882c-7f72dd7bd506" alt="image"></p> <p>One distinctive feature of the Fermi L1 cache is that its replacement policy is not LRU, as pointed out by Meltzer et.al.
Among the four cache ways, cache way 2 is three times more likely to be replaced than the other three cache ways.</p> <p><strong>Another paper[4]</strong> We found that when the L1 data cache saturates, Turing randomly evicts 4 consecutive cache lines (128 B).<br>
We observed that once a block of cache lines are evicted, the second scan will cause more cache lines from the same set to be evicted.</p> <p><img src="https://github.com/user-attachments/assets/ae6a8abd-7d57-4e0c-98ea-12264a37ae75" alt="image"></p> <h4 id="l2-data-cache"><a href="#l2-data-cache" class="header-anchor">#</a> L2 Data Cache</h4> <ul><li>The replacement policy of the L2 cache is not LRU</li> <li><strong>The L2 cache line size is 32 bytes</strong> by observing the memory access pattern of overflowing the cache and visiting array element one by one.</li> <li>The data mapping is sophisticated and not conventional bits-defined</li> <li>a hardware-level pre-fetching mechanism from the DRAM to the L2 data cache on all three platforms.<br> <strong>The pre-fetching size is about 2/3 of the L2 cache size and the prefetching is sequential. This is deduced from that if we load an array smaller than 2/3 of the L2 data cache size, there is no cold cache miss patterns.</strong><br>
üôã(Maybe they can cover the gap just by prefetching sequential line.)</li></ul> <h4 id="global-memory"><a href="#global-memory" class="header-anchor">#</a> Global Memory</h4> <p>global memory access involves accessing the DRAM, L1 and L2 data caches, TLBs and page tables.</p> <h5 id="global-memory-throughput"><a href="#global-memory-throughput" class="header-anchor">#</a> Global Memory Throughput</h5> <p>The theoretical bandwidth is calculated as fmem * bus width * DDR factor.
<img src="https://github.com/user-attachments/assets/dbb8cdc6-e0cd-4bc6-aec8-f9450ea6d0bf" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/707f8b05-88e6-40b2-b3e4-426f984d4405" alt="image"></p> <p>the throughput of a larger ILP saturates faster.</p> <p>The GTX780 has the highest throughput as it benefits from the highest bus width,<br>
but its convergence speed is the slowest, i.e., it requires the most memory requests to hide the pipeline latency.</p> <p><strong>This could be part of the reason that NVIDIA reduced the bus width back to 256 bits in Maxwell devices.</strong></p> <h5 id="global-memory-latency"><a href="#global-memory-latency" class="header-anchor">#</a> Global Memory Latency</h5> <p><strong>The global memory access latency is the whole time accessing a data located in DRAM/L2 or L1 cache, including the latency of page table look-ups.</strong></p> <ul><li>very large s1 = 32 MB to construct the TLB/page table miss and cache miss (P5&amp;P6)</li> <li>set s2 = 1 MB to construct the L1 TLB hit but cache miss (P4)</li> <li>After a total of 65 data accesses, 65 data lines are loaded into the cache.<br>
We then visit the cached data lines with s1 again for several times, to construct cache hit but TLB miss (P2&amp;P3).</li> <li>set s3 = 1 element and repeatedly load the data in a cache line so that every memory access is a cache hit (P1).</li></ul> <p><img src="https://github.com/user-attachments/assets/e5c397e8-f4b4-46ba-b7ee-41c34fa08b33" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/1f616c8e-a758-4151-b1eb-61f15c810246" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/822fc563-d0dc-4708-afd2-89549adb7ec4" alt="image"></p> <ul><li>The Maxwell and Kepler devices have a unique memory access pattern (P6) for page table context switching. <br>
When a kernel is launched, only memory page entries of 512 MB are activated. <br>
If the thread visits an inactivate page entry, the hardware needs a rather long time to switch between page tables.<br>
This phenomena is also reported in [22] as page table ‚Äúmiss‚Äù.</li> <li>The Maxwell L1 data cache addressing does not go through the TLBs or page tables.<br>
On the GTX980, there is no TLB miss pattern (i.e., P2 and P3) when the L1 data cache is hit.<br>
Once the L1 cache is missed, the access latency increases from tens of cycles to hundreds or even thousands of cycles.
<strong>My comments: But if we look at GTX560Ti in P2, the latency is different with P1. So does this means that in Fermi, the memory request has to go through TLB first, and then access L1 DataCache? This might be the reason that the latency is longer. But this will degrade the performance....</strong></li> <li>The TLBs are off-chip. we infer that the physical memory locations of the L1 TLB and L2 data cache are close. <br>
The physical memory locations of the L1 TLB and L2 TLB are also close, which means that the L1/L2 TLB and L2 data cache are shared off-chip by all SMs.</li> <li>The GTX780 generally has the shortest global memory latencies, almost half that of the Fermi, with an access pattern of P2-P5.<br>
The page table context switching of the GTX980 is also much more expensive than that of the GTX780.</li></ul> <p>To summarize, the Maxwell device has <em>long global memory access latencies</em> for cold cache misses and page table context switching.<br>
Except for these rare access patterns, its access latency cycles are close to those of the Kepler device. <br>
because the GTX980 has higher fmem than the GTX780, it actually offers the shortest global memory access time (P2-P4).</p> <p><img src="https://github.com/user-attachments/assets/8d12e01f-1a6e-49e7-894c-28de28c9f864" alt="image"></p> <h3 id="shared-memory"><a href="#shared-memory" class="header-anchor">#</a> Shared Memory</h3> <p>In CUDA programming, different CTAs assigned to the same SM have to share the same physical memory space.<br>
On the Fermi and Kepler platforms, the shared memory is physically integrated with the L1 cache.<br>
On the Maxwell platform, it occupies a separate memory space.
<strong>Note that the shared memory and L1 cache are separated since Maxwell architecture.</strong></p> <p><em>Programmers</em> move the data into and out of shared memory from global memory before and after arithmetic execution,<br>
to avoid the frequent occurrence of long global memory access latencies.</p> <p><strong>We report a dramatic improvement in performance for the Maxwell device.</strong></p> <h5 id="shared-memory-throughput"><a href="#shared-memory-throughput" class="header-anchor">#</a> Shared Memory Throughput</h5> <p>the shared memory is organized as 32 memory banks [15].<br>
The bank width of the <strong>Fermi and Maxwell devices is 4 bytes</strong>, while that of the Kepler device is 8 bytes.
The theoretical peak throughput of each SM (WSM) is calculated as fcore ‚àó Wbank ‚àó 32.</p> <p><img src="https://github.com/user-attachments/assets/3bf6da77-b196-4e13-b2fe-af410ee750a4" alt="image"></p> <p><strong>The achieved throughput per SM is calculated as 2 * fcore * sizeof(int) * (number of active threads per SM) * ILP / (total latency of each SM).</strong>
Usually a large value of ILP results in less active warps per SM.<br>
The peak throughput W0SM denotes the respective maximum throughput of the abovecombinations.<br>
Two key factors that affect the throughput are the number of active warps per SM and the ILP level.</p> <p>The GTX980 reaches its peak throughput when the CTA size = 256, CTAs per SM = 2 and ILP = 8, i.e., 16 active warps per SM. The peak throughput is 137.41 GB/s, about <em>83.9%</em> of the theoretical bandwidth.
The Maxwell device shows the best use of its shared memory bandwidth, and the Kepler device shows the worst.</p> <p>GTX980 exhibits similar behavior as GTX780: high ILP is required to achieve high throughput for high SM occupancy.</p> <p>According to Little‚Äôs Law, we roughly have: number of active warps * ILP = latency cycles * throughput.</p> <p><strong>GTX780 sucks in ILP = 1, since its limited 64 warps at most to be scheduled concurrently.</strong><br>
We consider this to be the main reason the achieved throughput of the GTX780 is poor compared with its designed value.</p> <h4 id="shared-memory-latency"><a href="#shared-memory-latency" class="header-anchor">#</a> Shared Memory Latency</h4> <p><strong>The shared memory latencies on Fermi, Kepler and Maxwell devices are 50, 47 and 28 cycles, respectively.</strong></p> <p><strong>Fermi and Maxwell devices have the same number of potential bank conflicts because they have the same architecture.</strong></p> <p>The shared memory space is divided into 32 banks.<br>
Successive words are allocated to successive banks.<br>
If two threads in the same warp access memory spaces in the same bank, a 2-way bank conflict occurs.</p> <p><img src="https://github.com/user-attachments/assets/c5ef66d3-c05e-46b7-84d6-ace224aafeab" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/c8e4560b-68aa-4188-9621-05a9f90fca32" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/f8e02655-9d5d-4d80-9bfb-fb6e5aefde8f" alt="image"></p> <p>For the Fermi and Kepler devices, where there is a 32-way bank conflict, it takes much longer to access shared memory than regular global memory (TLB hit, cache miss). <br>
Surprisingly, the effect of a bank conflict on shared memory access latency on the Maxwell device is mild.<br>
Even the longest shared memory access latency is still at the same level as L1 data cache latency.</p> <p>In summary, although the shared memory has very short access latency, it can be rather long if there are many ways of bank conflicts.<br>
This is most obvious on the Fermi hardware.<br>
The Kepler device tries to solve it by doubling the bank width of shared memory.<br>
Compared with the Fermi, the Kepler‚Äôs 4-byte mode shared memory halves the chance of bank conflict, and the 8-byte mode reduces it further.</p> <p>However, we also find that the Kepler‚Äôs shared memory is inefficient in terms of throughput.<br>
The Maxwell device has the best shared memory performance.<br>
With the same architecture as the Fermi device, the Maxwell hardware shows a 2x size, 2x memory access speedup and achieves the highest throughput.<br>
Most importantly, the Maxwell device‚Äôs shared memory has been optimized to avoid the long latency caused by bank conflicts.</p> <h4 id="conclusion"><a href="#conclusion" class="header-anchor">#</a> Conclusion</h4> <p>The memory capacity is significantly enhanced in both Kepler and Maxwell as compared with Fermi.<br>
The Kepler device is performance-oriented and incorporates several aggressive elements in its design, such as increasing the bus width of DRAM and doubling the bank width of shared memory.<br>
These designs have some side-effects.<br>
The theoretical bandwidths of both global memory and shared memory are difficult to saturate, and hardware resources are imbalanced with a low utilization rate.<br>
The Maxwell device has a more efficient and conservative design.<br>
It has a reduced bus width and bank width, and the on-chip cache architectures are adjusted, including doubling the shared memory size and the read-only data cache size.<br>
Furthermore, it sharply decreases the shared memory latency caused under bank conflicts.</p> <h3 id="_4-dissecting-the-nvidia-turing-t4-gpu-via-microbenchmarking"><a href="#_4-dissecting-the-nvidia-turing-t4-gpu-via-microbenchmarking" class="header-anchor">#</a> 4. Dissecting the NVidia Turing T4 GPU via Microbenchmarking</h3> <h4 id="result"><a href="#result" class="header-anchor">#</a> Result</h4> <p><img src="https://github.com/user-attachments/assets/eb21b04f-6ce8-44ef-8307-d26c35fa8a86" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/bae4d3b8-b2df-463b-a8f8-c095fbb53c9d" alt="image"></p> <h4 id="shared-memory-latency-2"><a href="#shared-memory-latency-2" class="header-anchor">#</a> Shared Memory Latency</h4> <p><img src="https://github.com/user-attachments/assets/ae7a4300-20fb-4be0-9404-e1c39a223d7d" alt="image"></p> <h4 id="bandwidth"><a href="#bandwidth" class="header-anchor">#</a> Bandwidth</h4> <p><img src="https://github.com/user-attachments/assets/74dc0901-e5a8-4084-b6a1-d8d71175926f" alt="image"></p> <h3 id="_4-benchmarking-the-gpu-memory-at-the-warp-level"><a href="#_4-benchmarking-the-gpu-memory-at-the-warp-level" class="header-anchor">#</a> 4. Benchmarking the GPU memory at the warp level</h3> <p>In this work, we investigate the data accessing capability of a warp of threads: broadcasting and parallel accessing.\</p> <ul><li>Broadcasting occurs when multiple threads access the same data element, i.e., multiple threads request a single data element (MTSD).</li> <li>We refer the case of multiple threads accessing multiple distinct data elements (MTMD) as parallel accessing.</li></ul> <h5 id="local-memory"><a href="#local-memory" class="header-anchor">#</a> Local Memory</h5> <ul><li>For the simple memory access patterns, we should allocate a sufficient small array to guarantee that it is located in registers.</li> <li>For the complex memory access patterns, we should simplify codes to exploit registers. For example, we merge a three-level loop into an one-level loop so that a larger temporal vector can be allocated in registers.</li> <li></li></ul> <h5 id="shared-memory-2"><a href="#shared-memory-2" class="header-anchor">#</a> Shared Memory</h5> <ul><li>Bank conflicts must be avoided by the ways of e.g., data padding.</li> <li>Shared memory supports both broadcasting and parallel accessing.</li> <li>Neither consecutively accessing nor aligned accessing is a must.</li> <li>The latency decreases when the number of threads increase, and thus we should use a sufficiently large thread block.</li> <li>Replacing global memory with shared memory, because the latency of shared memory is smaller than that of global memory.</li> <li>Using shared memory bares an overhead (i.e., buffer allocation and data movement) and reusing data in it is a must for improved performance.</li></ul> <h5 id="constant-memory"><a href="#constant-memory" class="header-anchor">#</a> Constant Memory</h5> <p>But constant memory does not support parallel accessing.<br>
That is, constant memory can only be accessed serially when requesting different data elements.<br>
On the one hand, constant memory is used to store a small amount of read-only data, which is not sensitive to bandwidth.<br>
So parallel accessing is not a must for constant memory.</p> <ul><li>Constant memory supports the accessing capability of broadcasting.</li> <li>Constant memory does not support parallel accessing, and satisfies parallel memory requests in a serial manner.</li></ul> <h5 id="shared-memory-3"><a href="#shared-memory-3" class="header-anchor">#</a> Shared Memory</h5> <ul><li>Global memory supports both broadcasting and parallel accessing.</li> <li>The data types of 4 or 8 bytes can obtain the near upper-bounded bandwidth of global memory, while the data types cannot.<br>
So the char data should be coalesced into the char4 type for improved bandwidth.</li> <li>Global memory accesses should be consecutive, but aligned accessing is not necessary for global memory.</li> <li>When memory accessing is non-consecutive, the latency changes with the number of threads, but not with the number of blocks.
So we should configure the thread dimensionality.</li></ul> <h3 id="_5-exploring-modern-gpu-memory-system-design-challenges-through-accurate-modeling"><a href="#_5-exploring-modern-gpu-memory-system-design-challenges-through-accurate-modeling" class="header-anchor">#</a> 5. Exploring Modern GPU Memory System Design Challenges through Accurate Modeling</h3> <p>üëç üëç üëç</p> <h5 id="memory-coalescer"><a href="#memory-coalescer" class="header-anchor">#</a> Memory Coalescer</h5> <p>the eviction granularity of the cache is 128B, indicating that the L1 cache has 128B lines with 32B sectors.<br>
Furthermore, the coalescer operates across eight threads, i.e. the coalescer tries to coalesce each group of eight threads separately to generate sectored accesses.</p> <p><img src="https://github.com/user-attachments/assets/c3d84400-b121-4152-931c-c40074848909" alt="image"></p> <p>When the stride=32, the memory access is converged, and all the threads within the same warp will access the same cache line,<br>
however we receive four read accesses at L1 cache.</p> <p><strong>8 Thread register 32bit == 32Byte.</strong></p> <h5 id="l2-cache"><a href="#l2-cache" class="header-anchor">#</a> L2 Cache</h5> <p>L2 cache applies something similar to <strong>write-validate</strong> not <strong>fetch on write</strong>.\ üò±
However, all the reads received by L2 caches from the coalescer are 32-byte sectored accesses.<br>
Thus, the read access granularity (32 bytes) is different from the write access granularity (one byte).<br>
To handle this, the L2 cache applies a different write allocation policy, which we named lazy fetch-on-read, that is a compromise between write-validate and fetch-on-write.</p> <p>When a sector read request is received to a modified sector, it first checks if the sector write-mask is complete, i.e. all the bytes have been written to and the line is
fully readable.<br>
If so, it reads the sector, otherwise, similar to fetch-on-write, it generates a read request for this sector and merges it with the modified bytes.</p> <h5 id="streaming-throughput-oriented-l1-cache"><a href="#streaming-throughput-oriented-l1-cache" class="header-anchor">#</a> Streaming Throughput-oriented L1 Cache</h5> <p><img src="https://github.com/user-attachments/assets/8edd3919-c6ff-40df-a207-a0853fcfa161" alt="image"></p> <p>The L1 cache in Volta is what NVIDIA is calling a streaming cache [33].<br>
It is streaming because the documentation states that it allows <strong>unlimited cache misses</strong> to be in flight regardless the number of cache lines per cache set [10].</p> <p>independent of the number of L1 configured size, the number of MSHRs available are the same, even if more of the on-chip SRAM storage is devoted to shared memory.</p> <p>We believe that unified cache is a plain SRAM where sectored data blocks are shared between the L1D and the CUDA shared memory.<br>
It can be configured adaptively by the driver as we discussed earlier.<br>
We assume that the L1D‚Äôs TAG and MSHR merging functionality are combined together in a separate table structure (TAG-MSHR table).<br>
Since, the filling policy is now ON FILL, we can have more TAG entries and outstanding requests than the assigned L1D cache lines.</p> <p>If it is a hit to a reserved sector (i.e. the status is pending), it sets its corresponding warp bit in the merging mask (64 bits for 64 warps).<br>
When the pending request comes back, it allocates a cache line/sector in the data block and sets the allocated block index in the table.<br>
Then, the merged warps access the sector, on a cycle-by-cyle basis.</p> <h3 id="_6-osm-off-chip-shared-memory-for-gpus"><a href="#_6-osm-off-chip-shared-memory-for-gpus" class="header-anchor">#</a> 6. OSM: Off-Chip Shared Memory for GPUs</h3> <p><img src="https://github.com/user-attachments/assets/0fadce93-433a-4889-b130-e2c330d3b334" alt="image"></p> <p>L1-D cache and shared memory use the same 32-bank memory structure (4 KB capacity per bank) as shown in Fig. 4;<br>
however, they have some differences.<br>
We can <strong>access 32-bit shared memory arrays via a thread-index directly</strong>, while for accessing L1-D cache, we should read 128B (four 32B sectors) of the cache block.<br>
In addition, L1 cache requires an extra hardware for managing tags and implementing LRU replacement policy.</p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/03.gpu/19.gpu_cache_mem.md" target="_blank" rel="noopener noreferrer">Â∏ÆÂä©Êàë‰ª¨ÊîπÂñÑÊ≠§È°µÈù¢</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">‰∏äÊ¨°Êõ¥Êñ∞:</span> <span class="time">2024/08/31, 16:00:47</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/45875/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">GPU Cache Coherency</div></a> <a href="/qishao-notes/pages/45877/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">GPU TLB</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ‚Üê
        <a href="/qishao-notes/pages/45875/" class="prev">GPU Cache Coherency</a></span> <span class="next"><a href="/qishao-notes/pages/45877/">GPU TLB</a>‚Üí
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="ÂèëÈÇÆ‰ª∂" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="Êú¨Á´ô‰∏ªÈ¢ò">Vdoing</a> 
    | Copyright ¬© 2022-2024
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="ËøîÂõûÈ°∂ÈÉ®" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="ÂéªËØÑËÆ∫" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="‰∏ªÈ¢òÊ®°Âºè" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          Ë∑üÈöèÁ≥ªÁªü
        </li><li class="iconfont icon-rijianmoshi">
          ÊµÖËâ≤Ê®°Âºè
        </li><li class="iconfont icon-yejianmoshi">
          Ê∑±Ëâ≤Ê®°Âºè
        </li><li class="iconfont icon-yuedu">
          ÈòÖËØªÊ®°Âºè
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.f2094338.js" defer></script><script src="/qishao-notes/assets/js/2.75973713.js" defer></script><script src="/qishao-notes/assets/js/48.55f1d93a.js" defer></script>
  </body>
</html>
