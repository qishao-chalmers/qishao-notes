<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Understanding Pytorch CUDA | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.b4fc0d6d.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.6d8a25ce.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/117.372eb54b.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.3e10e050.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.1dbf4645.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.6006be8e.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.02d8395d.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.295cb0f9.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.a4712551.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.80b95ecd.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.cdbcd2ce.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.d8b905c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.13654181.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.5c90f334.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.86efcdc5.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.3c526caa.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.98ba0d63.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.f63c27cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.2bbbf3ce.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.4130a043.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.1f417c1f.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.c1e87a28.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.2cca0b69.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.7376b712.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.dc4136f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.7edc9e29.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.c68461e3.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.9462a194.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.187f7d46.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.33791185.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.35cdaa5c.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.936b94c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.1c803b2c.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.b688542f.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.d4a145d2.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.13806125.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.31f2ec6d.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.1adc4cd7.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.58ec3c1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.049ba004.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.b08a9f3b.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.db192854.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.ff0533c0.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.223b71e2.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.f0a00d23.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.e1097275.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.5e9a8a76.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.a37871e7.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.9c58429f.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.9c58d827.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.ac7b19be.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.84f10398.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.464f648d.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.588864da.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.c95eff2e.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.d63a3d79.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.fdab2a0b.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.bae99c2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.89b382c5.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.ea1753be.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.f70b72b4.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.56a470f5.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.3cc58cc5.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.dde88fd3.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.4edc7d96.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.f311eaa5.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.526dc690.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.28e90c7e.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.8b03d117.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.b803c07a.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.c400caa3.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.28e64713.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.bfe1dda8.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.9480e434.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.9a082998.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.a3b517e3.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.e6a3f4e4.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.e1d3c447.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.fe5b5324.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.f0fbf90b.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.1780d091.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.65de7c88.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.6190f2a9.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.19c15ad2.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.b80972c3.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.d63f1f61.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.021a0ef2.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.747af7e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.80f9f6ee.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.01ba40c0.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.1e07ff8f.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.3b4ae10c.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.5b06b360.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.bc22dca0.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.3a589a2f.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.8deb1a9c.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.83e128af.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.5116e0c3.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.6437ce99.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.e6fe9a70.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.d6020eb8.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.f90e1643.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.41b3505b.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.fee7524e.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.c7a01bc6.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.25b8022f.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.c2816937.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.fab47ee4.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.07809e54.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.a852b30e.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.06f1629c.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.b8e31aef.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.7b957d82.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.7ccee246.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.8d679031.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.2852c8c2.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.9de1babd.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.08e332f5.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.4bc9f5d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.042f6fe8.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.bcda7059.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.4d7b14f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.a3b16e03.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/f00000/" class="sidebar-link">leakage current moores law meets static power</a></li><li><a href="/qishao-notes/pages/f00001/" class="sidebar-link">Blogs to be read</a></li><li><a href="/qishao-notes/pages/f00002/" class="sidebar-link">Understanding Pytorch Source Code</a></li><li><a href="/qishao-notes/pages/f00003/" class="sidebar-link">Understanding Pytorch Source Code AOT - Inductor IR - Codegen</a></li><li><a href="/qishao-notes/pages/f00004/" class="sidebar-link">CUDA Merge</a></li><li><a href="/qishao-notes/pages/f00006/" class="sidebar-link">CUDA Softmax</a></li><li><a href="/qishao-notes/pages/f00007/" class="sidebar-link">CUDA LayerNorm</a></li><li><a href="/qishao-notes/pages/f00008/" aria-current="page" class="active sidebar-link">Understanding Pytorch CUDA</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/f00008/#pytorch-cuda-memory-allocation" class="sidebar-link">Pytorch CUDA Memory Allocation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#lower-level" class="sidebar-link">Lower level</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#intermediate-level" class="sidebar-link">intermediate level</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#python-level" class="sidebar-link">python level</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#code-gen-for-memory-allocation" class="sidebar-link">code gen for memory allocation</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/f00008/#pytorch-adam-cuda-kernel" class="sidebar-link">Pytorch Adam CUDA Kernel</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#native-functions-yaml" class="sidebar-link">native_functions.yaml</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#fusedadamkernel-cu" class="sidebar-link">FusedAdamKernel.cu</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#fused-adam-impl-cu" class="sidebar-link">fusedadamimpl.cu</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#multitensorapply-cuh" class="sidebar-link">MultiTensorApply.cuh</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/f00008/#pytorch-linear-lowering" class="sidebar-link">Pytorch linear lowering</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#python-nn-functions-cpp" class="sidebar-link">pythonnnfunctions.cpp</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#linear-h" class="sidebar-link">linear.h</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#operators-0-cpp" class="sidebar-link">Operators_0.cpp</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#dispatch" class="sidebar-link">dispatch</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#boring-part-traversing-from-dispatcher-to-actual-kernel-call" class="sidebar-link">boring part, traversing from dispatcher to actual kernel call</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#redispatchfunction" class="sidebar-link">RedispatchFunction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#blas-cpp" class="sidebar-link">Blas.cpp</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/f00009/" class="sidebar-link">Understanding Pytorch Source Torch</a></li><li><a href="/qishao-notes/pages/f00010/" class="sidebar-link">CUDA Piceces</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/mix/#mix" data-v-06225672>mix</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2024-12-30</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">Understanding Pytorch CUDA<!----></h1> <!----> <div class="theme-vdoing-content content__default"><h1 id="pytorch-cuda"><a href="#pytorch-cuda" class="header-anchor">#</a> Pytorch CUDA</h1> <p>before dive into deeper source code, this is the instinct of pytorch source code:</p> <p>gradually lower code from python to cuda code</p> <ul><li><strong>torch</strong> <ul><li>pytorch interface, python oriented
-./csrc.<strong>autograd mechanism</strong> <ul><li>engine</li> <li>function</li> <li>variable</li></ul></li></ul></li> <li><strong>c10</strong> core functions
-<strong>cuda memory management</strong>
-<strong>dispatcher mechanism</strong></li> <li><strong>aten</strong> <ul><li>native_functions.yaml
<ul><li>register all implementations with backend specification</li></ul></li></ul></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>    - func: _slow_conv2d_forward.output(Tensor self, Tensor weight, SymInt[2] kernel_size,
      Tensor? bias, SymInt[2] stride, SymInt[2] padding, *, Tensor(a!) output) -&gt; Tensor(a!)
    python_module: nn
    dispatch:
    CPU: slow_conv2d_forward_out_cpu
    CUDA: slow_conv2d_forward_out_cuda
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><ul><li>src/ATen/Native/core/boxing/
<ul><li>WrapFunctionIntoFunctor.h support dispatcher, unbox function and call into real implementation</li></ul></li> <li>./src/ATen/cuda
<ul><li><p>general functions</p> <p><img src="https://github.com/user-attachments/assets/0e78c6b5-ac3e-488c-9296-670f1e8558f6" alt="image"></p></li></ul></li> <li>./src/ATen/native/cuda
<ul><li><p>implementation of different layers</p> <p><img src="https://github.com/user-attachments/assets/41d8b9a7-60cf-4c4b-8f6f-45570df2c49e" alt="image"></p></li></ul></li></ul> <p>Pytorch code of torch.randn(), it will calls empty() function to allocate memory and cudacachingallocator malloc will be called.</p> <p>Pytorch code of nn.linear(), it will lowered into addmm_out_cuda_impl cuda code in ./aten/src/ATen/native/cuda/Blas.cpp</p> <h1 id="pending-to-be-understood"><a href="#pending-to-be-understood" class="header-anchor">#</a> Pending to be understood</h1> <p>waiting task: learn deeper how pytorch autograd works
related blog:</p> <ul><li>http://blog.ezyang.com/2019/05/pytorch-internals/</li> <li>https://www.52coding.com.cn/2019/05/05/PyTorch4/</li></ul> <h2 id="pytorch-cuda-memory-allocation"><a href="#pytorch-cuda-memory-allocation" class="header-anchor">#</a> Pytorch CUDA Memory Allocation</h2> <h3 id="lower-level"><a href="#lower-level" class="header-anchor">#</a> Lower level</h3> <p>./c10/cuda/CUDACachingAllocator.cpp</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Block* malloc(
      c10::DeviceIndex device,
      size_t orig_size,
      cudaStream_t stream) {
    ...
    // First, try to get a block from the existing pool.
    bool block_found =
        // Search pool
        get_free_block(params)
        // Trigger callbacks and retry search
        || (trigger_free_memory_callbacks(params) &amp;&amp; get_free_block(params));
    ...
    if (!block_found) {
      // Do garbage collection if the flag is set.
      if (C10_UNLIKELY(
              set_fraction &amp;&amp;
              CUDAAllocatorConfig::garbage_collection_threshold() &gt; 0.0)) {
        garbage_collect_cached_blocks(context);
      }
    ...
      // Attempt allocate
      // WARNING: alloc_block may release the allocator lock when calling
      // cudaMalloc. So far this function has not modified allocator state, but
      // keep in mind that any observed allocator state may change across calls
      // to alloc_block since it may release the lock.
      block_found = alloc_block(params, false, context, lock)
          // Free enough available cached blocks to satisfy alloc and retry
          // alloc.
          || (release_available_cached_blocks(params, context) &amp;&amp;
              alloc_block(params, false, context, lock))
          // Free all non-split cached blocks and retry alloc.
          || (C10_LIKELY(captures_underway.size() == 0) &amp;&amp;
              release_cached_blocks(context) &amp;&amp;
              alloc_block(params, true, context, lock));
...
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br></div></div><p>./aten/src/ATen/cuda</p> <ul><li>CachingHostAllocator.h</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>inline TORCH_CUDA_CPP_API at::DataPtr HostAlloc(size_t size) {
  return getCachingHostAllocator()-&gt;allocate(size);
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li>CachingHostAllocator.cpp</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>struct CUDACachingHostAllocatorImpl
    : public CachingHostAllocatorImpl&lt;CUDAStream, EventPool::Event&gt; {
 private:
  void allocate_host_memory(size_t size, void** ptr) override {
    // Pinned memory pointers allocated by any device can be directly used by
    // any other device, regardless of the current device at the time of
    // allocation, since we assume unified addressing. So we grab any existing
    // primary context, if available. See pytorch/pytorch#21081.
    ...
    // Use cudaHostAlloc for allocating pinned memory (global lock in driver)
    C10_CUDA_CHECK(cudaHostAlloc(ptr, size, cudaHostAllocDefault));
    ...
    }
  }

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><h3 id="intermediate-level"><a href="#intermediate-level" class="header-anchor">#</a> intermediate level</h3> <p>code like cudnn use get_workspace_size to allocate space</p> <p>./aten/src/ATen/native/cuda/MixedDtypesLinear.cu</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>  // Allocate workspace for CUTLASS mixed datatypes GEMM kernel.
  const auto workspace_size = Gemm::get_workspace_size(arguments);
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>code like:
./aten/src/ATen/native/cuda/ForeachReduceOp.cu</p> <p>allocate memory by using at::zeros or at::empty</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>  auto output_per_tensor = at::zeros(
      {static_cast&lt;int64_t&gt;(ntensors) * max_chunks_per_tensor}, options);
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>at::zeros is based on:<br>
./aten/src/ATen/native/cuda/EmptyTensor.cpp</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>TensorBase empty_cuda(
    IntArrayRef size,
    ScalarType dtype,
    std::optional&lt;Device&gt; device_opt,
    std::optional&lt;c10::MemoryFormat&gt; memory_format_opt) {
  at::globalContext().lazyInitDevice(c10::DeviceType::CUDA);
  const auto device = device_or_default(device_opt);
  TORCH_INTERNAL_ASSERT(device.is_cuda());
  const DeviceGuard device_guard(device);
  auto* allocator = at::cuda::getCUDADeviceAllocator();
  constexpr c10::DispatchKeySet cuda_dks(c10::DispatchKey::CUDA);
  return at::detail::empty_generic(
      size, allocator, cuda_dks, dtype, memory_format_opt);
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>./aten/src/ATen/EmptyTensor.cpp</p> <p>We have specify allocator and size, so we will call cuda caching allocator to allocate memory.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>TensorBase empty_generic(
    IntArrayRef size,
    c10::Allocator* allocator,
    c10::DispatchKeySet ks,
    ScalarType scalar_type,
    std::optional&lt;c10::MemoryFormat&gt; memory_format_opt) {
  return _empty_generic(size, allocator, ks, scalar_type, memory_format_opt);
}

template &lt;typename T&gt;
TensorBase _empty_generic(
    ArrayRef&lt;T&gt; size,
    c10::Allocator* allocator,
    c10::DispatchKeySet ks,
    ScalarType scalar_type,
    std::optional&lt;c10::MemoryFormat&gt; memory_format_opt) {
  at::detail::check_size_nonnegative(size);
  at::detail::raise_warning_for_complex_half(scalar_type);
  caffe2::TypeMeta dtype = scalarTypeToTypeMeta(scalar_type);
  auto size_bytes = computeStorageNbytesContiguous(size, dtype.itemsize());
  auto storage_impl = c10::make_intrusive&lt;StorageImpl&gt;(
      c10::StorageImpl::use_byte_size_t(),
      size_bytes,
      allocator,
      /*resizeable=*/true);

  auto tensor = detail::make_tensor_base&lt;TensorImpl&gt;(
      std::move(storage_impl), ks, dtype);
  // Default TensorImpl has size [0]
  // NB: test for meta dispatch key to avoid guarding on zero-ness
  if (ks.has(c10::DispatchKey::Meta) || size.size() != 1 || size[0] != 0) {
    tensor.unsafeGetTensorImpl()-&gt;generic_set_sizes_contiguous(size);
  }

  if (memory_format_opt.has_value()) {
    // Restriding a just-created empty contiguous tensor does nothing.
    if (*memory_format_opt != MemoryFormat::Contiguous) {
      tensor.unsafeGetTensorImpl()-&gt;empty_tensor_restride(*memory_format_opt);
    }
  }

  return tensor;
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br></div></div><h3 id="python-level"><a href="#python-level" class="header-anchor">#</a> python level</h3> <p><strong>init</strong>.pyi.in</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>class _cuda_CUDAAllocator: ...

def _cuda_customAllocator(alloc_fn: _int, free_fn: _int) -&gt; _cuda_CUDAAllocator: ...
def _cuda_changeCurrentAllocator(allocator: _cuda_CUDAAllocator) -&gt; None: ...
def _cuda_getAllocator() -&gt; _cuda_CUDAAllocator: ...
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>./torch/cuda/memory.py</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>__all__ = [
    &quot;caching_allocator_alloc&quot;,
    &quot;caching_allocator_delete&quot;,
    &quot;caching_allocator_enable&quot;,
    ...
    &quot;memory_allocated&quot;,
    ...
]

def caching_allocator_alloc(size, device: Union[Device, int] = None, stream=None):
    r&quot;&quot;&quot;Perform a memory allocation using the CUDA memory allocator.

    Memory is allocated for a given device and a stream, this
    function is intended to be used for interoperability with other
    frameworks. Allocated memory is released through
    :func:`~torch.cuda.caching_allocator_delete`.

    Args:
        size (int): number of bytes to be allocated.
        device (torch.device or int, optional): selected device. If it is
            ``None`` the default CUDA device is used.
        stream (torch.cuda.Stream or int, optional): selected stream. If is ``None`` then
            the default stream for the selected device is used.

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    &quot;&quot;&quot;
    if device is None:
        device = torch.cuda.current_device()
    device = _get_device_index(device)
    if stream is None:
        stream = torch.cuda.current_stream(device)
    if isinstance(stream, torch.cuda.streams.Stream):
        stream = stream.cuda_stream
    ...
    with torch.cuda.device(device):
        return torch._C._cuda_cudaCachingAllocator_raw_alloc(size, stream)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br></div></div><p>./torch/csrc/cuda/Module.cpp</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>    {&quot;_cuda_cudaCachingAllocator_raw_alloc&quot;,
     THCPModule_cudaCachingAllocator_raw_alloc,
     METH_VARARGS,
     nullptr},

PyObject* THCPModule_cudaCachingAllocator_raw_alloc(
    PyObject* _unused,
    PyObject* args) {
  HANDLE_TH_ERRORS
  PyObject* size_o = nullptr;
  PyObject* stream_o = nullptr;
  if (!PyArg_ParseTuple(args, &quot;OO&quot;, &amp;size_o, &amp;stream_o)) {
    THPUtils_invalidArguments(
        args,
        nullptr,
        &quot;caching_allocator_alloc&quot;,
        1,
        &quot;(ssize_t size, intptr_t stream);&quot;);
    return nullptr;
  }
  auto size = PyLong_AsSsize_t(size_o);
  cudaStream_t stream = static_cast&lt;cudaStream_t&gt;(PyLong_AsVoidPtr(stream_o));
  void* mem = nullptr;
  {
    pybind11::gil_scoped_release no_gil;
    mem = c10::cuda::CUDACachingAllocator::raw_alloc_with_stream(size, stream);
  }
  return PyLong_FromVoidPtr(mem);
  END_HANDLE_TH_ERRORS
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br></div></div><h3 id="code-gen-for-memory-allocation"><a href="#code-gen-for-memory-allocation" class="header-anchor">#</a> code gen for memory allocation</h3> <p>./torch/_inductor/codegen/cuda/cuda_kernel.py</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>    def call_kernel(
        self,
        name: str,
        node: &quot;CUDATemplateBuffer&quot;,  # type: ignore[name-defined]
    ) -&gt; None:
        if node.get_workspace_size() &gt; 0:
            ws = WorkspaceArg(
                count=node.get_workspace_size(),
                device=V.graph.get_current_device_or_throw(),
                zero_mode=WorkspaceZeroMode.UNINITIALIZED,
                outer_name=WorkspaceArg.unique_name(),
            )
            wrapper.generate_workspace_allocation(ws)
            workspace = str(ws.outer_name)
            call_args.append(
                workspace
                if V.graph.cpp_wrapper
                else f&quot;c_void_p({workspace}.data_ptr())&quot;
            )
      .....
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>./torch/_inductor/codegen/wrapper.py</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>    def generate_workspace_allocation(self, ws: WorkspaceArg):
        name = ws.get_name()
        line = AllocateLine(self, ws)
        if ws.zero_mode == WorkspaceZeroMode.UNINITIALIZED:
            self.writeline(line)
        elif ws.zero_mode == WorkspaceZeroMode.ZERO_ON_CALL:
            self.writeline(line)
            self.writeline(self.make_zero_buffer(name))
        elif ws.zero_mode == WorkspaceZeroMode.ZERO_PER_GRAPH:
            prior = self.allocated_workspaces.get(name)
            if prior:
                assert isinstance(prior, AllocateLine)
                # expand existing allocation
                prior.node = WorkspaceArg.maximum(prior.node, ws)
            else:
                self.writeline(line)
                self.writeline(self.make_zero_buffer(name))
                self.allocated_workspaces[name] = line
        else:
            raise AssertionError(ws.zero_mode)

        if config.triton.autotune_at_compile_time:
            self.kernel_autotune_calls.writeline(
                PythonWrapperCodegen.make_allocation(
                    self,
                    name,
                    ws.device,
                    ws.dtype,
                    shape=(V.graph.sizevars.size_hint(ws.count),),
                    stride=(1,),
                )
            )
            if ws.zero_mode != WorkspaceZeroMode.UNINITIALIZED:
                self.kernel_autotune_calls.writeline(
                    PythonWrapperCodegen.make_zero_buffer(self, name)
                )
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br></div></div><h2 id="pytorch-adam-cuda-kernel"><a href="#pytorch-adam-cuda-kernel" class="header-anchor">#</a> Pytorch Adam CUDA Kernel</h2> <h3 id="native-functions-yaml"><a href="#native-functions-yaml" class="header-anchor">#</a> native_functions.yaml</h3> <p>./aten/src/ATen/native/native_functions.yaml</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>- func: _fused_adam_(Tensor(a!)[] self,
Tensor(b!)[] grads,
Tensor(c!)[] exp_avgs,
Tensor(d!)[] exp_avg_sqs,
Tensor(e!)[] max_exp_avg_sqs,
Tensor[] state_steps, *,
float lr, float beta1, float beta2,
float weight_decay, float eps, bool amsgrad,
bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; ()
  # Unlike &quot;foreach&quot; functions, lists of tensors should be guaranteed to be on the same device (for now).
  variants: function
  dispatch:
    CPU: _fused_adam_kernel_cpu_
    CUDA: _fused_adam_kernel_cuda_
    MPS: _fused_adam_kernel_mps_
  autogen: _fused_adam, _fused_adam.out
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><h3 id="fusedadamkernel-cu"><a href="#fusedadamkernel-cu" class="header-anchor">#</a> FusedAdamKernel.cu</h3> <p>./aten/arc/ATen/native/cuda</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>// The following overload simply has a Tensor lr
void _fused_adam_kernel_cuda_(
    at::TensorList params,
    at::TensorList grads,
    at::TensorList exp_avgs,
    at::TensorList exp_avg_sqs,
    at::TensorList max_exp_avg_sqs,
    at::TensorList state_steps,
    const at::Tensor&amp; lr,
    const double beta1,
    const double beta2,
    const double weight_decay,
    const double eps,
    const bool amsgrad,
    const bool maximize,
    const std::optional&lt;at::Tensor&gt;&amp; grad_scale,
    const std::optional&lt;at::Tensor&gt;&amp; found_inf) {
  if (lr.is_cpu()) {
    _fused_adam_kernel_cuda_(
        params,
        grads,
        exp_avgs,
        exp_avg_sqs,
        max_exp_avg_sqs,
        state_steps,
        lr.item&lt;double&gt;(),
        beta1,
        beta2,
        weight_decay,
        eps,
        amsgrad,
        maximize,
        grad_scale,
        found_inf);
    return;
  }

  // Manually check devices since we specify no device check in
  // native_functions.yaml
  ...

  if (amsgrad) {
    ...
  } else {
    TORCH_CHECK(
        at::native::check_fast_path_restrictions(
            {params, grads, exp_avgs, exp_avg_sqs}),
        &quot;params, grads, exp_avgs, and exp_avg_sqs must have same dtype, device, and layout&quot;);
    _fused_adam_cuda_impl_(
        params,
        grads,
        exp_avgs,
        exp_avg_sqs,
        state_steps,
        lr,
        beta1,
        beta2,
        weight_decay,
        eps,
        maximize,
        grad_scale,
        found_inf);
  }
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br></div></div><h3 id="fused-adam-impl-cu"><a href="#fused-adam-impl-cu" class="header-anchor">#</a> fused_adam_impl.cu</h3> <p>./aten/arc/ATen/native/cuda</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>void _fused_adam_cuda_impl_(
    at::TensorList params,
    at::TensorList grads,
    at::TensorList exp_avgs,
    at::TensorList exp_avg_sqs,
    at::TensorList state_steps,
    const double lr,
    const double beta1,
    const double beta2,
    const double weight_decay,
    const double eps,
    const bool maximize,
    const std::optional&lt;at::Tensor&gt;&amp; grad_scale,
    const std::optional&lt;at::Tensor&gt;&amp; found_inf) {
  std::vector&lt;std::vector&lt;at::Tensor&gt;&gt; tensor_lists{
      params.vec(), grads.vec(), exp_avgs.vec(), exp_avg_sqs.vec()};

  const float* grad_scale_ptr =
      grad_scale.has_value() ? grad_scale-&gt;data_ptr&lt;float&gt;() : nullptr;
  const float* found_inf_ptr =
      found_inf.has_value() ? found_inf-&gt;data_ptr&lt;float&gt;() : nullptr;
  const float* lr_ptr = nullptr;

  AT_DISPATCH_FLOATING_TYPES_AND2(
      kHalf,
      kBFloat16,
      params[0].scalar_type(),
      &quot;fused_adam_kernel_cuda&quot;,
      [&amp;]() {
        multi_tensor_apply_for_fused_optimizer&lt;4&gt;(
            tensor_lists,
            state_steps,
            FusedAdamMathFunctor&lt;scalar_t, 4, ADAM_MODE::ORIGINAL, false&gt;(),
            lr_ptr, // unused
            lr,
            beta1,
            beta2,
            weight_decay,
            eps,
            maximize,
            grad_scale_ptr,
            found_inf_ptr);
      });
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br></div></div><p>The callable function is <strong>FusedAdamMathFunctor&lt;scalar_t, 4, ADAM_MODE::ORIGINAL, false&gt;()</strong>.</p> <h3 id="multitensorapply-cuh"><a href="#multitensorapply-cuh" class="header-anchor">#</a> MultiTensorApply.cuh</h3> <div class="language- line-numbers-mode"><pre class="language-text"><code>
template &lt;typename T, typename U, typename... ArgTypes&gt;
C10_LAUNCH_BOUNDS_1(kBlockSize)
__global__ void multi_tensor_apply_kernel(
    T tensorListMeta,
    U callable,
    ArgTypes... args) {
  // Hand the chunk information to the user-supplied functor to process however
  // it likes.
  callable(kChunkSize, tensorListMeta, args...);
}

template &lt;int depth, typename T, typename... ArgTypes&gt;
void multi_tensor_apply_for_fused_optimizer(
    std::vector&lt;std::vector&lt;at::Tensor&gt;&gt;&amp; tensor_lists,
    at::TensorList state_steps,
    T callable,
    ArgTypes... args) {
    ...
    for (const auto&amp; chunk : c10::irange(chunks)) {
            multi_tensor_apply_kernel&lt;&lt;&lt;
            loc_block_info,
            kBlockSize,
            0,
            at::cuda::getCurrentCUDAStream()&gt;&gt;&gt;(
            tensorListMeta, callable, args...);
        C10_CUDA_KERNEL_LAUNCH_CHECK();
    }
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br></div></div><hr> <h2 id="pytorch-linear-lowering"><a href="#pytorch-linear-lowering" class="header-anchor">#</a> Pytorch linear lowering</h2> <h3 id="python-nn-functions-cpp"><a href="#python-nn-functions-cpp" class="header-anchor">#</a> python_nn_functions.cpp</h3> <p>Depth 31/29</p> <p>./torch/csrc/autograd/generated/python_nn_functions.cpp</p> <p>operator() (__closure=0x7ffc7dcf9e88, input=..., weight=..., bias=...)</p> <p>torch::autograd::THPVariable_linear</p> <h3 id="linear-h"><a href="#linear-h" class="header-anchor">#</a> linear.h</h3> <p>Depth 29</p> <p>./build/aten/src/ATen/ops/linear.h</p> <p>at::linear (input=..., weight=..., bias=...)</p> <h3 id="operators-0-cpp"><a href="#operators-0-cpp" class="header-anchor">#</a> Operators_0.cpp</h3> <p>at namespace
Depth 28</p> <p>at::_ops::linear::call (input=..., weight=..., bias=...)
build/aten/src/ATen/Operators_0.cpp:3601</p> <h3 id="dispatch"><a href="#dispatch" class="header-anchor">#</a> dispatch</h3> <p>c10 namespace</p> <p>./aten/src/ATen/Core/dispatch/Dispatcher.h dispatch</p> <p><img src="https://github.com/user-attachments/assets/cf488804-1b5d-4ade-ba21-fa0ec46733f7" alt="image"></p> <h3 id="boring-part-traversing-from-dispatcher-to-actual-kernel-call"><a href="#boring-part-traversing-from-dispatcher-to-actual-kernel-call" class="header-anchor">#</a> boring part, traversing from dispatcher to actual kernel call</h3> <h4 id="boxing-and-unbox"><a href="#boxing-and-unbox" class="header-anchor">#</a> boxing and unbox</h4> <p>Depth 23</p> <p>c10 namespace</p> <p>aten/src/ATen/core/boxing/KernelFunction_impl.h 105</p> <p>c10::callUnboxedKernelFunction at</p> <p>aten/src/ATen/core/boxing/KernelFunction_impl.h:53</p> <p>c10::impl::wrap_kernel_functor_unboxed_ at</p> <p>aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:468</p> <h4 id="from-c10-to-at-namespace"><a href="#from-c10-to-at-namespace" class="header-anchor">#</a> From c10 to at namespace</h4> <p>Please notice the dispatchkeyset, that is related to dispatch mechanism in pytorch.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>c10::impl::detail::WrapFunctionIntoFunctor_ in aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13\
to
at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__linear in
build/aten/src/ATen/RegisterCompositeImplicitAutograd.cpp:2620


Depth 20

at::native::linear (input=..., weight=..., bias_opt=...) at aten/src/ATen/native/Linear.cpp:95

Tensor linear(const Tensor&amp; input, const Tensor&amp; weight, const c10::optional&lt;Tensor&gt;&amp; bias_opt) {
  ...
  if (input_dim == 2 &amp;&amp; bias-&gt;defined()) {
    // Fused op is marginally faster.
    return at::addmm(*bias, input, weight.t());
  }
  ...
}


Depth 19

at::addmm (self=..., mat1=..., mat2=..., beta=..., alpha=...) at build/aten/src/ATen/ops/addmm.h:36

Depth 18

at::_ops::addmm::call (self=..., mat1=..., mat2=..., beta=..., alpha=...) at aten/src/ATen/Operators_0.cpp:7151

Depth 12

c10::impl::detail::WrapFunctionIntoFunctor_&lt;
c10::CompileTimeFunctionPointer&lt;
at::Tensor(c10::DispatchKeySet, const at::Tensor&amp;, const at::Tensor&amp;, const at::Tensor&amp;, const c10::Scalar&amp;, const c10::Scalar&amp;),
torch::autograd::VariableType::(anonymous namespace)::addmm&gt;,
at::Tensor,
c10::guts::typelist::typelist&lt;c10::DispatchKeySet, const at::Tensor&amp;, const at::Tensor&amp;, const at::Tensor&amp;, const c10::Scalar&amp;, const c10::Scalar&amp;&gt;
&gt;::operator() (args#5=..., args#4=..., args#3=..., args#2=..., args#1=..., args#0=..., this=0x30c7a930)
aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br></div></div><p>to torch::autograd::VariableType::(anonymous namespace)::addmm (ks=..., self=..., mat1=..., mat2=..., beta=..., alpha=...) at aten/src/autograd/generated/VariableType_0.cpp:6898</p> <h3 id="redispatchfunction"><a href="#redispatchfunction" class="header-anchor">#</a> RedispatchFunction</h3> <p>This is generated function, but I guess most of redispatch function will be generated here.</p> <p>👍</p> <p>at::redispatch::addmm (dispatchKeySet=..., self=..., mat1=..., mat2=..., beta=..., alpha=...) at build/aten/src/ATen/RedispatchFunctions.h:8517</p> <p>And then:
at::(anonymous namespace)::wrapper_CUDA_addmm build/aten/src/ATen/RegisterCUDA.cpp</p> <p><strong>linear</strong></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>    // aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -&gt; Tensor
    inline at::Tensor linear(c10::DispatchKeySet dispatchKeySet, const at::Tensor &amp; input, const at::Tensor &amp; weight, const c10::optional&lt;at::Tensor&gt; &amp; bias={}) {
        return at::_ops::linear::redispatch(dispatchKeySet, input, weight, bias);
    }
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p><strong>relu</strong></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>    inline at::Tensor relu(c10::DispatchKeySet dispatchKeySet, const at::Tensor &amp; self) {
        return at::_ops::relu::redispatch(dispatchKeySet, self);
    }
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><strong>softmax</strong></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>    inline at::Tensor softmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor &amp; self, int64_t dim, c10::optional&lt;at::ScalarType&gt; dtype=c10::nullopt) {
        return at::_ops::softmax_int::redispatch(dispatchKeySet, self, dim, dtype);
    }
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h3 id="blas-cpp"><a href="#blas-cpp" class="header-anchor">#</a> Blas.cpp</h3> <p>Blas.cpp calls to at::cuda::blas::gemm function.</p> <p>./aten/src/ATen/native/cuda/Blas.cpp</p> <p>Notice at::cuda::blas::gemm function. it calls into gemm function.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>    // addmm_out_cuda_impl function

    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(
        at::ScalarType::Half,
        at::ScalarType::BFloat16,
        scalar_type,
        &quot;addmm_cuda&quot;,
        [&amp;] {
          using opmath_t = at::opmath_type&lt;scalar_t&gt;;
          opmath_t alpha_val = alpha.to&lt;opmath_t&gt;();
          opmath_t beta_val = beta.to&lt;opmath_t&gt;();
          const scalar_t* mat1_ptr = args.mata-&gt;const_data_ptr&lt;scalar_t&gt;();
          const scalar_t* mat2_ptr = args.matb-&gt;const_data_ptr&lt;scalar_t&gt;();
          scalar_t* result_ptr = args.result-&gt;mutable_data_ptr&lt;scalar_t&gt;();
          at::cuda::blas::gemm&lt;scalar_t&gt;(
              args.transa,
              args.transb,
              args.m,
              args.n,
              args.k,
              alpha_val,
              mat1_ptr,
              args.lda,
              mat2_ptr,
              args.ldb,
              beta_val,
              result_ptr,
              args.result_ld);
        });
    switch (activation) {
      case Activation::RELU:
        at::relu_(const_cast&lt;Tensor&amp;&gt;(*args.result));
        break;
      case Activation::GELU:
        at::gelu_(const_cast&lt;Tensor&amp;&gt;(*args.result), &quot;tanh&quot;);
        break;
      default: break;
    }
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br></div></div></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/10.mix/08.learn_pytorch_cuda.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/03/21, 04:47:58</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/f00007/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">CUDA LayerNorm</div></a> <a href="/qishao-notes/pages/f00009/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Understanding Pytorch Source Torch</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/f00007/" class="prev">CUDA LayerNorm</a></span> <span class="next"><a href="/qishao-notes/pages/f00009/">Understanding Pytorch Source Torch</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.b4fc0d6d.js" defer></script><script src="/qishao-notes/assets/js/2.6d8a25ce.js" defer></script><script src="/qishao-notes/assets/js/117.372eb54b.js" defer></script>
  </body>
</html>
