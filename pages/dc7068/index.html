<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>From Attention Sink to Massive Activation | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.3856e1e2.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.0833fe67.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/114.48c15a29.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.2e68d5ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.8121a17d.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.d344979b.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.6bc1dc1a.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.752dc056.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.fb36554a.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.c328df02.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.d4d701c5.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.dec277b2.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.9462122c.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.8add2597.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.8a8e0de2.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.d583abd0.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.f50c04de.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.c72d74fe.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.892e051a.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.6a8ed08a.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.e0bf79a5.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.85d39ce4.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.2e9a2880.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.3a815ecd.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.8535d463.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.d3f8dc72.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.058fb67a.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.67c7b28b.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.d96cccd4.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.01113fcc.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.c5c8d21d.js"><link rel="prefetch" href="/qishao-notes/assets/js/126.e94356a3.js"><link rel="prefetch" href="/qishao-notes/assets/js/127.a5bab308.js"><link rel="prefetch" href="/qishao-notes/assets/js/128.ff0f0892.js"><link rel="prefetch" href="/qishao-notes/assets/js/129.3a27b433.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.7de2e04b.js"><link rel="prefetch" href="/qishao-notes/assets/js/130.e45a571b.js"><link rel="prefetch" href="/qishao-notes/assets/js/131.7bf8633a.js"><link rel="prefetch" href="/qishao-notes/assets/js/132.8f6e1e21.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.466ec3ac.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.e89652e9.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.383840b9.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.63fdd997.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.bc4a5b1d.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.a2c2ac4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a4b6fb1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.58619e3b.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.b2f9c634.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.4d4cd4e1.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.9538dbb9.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.420a689a.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.2aa82bc2.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.27e20f98.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.c9887203.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.c8b0d700.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.b67a85e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.0858b5b0.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.aa6fd9d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.96306ad4.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.f2770d51.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.5e48ab77.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.01d6cf82.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.c355ce28.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.6cb48c78.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.9f9585d9.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.60eee2cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.2dcb8de5.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.768b44d0.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.bcc7509d.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.36f25851.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.18008ea4.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.be5d1eda.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.ebba6389.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.5ad3e7da.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.fce7d840.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.ad514339.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.14c8f20a.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.9d58ea7e.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.1d0bbfe1.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.26a65cae.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.7a34f898.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.f3dbccbd.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.c2b025f5.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.545b33b8.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.63d5abb5.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.89586274.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.03d7c65b.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.274dbddd.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.e3d0834d.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.b0034962.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.f1418ea1.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.f62341ae.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.b9c3a1f9.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.57f3eb17.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.db55939d.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.7bd2530e.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.4fb49cdd.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.b7cb5686.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.f45db467.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.de46b925.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.cef4957c.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.abfe714d.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.d2933de4.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.0330ff48.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.c29a1485.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.cbac4120.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.65bac2a1.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.2bc797df.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.1ac71d6e.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.d0a192f2.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.b6a1e324.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.994852f8.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.93e5fd34.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.e760ae06.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.65876b92.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.c2ba0b36.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.6fa53d3a.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.6b8db046.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.d5b59f1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.0a9b27e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.48a79619.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.a15ac68d.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.640a49e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.f13c6793.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.5a1e5fd5.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.7a8c0983.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.4fa07916.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.96018905.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.ad1b24f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.51ab3a40.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.36548244.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.3f361b43.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Paper List</a></li><li><a href="/qishao-notes/pages/dc7043/" class="sidebar-link">Efficient LLM</a></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" class="sidebar-link">Summery of Inner Workings of LLM</a></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" class="sidebar-link">Memory Optimizations in LLM</a></li><li><a href="/qishao-notes/pages/dc7049/" class="sidebar-link">Reasoning in LLM</a></li><li><a href="/qishao-notes/pages/dc7050/" class="sidebar-link">LLM Mixed Precision &amp; Quantization &amp; Outlier</a></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" class="sidebar-link">LLM Scaling Law</a></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7059/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li><li><a href="/qishao-notes/pages/dc7060/" class="sidebar-link">LLM MOE Inference</a></li><li><a href="/qishao-notes/pages/dc7061/" class="sidebar-link">LLM Compression</a></li><li><a href="/qishao-notes/pages/dc7062/" class="sidebar-link">LLM Optimizer Optimization</a></li><li><a href="/qishao-notes/pages/dc7063/" class="sidebar-link">LLM Posttraining</a></li><li><a href="/qishao-notes/pages/dc7064/" class="sidebar-link">LLM MICRO - ISCA - HPCA</a></li><li><a href="/qishao-notes/pages/dc7066/" class="sidebar-link">LLM Prefilling &amp; Decoding Split</a></li><li><a href="/qishao-notes/pages/dc7067/" class="sidebar-link">Thinking of LLM Prefilling &amp; Decoding Split</a></li><li><a href="/qishao-notes/pages/dc7068/" aria-current="page" class="active sidebar-link">From Attention Sink to Massive Activation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7068/#_1-596-y2023-efficient-steaming-language-models-with-attention-sinks" class="sidebar-link">1. [596 Y2023] Efficient Steaming Language Models with Attention Sinks</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7068/#_2-y2025-when-attention-sink-emerges-in-language-models" class="sidebar-link">2. [Y2025] When Attention Sink Emerges in Language Models</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7068/#what-is-attention-sink" class="sidebar-link">What is Attention Sink?</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7068/#main-contributions" class="sidebar-link">Main Contributions</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7068/#_3-192-y2024-massive-activations-in-large-language-models" class="sidebar-link">3. [192 Y2024] Massive Activations in Large Language Models</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7068/#which-layers" class="sidebar-link">Which Layers?</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7068/#which-feature-and-sequence-dimensions" class="sidebar-link">Which Feature and Sequence Dimensions?</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7068/#difference-from-outlier" class="sidebar-link">Difference from Outlier</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7068/#massive-activations-act-as-biases-in-llms" class="sidebar-link">Massive Activations Act as Biases in LLMs</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7068/#attention-is-concentrated-on-massive-activations" class="sidebar-link">Attention is Concentrated on Massive Activations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7068/#massive-activations-impose-implicit-attention-biases" class="sidebar-link">Massive Activations Impose Implicit Attention Biases</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7068/#explicit-attention-biases-eliminate-massive-activations" class="sidebar-link">Explicit Attention Biases Eliminate Massive Activations</a></li></ul></li></ul></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-04-30</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">From Attention Sink to Massive Activation<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[596 Y2023] Efficient Steaming Language Models with Attention Sinks</li> <li>[Y2025] When Attention Sink Emerges in Language Models</li> <li>[192 Y2024] Massive Activations in Large Language Models</li> <li>[2025] A Refined Analysis of Massive Activations in LLMs</li></ol> <h2 id="_1-596-y2023-efficient-steaming-language-models-with-attention-sinks"><a href="#_1-596-y2023-efficient-steaming-language-models-with-attention-sinks" class="header-anchor">#</a> 1. [596 Y2023] Efficient Steaming Language Models with Attention Sinks</h2> <p>Attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention.</p> <p>In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a “sink” even if they are not semantically important.</p> <img src="https://github.com/user-attachments/assets/e78d12d3-80c5-4771-bb09-4a39440d089c" style="width:600px;height:auto;"> <p>The results show the insufficiency of introducing merely one or two initial tokens, whereas a threshold of four initial tokens appears enough, with subsequent additions contributing marginal effects.</p> <p>The nature of the SoftMax function prevents all attended tokens from having zero values.</p> <p>This requires aggregating some information from other tokens across all heads in all layers, even if the current embedding has sufficient self-contained information for its prediction.</p> <p>Consequently, the model tends to dump unnecessary attention values to specific tokens.</p> <p>This result justifies our choice of introducing 4 initial tokens as attention sinks in StreamingLLM.</p> <p>We’ve noted that LLMs are typically trained to utilize multiple initial tokens as attention sinks rather than just one.</p> <img src="https://github.com/user-attachments/assets/fb405010-15cb-4e44-aac4-95a91b98df55" style="width:600px;height:auto;"> <p>The KV cache in StreamingLLM can be conceptually divided into two parts, as illustrated in Figure 4: (1) Attention sinks (four initial tokens) stabilize the attention computation; 2) Rolling KV Cache retains the most recent tokens, crucial for language modeling.</p> <img src="https://github.com/user-attachments/assets/ff71fbb0-f72d-42e1-8dd7-b6cf41f7d3df" style="width:600px;height:auto;"> <p>They also test pretraining with a single sink token.</p> <p>The vanilla model requires the addition of multiple tokens as attention sinks to maintain stable streaming perplexity.</p> <p>In contrast, the model trained with a sink token achieves satisfactory streaming performance <strong>using just the sink token</strong>.</p> <img src="https://github.com/user-attachments/assets/96760ee8-9989-483f-8a2e-af2f2894eed2" style="width:600px;height:auto;"> <hr> <h2 id="_2-y2025-when-attention-sink-emerges-in-language-models"><a href="#_2-y2025-when-attention-sink-emerges-in-language-models" class="header-anchor">#</a> 2. [Y2025] When Attention Sink Emerges in Language Models</h2> <p><strong>Background</strong></p> <p>Xiao et al. (2023b) revealed that LLMs allocate significant attention scores to specific token positions, e.g. the first token (not necessary to be a BOS token), resulting in “vertical” attention patterns.</p> <p><strong>Contribution</strong></p> <ul><li>Attention sink emerges after LMs are trained effectively on sufficient training data.
It appears less obvious in LMs trained with small learning rates.
While weight decay encourages the emergence of attention sink.</li> <li>The sink position is highly related to the loss function and data distribution and can be shifted to other positions rather than the first token.</li> <li>Attention sink acts more like key biases, storing extra attention and meanwhile not contributing to the value computation.
This phenomenon (at least partially) stems from tokens’inner dependence on attention scores due to the softmax normalization.
After relaxing such dependence by replacing softmax attention with other attention operations, e.g., sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters.</li></ul> <h3 id="what-is-attention-sink"><a href="#what-is-attention-sink" class="header-anchor">#</a> What is Attention Sink?</h3> <p>An attention sink is when tokens disproportionately attend to the first token (often the BOS token or first word), even when it holds little semantic importance.</p> <p>This creates a vertical attention pattern, frequently leveraged for efficiency in:</p> <ul><li>Streaming inference</li> <li>KV cache optimization</li> <li>Quantization-aware training</li></ul> <h3 id="main-contributions"><a href="#main-contributions" class="header-anchor">#</a> Main Contributions</h3> <h4 id="empirical-demonstration-of-universality"><a href="#empirical-demonstration-of-universality" class="header-anchor">#</a> Empirical Demonstration of Universality</h4> <p>Attention sink appears across LMs (GPT2-XL, LLaMA2/3, Mistral) — even with:</p> <ul><li>Random token sequences</li> <li>Small-scale models
This suggests the phenomenon is model-agnostic and tied to the training process, not the data or size.</li></ul> <h4 id="mechanistic-understanding"><a href="#mechanistic-understanding" class="header-anchor">#</a> Mechanistic Understanding</h4> <p>First token’s key vector acts like a bias: due to angle alignment with other queries, not due to large vector norms.</p> <p>The cosine similarity, not norm product, drives attention sink — meaning high attention persists even with small key norms.</p> <h4 id="emergence-during-pretraining"><a href="#emergence-during-pretraining" class="header-anchor">#</a> Emergence During Pretraining</h4> <p>Attention sink becomes prominent after sufficient optimization.</p> <p>It's less obvious with:</p> <ul><li>Fewer training steps</li> <li>Smaller learning rates</li> <li>Less training data</li></ul> <h4 id="influence-of-data-distribution"><a href="#influence-of-data-distribution" class="header-anchor">#</a> Influence of Data Distribution</h4> <p>Sink position can shift:</p> <ul><li>From token 1 to 2 if token 1 is randomized</li> <li>To fixed tokens if injected intentionally</li> <li><strong>repeated tokens in input suppress attention sink in relative positional encoding models (e.g., LLaMA)</strong>.</li></ul> <h4 id="loss-function-optimization-effects"><a href="#loss-function-optimization-effects" class="header-anchor">#</a> Loss Function &amp; Optimization Effects</h4> <ul><li>Weight decay encourages attention sink, but excessive decay suppresses it by harming learning.</li> <li>In prefix LM tasks, sink moves from token 1 to the prefix span.</li> <li>Shifted window attention (e.g., in Mistral) localizes the sink to absolute positions — not relative ones.</li></ul> <h4 id="attention-sink-key-biases"><a href="#attention-sink-key-biases" class="header-anchor">#</a> Attention Sink = Key Biases</h4> <img src="https://github.com/user-attachments/assets/1faf375f-a01c-4b7d-af36-8d41e2f5b1fa" style="width:600px;height:auto;"> <p>We further show that due to the different manifold of $\mathbf{k}_1^{l,h}$, the angles between $\mathbf{k}_1^{l,h}$ and $\mathbf{q}_t^{l,h}$ play an important role. Considering
$\mathbf{q}_t^{l,h} {\mathbf{k}_j^{l,h}}^\top = |\mathbf{q}_t^{l,h}| \cdot |\mathbf{k}_j^{l,h}| \cdot \cos(\mathbf{q}_t^{l,h}, \mathbf{k}_j^{l,h}),$</p> <p>we visualize the cosine similarity between keys and values, and the product of $\ell_2$-norm between keys and values in Figure 2(<em>Bottom</em>)** .</p> <blockquote><p><strong>Although</strong>  $$|\mathbf{q}_t^{l,h}| \cdot |\mathbf{k}_1^{l,h}|$$ <strong>is comparatively small,</strong></p></blockquote> <p>$$\cos(\mathbf{q}_t^{l,h}, \mathbf{k}_1^{l,h})$$ <strong>is significantly large, leading to attention sink.</strong></p> <p>This explains why attention sink exists despite the small $$\ell_2$$-norm of keys of the first token.</p> <p>To conclude, the first token leverages its keys to act as biases, thus minimizing the angles between $$\mathbf{k}_1^{l,h}$$ and $$\mathbf{q}_t^{l,h}$$, and <strong>exhibiting attention sink</strong> .</p> <ul><li>Attention sink does not contribute to value computation, acting more like a <strong>key-space artifact</strong>.</li> <li>Simply <strong>adding key biases (even without value biases) shifts the sink away from real tokens, proving it’s an optimization artifact</strong>.</li></ul> <h4 id="attention-sink-under-different-inputs"><a href="#attention-sink-under-different-inputs" class="header-anchor">#</a> Attention Sink Under Different Inputs</h4> <ul><li>input domains have negligible effects on our attention sink metric Sinkϵ1</li> <li>(i) randomly sample T tokens from the tokenizer vocabulary V to construct a sequence</li> <li>(ii) randomly sample 1 token from the tokenizer V and repeat it T times.
As present in Table 1(Left), attention sink still exists when the inputs are <strong>random tokens instead of natural language</strong>.</li></ul> <p>However, with repeated tokens, attention sink in Mistral (Jiang et al., 2023) and LLaMA models disappears.
<strong>we prove that for LMs with NoPE/relative PE/ALiBI/Rotary, if the first T tokens are the same, their corresponding hidden states are the same. They all have massive activations, thus dispersing the attention sink.</strong></p> <h4 id="effects-of-optimization-on-attention-sink"><a href="#effects-of-optimization-on-attention-sink" class="header-anchor">#</a> Effects of Optimization on Attention Sink</h4> <ul><li>Attention sink emerges after LMs are trained effectively.</li> <li>Attention sink appears less obvious in LMs trained with small learning rates.</li></ul> <h4 id="effects-of-data-distribution-pdata-on-attention-sink"><a href="#effects-of-data-distribution-pdata-on-attention-sink" class="header-anchor">#</a> Effects of Data Distribution pData on Attention Sink</h4> <ol><li>Attention sink emerges after LMs are trained on sufficient training data.</li> <li>Attention sink could be shifted to other positions rather than the first token if modifying pdata.</li></ol> <h4 id="effects-of-loss-function-on-attention-sink"><a href="#effects-of-loss-function-on-attention-sink" class="header-anchor">#</a> Effects of Loss Function on Attention Sink</h4> <img src="https://github.com/user-attachments/assets/c2dbbdd3-68f5-4b76-8e5f-e3cc788fd0ae" style="width:600px;height:auto;"> <ol><li>Weight decay encourages the emergence of attention sink.</li> <li>With prefix language modeling, attention sink appears among the prefix tokens rather than the first token
only.</li> <li>With shifted window attention, attention sink appears on the “absolute”, not the “relative” first token. Smaller window size prevents the emergence of attention sink.</li></ol> <h4 id="effects-of-model-architecture-on-attention-sink"><a href="#effects-of-model-architecture-on-attention-sink" class="header-anchor">#</a> Effects of Model Architecture on Attention Sink</h4> <ul><li>we note that all these LMs, even the one without explicit PE (NoPE), have attention sink.</li></ul> <h5 id="attention-bias"><a href="#attention-bias" class="header-anchor">#</a> Attention Bias</h5> <p>considered a learnable sink token in each chunk before the input tokens during LM pre-training.<br>
As this token is fixed in the first token, this could be considered as implicitly introducing biases k,v,q in attention.<br>
as long as there are key biases k*<sup>l,h</sup> attention sink disappears on the first token but on the biases.</p> <p><strong>So they prove that v*<sup>l,h</sup> is not important, could just be zero.</strong></p> <img src="https://github.com/user-attachments/assets/3f9afb16-8169-4262-a8a8-665574b1d971" style="width:600px;height:auto;"> <ol><li>Positional embedding, FFN design, LN location, and multi-head design do not affect the emergence of attention sink.</li> <li>Attention sink acts more like key biases, storing extra attention and meanwhile not contributing to the value computation.</li> <li>When relaxing tokens’ inner dependence on attention scores, attention sink does not emerge in LMs.
We note that the LMs with no attention sink typically relax tokens’ inner dependence on attention scores.<br>
Their attention scores during pre-training could be negative or not add up to one.<br>
This indicated that attention sink (at least partially) stems from such inner dependence.<br>
Besides the attention metric computed by proxy attention scores, we also observe that the above LMs also have no massive activations.</li></ol> <img src="https://github.com/user-attachments/assets/4b0d5fe4-940b-45b9-a9bd-6f151dd163b5" style="width:600px;height:auto;"> <h4 id="role-of-attention-normalization"><a href="#role-of-attention-normalization" class="header-anchor">#</a> Role of Attention Normalization</h4> <ul><li><strong>Softmax normalization creates inter-token dependence, reinforcing attention sink</strong></li> <li><strong>Replacing softmax with sigmoid or non-normalized attention (e.g., ELU+1) eliminates attention sink — even in 1B-parameter models</strong></li></ul> <hr> <h2 id="_3-192-y2024-massive-activations-in-large-language-models"><a href="#_3-192-y2024-massive-activations-in-large-language-models" class="header-anchor">#</a> 3. [192 Y2024] Massive Activations in Large Language Models</h2> <ul><li>Certain activations exhibit huge magnitudes, e.g., more than 4 orders of magnitude larger than the median.
<ul><li>These activations are also extremely rare, often numbering fewer than 10 among tens of millions of total activations.</li></ul></li> <li>Regarding the depth dimension of LLMs, the appearance of massive activations is mostly abrupt: they emerge suddenly after a single layer of computation, and diminish at the last few layers.
<ul><li>Further, we find massive activations occur in a small number of feature dimensions that are input agnostic.</li> <li>Many of these activations are found within the starting word token and delimiter tokens.</li> <li>Additionally, we show that massive activations are not the same as outlier features (Dettmers et al., 2022), a previously known phenomenon in LLMs.</li></ul></li> <li>Massive activations act as fixed but crucial bias terms in LLMs.
<ul><li>Certain internal states of the models that are independent from the inputs, analogous to the bias term b in a linear layer y = W x + b.</li> <li>First, we show that massive activations play a critical role in LLMs’ capabilities. For instance, in LLaMA2-7B, setting merely four massive activations (out of millions of activations) to zero would result in catastrophic collapse in model performance.</li> <li>Further, <strong>setting them to their mean values does not hurt the model, suggesting their role is equivalent to simple constant biases</strong>.</li> <li>Our analysis reveals that after the initial layers, LLMs repurpose the tokens linked with massive activations to store these important biases.</li></ul></li> <li>Massive activations are closely connected with self-attention.
<ul><li>In particular, we show massive activations cause attention to be attracted to the tokens associated with them.</li> <li>Our findings extend the observations from “attention sinks” (Xiao et al., 2023b)—we demonstrate that LLMs allocate excessive attention to more than just the first token, and provide an in-depth analysis on how such attention concentration patterns arise.</li> <li>Our analysis suggests that LLMs try to learn implicit bias components in self-attention via massive activations, during their pretraining phase.</li> <li>We thus experiment with augmenting self-attention with additional key and value embeddings that are explicitly designed as biases.</li> <li>Remarkably, we demonstrate that training with them eliminates the need for LLMs to learn massive activations.</li></ul></li></ul> <p><img src="https://github.com/user-attachments/assets/96b97b21-81a5-41fb-af6d-dd382b7beaa1" alt="image"></p> <p>The most notable property is that these activations possess massive values and their magnitudes are significantly larger than other activations, often several orders of magnitude larger than the median value.</p> <p>Another property is that they are exceptionally few in number.<br>
For LLaMA2-7B in Figure 1, there are approximately 40,000 total activations in each presented hidden state but at most four massive activations can be identified.</p> <p><strong>an activation qualifies as a massive activation if its magnitude surpasses 100 and is at least or around 1,000 times larger than the median magnitude of its hidden state.</strong></p> <p><img src="https://github.com/user-attachments/assets/efa19455-5a67-4705-aa35-50879093f7fc" alt="image"></p> <h3 id="which-layers"><a href="#which-layers" class="header-anchor">#</a> Which Layers?</h3> <p><em>Massive activations exist and remain as largely constant values throughout most of the intermediate layers.</em></p> <p><em>They emerge in the initial layers and start to diminish in the last few layers.</em></p> <h3 id="which-feature-and-sequence-dimensions"><a href="#which-feature-and-sequence-dimensions" class="header-anchor">#</a> Which Feature and Sequence Dimensions?</h3> <p>We find that massive activations appear at:</p> <ol><li>the starting word token,</li> <li>the token representing the first period (.) or newline token (\n) in the sequence.</li> <li>These activations are exclusively located within the starting token of the sequence, regardless of its semantics.</li> <li>They are associated with the starting token, delimiter tokens and also certain word tokens, e.g., token “and” and token “of”.</li></ol> <ul><li>For feature dimensions, massive activations are consistently present in very few fixed dimensions.</li> <li>For sequence dimensions, we classify LLMs into three categories based on massive activations’ locations:
<ul><li>Starting token only.
Models include LLaMA2-13B, MPT and GPT-2.</li> <li>Starting token and the first “strong” delimiter token (i.e., “.” or “\n”)
Models include LLaMA2-7B and LLaMA2-7B-Chat.</li> <li>Starting token, delimiter tokens (such as “.”, “\n”, “’” or “,”), and certain word tokens with weak semantics (such as “and”, “from”, “of” or “2”2) <br>
Models include LLaMA2-70B, Mistral-7B, Mixtral-8x7B, Falcon-40B and Phi-2.</li></ul></li></ul> <h3 id="difference-from-outlier"><a href="#difference-from-outlier" class="header-anchor">#</a> Difference from Outlier</h3> <p><em>Conceptually, a massive activation is a scalar value, determined jointly by the sequence and feature dimensions ; in contrast, an outlier feature is a vector, corresponding to activations at all tokens. Further, massive activations are present at extremely few tokens, while outlier features expect most activations in them to be large. In practice, we find that massive activations do not overlap with outlier feature dimensions.</em></p> <h3 id="massive-activations-act-as-biases-in-llms"><a href="#massive-activations-act-as-biases-in-llms" class="header-anchor">#</a> Massive Activations Act as Biases in LLMs</h3> <p>Variances of massive activations are considerably smaller relative to their mean values when compared to other activations.</p> <p>Massive activations act as fixed but important biases in LLMs.</p> <p>Why these layers and tokens? The fact that these activations act as biases may explain why LLMs store them at certain layers and tokens:</p> <ul><li>The tendency of these activations to appear at the starting token could be attributed to <strong>the fact that every autoregressive training instance contains an initial token</strong>.
<ul><li>Since LLMs are based on next word prediction, the starting token is the only token used in all forward passes within a sequence.</li></ul></li> <li>The existence of these activations in <strong>delimiter tokens might be due to the relatively low semantic value of these tokens, rendering them a low-cost option for storing such biases</strong>.
<ul><li>Conversely, tokens with rich semantics would risk significant loss of input information, if they are repurposed to store biases.</li></ul></li> <li>The fact that massive activations emerge only after a few initial layers may be because LLMs would require <strong>some initial layers</strong> to process the meaning of the tokens associated with massive activations.
<ul><li>At these layers, their semantics may be transferred to other token positions via self-attention, and preserved moving forward.</li></ul></li></ul> <h3 id="attention-is-concentrated-on-massive-activations"><a href="#attention-is-concentrated-on-massive-activations" class="header-anchor">#</a> Attention is Concentrated on Massive Activations</h3> <p><img src="https://github.com/user-attachments/assets/77194c6f-677f-4c9a-ac21-84b64b3e9101" alt="image"></p> <p>We find that in layer 3 and deeper layers (e.g., layer 31), attention is mostly concentrated on the two tokens associated with massive activations.</p> <h3 id="massive-activations-impose-implicit-attention-biases"><a href="#massive-activations-impose-implicit-attention-biases" class="header-anchor">#</a> Massive Activations Impose Implicit Attention Biases</h3> <p>We find that at all stages, features of the two tokens associated with massive activations are drastically different from other tokens.</p> <p>Specifically, after the first “normalize” step, the embeddings of these two tokens appear as a sparse vector with two distinct non-zero elements.</p> <p><img src="https://github.com/user-attachments/assets/8084bcb2-a03a-468d-b026-8f5e13f560cb" alt="image"></p> <p>We can see that the value updates from C are nearly identical across tokens, i.e., they serve as additive bias terms, although not explicitly imposed.</p> <p>Furthermore, we note that this pattern of value update is strikingly similar across various inputs.</p> <p>Overall, our results indicate that LLMs use massive activations to allocate substantial attention at certain tokens.</p> <p>These tokens are then utilized to form a constant bias term when computing the attention output.</p> <h3 id="explicit-attention-biases-eliminate-massive-activations"><a href="#explicit-attention-biases-eliminate-massive-activations" class="header-anchor">#</a> Explicit Attention Biases Eliminate Massive Activations</h3> <p>Introduce additional learnable parameters k′, v′ ∈ Rd for each head.</p> <p><img src="https://github.com/user-attachments/assets/61137f43-91f7-4b81-af72-6da99aecbf9f" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/81552ba0-c3b8-4dcf-866b-f8156a24b057" alt="image"></p> <p><em>Massive activations are connected to self-attention.</em></p> <p><em>LLMs use massive activations to concentrate substantial attention on very few tokens, injecting implicit bias terms in the attention computation.</em></p> <p><em>Further, massive activations can be eliminated by augmenting LLMs with explicit attention biases.</em></p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/30.llm_sink_activation.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/04/30, 20:05:19</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7067/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Thinking of LLM Prefilling &amp; Decoding Split</div></a> <!----></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7067/" class="prev">Thinking of LLM Prefilling &amp; Decoding Split</a></span> <!----></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.3856e1e2.js" defer></script><script src="/qishao-notes/assets/js/2.0833fe67.js" defer></script><script src="/qishao-notes/assets/js/114.48c15a29.js" defer></script>
  </body>
</html>
