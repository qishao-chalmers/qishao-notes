<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>From Attention Sink to Massive Activation | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.542e2c29.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.0833fe67.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/114.1989db51.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.1e500a78.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.efe4e18e.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.d344979b.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.aad8bc68.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.69f251e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.fb36554a.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.c328df02.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.f0ba8661.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.e17d34f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.695af16c.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.cc2a8c8c.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.8a8e0de2.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.d583abd0.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.2d0c3ec5.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.c72d74fe.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.de852cdc.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.ce4015db.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.03ac3f53.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.d217c2fa.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.80d6079b.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.3a815ecd.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.8535d463.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.d3f8dc72.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.8f4b9589.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.d2179baf.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.d96cccd4.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.496a473d.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.2742eaf0.js"><link rel="prefetch" href="/qishao-notes/assets/js/126.4e34b166.js"><link rel="prefetch" href="/qishao-notes/assets/js/127.17809496.js"><link rel="prefetch" href="/qishao-notes/assets/js/128.4772efd7.js"><link rel="prefetch" href="/qishao-notes/assets/js/129.ca41e3d9.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.66b81125.js"><link rel="prefetch" href="/qishao-notes/assets/js/130.75267670.js"><link rel="prefetch" href="/qishao-notes/assets/js/131.39b62a70.js"><link rel="prefetch" href="/qishao-notes/assets/js/132.8f6e1e21.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.02671f9c.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.906dcdfc.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.383840b9.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.71442f47.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.3f69064e.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.a2c2ac4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a4b6fb1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.8a5b62d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.4d84fdc5.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.637bf54a.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.d8b4d04c.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.23be87ee.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.ab4d38ce.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.ae348b88.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.781915d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.c8b0d700.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.b67a85e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.0858b5b0.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.aa6fd9d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.feb23e67.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.530df744.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.3e885138.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.8774cef4.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.cdfd3aa9.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.6cb48c78.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.ac79142f.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.60ab6920.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.2dcb8de5.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.5f352ae8.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.bcc7509d.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.36f25851.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.20d7ac8f.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.ad822e3a.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.ebba6389.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.86177f5e.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.fce7d840.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.ad514339.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.14c8f20a.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.9d58ea7e.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.1f9658dd.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.26a65cae.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.b180c3b8.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.00cfb273.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.2777a73c.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.e5d7e5b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.63d5abb5.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.89586274.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.a1fbdc11.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.5a90503c.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.bca7e213.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.257e363f.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.cd292c5d.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.f62341ae.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.b9c3a1f9.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.163b25e0.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.8aff9616.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.7bd2530e.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.4fb49cdd.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.76a443ae.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.a14cc445.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.3eecfe51.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.bcd1d302.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.a4f4e7d5.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.dcece52f.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.0330ff48.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.c29a1485.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.0d1177a0.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.40588fe2.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.2bc797df.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.1ac71d6e.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.aa9320dc.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.b6a1e324.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.0f05ece6.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.f990a16a.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.f1dfb30c.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.4ac84047.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.cc7ba318.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.7a21298c.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.6b8db046.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.9eed17aa.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.0a9b27e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.48a79619.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.eaed4e75.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.77ba0b2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.aa523a54.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.518be617.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.7a8c0983.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.ae5fe203.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.2538d6f3.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.ad1b24f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.3c98d117.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.6dce913e.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.3f361b43.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.b5ad55b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Paper List</a></li><li><a href="/qishao-notes/pages/dc7043/" class="sidebar-link">Efficient LLM</a></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" class="sidebar-link">Summery of Inner Workings of LLM</a></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" class="sidebar-link">Memory Optimizations in LLM</a></li><li><a href="/qishao-notes/pages/dc7049/" class="sidebar-link">Reasoning in LLM</a></li><li><a href="/qishao-notes/pages/dc7050/" class="sidebar-link">LLM Mixed Precision &amp; Quantization &amp; Outlier</a></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" class="sidebar-link">LLM Scaling Law</a></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7059/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li><li><a href="/qishao-notes/pages/dc7060/" class="sidebar-link">LLM MOE Inference</a></li><li><a href="/qishao-notes/pages/dc7061/" class="sidebar-link">LLM Compression</a></li><li><a href="/qishao-notes/pages/dc7062/" class="sidebar-link">LLM Optimizer Optimization</a></li><li><a href="/qishao-notes/pages/dc7063/" class="sidebar-link">LLM Posttraining</a></li><li><a href="/qishao-notes/pages/dc7064/" class="sidebar-link">LLM MICRO - ISCA - HPCA</a></li><li><a href="/qishao-notes/pages/dc7066/" class="sidebar-link">LLM Prefilling &amp; Decoding Split</a></li><li><a href="/qishao-notes/pages/dc7067/" class="sidebar-link">Thinking of LLM Prefilling &amp; Decoding Split</a></li><li><a href="/qishao-notes/pages/dc7068/" aria-current="page" class="active sidebar-link">From Attention Sink to Massive Activation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7068/#_1-596-y2023-efficient-steaming-language-models-with-attention-sinks" class="sidebar-link">1. [596 Y2023] Efficient Steaming Language Models with Attention Sinks</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7068/#_2-y2025-when-attention-sink-emerges-in-language-models" class="sidebar-link">2. [Y2025] When Attention Sink Emerges in Language Models</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7068/#what-is-attention-sink" class="sidebar-link">What is Attention Sink?</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7068/#main-contributions" class="sidebar-link">Main Contributions</a></li></ul></li></ul></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-04-30</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">From Attention Sink to Massive Activation<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[596 Y2023] Efficient Steaming Language Models with Attention Sinks</li> <li>[Y2025] When Attention Sink Emerges in Language Models</li></ol> <h2 id="_1-596-y2023-efficient-steaming-language-models-with-attention-sinks"><a href="#_1-596-y2023-efficient-steaming-language-models-with-attention-sinks" class="header-anchor">#</a> 1. [596 Y2023] Efficient Steaming Language Models with Attention Sinks</h2> <p>Attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention.</p> <p>In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a “sink” even if they are not semantically important.</p> <img src="https://github.com/user-attachments/assets/e78d12d3-80c5-4771-bb09-4a39440d089c" style="width:600px;height:auto;"> <p>The results show the insufficiency of introducing merely one or two initial tokens, whereas a threshold of four initial tokens appears enough, with subsequent additions contributing marginal effects.</p> <p>The nature of the SoftMax function prevents all attended tokens from having zero values.</p> <p>This requires aggregating some information from other tokens across all heads in all layers, even if the current embedding has sufficient self-contained information for its prediction.</p> <p>Consequently, the model tends to dump unnecessary attention values to specific tokens.</p> <p>This result justifies our choice of introducing 4 initial tokens as attention sinks in StreamingLLM.</p> <p>We’ve noted that LLMs are typically trained to utilize multiple initial tokens as attention sinks rather than just one.</p> <img src="https://github.com/user-attachments/assets/fb405010-15cb-4e44-aac4-95a91b98df55" style="width:600px;height:auto;"> <p>The KV cache in StreamingLLM can be conceptually divided into two parts, as illustrated in Figure 4: (1) Attention sinks (four initial tokens) stabilize the attention computation; 2) Rolling KV Cache retains the most recent tokens, crucial for language modeling.</p> <img src="https://github.com/user-attachments/assets/ff71fbb0-f72d-42e1-8dd7-b6cf41f7d3df" style="width:600px;height:auto;"> <p>They also test pretraining with a single sink token.</p> <p>The vanilla model requires the addition of multiple tokens as attention sinks to maintain stable streaming perplexity.</p> <p>In contrast, the model trained with a sink token achieves satisfactory streaming performance <strong>using just the sink token</strong>.</p> <img src="https://github.com/user-attachments/assets/96760ee8-9989-483f-8a2e-af2f2894eed2" style="width:600px;height:auto;"> <h2 id="_2-y2025-when-attention-sink-emerges-in-language-models"><a href="#_2-y2025-when-attention-sink-emerges-in-language-models" class="header-anchor">#</a> 2. [Y2025] When Attention Sink Emerges in Language Models</h2> <p><strong>Background</strong></p> <p>Xiao et al. (2023b) revealed that LLMs allocate significant attention scores to specific token positions, e.g. the first token (not necessary to be a BOS token), resulting in “vertical” attention patterns.</p> <p><strong>Contribution</strong></p> <ul><li>Attention sink emerges after LMs are trained effectively on sufficient training data.
It appears less obvious in LMs trained with small learning rates.
While weight decay encourages the emergence of attention sink.</li> <li>The sink position is highly related to the loss function and data distribution and can be shifted to other positions rather than the first token.</li> <li>Attention sink acts more like key biases, storing extra attention and meanwhile not contributing to the value computation.
This phenomenon (at least partially) stems from tokens’inner dependence on attention scores due to the softmax normalization.
After relaxing such dependence by replacing softmax attention with other attention operations, e.g., sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters.</li></ul> <h3 id="what-is-attention-sink"><a href="#what-is-attention-sink" class="header-anchor">#</a> What is Attention Sink?</h3> <p>An attention sink is when tokens disproportionately attend to the first token (often the BOS token or first word), even when it holds little semantic importance.</p> <p>This creates a vertical attention pattern, frequently leveraged for efficiency in:</p> <ul><li>Streaming inference</li> <li>KV cache optimization</li> <li>Quantization-aware training</li></ul> <h3 id="main-contributions"><a href="#main-contributions" class="header-anchor">#</a> Main Contributions</h3> <h4 id="empirical-demonstration-of-universality"><a href="#empirical-demonstration-of-universality" class="header-anchor">#</a> Empirical Demonstration of Universality</h4> <p>Attention sink appears across LMs (GPT2-XL, LLaMA2/3, Mistral) — even with:</p> <ul><li>Random token sequences</li> <li>Small-scale models
This suggests the phenomenon is model-agnostic and tied to the training process, not the data or size.</li></ul> <h4 id="mechanistic-understanding"><a href="#mechanistic-understanding" class="header-anchor">#</a> Mechanistic Understanding</h4> <p>First token’s key vector acts like a bias: due to angle alignment with other queries, not due to large vector norms.</p> <p>The cosine similarity, not norm product, drives attention sink — meaning high attention persists even with small key norms.</p> <h4 id="emergence-during-pretraining"><a href="#emergence-during-pretraining" class="header-anchor">#</a> Emergence During Pretraining</h4> <p>Attention sink becomes prominent after sufficient optimization.</p> <p>It's less obvious with:</p> <ul><li>Fewer training steps</li> <li>Smaller learning rates</li> <li>Less training data</li></ul> <h4 id="influence-of-data-distribution"><a href="#influence-of-data-distribution" class="header-anchor">#</a> Influence of Data Distribution</h4> <p>Sink position can shift:</p> <ul><li>From token 1 to 2 if token 1 is randomized</li> <li>To fixed tokens if injected intentionally</li> <li><strong>repeated tokens in input suppress attention sink in relative positional encoding models (e.g., LLaMA)</strong>.</li></ul> <h4 id="loss-function-optimization-effects"><a href="#loss-function-optimization-effects" class="header-anchor">#</a> Loss Function &amp; Optimization Effects</h4> <ul><li>Weight decay encourages attention sink, but excessive decay suppresses it by harming learning.</li> <li>In prefix LM tasks, sink moves from token 1 to the prefix span.</li> <li>Shifted window attention (e.g., in Mistral) localizes the sink to absolute positions — not relative ones.</li></ul> <h4 id="attention-sink-key-biases"><a href="#attention-sink-key-biases" class="header-anchor">#</a> Attention Sink = Key Biases</h4> <img src="https://github.com/user-attachments/assets/1faf375f-a01c-4b7d-af36-8d41e2f5b1fa" style="width:600px;height:auto;"> <p>We further show that due to the different manifold of $\mathbf{k}_1^{l,h}$, the angles between $\mathbf{k}_1^{l,h}$ and $\mathbf{q}_t^{l,h}$ play an important role. Considering
$\mathbf{q}_t^{l,h} {\mathbf{k}_j^{l,h}}^\top = |\mathbf{q}_t^{l,h}| \cdot |\mathbf{k}_j^{l,h}| \cdot \cos(\mathbf{q}_t^{l,h}, \mathbf{k}_j^{l,h}),$</p> <p>we visualize the cosine similarity between keys and values, and the product of $\ell_2$-norm between keys and values in Figure 2(<em>Bottom</em>)** .</p> <blockquote><p><strong>Although</strong>  $$|\mathbf{q}_t^{l,h}| \cdot |\mathbf{k}_1^{l,h}|$$ <strong>is comparatively small,</strong></p></blockquote> <p>$$\cos(\mathbf{q}_t^{l,h}, \mathbf{k}_1^{l,h})$$ <strong>is significantly large, leading to attention sink.</strong></p> <p>This explains why attention sink exists despite the small $$\ell_2$$-norm of keys of the first token.</p> <p>To conclude, the first token leverages its keys to act as biases, thus minimizing the angles between $$\mathbf{k}_1^{l,h}$$ and $$\mathbf{q}_t^{l,h}$$, and <strong>exhibiting attention sink</strong> .</p> <ul><li>Attention sink does not contribute to value computation, acting more like a <strong>key-space artifact</strong>.</li> <li>Simply <strong>adding key biases (even without value biases) shifts the sink away from real tokens, proving it’s an optimization artifact</strong>.</li></ul> <h4 id="attention-sink-under-different-inputs"><a href="#attention-sink-under-different-inputs" class="header-anchor">#</a> Attention Sink Under Different Inputs</h4> <ul><li>input domains have negligible effects on our attention sink metric Sinkϵ1</li> <li>(i) randomly sample T tokens from the tokenizer vocabulary V to construct a sequence</li> <li>(ii) randomly sample 1 token from the tokenizer V and repeat it T times.
As present in Table 1(Left), attention sink still exists when the inputs are <strong>random tokens instead of natural language</strong>.</li></ul> <p>However, with repeated tokens, attention sink in Mistral (Jiang et al., 2023) and LLaMA models disappears.
<strong>we prove that for LMs with NoPE/relative PE/ALiBI/Rotary, if the first T tokens are the same, their corresponding hidden states are the same. They all have massive activations, thus dispersing the attention sink.</strong></p> <h4 id="effects-of-optimization-on-attention-sink"><a href="#effects-of-optimization-on-attention-sink" class="header-anchor">#</a> Effects of Optimization on Attention Sink</h4> <ul><li>Attention sink emerges after LMs are trained effectively.</li> <li>Attention sink appears less obvious in LMs trained with small learning rates.</li></ul> <h4 id="effects-of-data-distribution-pdata-on-attention-sink"><a href="#effects-of-data-distribution-pdata-on-attention-sink" class="header-anchor">#</a> Effects of Data Distribution pData on Attention Sink</h4> <ol><li>Attention sink emerges after LMs are trained on sufficient training data.</li> <li>Attention sink could be shifted to other positions rather than the first token if modifying pdata.</li></ol> <h4 id="effects-of-loss-function-on-attention-sink"><a href="#effects-of-loss-function-on-attention-sink" class="header-anchor">#</a> Effects of Loss Function on Attention Sink</h4> <img src="https://github.com/user-attachments/assets/c2dbbdd3-68f5-4b76-8e5f-e3cc788fd0ae" style="width:600px;height:auto;"> <ol><li>Weight decay encourages the emergence of attention sink.</li> <li>With prefix language modeling, attention sink appears among the prefix tokens rather than the first token
only.</li> <li>With shifted window attention, attention sink appears on the “absolute”, not the “relative” first token. Smaller window size prevents the emergence of attention sink.</li></ol> <h4 id="effects-of-model-architecture-on-attention-sink"><a href="#effects-of-model-architecture-on-attention-sink" class="header-anchor">#</a> Effects of Model Architecture on Attention Sink</h4> <ul><li>we note that all these LMs, even the one without explicit PE (NoPE), have attention sink.</li></ul> <h5 id="attention-bias"><a href="#attention-bias" class="header-anchor">#</a> Attention Bias</h5> <p>considered a learnable sink token in each chunk before the input tokens during LM pre-training.<br>
As this token is fixed in the first token, this could be considered as implicitly introducing biases k,v,q in attention.<br>
as long as there are key biases k*<sup>l,h</sup> attention sink disappears on the first token but on the biases.</p> <p><strong>So they prove that v*<sup>l,h</sup> is not important, could just be zero.</strong></p> <img src="https://github.com/user-attachments/assets/3f9afb16-8169-4262-a8a8-665574b1d971" style="width:600px;height:auto;"> <ol><li>Positional embedding, FFN design, LN location, and multi-head design do not affect the emergence of attention sink.</li> <li>Attention sink acts more like key biases, storing extra attention and meanwhile not contributing to the value computation.</li> <li>When relaxing tokens’ inner dependence on attention scores, attention sink does not emerge in LMs.
We note that the LMs with no attention sink typically relax tokens’ inner dependence on attention scores.<br>
Their attention scores during pre-training could be negative or not add up to one.<br>
This indicated that attention sink (at least partially) stems from such inner dependence.<br>
Besides the attention metric computed by proxy attention scores, we also observe that the above LMs also have no massive activations.</li></ol> <img src="https://github.com/user-attachments/assets/4b0d5fe4-940b-45b9-a9bd-6f151dd163b5" style="width:600px;height:auto;"> <h4 id="role-of-attention-normalization"><a href="#role-of-attention-normalization" class="header-anchor">#</a> Role of Attention Normalization</h4> <ul><li><strong>Softmax normalization creates inter-token dependence, reinforcing attention sink</strong></li> <li><strong>Replacing softmax with sigmoid or non-normalized attention (e.g., ELU+1) eliminates attention sink — even in 1B-parameter models</strong></li></ul></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/30.llm_sink_activation.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/04/30, 16:40:18</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7067/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Thinking of LLM Prefilling &amp; Decoding Split</div></a> <!----></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7067/" class="prev">Thinking of LLM Prefilling &amp; Decoding Split</a></span> <!----></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.542e2c29.js" defer></script><script src="/qishao-notes/assets/js/2.0833fe67.js" defer></script><script src="/qishao-notes/assets/js/114.1989db51.js" defer></script>
  </body>
</html>
