<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Efficient LLM | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.6a992a9d.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.7441cbb4.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/83.ad9f799f.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.3e10e050.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.bdf0afb2.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.8843afa2.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.2b10bd09.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.79ad9458.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.ff8e8f1e.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.a14c0b58.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.68b02da3.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.f8c2c5fa.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.6daf8339.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.bf4aac26.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.b6f7d42d.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.ddda0100.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.fa3320d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.200aa588.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.708c1178.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.dc4136f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.a28834fc.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.bfce2e69.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.936b94c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.1c803b2c.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.b688542f.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.d4a145d2.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.13806125.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.31f2ec6d.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.1adc4cd7.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.58ec3c1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.049ba004.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.b08a9f3b.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.db192854.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.ff0533c0.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.65953f3e.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.0ecf0c5e.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.e1097275.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.b9f822cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.c28eefd0.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.9c58429f.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.d31e1099.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.5feb66b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.63e48fb0.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.6c9a33c2.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.6cac6816.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.8607e1f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.f874a546.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.865ae9b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.1c8ad3d7.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.d1c157c7.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.02b760bc.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.3b4fb4a9.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.b0d968c9.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.2bff55cb.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.df5a29a0.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.3ce5b7fb.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.81a4a21e.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.8e78455c.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.45f488bc.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.8b03d117.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.f7d9e54d.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.799b9cd7.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.1e6cb5dc.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.75040c9a.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.245facf2.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.4a761ee9.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.e66e714b.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.3abddccb.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.3acbf519.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.4e32926a.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.2827b429.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.6498da50.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.3a0146e0.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.2df0c6f1.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.02c5e9c0.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.c1fd8b9c.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.440d8122.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.6e3b44f3.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.4bfd230c.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.54a8be0f.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.93ce3c8c.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.5d95b052.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.b9fa42d0.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.26cb8349.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.e6689334.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.0435df4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.88d7bc59.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.7d03d8a3.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.25811434.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.0d058120.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.87ee8000.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.8c524a54.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.f90e1643.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.37d8e037.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.c7815d38.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.632deb10.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.9b3fe0ac.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.461eeffb.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.6439298a.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.804a24b9.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.e60bd10c.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.a6d792e2.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.0dfd9c33.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.f31553f7.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.71f6c077.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.ca2b86a0.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.af058ea0.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.3c6e339b.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.941f3a09.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.c4853b39.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.abd731ae.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.c917c656.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.17a81924.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="ÁõÆÂΩï" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7043/" aria-current="page" class="active sidebar-link">Efficient LLM</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7043/#_1-resource-efficient-architectures" class="sidebar-link">1. Resource-Efficient Architectures</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_1-1-efficient-attention" class="sidebar-link">1.1 Efficient Attention</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_1-2-dynamic-neural-network" class="sidebar-link">1.2 Dynamic Neural Network</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_1-3-diffusion-specific-optimization" class="sidebar-link">1.3 Diffusion-Specific Optimization</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_1-4-vit-specific-optimizations" class="sidebar-link">1.4 ViT-Specific Optimizations</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7043/#_2-resource-efficient-algorithms" class="sidebar-link">2. Resource-Efficient Algorithms</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_2-1-pre-training-algorithms" class="sidebar-link">2.1 Pre-Training Algorithms</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_2-2-fine-tuning-algorithms" class="sidebar-link">2.2 Fine-Tuning Algorithms</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_2-3-inference-algorithms" class="sidebar-link">2.3 Inference Algorithms</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_2-4-model-compression" class="sidebar-link">2.4 Model Compression</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7043/#_3-resource-efficient-systems" class="sidebar-link">3. Resource-Efficient Systems</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_3-1-distributed-training" class="sidebar-link">3.1 Distributed Training</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_3-2-hardware-aware-optimizations" class="sidebar-link">3.2 Hardware-Aware Optimizations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_3-3-serving-on-cloud" class="sidebar-link">3.3 Serving on Cloud</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_3-4-serving-on-edge" class="sidebar-link">3.4 Serving on Edge</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" class="sidebar-link">Summery of Inner Workings of LLM</a></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" class="sidebar-link">Memory Optimizations in LLM</a></li><li><a href="/qishao-notes/pages/dc7049/" class="sidebar-link">Reasoning in LLM</a></li><li><a href="/qishao-notes/pages/dc7050/" class="sidebar-link">LLM Mixed Precision &amp; Quantization &amp; Outlier</a></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" class="sidebar-link">LLM Scaling Law</a></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="È¶ñÈ°µ" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="‰ΩúËÄÖ" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="‰ΩúËÄÖ" class="beLink" data-v-06225672>hitqishao</a></div> <div title="ÂàõÂª∫Êó∂Èó¥" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-01-20</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">ÁõÆÂΩï</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">Efficient LLM<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ul><li>[1] Resource-efficient Algorithms and Systems of Foundation Models: A Survey üòÑ</li> <li>[2] A Survey on Efficient Inference for Large Language Models üôã</li> <li>[3] Efficient Large Language Models: A Survey üôã</li> <li>[4] Efficient Transformers: A Survey üôã</li></ul> <hr> <p><img src="https://github.com/user-attachments/assets/539f3a36-1e67-486a-a1af-d7b637cfaf8e" alt="image"></p> <p><em>Source: Resource-efficient</em></p> <p>Computation complexity of attention is O(T 2D), whereas that of the FFN is O(TD2), where T represents the sequence length and D the hidden state dimension of the model.<br>
The FFN layer is the most computationally intensive component.</p> <h2 id="_1-resource-efficient-architectures"><a href="#_1-resource-efficient-architectures" class="header-anchor">#</a> 1. Resource-Efficient Architectures</h2> <h3 id="_1-1-efficient-attention"><a href="#_1-1-efficient-attention" class="header-anchor">#</a> 1.1 Efficient Attention</h3> <p><img src="https://github.com/user-attachments/assets/d87f3dae-6a64-46ed-af28-51b4f2c16f3d" alt="image"></p> <ul><li><strong>Sparse Attention</strong>: Reduces complexity (e.g., <strong>Longformer[C4522 2020]</strong>, BigBird).<br>
Motivated by graph sparsification, sparse attention aims to build a sparse attention matrix.<br>
This approach aims to retain the empirical advantages of a fully quadratic self-attention scheme while employing a reduced number of inner products.</li></ul> <p>[C2016 2019] Generating Long Sequences with Sparse Transformers<br>
[C602 2020] Efficient Content-Based Sparse Attention with Routing Transformers
[C136 2021] Scatterbrain: Unifying Sparse and Low-rank Attention Approximation
[C55 2021] Is Sparse Attention more Interpretable?</p> <ul><li><p><strong>Approximate Attention</strong>: Low-rank approximations (e.g., Linformer, Reformer).
Approximate attention mainly includes low-rank approximations of the self-attention matrix and innovative reformulations of the self-attention.</p></li> <li><p><strong>Attention-Free Approaches</strong>: Alternatives like Hyena, Mamba.<br>
Despite the dominance of attention-based transformer architectures in large FMs, several works have put forth innovative architectures that hold the potential to replace the traditional transformer model.</p></li></ul> <h3 id="_1-2-dynamic-neural-network"><a href="#_1-2-dynamic-neural-network" class="header-anchor">#</a> 1.2 Dynamic Neural Network</h3> <p><img src="https://github.com/user-attachments/assets/baefb07e-0c3b-46c2-a728-5cb2e94a21b5" alt="image"></p> <ul><li><strong>Mixture of Experts (MoE)</strong>: (e.g., Switch Transformer, GLaM, MoEfication, FFF).
<ul><li><em>C1950 2021</em> Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</li> <li><em>Janus[C20 2023]</em> Janus: A Unified Distributed Training Framework for Sparse Mixture-of-Experts Models</li> <li><em>C124 2018</em> Deep Mixture of Experts via Shallow Embedding</li> <li><em>C364 2013</em> Learning Factored Representations in a Deep Mixture of Experts</li> <li><em>C264 2022</em> Mixture-of-Experts with Expert Choice Routing</li> <li><em>C594 2022</em> GLaM: Efficient Scaling of Language Models with Mixture-of-Experts</li> <li><em>C113 2021</em> MoEfication: Transformer Feed-forward Layers are Mixtures of Experts</li></ul></li> <li><strong>Early Exiting</strong>: Premature termination based on confidence (e.g., FREE, SkipDecode,DeeBERT, PABEE).
early-exiting optimization is a strategy that allows a model to terminate its computational process prematurely when it attains high confidence in the prediction or encounters resource constraints.
[C342 2020] BERT Loses Patience: Fast and Robust Inference with Early Exit
[C383 2020] DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference
[C46 2023] SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference</li></ul> <h3 id="_1-3-diffusion-specific-optimization"><a href="#_1-3-diffusion-specific-optimization" class="header-anchor">#</a> 1.3 Diffusion-Specific Optimization</h3> <ul><li><strong>Efficient Sampling</strong></li> <li><strong>Diffusion in Latent Space</strong></li> <li><strong>Diffusion Architecture Variants</strong></li></ul> <h3 id="_1-4-vit-specific-optimizations"><a href="#_1-4-vit-specific-optimizations" class="header-anchor">#</a> 1.4 ViT-Specific Optimizations</h3> <ul><li><strong>Efficient ViT Variants</strong>: MobileViT, EfficientFormer, EdgeViT.</li></ul> <h2 id="_2-resource-efficient-algorithms"><a href="#_2-resource-efficient-algorithms" class="header-anchor">#</a> 2. Resource-Efficient Algorithms</h2> <h3 id="_2-1-pre-training-algorithms"><a href="#_2-1-pre-training-algorithms" class="header-anchor">#</a> 2.1 Pre-Training Algorithms</h3> <ul><li><strong>Training Data Quality Control</strong>: DataComp, DFN.<br>
A portion of work focus on controlling the quality of training data.</li> <li><strong>Training Data Reduction</strong>: Deduplication, image patch removal.<br>
Pre-training for large FMs needs a dataset at the trillion scale, exemplified by 0.3 trillion tokens for GPT-3-175B [25] and 2 trillion tokens for LLaMa-2-70B [238].<br>
prior literature resorts to reduce vast training data through two aspects: deduplicating text datasets and image patch removal.</li> <li><strong>Progressive Learning</strong>: StackingBERT, CompoundGrow.
Progressive learning is a training strategy that begins by training a small model and then gradually increases the model size, throughout the training process.</li> <li><strong>Mixed Precision Training</strong>: Mesa, GACT.
Mixed precision training often utilizes half-precision floating-point data representation instead of single precision. This approach significantly reduces memory
requirements, approximately halving the storage space needed for weights, activations, and gradients.</li></ul> <h3 id="_2-2-fine-tuning-algorithms"><a href="#_2-2-fine-tuning-algorithms" class="header-anchor">#</a> 2.2 Fine-Tuning Algorithms</h3> <p><img src="https://github.com/user-attachments/assets/1ed489c1-3451-4874-8f36-fa99fc65e0ca" alt="image"></p> <ul><li><p><strong>Additive Tuning</strong>:</p> <ul><li><em>Adapter tuning</em> aims to reduce training costs by introducing adapter modules to specific layers (or all layers) of pre-trained large FMs.
During tuning, the backbone of the pre-trained model remains frozen, and adapter modules are utilized to acquire task-specific knowledge.</li> <li><em>prompt tuning</em> involves designing a task-specific prompt for each task, with the aim of replacing the traditional fine-tuning of pre-trained large FMs parameters.<br>
By tuning the input prompts instead, this method significantly reduces the resources and time required for the fine-tuning.<br>
[C3778] The Power of Scale for Parameter-Efficient Prompt Tuning</li> <li><em>prefix tuning</em> introduces a trainable, task-specific prefix part to each layer of large FMs.
This technique aims to reduce the tuning cost by limiting the updates to the parameters in this prefix.</li></ul></li> <li><p><strong>Selective Tuning</strong>: Freezing most parameters, selective updates.
Selective tuning aims to maintain high performance on new tasks with low training costs by freezing the majority of parameters in large FMs and selectively updating only a small portion of the parameters.</p></li> <li><p><strong>Re-parameter Tuning</strong>: Low-rank adaptation (e.g., <strong>LoRA</strong>, Delta-LoRA).
<img src="https://github.com/user-attachments/assets/920fd758-54f1-492c-bf1e-0a5b4209f2b4" alt="image"></p> <p>Re-parameter tuning adapts large FMs by targeting a significantly smaller subspace than the original, expansive training space.<br>
This approach involves fine-tuning low-rank matrix parameters, a technique that effectively reduces the overall training cost.</p></li></ul> <h3 id="_2-3-inference-algorithms"><a href="#_2-3-inference-algorithms" class="header-anchor">#</a> 2.3 Inference Algorithms</h3> <ul><li><p><strong>Opportunistic Decoding</strong>:</p> <ul><li>Speculative decoding (<em>SpecInfer, LLMCad</em>) generating sequences autoregressively with a cost-efficient small model, followed by parallel token verification using a larger model.</li> <li>Look-ahead decoding accelerates inference in large FMs without relying on a draft model or data store, reducing decoding steps in proportion to log(FLOPs).</li></ul></li> <li><p><strong>Input Filtering and Compression</strong>:</p> <ul><li>Prompt compression(LLMLingua,LLMZip,ICAE,COT-Max)
LLMZip [241] employs LLaMA-7B for compressing natural language. Experimental results demonstrate that LLMZip outperforms cutting-edge text compression methods, including BSC, ZPAQ, and paq8h.</li> <li>Token pruning Pruning of input sequences for transformers, often involving the incremental removal of less important tokens during inference.</li></ul></li> <li><p><strong>KV Cache Optimization</strong>: Memory-efficient sparse attention.
most sparse attention designs, which primarily target the reduction of computational complexity [24, 294], do not necessarily lead to a reduction in KV cache memory consumption.<br>
This is because achieving a reduced memory footprint for the KV cache necessitates a more stringent sparsity pattern.</p> <ul><li><em>H2O</em> KV cache eviction stragegy: employs attention scores to identify and select the least important KV cache tokens in the current state for eviction</li> <li><em>Dynamic Context Pruning</em> learns a memory-efficient KV cache eviction strategy during the pre-training phase.</li> <li><em>Scissorhands</em>: innovative compact KV cache</li> <li><em>Landmark Attention</em> enables the storage of most KV caches in a slower but larger capacity memory</li> <li>[C1 2025] Efficient LLM Inference with Activation Checkpointing and Hybrid Caching</li> <li>[C1 2024] Unifying KV Cache Compression for Large Language Models with LeanKV</li> <li>[C316 2023] Efficient Streaming Language Models with Attention Sinks</li></ul></li> <li><p><strong>Long Context Handling</strong>: LM-Infinite, StreamingLLM. Due to the quadratic computational cost associated with attention mechanisms, various resource-efficient optimizations have been proposed to handle long inputs.</p> <ul><li>LM-Infinite introduces a Œõ-shaped attention mechanism to handle long contexts efficiently.</li> <li>StreamingLLM facilitates large FMs trained with a finite-length attention window to generalize to infinite stream decoding without the need for any fine-tuning.</li></ul></li></ul> <h3 id="_2-4-model-compression"><a href="#_2-4-model-compression" class="header-anchor">#</a> 2.4 Model Compression</h3> <p><img src="https://github.com/user-attachments/assets/d3d1214d-8f83-4553-97bd-467a2b914dd4" alt="image"></p> <ul><li><p><strong>Pruning</strong></p> <ul><li>Structured pruning eliminates entire structural components, such as groups of consecutive parameters or hierarchical structures<br>
LLM Pruner[C372 2023] selectively removes non-essential model structures based on gradient information and incorporates LoRA to recover the model‚Äôs accuracy after pruning.<br>
Structured pruning is also employed in training.<br> <em>Sheared LLaMA</em> adopts an end-to-end to remove channels, encompassing layers, attention heads, intermediate layers, and hidden layers.<br> <em>AdaPrune</em> accelerates neural network training using transposable masks.<br> <em>GUM</em> considers neuron specificity and introduces pruning through network component-based global mobility and local uniqueness scores.<br> <em>PLATON</em> tackles the uncertainty in importance scores during pruning by employing the upper confidence bound of importance estimation.</li> <li>Unstructred pruning It removes neurons with weights below a threshold, thereby compressing the model.„ÄÅ
<em>SparseGPT[C497 2023]</em> sparse regression solver<br> <em>Wanda[C346 2023]</em> prunes weights with smallest magnitude multiplied by corresponding input activations.<br> <em>SIGE</em> converts computation reduction into latency reduction.</li> <li><strong>Contextual pruning</strong> selects the sparse state of each layer.„ÄÅ
<em>Deja Vu</em> üôã dynamically predicts the sparsity of the next layer using the activations of the previous layer. It determines which neurons of MLP blocks and the heads of attention blocks need to be retained. To mitigate the overhead of this predictor, Deja Vu asynchronously predicts the next layer.<br> <em>PowerInfer</em> utilizes the sparsity of activation to dynamically predict the hotactivated neurons of the next layer and computes them on the GPU, whereas other cold-activated neurons are computed on the CPU.</li></ul></li> <li><p><strong>Knowledge Distillation</strong>: Black-box and white-box KD.</p></li> <li><p><strong>Quantization</strong>: Quantization-aware training (QAT), post-training quantization (PTQ).</p> <ul><li>Quantization-aware training<br> <em>LLM-QAT obtains training data for LLMs by leveraging pre-trained models to generate samples through data-free distillation. it quantizes weights, activations, and KV cache, thereby improving training throughput.<br>
QuantGPT incorporats contrastive distillation from a full-precision teacher model and distilling logit information to a quantized student model during autoregressive pre-training.<br>
BitNet pioneers QAT for 1-bit language models, training the language model with 1-bit weights and activations.</em>
[C31 2024] KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</li> <li>Post-training quantization
<ul><li><p>Weights-only Quantization</p> <p><em>identification of outliers and important weights in weights that significantly contribute to accuracy and treating these outliers specially.</em></p> <p><em>SpQR identifies outlier weights and maintains them with high precision while quantizing the rest of the weights.</em></p> <p><em>LLM.int8() employs vectorized quantization and mixed-precision decomposition to handle outlier values for efficient inference.</em></p> <p><em>AWQ[C523 2023] reduces quantization error by protecting the top 1% important weights in the model, utilizing per-channel scaling to determine the optimal scaling factor.</em></p> <p><em>OWQ[C37 2023] analysis suggests that abnormal activations amplify quantization errors, and it employs a mixed-precision scheme, applying higher-precision quantization to weights with a significant impact from activated outlier values.</em></p> <p><em>SqueezeLLM[C160 2023] observes that sensitive weights determine the final model‚Äôs quantization performance and proposes a non-uniform quantization approach to minimize quantization errors in these sensitive weights.</em></p></li> <li><p>Weights-Activation Coquantization</p> <p><em>SmoothQuant[C770 2022] takes advantage of the similarity in the channel-wise activations of different tokens and performs quantization on both weight and activation using per-channel scaling transforms.</em></p> <p><em>RPTQ recognizes the substantial range differences across different channels, reordering the channels for quantization and integrating them into layer normalization and linear layer weights.</em></p> <p><em>OliVe adopts outlier-victim pair quantization and locally processes outliers.</em></p> <p><em>Outlier Suppression+ builds upon Outlier Suppression, discovering that harmful outliers exhibit an asymmetric distribution mainly concentrated in specific channels.<br>
Considering the asymmetry of outliers and quantization errors from the weights of the next layer, this approach performs channel-level translation and scaling operations.</em></p> <p><em>QLLM addresses the issue of activation outliers through an adaptive channel reassembly method and mitigates the information loss caused by quantization using calibration data.</em></p> <p><em>LLM-FP4 quantizes weights into 4-bit float points, proposes per-channel activation quantization, and reparameters additional scaling factors as exponential biases of weights.</em></p> <p><em>ZeroQuant combines layer-wise KD and optimized quantization support to achieve 8-bit quantization.</em></p> <p><em>FlexRound updates the quantization scale of weights and activations by minimizing the error between the quantized values and the full-precision values.</em></p> <p><em>ATOM significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization.</em></p></li></ul></li></ul></li> <li><p><strong>Low-Rank Decomposition (LoRD)</strong>: TensorGPT, LoSparse.</p></li></ul> <h2 id="_3-resource-efficient-systems"><a href="#_3-resource-efficient-systems" class="header-anchor">#</a> 3. Resource-Efficient Systems</h2> <h3 id="_3-1-distributed-training"><a href="#_3-1-distributed-training" class="header-anchor">#</a> 3.1 Distributed Training</h3> <ul><li><p><strong>Resilience</strong>: Checkpointing, redundant computation.</p></li> <li><p><strong>Parallelism</strong>: Data, model, and sequence parallelism.</p></li> <li><p><strong>Communication</strong>: Compression, overlapping with computation.</p></li> <li><p><strong>Storage</strong>: Offloading, heterogeneous GPUs.<br>
ZeRO-Offload offloads data and computations to CPU to train large models on a single GPU.<br>
FlashNeuron[C52 2021] offloads selective data to the SSD for higher throughput.</p></li> <li><p><strong>MoE Optimization</strong>: optimizes the dynamism-related mechanisms, parallelism, and communication in MoE training.
<em>MegaBlocks[C74 2022] leverages sparse primitives to handle dynamic routing and load-imbalanced computation.</em></p> <p><em>FlexMoE focuses on the dynamic expert management and device placement problem.</em></p> <p><em>Tutel designs dynamic adaptive parallelism and pipelining strategies.</em></p></li></ul> <h3 id="_3-2-hardware-aware-optimizations"><a href="#_3-2-hardware-aware-optimizations" class="header-anchor">#</a> 3.2 Hardware-Aware Optimizations</h3> <ul><li><strong>EdgeBERT</strong>: Latency-aware energy optimization.</li> <li><strong>FlightLLM</strong>: FPGA-based LLM inference.</li> <li><strong>SpAtten</strong>: Sparse attention with cascade token pruning.</li> <li><strong>A3[C227 2020]</strong></li></ul> <h3 id="_3-3-serving-on-cloud"><a href="#_3-3-serving-on-cloud" class="header-anchor">#</a> 3.3 Serving on Cloud</h3> <ul><li><p><strong>Inference Accelerating</strong>:</p> <ul><li><em>FlashAttention, FlashAttention-2</em></li> <li><em>Flash-Decoding, FlashDecoding++</em></li> <li><em>DeepSpeed-Inference</em></li> <li><em>ByteTransformer</em></li> <li><em>Google PaLM</em></li></ul> <p><strong>Batching and scheduling</strong></p> <p><em>Orca proposes selective batching and iteration-level scheduling to batch requests of different lengths at the granularity of iterations to increase the maximum batch size.</em></p> <p><em>FlexGen proposes a request scheduling algorithm to mitigate the impact of offloading on the performance of latencyinsensitive FM serving in a single GPU.</em></p> <p><em>FastServe proposes an iteration-level preemptive scheduling and proactive KV cache swapping to mitigate the impact of head-of-line blocking on the performance of distributed FM serving.</em></p> <p><em>SARATHI and DeepSpeed-FastGen split the computation of the prefill phase into small chunks and schedule these chunks with the decoding phase to mitigate the impact of the prefill phase on the performance of large FMs serving.</em></p> <p><em>Splitwise splits the prefill phase and the decoding phase onto different machines according to their different computation and memory requirements.</em></p> <p><em>Sarathi-Serve introduces a chunked-prefills scheduler which splits a prefill request into near equal sized chunks and creates stall-free schedules that adds new requests in a batch without pausing ongoing decodes.</em></p> <p><em>dLoRA dynamically merges/unmerges adapters with the base model and migrating requests/adapters between worker replicas, significantly improving the serving throughput.</em></p></li> <li><p><strong>Memory Saving</strong></p> <ul><li>DeepSpeed-Inference and FlexGen offload activations or model parameters to the DRAM or NVMe memories</li> <li>KV cache managing
<ul><li><strong>vLLM</strong> adopts block-level on-demand memory allocation mechanism.</li> <li>S-LoRA extends this idea to Unified Paging to manage multiple LoRA adapters at the same time.</li></ul></li></ul></li> <li><p><strong>Emerging Platforms</strong>: Spot instances, heterogeneous GPUs.</p></li></ul> <h3 id="_3-4-serving-on-edge"><a href="#_3-4-serving-on-edge" class="header-anchor">#</a> 3.4 Serving on Edge</h3> <ul><li><p><strong>Edge-Cloud Collaboration</strong></p> <p><em>EdgeFM</em> queries and adapts the large FMs to the specific edge models with customized knowledge and architectures so that the dynamic edge model can ensure both low latency and close accuracy to the original large FMs.</p></li> <li><p><strong>On-Device MoE</strong></p> <p><em>EdgeMoe identifies the problem that experts have to be dynamically loaded into memory during inference. To tackle this issue, this approach proposes expert-wise bit-width adaptation to reduce the size of expert parameters with acceptable accuracy loss, saving parameters‚Äô loading time.</em></p> <p><em>PC-MoE is based on a crucial observation that expert activations are subject to temporal locality. Based on this observation, PC-MoE proposes Parameter Committee, which intelligently maintains a subset of crucial experts in use to reduce resource consumption.</em></p> <p>[C21 2024] Mobile foundation model as firmware</p></li> <li><p><strong>Memory Optimization</strong>: LLMCad, <strong>PowerInfer</strong>.</p> <p><em>LLMCad utilizes speculative decoding, which can offload most workloads to a smaller memory-resident draft model.</em></p> <p><em>PowerInfer relies on large FMs runtime sparsity (i.e., only hot neurons are consistently activated across inputs). PowerInfer pre-loads hot-activated neurons onto the GPU for fast access, whereas cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPUGPU data transfers.</em></p></li> <li><p><strong>I/O Optimization</strong>: STI, LLM in a flash.
<em>STI [C16] identifies that loading parameters time is highly longer than computation time. To address this problem, STI proposes dynamically adapting weights bit-width during the loading procedure according to parameters importance, minimizing loading overhead under maximum inference accuracy.</em></p></li> <li><p><strong>Kernel Optimization</strong>: Integer-based edge kernels.</p></li></ul></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/09.eff_llm.md" target="_blank" rel="noopener noreferrer">Â∏ÆÂä©Êàë‰ª¨ÊîπÂñÑÊ≠§È°µÈù¢</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">‰∏äÊ¨°Êõ¥Êñ∞:</span> <span class="time">2025/02/10, 17:50:31</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7042/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">LLM Internals</div></a> <a href="/qishao-notes/pages/dc7045/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Estimation of LLM</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ‚Üê
        <a href="/qishao-notes/pages/dc7042/" class="prev">LLM Internals</a></span> <span class="next"><a href="/qishao-notes/pages/dc7045/">Estimation of LLM</a>‚Üí
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="ÂèëÈÇÆ‰ª∂" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="Êú¨Á´ô‰∏ªÈ¢ò">Vdoing</a> 
    | Copyright ¬© 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="ËøîÂõûÈ°∂ÈÉ®" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="ÂéªËØÑËÆ∫" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="‰∏ªÈ¢òÊ®°Âºè" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          Ë∑üÈöèÁ≥ªÁªü
        </li><li class="iconfont icon-rijianmoshi">
          ÊµÖËâ≤Ê®°Âºè
        </li><li class="iconfont icon-yejianmoshi">
          Ê∑±Ëâ≤Ê®°Âºè
        </li><li class="iconfont icon-yuedu">
          ÈòÖËØªÊ®°Âºè
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.6a992a9d.js" defer></script><script src="/qishao-notes/assets/js/2.7441cbb4.js" defer></script><script src="/qishao-notes/assets/js/83.ad9f799f.js" defer></script>
  </body>
</html>
