<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>LLM Optimizer Optimization | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.8ec4265c.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.6d8a25ce.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/109.d64b47e0.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.c53d023c.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.e2b30df6.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.cd5b5485.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.feae495b.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.66edd4a4.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.bcfab81d.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.da764b86.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.847ccb05.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.5827ba3f.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.fbd753e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.3efa7066.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.a2683c1e.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.0633f77b.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.033969bf.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.f159436a.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.8bc8aefd.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.916e4961.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.c1214fe6.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.f53ebf4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.93f0cf4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.905af094.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.dc4136f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.2c8faca4.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.27286009.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.a93e01e7.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.8aa890c1.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.09ad2136.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.af232804.js"><link rel="prefetch" href="/qishao-notes/assets/js/126.3543de21.js"><link rel="prefetch" href="/qishao-notes/assets/js/127.10d86fde.js"><link rel="prefetch" href="/qishao-notes/assets/js/128.f0a20af7.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.33791185.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.35cdaa5c.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.936b94c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.cc28076c.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.b688542f.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.80c6d9c9.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.42f21f4f.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a7b3ac76.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.e5e299c3.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.96a79aa3.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.049ba004.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.f485ff78.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.18fc9823.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.f10809d3.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.65953f3e.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.0ecf0c5e.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.a9c55c08.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.18c7702c.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.eccee23d.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.9c58429f.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.1c45955e.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.5feb66b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.84f10398.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.b14f6e3a.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.21d1843c.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.c95eff2e.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.d63a3d79.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.fdab2a0b.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.bae99c2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.89b382c5.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.ea1753be.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.d6058b82.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.56a470f5.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.ba0e48ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.92df1709.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.8538c3c9.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.f311eaa5.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.526dc690.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.28e90c7e.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.8b03d117.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.b803c07a.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.c400caa3.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.d8688b9f.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.a9de2d3c.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.9480e434.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.9a082998.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.a3b517e3.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.e6a3f4e4.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.e1d3c447.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.42cf2968.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.2827b429.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.3c9be637.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.c7e8115d.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.0e73c7e2.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.e86b7a00.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.adfe2eec.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.d63f1f61.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.021a0ef2.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.747af7e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.7d99e80d.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.7d8bf00f.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.5d95b052.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.1905d6a8.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.4853193a.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.8f7da019.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.3a589a2f.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.8deb1a9c.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.83e128af.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.6f29df79.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.2129eea1.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.ce7c920c.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.45ce4740.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.f90e1643.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.0540653b.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.47a14f22.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.f2525f33.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.9f8cc0d1.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.1fb87437.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.17b26a24.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.e4e19469.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.2d5d9b41.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.981f151e.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.daeb3e0b.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.7b957d82.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.743ea4b0.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.4e398438.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.dd2487ce.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.67524fb4.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.2dcc72f2.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.8b1b444f.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.f04fc93e.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.1f728656.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.747a996c.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.c6d4c0d1.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Paper List</a></li><li><a href="/qishao-notes/pages/dc7043/" class="sidebar-link">Efficient LLM</a></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" class="sidebar-link">Summery of Inner Workings of LLM</a></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" class="sidebar-link">Memory Optimizations in LLM</a></li><li><a href="/qishao-notes/pages/dc7049/" class="sidebar-link">Reasoning in LLM</a></li><li><a href="/qishao-notes/pages/dc7050/" class="sidebar-link">LLM Mixed Precision &amp; Quantization &amp; Outlier</a></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" class="sidebar-link">LLM Scaling Law</a></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7059/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li><li><a href="/qishao-notes/pages/dc7060/" class="sidebar-link">LLM MOE Inference</a></li><li><a href="/qishao-notes/pages/dc7061/" class="sidebar-link">LLM Compression</a></li><li><a href="/qishao-notes/pages/dc7062/" aria-current="page" class="active sidebar-link">LLM Optimizer Optimization</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7062/#_1-1-badam-a-memory-efficient-full-parameter-optimization-method-for-large-language-models" class="sidebar-link">1. [1] BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7062/#_2-1075-adafactor-adaptive-learning-rates-with-sublinear-memory-cost" class="sidebar-link">2. [1075] Adafactor Adaptive Learning Rates with Sublinear Memory Cost</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7062/#_3-adam-accumulation-to-reduce-memory-footprints-of-both-activations-and-gradients-for-large-scale-dnn-training" class="sidebar-link">[3] Adam Accumulation to Reduce Memory Footprints of both Activations and Gradients for Large-scale DNN Training</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7062/#gradient-accumulation-gradient-release" class="sidebar-link">Gradient Accumulation &amp; Gradient Release</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7062/#adam-accumulation-adama" class="sidebar-link">Adam Accumulation (AdamA)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7062/#elaboration" class="sidebar-link">Elaboration</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7062/#mechanism" class="sidebar-link">Mechanism</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/dc7063/" class="sidebar-link">LLM Posttraining</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-04-02</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">LLM Optimizer Optimization<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[1] BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models</li> <li>[1075] Adafactor Adaptive Learning Rates with Sublinear Memory Cost</li> <li>[3] Adam Accumulation to Reduce Memory Footprints of both Activations and Gradients for Large-scale DNN Training</li></ol> <hr> <h2 id="_1-1-badam-a-memory-efficient-full-parameter-optimization-method-for-large-language-models"><a href="#_1-1-badam-a-memory-efficient-full-parameter-optimization-method-for-large-language-models" class="header-anchor">#</a> 1. [1] BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models</h2> <p><strong>It change the finetuing into block coordinate descent (BCD)-type optimization</strong> which I dont understand.</p> <p>For instance, to finetune an LLM with M billion parameters, Adam [23] necessitates roughly 18M GB of GPU memory for successful training, and
this estimate does not even account for the storage of activations used in the backpropagation (BP) process.</p> <p>Despite the success of PEFT methods, finetuning within a substantially lower-dimensional subspace may potentially <strong>limit downstream performance</strong>.</p> <p><img src="https://github.com/user-attachments/assets/8def8b42-d493-4358-80bc-515746a2cc17" alt="image"></p> <p>We first analyze the memory cost of Adam with mixed precision training.</p> <p>One needs to store the FP16 model parameters for the BP process, which costs 2M memory.</p> <p>For a more precise update, the optimizer also maintains a master copy of a FP32 model, which costs 4M memory.</p> <p>Then, it comes to store the <strong>gradient (converted to FP32), momentum, and second moment</strong> in FP32 precision, costing <strong>4M + 4M + 4M = 12M</strong> memory.</p> <p>In total, Adam needs roughly <strong>18M memory</strong>.</p> <p>In terms of BAdam, it needs to store the up-to-date model parameters (see Figure 1) in FP16 precision, which costs 2M memory. Importantly, since BAdam only updates the active block at one time, we
can store the model parameters, gradient, momentum, and second moment only for the active block θπi in FP32 precision, where the FP32 model parameters and gradient of the active block can be
obtained by transforming their FP16 versions to the FP32 versions.</p> <p>Let us consider the simple case where the partitioned D blocks are equal-sized. Then, BAdam only needs in total</p> <p><img src="https://github.com/user-attachments/assets/f0ee0fbd-cf93-4b2b-a1b0-f7df945d5d16" alt="image"></p> <hr> <h2 id="_2-1075-adafactor-adaptive-learning-rates-with-sublinear-memory-cost"><a href="#_2-1075-adafactor-adaptive-learning-rates-with-sublinear-memory-cost" class="header-anchor">#</a> 2. [1075] Adafactor Adaptive Learning Rates with Sublinear Memory Cost</h2> <p>This paper introduces <strong>Adafactor</strong> , an optimization algorithm designed to provide <strong>adaptive learning rates with significantly reduced memory overhead</strong>  compared to traditional methods like Adam.</p> <p>The core innovation is replacing full per-parameter second-moment estimators (used to scale gradients) with <strong>factored approximations</strong>  based on the <strong>row and column sums</strong>  of squared gradients.</p> <p><strong>Similar to LORA, using only the per-row and percolumn sums of these moving averages, and estimating the per-parameter second moments based on these sums.</strong></p> <p>This change reduces memory usage from O(nm) to O(n + m) for matrix-shaped parameters.</p> <p>Key contributions and details include:</p> <ul><li><p><strong>Factored Second Moment Estimation</strong> :</p> <ul><li>Adafactor leverages a rank-1 approximation using the generalized Kullback-Leibler divergence to estimate the second moment of gradients.</li> <li>Instead of storing full-size accumulators, it maintains exponential moving averages of row and column sums, achieving memory savings while preserving empirical performance.</li> <li>A closed-form solution for the rank-1 approximation ensures computational efficiency and compatibility with exponential smoothing.</li></ul></li> <li><p><strong>Removing Momentum</strong> :</p> <ul><li><strong>Momentum (first moment) is omitted (β₁ = 0)</strong> to further reduce memory cost. This change initially causes instability, especially without learning rate warmup.</li></ul></li> <li><p><strong>Stabilizing Updates</strong> :</p> <ul><li><strong>Update Clipping</strong> : Caps the RMS of unscaled updates to avoid large, destabilizing parameter jumps due to outdated second-moment estimators.</li> <li><strong>Increasing β₂ Schedule</strong> : Proposes a decay schedule like β̂₂ₜ = 1 − t^(-c), which adapts over time and avoids the need for bias correction.</li> <li>These methods independently and jointly stabilize training in the absence of momentum and warmup.</li></ul></li> <li><p><strong>Relative Step Sizes</strong> :</p> <ul><li>Instead of fixed absolute learning rates, Adafactor uses <strong>parameter-relative step sizes</strong> , scaling updates based on the parameter norm, making it more resilient to differing parameter magnitudes (e.g., in embeddings).</li></ul></li> <li><p><strong>Experiments</strong> :</p> <ul><li>Conducted on Transformer models for WMT’14 En→De machine translation.</li> <li>Adafactor with factored moments, no momentum, update clipping, increasing decay rate, and relative step sizes performs comparably to Adam while using less memory.</li> <li>Results show robustness to poor parameter initialization and scaling, unlike Adam.</li></ul></li> <li><p><strong>Practical Use</strong> :</p> <ul><li>Adafactor enables training larger models on memory-constrained hardware.</li> <li>Implementation is available in the Tensor2Tensor library.</li></ul></li></ul> <p><strong>Three-Sentence Summary:</strong></p> <p>The paper proposes <strong>Adafactor</strong> , a memory-efficient optimizer that approximates second-moment estimators using factored row and column sums, drastically reducing auxiliary memory usage from O(nm) to O(n + m).</p> <p>To address instability caused by omitting momentum and slow-decaying estimators, the authors introduce <strong>update clipping</strong>  and an <strong>increasing decay schedule</strong> , both of which stabilize training.</p> <p>Combined with <strong>relative step sizes</strong> , Adafactor achieves comparable performance to Adam on large-scale Transformer tasks while enabling significantly larger models on limited hardware.</p> <hr> <h2 id="_3-adam-accumulation-to-reduce-memory-footprints-of-both-activations-and-gradients-for-large-scale-dnn-training"><a href="#_3-adam-accumulation-to-reduce-memory-footprints-of-both-activations-and-gradients-for-large-scale-dnn-training" class="header-anchor">#</a> [3] Adam Accumulation to Reduce Memory Footprints of both Activations and Gradients for Large-scale DNN Training</h2> <h3 id="gradient-accumulation-gradient-release"><a href="#gradient-accumulation-gradient-release" class="header-anchor">#</a> Gradient Accumulation &amp; Gradient Release</h3> <p><strong>Gradient accumulation</strong> reduces the activation memory by splitting a mini-batch into a sequence of micro batches and accumulating the gradients of all micro-batches.</p> <p><strong>Gradient accumulation</strong> must preserve accumulated value of gradients until the last micro-batch.</p> <p><strong>Gradient release</strong> reduces the gradient memory by freeing up the gradient-occupied space in a layer-by-layer manner.</p> <p><strong>Gradient release</strong> releases the gradients immediately after use.</p> <h3 id="adam-accumulation-adama"><a href="#adam-accumulation-adama" class="header-anchor">#</a> Adam Accumulation (AdamA)</h3> <p>Specifically, instead of accumulating gradients, AdamA integrates gradients into optimizer states (m and v in Adam)
immediately after the gradients are produced, and accumulates optimizer states sequentially over micro-batches.</p> <p>This subtle change of directly integrating gradients to optimizer states makes the memory space for whole model gradients no longer needed, eliminating the aforementioned contradiction between preserving gradients and releasing gradients.</p> <p>Consequently, AdamA can reduce the gradient memory to 1/M of the original (M is the number of layers), and the activation memory to 1/N of the original (N is the number of micro-batches).</p> <p><img src="https://github.com/user-attachments/assets/1dfb4486-d838-42d9-a412-5c5f818a2fb8" alt="image"></p> <h3 id="elaboration"><a href="#elaboration" class="header-anchor">#</a> Elaboration</h3> <p>The key idea behind gradient accumulation is to split a mini-batch into several micro-batches.</p> <p>This method computes the gradients of micro-batches sequentially and accumulates them to reduce the memory footprint of activations as well as to keep the same convergence properties as the original mini-batch.</p> <p>Gradient release executes the backward process in a layer-by-layer manner, which immediately releases the gradient-occupied
memory after the weight updating is finished, so that the memory allocated for gradients can be reduced from the size of whole model size to the size of the maximum layer.</p> <h3 id="mechanism"><a href="#mechanism" class="header-anchor">#</a> Mechanism</h3> <p>Intuitively, as gradients are eventually used to update the optimizer states (m and v in Adam), if we can integrate gradients
into optimizer states in advance, the gradients memory can be released, thus resolving this dilemma.</p> <p>Inspired by this insight, we for the first time propose an optimizer accumulation method, namely AdamA, that integrates gradients into optimizer states immediately after produced and then accumulates optimizer states sequentially over micro-batches.</p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/25.llm_optimizer.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/04/04, 05:07:18</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7061/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">LLM Compression</div></a> <a href="/qishao-notes/pages/dc7063/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">LLM Posttraining</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7061/" class="prev">LLM Compression</a></span> <span class="next"><a href="/qishao-notes/pages/dc7063/">LLM Posttraining</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.8ec4265c.js" defer></script><script src="/qishao-notes/assets/js/2.6d8a25ce.js" defer></script><script src="/qishao-notes/assets/js/109.d64b47e0.js" defer></script>
  </body>
</html>
