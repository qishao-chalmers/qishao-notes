<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Reasoning in LLM | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.8a3e31ba.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.7441cbb4.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/93.a4ea562b.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.3e10e050.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.520553a6.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.83d4d4de.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.16fde198.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.51a049c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.d140062e.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.d3ccd1b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.026ccec6.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.ee1d53d0.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.49858ffd.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.2fc05b0f.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.b6f7d42d.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.63367d2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.3aca2744.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.9a3962c3.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.d4565194.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.37ca8f46.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.cbb19e58.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.710178d2.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.b491cd26.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.20c16677.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.c8c4342b.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.8193bd1a.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.3f209eda.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.bfce2e69.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.18b8bfd8.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.1c803b2c.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.b688542f.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.470802ea.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.13806125.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a7b3ac76.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.c0805dc6.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.96a79aa3.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.53082807.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.b08a9f3b.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.db192854.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.ff0533c0.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.65953f3e.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.c0382aef.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.a9c55c08.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.38386ce5.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.a37871e7.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.9c58429f.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.1c45955e.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.02dd6ae5.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.84f10398.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.464f648d.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.90c421dc.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.1537fba4.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.d63a3d79.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.d53a91bf.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.1c8ad3d7.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.b6f563a6.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.95a3d3d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.bab9140e.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.45bd69b1.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.d31efcc0.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.744a65a1.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.6eb12a3b.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.d36cb9bb.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.adef552b.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.5b854a82.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.8b03d117.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.66a8db9d.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.a683348c.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.1cf7c83a.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.ad1d5ea5.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.a0cacab0.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.4b1473ac.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.ffd123a2.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.0285dd8f.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.261fb71a.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.15c3daf0.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.f0fbf90b.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.d72928d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.e7981e43.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.baee9bd1.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.6fc0140c.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.4aaae739.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.8cc5783e.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.e36d1294.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.95305cc9.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.379ddcc5.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.474f6ed0.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.1e07ff8f.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.5077d9b5.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.5aa14f8d.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.af9db9a7.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.8cbf6856.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.6b0f426e.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.ece63242.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.d983d9f1.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.4820f894.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.404f66fb.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.66f0b3bb.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.f90e1643.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.f4c46206.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.4972d0fb.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.16da0a95.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.f29ed32f.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.52308383.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.c1f7c2a6.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.1af504be.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.5f136ed6.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.c88f3056.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.554ef98d.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.0dfd9c33.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.d464bbc9.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.9210a847.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.36a14662.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.5eeef50b.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.5cf197db.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.e380fab9.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.a0e3b23f.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.029bc410.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.9ce0d5d5.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Paper List</a></li><li><a href="/qishao-notes/pages/dc7043/" class="sidebar-link">Efficient LLM</a></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" class="sidebar-link">Summery of Inner Workings of LLM</a></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" class="sidebar-link">Memory Optimizations in LLM</a></li><li><a href="/qishao-notes/pages/dc7049/" aria-current="page" class="active sidebar-link">Reasoning in LLM</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7049/#_331-language-models-dont-always-say-what-they-think-unfaithful-explanations-in-chain-of-thought-prompting" class="sidebar-link">[331] Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7049/#_9831-chain-of-thought-prompting-elicits-reasoning-in-large-language-models" class="sidebar-link">[9831] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7049/#_641-towards-reasoning-in-large-language-models-a-survey" class="sidebar-link">[641] Towards Reasoning in Large Language Models: A Survey</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7049/#what-is-reasoning" class="sidebar-link">What is Reasoning?</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7049/#techniques-for-enhancing-reasoning-in-llms" class="sidebar-link">Techniques for Enhancing Reasoning in LLMs</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7049/#evaluating-reasoning-in-llms" class="sidebar-link">Evaluating Reasoning in LLMs</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7049/#key-findings-and-implications" class="sidebar-link">Key Findings and Implications</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7049/#open-questions-and-future-directions" class="sidebar-link">Open Questions and Future Directions</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/dc7050/" class="sidebar-link">LLM Mixed Precision &amp; Quantization &amp; Outlier</a></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" class="sidebar-link">LLM Scaling Law</a></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-01-27</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">Reasoning in LLM<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[331] Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</li> <li>[9831] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</li> <li>[641] Towards Reasoning in Large Language Models: A Survey</li></ol> <hr> <h2 id="_331-language-models-dont-always-say-what-they-think-unfaithful-explanations-in-chain-of-thought-prompting"><a href="#_331-language-models-dont-always-say-what-they-think-unfaithful-explanations-in-chain-of-thought-prompting" class="header-anchor">#</a> [331] Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</h2> <ul><li>Key Findings:
<ul><li>Unfaithful Explanations: CoT explanations can be plausible but systematically unfaithful, failing to reflect the true reasoning process.</li> <li>Biasing Features: Models are heavily influenced by biasing features (e.g., reordering multiple-choice options), which are not mentioned in explanations.</li> <li>Accuracy Drop: Biasing models toward incorrect answers leads to a 36% drop in accuracy on BIG-Bench Hard tasks.</li> <li>Social Bias: Models justify stereotype-aligned answers without acknowledging the influence of social biases.</li> <li>Counterfactual Simulatability: Models rarely acknowledge biasing features, making explanations systematically unfaithful.</li></ul></li></ul> <p>They instruct llm with bias：</p> <p><img src="https://github.com/user-attachments/assets/9c02c120-64d7-488c-b1af-bebeb28e8582" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/c11dfc4e-35af-4ce2-b88f-fd51f8980805" alt="image"></p> <hr> <h2 id="_9831-chain-of-thought-prompting-elicits-reasoning-in-large-language-models"><a href="#_9831-chain-of-thought-prompting-elicits-reasoning-in-large-language-models" class="header-anchor">#</a> [9831] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</h2> <p>Key findings:</p> <ul><li>Chain-of-thought prompting significantly improves the performance of large language models on a variety of reasoning tasks.</li> <li>Chain-of-thought prompting is an emergent property of model scale, meaning that it only provides significant performance gains when used with very large language models (around 100 billion parameters).</li> <li>The improvements from chain-of-thought prompting are robust across different language models, datasets, and annotators</li></ul> <p>Usage cases:</p> <ul><li>Arithmetic reasoning: CoT prompting can help language models solve math word problems that require multiple steps, such as the GSM8K benchmark.</li> <li>Commonsense reasoning: CoT prompting can also improve the performance of language models on tasks that require commonsense reasoning, such as the StrategyQA dataset, which requires models to infer a multi-hop strategy to answer questions.</li> <li>Symbolic reasoning: CoT prompting has also been shown to be effective for symbolic reasoning tasks, such as last letter concatenation, which requires the model to concatenate the last letters of words in a name.</li></ul> <hr> <h2 id="_641-towards-reasoning-in-large-language-models-a-survey"><a href="#_641-towards-reasoning-in-large-language-models-a-survey" class="header-anchor">#</a> [641] Towards Reasoning in Large Language Models: A Survey</h2> <p><img src="https://github.com/user-attachments/assets/a54b39f5-4293-4d16-ad88-ce289ed787a9" alt="image"></p> <p>Large language models (LLMs) have made impressive strides in natural language processing, but their ability to reason remains a hot topic. This blog post delves into the fascinating world of reasoning in LLMs, exploring the techniques, evaluations, and key findings that are shaping this field.</p> <h3 id="what-is-reasoning"><a href="#what-is-reasoning" class="header-anchor">#</a> What is Reasoning?</h3> <p>Reasoning is the process of using evidence, logic, and past experiences to form conclusions or make decisions. It's a fundamental aspect of human intelligence, allowing us to solve problems, think critically, and understand the world around us.   There are different types of reasoning, including:</p> <ul><li>Deductive reasoning: Drawing a conclusion based on the truth of the premises (e.g., if all mammals have kidneys and all whales are mammals, then all whales have kidneys).</li> <li>Inductive reasoning: Drawing a conclusion based on observations or evidence (e.g., if every winged creature we've seen is a bird, then a new winged creature is likely a bird).</li> <li>Abductive reasoning: Drawing a conclusion based on the best explanation for a set of observations (e.g., if the car won't start and there's a puddle under it, then the car probably has a leak).</li></ul> <h3 id="techniques-for-enhancing-reasoning-in-llms"><a href="#techniques-for-enhancing-reasoning-in-llms" class="header-anchor">#</a> Techniques for Enhancing Reasoning in LLMs</h3> <p>Researchers are constantly developing new techniques to improve or elicit reasoning in LLMs. Some of the most promising methods include:</p> <ul><li>Fully supervised fine-tuning: This involves fine-tuning a pre-trained LLM on a dataset containing explicit reasoning examples. For instance, a model could be trained to generate rationales explaining its predictions.</li> <li>Prompting and in-context learning: This approach involves prompting LLMs with a question and a few examples of how to solve similar questions. Chain-of-thought prompting is a popular technique where the examples include intermediate reasoning steps, guiding the LLM to generate its own reasoning process.
<ul><li>Prompting &amp; In-Context Learning: in CoT prompting, ⟨input, output⟩ demonstrations are replaced with ⟨input, chain of thought, output⟩ tripples
<ul><li>manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation</li></ul></li> <li>Rationale Engieering： creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by LLMs.</li> <li>Rationale refinement</li> <li>complexity-based prompting to create rationales with more reasoning steps. Their experiments show that the performance of LLMs improves with the increased rationale complexity</li> <li>algorithmic prompting, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations</li> <li>Rationale exploration: decoding strategy, sampling a divese set of rationale, instead of the greedy one</li> <li>Rationale verification</li></ul></li></ul> <p><img src="https://github.com/user-attachments/assets/3cda04b5-8ce4-4149-910e-920c0113efa0" alt="image"></p> <ul><li>Hybrid methods: These methods combine techniques like pre-training or fine-tuning LLMs on datasets that include reasoning, along with prompting techniques to elicit reasoning.
<ul><li>LLMs trained on datasets containing scientific and mathematical data can achieve better performance on reasoning tasks like quantitative reasoning problems when using CoT prompting.</li> <li>bootstrapping &amp; self improving: using LLMs to self-improve their reasoning abilities through a process known  as bootstrapping.
<ul><li>Specifically, with CoT prompting, the model first generates initial rationales. And then, the model is finetuned on rationales that lead to correct answers. This process can be repeated, with each iteration resulting in an improved model that can generate better training data.</li></ul></li></ul></li></ul> <h3 id="evaluating-reasoning-in-llms"><a href="#evaluating-reasoning-in-llms" class="header-anchor">#</a> Evaluating Reasoning in LLMs</h3> <p>Evaluating the reasoning abilities of LLMs is crucial. Researchers use various methods and benchmarks to assess their performance, including:</p> <ul><li>End task performance: This involves measuring the accuracy of LLMs on <strong>tasks requiring reasoning, such as arithmetic, commonsense, and symbolic reasoning benchmarks.</strong></li> <li>Analysis of reasoning: This approach focuses on directly assessing the reasoning steps taken by LLMs, rather than just the final answer. This can involve analyzing the quality of the generated rationales or using formal metrics to evaluate the reasoning process.</li></ul> <h3 id="key-findings-and-implications"><a href="#key-findings-and-implications" class="header-anchor">#</a> Key Findings and Implications</h3> <p>Research in reasoning in LLMs has yielded some interesting findings:</p> <ul><li>Emergent ability: Reasoning seems to be an emergent ability of LLMs, <strong>becoming more pronounced as the models get larger (around 100 billion parameters or more).</strong></li> <li>Chain-of-thought prompting: This technique has been shown to significantly improve the performance of LLMs on various reasoning tasks.</li> <li>Complex reasoning challenges: Despite progress, LLMs still struggle with complex reasoning tasks, suggesting that current benchmarks might be too simple.</li></ul> <h3 id="open-questions-and-future-directions"><a href="#open-questions-and-future-directions" class="header-anchor">#</a> Open Questions and Future Directions</h3> <p>The field of reasoning in LLMs is still evolving, with many open questions and exciting avenues for future research:</p> <ul><li>True reasoning or mimicry?: Are LLMs truly capable of reasoning, or are they simply learning to mimic human reasoning through pattern recognition?</li> <li>Improving reasoning capabilities: How can we further enhance the reasoning capabilities of LLMs? This could involve developing new training methods, model architectures, or prompting techniques.</li></ul> <p>By addressing these questions and continuing to explore the intricacies of reasoning in LLMs, we can unlock their full potential and pave the way for more intelligent and reliable language-based AI systems.</p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/14.llm_reasoning.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/03/05, 03:07:21</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7048/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Memory Optimizations in LLM</div></a> <a href="/qishao-notes/pages/dc7050/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">LLM Mixed Precision &amp; Quantization &amp; Outlier</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7048/" class="prev">Memory Optimizations in LLM</a></span> <span class="next"><a href="/qishao-notes/pages/dc7050/">LLM Mixed Precision &amp; Quantization &amp; Outlier</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.8a3e31ba.js" defer></script><script src="/qishao-notes/assets/js/2.7441cbb4.js" defer></script><script src="/qishao-notes/assets/js/93.a4ea562b.js" defer></script>
  </body>
</html>
