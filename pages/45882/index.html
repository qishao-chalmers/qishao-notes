<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>GPU Unified Memory Innovations | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.7fcedfe3.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.7441cbb4.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/60.fe0350ce.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.3e10e050.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.fc2abc73.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.8843afa2.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.a6918a3d.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.b0057dfd.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.79e52b4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.a14c0b58.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.632f6117.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.918adf06.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.6daf8339.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.bf4aac26.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.86efcdc5.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.aa99c50c.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.fa3320d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.d6ac0ee4.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.875270e3.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.205db7cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.3f209eda.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.bfce2e69.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.936b94c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.1c803b2c.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.b688542f.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.ba9d4baf.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.42f21f4f.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a7b3ac76.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.1adc4cd7.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.58ec3c1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.049ba004.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.f485ff78.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.18fc9823.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.f10809d3.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.65953f3e.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.dfda1a33.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.e1097275.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.b9f822cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.a37871e7.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.5f059ad4.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.317fcf55.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.ac7b19be.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.0b761d24.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.dae4dbf8.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.2bbb98ee.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.ce927bde.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.1a265d46.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.eb741f11.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.1c8ad3d7.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.4ceb442b.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.02b760bc.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.1c1afaeb.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.74f957d1.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.6d7767ac.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.a77ec10c.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.f552acdd.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.81a4a21e.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.8e78455c.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.45f488bc.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.048a44f6.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.f7d9e54d.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.799b9cd7.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.2248139c.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.4ad778e1.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.5a249910.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.4a761ee9.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.d7e1015a.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.3abddccb.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.942dd3d6.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.b181d359.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.ddcf1d19.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.d96a9be4.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.2df0c6f1.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.1e1dc6d9.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.c5f8aaa0.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.2c0faaa3.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.58d85c05.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.7722f887.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.a6e7b8aa.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.6936f79c.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.5d95b052.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.fd1ba165.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.1be18098.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.13410603.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.50a41f16.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.88d7bc59.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.06057897.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.e1ace481.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.5e89e75c.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.87ee8000.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.8c524a54.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.de603d6f.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.a85ea3da.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.e305f5b1.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.0d12552d.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.8503f2c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.9864939e.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.28b859b3.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.6439298a.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.8f9fd9a3.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.e60bd10c.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.efe5c9cd.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.7d0e7d9f.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.04fe12c9.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.71f6c077.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.ca2b86a0.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.af058ea0.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.3c6e339b.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.941f3a09.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.c4853b39.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.abd731ae.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.c917c656.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.c2b1a1d3.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="ÁõÆÂΩï" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/cc7034/" class="sidebar-link">Operand Collector</a></li><li><a href="/qishao-notes/pages/2476ae/" class="sidebar-link">GPU WARP Scheduler</a></li><li><a href="/qishao-notes/pages/14769f/" class="sidebar-link">Precision Exception</a></li><li><a href="/qishao-notes/pages/44771e/" class="sidebar-link">Unified Memory Paper List</a></li><li><a href="/qishao-notes/pages/44871e/" class="sidebar-link">TensorCore Paper List</a></li><li><a href="/qishao-notes/pages/45871e/" class="sidebar-link">Memory Behaviour Paper List</a></li><li><a href="/qishao-notes/pages/45871f/" class="sidebar-link">GPU Virtualization Paper List</a></li><li><a href="/qishao-notes/pages/458720/" class="sidebar-link">Large Language Model Paper List</a></li><li><a href="/qishao-notes/pages/458721/" class="sidebar-link">GPU Simulator</a></li><li><a href="/qishao-notes/pages/458722/" class="sidebar-link">Architectural Survey</a></li><li><a href="/qishao-notes/pages/458724/" class="sidebar-link">Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper</a></li><li><a href="/qishao-notes/pages/458725/" class="sidebar-link">Understanding GPGPU-SIM 1 How to get Instruction</a></li><li><a href="/qishao-notes/pages/458726/" class="sidebar-link">Understanding GPGPU-SIM 2 Instruction Execution</a></li><li><a href="/qishao-notes/pages/458727/" class="sidebar-link">Understanding GPGPU-SIM 3 How is the simulation started</a></li><li><a href="/qishao-notes/pages/45872/" class="sidebar-link">Understanding GPGPU-SIM 4 Microarchitecture</a></li><li><a href="/qishao-notes/pages/45874/" class="sidebar-link">Understanding GPGPU-SIM 5  Memory Interface</a></li><li><a href="/qishao-notes/pages/45873/" class="sidebar-link">Warp Related Memory Optimization</a></li><li><a href="/qishao-notes/pages/45875/" class="sidebar-link">GPU Cache Coherency</a></li><li><a href="/qishao-notes/pages/45876/" class="sidebar-link">GPU Cache &amp; Memory Hirerarchy</a></li><li><a href="/qishao-notes/pages/45877/" class="sidebar-link">GPU TLB</a></li><li><a href="/qishao-notes/pages/45878/" class="sidebar-link">GPU Page Table Walk</a></li><li><a href="/qishao-notes/pages/45879/" class="sidebar-link">GPU Cache's Papers</a></li><li><a href="/qishao-notes/pages/45880/" class="sidebar-link">GPU WARP Mangement Papers</a></li><li><a href="/qishao-notes/pages/45882/" aria-current="page" class="active sidebar-link">GPU Unified Memory Innovations</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/45882/#_1-2023-evaluating-unified-memory-performance-in-hip" class="sidebar-link">1.[2023] Evaluating Unified Memory Performance in HIP</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45882/#result-analysis" class="sidebar-link">Result Analysis</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/45882/#_2-97-unlocking-bandwidth-for-gpus-in-cc-numa-systems" class="sidebar-link">2.[97] Unlocking Bandwidth for GPUs in CC-NUMA Systems</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45882/#main-idea-in-short" class="sidebar-link">Main Idea in Short</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45882/#introduction" class="sidebar-link">Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45882/#contribution" class="sidebar-link">Contribution</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45882/#interesting-experiment" class="sidebar-link">Interesting Experiment</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45882/#interesting-finding" class="sidebar-link">Interesting Finding</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45882/#tlb" class="sidebar-link">TLB</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45882/#_3-asplos-deepum-tensor-migration-and-prefetching-in-unified-memory" class="sidebar-link">3. [ASPLOS] DeepUM: Tensor Migration and Prefetching in Unified Memory</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45882/#_4-asplos-capuchin-tensor-based-gpu-memory-management-for-deep-learning" class="sidebar-link">4. [ASPLOS] Capuchin: Tensor-based GPU Memory Management for Deep Learning</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45882/#framework" class="sidebar-link">Framework</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45882/#_5-pump-profiling-free-unified-memory-prefetcher-for-large-dnn-model-support" class="sidebar-link">5. PUMP Profiling-free Unified Memory Prefetcher for Large DNN Model Support</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45882/#_6-micro-g10-enabling-an-efficient-unified-gpu-memory-and-storage-architecture-with-smart-tensor-migrations" class="sidebar-link">6. [MICRO] G10 Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/45883/" class="sidebar-link">GPU MultiTask</a></li><li><a href="/qishao-notes/pages/45884/" class="sidebar-link">GPU Training Notes</a></li><li><a href="/qishao-notes/pages/45885/" class="sidebar-link">GPU Paper with Code</a></li><li><a href="/qishao-notes/pages/45886/" class="sidebar-link">GPU Driver &amp; Runtime &amp; Compliation</a></li><li><a href="/qishao-notes/pages/45887/" class="sidebar-link">Accel-Sim Simulator</a></li><li><a href="/qishao-notes/pages/45889/" class="sidebar-link">Understanding GPGPU-SIM 6 Memory Space</a></li><li><a href="/qishao-notes/pages/47871e/" class="sidebar-link">TO READ</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="È¶ñÈ°µ" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/gpu/#gpu" data-v-06225672>gpu</a></li></ul> <div class="info" data-v-06225672><div title="‰ΩúËÄÖ" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="‰ΩúËÄÖ" class="beLink" data-v-06225672>hitqishao</a></div> <div title="ÂàõÂª∫Êó∂Èó¥" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2024-10-29</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">ÁõÆÂΩï</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">GPU Unified Memory Innovations<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[2023] Evaluating Unified Memory Performance in HIP</li> <li>[97] Unlocking Bandwidth for GPUs in CC-NUMA Systems</li> <li>[ASPLOS] DeepUM: Tensor Migration and Prefetching in Unified Memory</li> <li>[ASPLOS] Capuchin: Tensor-based GPU Memory Management for Deep Learning</li> <li>PUMP Profiling-free Unified Memory Prefetcher for Large DNN Model Support</li> <li>[MICRO] G10 Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations</li></ol> <hr> <p><strong>Not Read</strong></p> <ol><li>MegTaiChi: Dynamic Tensor-based Memory Management Optimization for DNN Training</li> <li>[14] Efficient GPU Memory Management for Nonlinear DNNs</li></ol> <hr> <h2 id="_1-2023-evaluating-unified-memory-performance-in-hip"><a href="#_1-2023-evaluating-unified-memory-performance-in-hip" class="header-anchor">#</a> 1.[2023] Evaluating Unified Memory Performance in HIP</h2> <p>UM only works on recent AMD GPUs, including Vega10 and MI100.<br>
There are two flavors of the support: XNACK-enabled and XNACK-disabled.\</p> <ul><li>In the XNACK-enabled mode,a GPU can handle retry of a memory access after page-faults, which enables mapping and migrating data on demand, as well
as memory overcommitment.</li> <li>In the XNACK-disabled mode, all memory must be resident and mapped in GPU page tables when the GPU is executing application code.<br>
The XNACK-enabled mode only has experimental support.</li></ul> <p>The experimental results show that the performance of the applications using UM is closely related to data transfer size and memory accesses of a kernel. Compared to ‚ÄúUM‚Äù, prefetching
memory as a memory usage hint leads to significant data transfers between the host and device.</p> <p>Compared to ‚ÄúUM‚Äù, prefetching memory as a memory usage hint leads to significant data transfers between the host and device.</p> <ul><li>‚ÄúUM-hint‚Äù and ‚ÄúUM‚Äù indicate unified memory with and without memory usage hints, respectively.</li> <li>‚ÄúZeroCopy‚Äù uses zero-copy buffers for data migration.</li> <li>‚ÄúPageableCopy‚Äù copies data from pageable host memory to device memory</li> <li>‚ÄúPageLockedCopy‚Äù transfers data from page-locked host memory to device memory</li></ul> <p><img src="https://github.com/user-attachments/assets/76a68408-3cee-458e-b1d0-ad3a2fc7ae0a" alt="image"></p> <h3 id="result-analysis"><a href="#result-analysis" class="header-anchor">#</a> Result Analysis</h3> <p>The result shows that the stall rate is highly sensitive to the increase of memory size in UM.</p> <p><img src="https://github.com/user-attachments/assets/84ec16fc-6579-407a-b624-17365e348d9b" alt="image"></p> <p>The decrease of the kernel execution time ranges from approximately 1.1X to 2.8X with respect to the vector length for the three optimization techniques.„ÄÅ
However, the execution time is still approximately 1.4X to 74.8X <strong>longer than that of the kernel that takes the copy-then-execute</strong> approach.</p> <p>In [28], the authors present 32 open-source UM benchmarks in CUDA and evaluate their performance on an NVIDIA Pascal GPU.<br>
They find that across the benchmarks the performance of the UM benchmarks is on average <strong>34.2%</strong> slower compared with the benchmarks without UM due to the cost of page fault
handling</p> <blockquote><p>[28] <em>UVMBench: A Comprehensive Benchmark Suite for Researching Unified Virtual Memory in GPU</em></p></blockquote> <hr> <h2 id="_2-97-unlocking-bandwidth-for-gpus-in-cc-numa-systems"><a href="#_2-97-unlocking-bandwidth-for-gpus-in-cc-numa-systems" class="header-anchor">#</a> 2.[97] Unlocking Bandwidth for GPUs in CC-NUMA Systems</h2> <p><em>Nvidia with umich</em></p> <h3 id="main-idea-in-short"><a href="#main-idea-in-short" class="header-anchor">#</a> Main Idea in Short</h3> <ul><li>Mainly focus on how many pages that covers the page-fault pages should be migrated.</li> <li>Prefetching with upgraded range, which balance the prefetching and also reduce the number of TLB shootdowns</li> <li>TLB shootdown is estimated at 100 cycles</li> <li><strong>Memory Oversubscription and Eviction is not considered.</strong></li> <li>Page Migration Threshold accustomed to each workload is complex. And not worth it. It is better to just migrate on first touch.</li></ul> <h3 id="introduction"><a href="#introduction" class="header-anchor">#</a> Introduction</h3> <p><img src="https://github.com/user-attachments/assets/369a641b-18b1-4eb2-8c4e-3d83c1861ade" alt="image"></p> <h3 id="contribution"><a href="#contribution" class="header-anchor">#</a> Contribution</h3> <ul><li>Counter-based metrics to determine when to migrate pages from the CPU to GPU are insufficient for finding an optimal migration policy to exploit GPU memory bandwidth.<br>
In streaming workloads, where each page may be accessed only a few times, waiting for N accesses to occur before migrating a page will actually limit the number of accesses that occur after migration, reducing the efficacy of the page migration operation.</li></ul> <ol start="2"><li>TLB shootdown and refill overhead can significantly degrade the performance of any page migration policy for GPUs.<br>
We show that combining reactive migration with virtual address locality information to aggressively prefetch pages can mitigate much of this overhead, resulting in increased GPU throughput.</li></ol> <h3 id="interesting-experiment"><a href="#interesting-experiment" class="header-anchor">#</a> Interesting Experiment</h3> <p>Performance comparson of DDR and GDDR Experiments</p> <p><img src="https://github.com/user-attachments/assets/e5c30717-100c-4792-b0b9-6d15d3f72144" alt="image"></p> <p>This choice is motivated by our observation that the performance of some GPU compute workloads would degrade by as much as 66% if the traditional GDDR memory on a GPU were replaced with standard DDR memory, as seen in Figure 2.</p> <p><img src="https://github.com/user-attachments/assets/7eaec0c6-bc35-4f6d-b9aa-ae31d8b23f06" alt="image"></p> <p><em>Still confused about the following Figure.</em></p> <p><img src="https://github.com/user-attachments/assets/76b5a722-f83b-458d-a8ab-bd03022702ff" alt="image"></p> <h3 id="interesting-finding"><a href="#interesting-finding" class="header-anchor">#</a> Interesting Finding</h3> <h4 id="clustered-page"><a href="#clustered-page" class="header-anchor">#</a> Clustered Page</h4> <p>Page Accessing is clusted by memory arranges.<br>
Part of continuous virtual address is hot.<br>
This clustering is key to range expansion because it suggests that if a page is identified for migration, then other neighboring pages in the virtual address space are likely to have a similar number of total touches.</p> <h4 id="threshold-to-trigger-page-migration"><a href="#threshold-to-trigger-page-migration" class="header-anchor">#</a> Threshold to trigger page migration</h4> <p><img src="https://github.com/user-attachments/assets/dced8b99-52e5-4390-9b4c-f825afe21cf2" alt="image"></p> <p>a first touch policy (threshold-1) requires no tracking information and can be trivially implemented by migrating a page the first time the GPU
translates an address for the page.</p> <p>Considering the performance differential seen across thresholds, we believe the overhead of implementing the necessary hardware counters to track all pages within a system to differentiate their access counts is not worth the improvement over a vastly simpler first-touch migration policy.</p> <h3 id="tlb"><a href="#tlb" class="header-anchor">#</a> TLB</h3> <p>The runtime system also must be cognizant that performing TLB invalidations (an integral part of page migration) on a GPU does not just halt a single processor, but thousands of compute pipelines that may be accessing these pages through a large shared TLB structure.<br>
This shared TLB structure makes page migrations between a CPU and GPU potentially much more costly (in terms of the opportunity cost of lost execution throughput) than in CPU-only systems.</p> <p>Recent papers have provided proposals about how to efficiently implement general purpose TLBs that are, or could be, optimized for a GPU‚Äôs needs [28]‚Äì[30].<br>
Others have recently looked at improving TLB reach by exploiting locality within the virtual to physical memory remapping, or avoiding this layer completely [31]‚Äì[33].<br>
Finally, Gerofi et al. [34] recently examined TLB performance of the Xeon Phi for applications with large footprints, while McCurdy et al. [35]
investigated the effect of superpages and TLB coverage for HPC applications in the context of CPUs.</p> <p><img src="https://github.com/user-attachments/assets/7365332a-8dc1-4314-9eca-11ae29d117c2" alt="image"></p> <hr> <h3 id="_3-asplos-deepum-tensor-migration-and-prefetching-in-unified-memory"><a href="#_3-asplos-deepum-tensor-migration-and-prefetching-in-unified-memory" class="header-anchor">#</a> 3. [ASPLOS] DeepUM: Tensor Migration and Prefetching in Unified Memory</h3> <p><img src="https://github.com/user-attachments/assets/a50fe29a-1100-4b36-9924-8c0b49ce28a1" alt="image"></p> <p>DeepUM automatically prefetches data using correlation prefetching.</p> <p>Two correlation table records the history of kenrel execution and page access patterns during training of prefetching.</p> <p>DeepUM‚Äôs correlation tables record the history of the kernel executions and their page accesses during the training phase of a DNN.<br>
It prefetches pages based on the information in the correlation tables by predicting which kernel will execute next.<br>
While traditional correlation prefetching uses a single table to store history and records the relationship between the CPU cache lines, DeepUM correlation prefetching uses two different table structures.</p> <p>Assign each kernel with a kernel ID and maitian kernel ID with its page access history.</p> <hr> <h3 id="_4-asplos-capuchin-tensor-based-gpu-memory-management-for-deep-learning"><a href="#_4-asplos-capuchin-tensor-based-gpu-memory-management-for-deep-learning" class="header-anchor">#</a> 4. [ASPLOS] Capuchin: Tensor-based GPU Memory Management for Deep Learning</h3> <p>üëç üëç üëç üëç üëç</p> <p>The key feature of Capuchin is that it makes memory management decisions based on dynamic tensor access pattern tracked at runtime. <br>
This design is motivated by the observation that the access pattern to tensors is regular during training iterations. <br>
Based on the identified patterns, one can exploit the total memory optimization space and offer the fine-grain and flexible control of when and how to perform
memory optimization techniques.</p> <h4 id="introduction-2"><a href="#introduction-2" class="header-anchor">#</a> Introduction</h4> <p>BERT 768 hiddenlayers 73GB memory int training.<br>
V100 32GB on-board memory and P100 16GB on-board memory.</p> <p>Feature maps are produced in the forward propagation and used again in the backward propagation.<br>
Major deep learning frameworks such as Tensorflow , MXNet and Pytorch usually maintain these feature maps in GPU memory until they are no longer
needed in backward propagation computation. <br>
However, there is usually a large gap between two accesses to the same feature map in forward and backward propagation, which incurs high memory consumption to store the intermediate results.</p> <p>To reduce memory consumption:</p> <ul><li>swapping</li> <li>recomputting</li></ul> <p>Key observaions:</p> <ul><li>we believe that dynamically tracking fine-grained tensor accesses is a fundamental and general technique that enables effective memory management optimizations.<br>
This paper demonstrates that this essential idea can be implemented efficiently on top of major deep learning frameworks.</li> <li>The training process is composed of millions of iterations with clear boundaries, and the tensor accesses have regular and repeated access patterns across iterations.<br>
This means that analyzing the timing and tensor access patterns can easily reveal the memory optimization opportunities with concrete guidance</li></ul> <h4 id="background"><a href="#background" class="header-anchor">#</a> Background</h4> <p><strong>Forward propagation</strong></p> <p>input feature maps, current layer‚Äôs weights and bias produce the output feature maps which become the next layer‚Äôs input data.<br>
The forward propagation concludes with the calculation of loss by comparing the output with ground truth label at the output layer.</p> <p><strong>Backward propagation</strong></p> <p>The backward propagation starts from the output layer and reversely traverses layers to optimize the weights and bias.</p> <p><strong>Memory Usage</strong></p> <ul><li>feature maps:output in the forward propagation</li> <li>gradient maps:output in the backward propagtaion</li> <li>convolution workspace: extra memory space needed by convolution algorithm.</li></ul> <p>Model weights consume very small amount of memory and are usually persistent in GPU memory to be continuously updated.</p> <p>the latter two are temporary memory usage which can be released immediately after current computations are finished.</p> <p>The feature maps are needed in both forward and backward propagation. However, there exists <em><strong>a large time gap between the two usage points for computations in forward and backward phase</strong></em>.</p> <p><strong>Deap learning framework execution modes</strong></p> <ul><li>eager mode: dynamic graph</li> <li>graph mode: static graph</li></ul> <h3 id="framework"><a href="#framework" class="header-anchor">#</a> Framework</h3> <p><img src="https://github.com/user-attachments/assets/bd0b963e-743e-4e1b-95b1-d2f77a4544a3" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/9a812147-f1a2-4ef9-9c43-fac1d9df67d9" alt="image"></p> <p>The idea is clear, simple and classic.</p> <p>Profile based on tensor reuse pattern.</p> <p>Based on history information choose early prefetch or recompute.</p> <hr> <h3 id="_5-pump-profiling-free-unified-memory-prefetcher-for-large-dnn-model-support"><a href="#_5-pump-profiling-free-unified-memory-prefetcher-for-large-dnn-model-support" class="header-anchor">#</a> 5. PUMP Profiling-free Unified Memory Prefetcher for Large DNN Model Support</h3> <p>Year: 2022</p> <p>üëç üëç üëç üëç üëç</p> <p><strong>This paper is published with source code. Real Stuff!</strong></p> <p>PUMP exploits GPU asynchronous execution for prefetch; that is, there exists a delay between the time that CPU launches a kernel and the time the kernel executes in GPU.</p> <p>PUMP extracts memory blocks accessed by the kernel when launching and swaps these blocks into GPU memory.</p> <p><img src="https://github.com/user-attachments/assets/8f1ec193-9be7-4be0-be6a-1fe7c735ab4e" alt="image"></p> <p>The idea behind this paper is also simple and classic.</p> <p>Before launch the kernel, launch the prefetch cudaEvent.</p> <p>They exploit dynamic linker of linux, with LD_PRELOAD, they could call their version of cuda wrapper.</p> <hr> <h3 id="_6-micro-g10-enabling-an-efficient-unified-gpu-memory-and-storage-architecture-with-smart-tensor-migrations"><a href="#_6-micro-g10-enabling-an-efficient-unified-gpu-memory-and-storage-architecture-with-smart-tensor-migrations" class="header-anchor">#</a> 6. [MICRO] G10 Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations</h3> <p>üëç üëç üëç üëç üëç</p> <p><strong>Paper with Code</strong></p> <p><img src="https://github.com/user-attachments/assets/4f27bd28-1c3f-4752-9d71-ecc76bc544dd" alt="image"></p> <p>Key Observations: tensors are not active for long time.</p> <p><img src="https://github.com/user-attachments/assets/1b914427-3254-48d1-8e48-02dc1cc4cf01" alt="image"></p> <p>Main Idea: insert pre-prefetch and evict function in CUDA code.
<img src="https://github.com/user-attachments/assets/4c0da2dd-190f-46c7-be33-8ce1a6545204" alt="image"></p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/03.gpu/24.gpu_novel_um.md" target="_blank" rel="noopener noreferrer">Â∏ÆÂä©Êàë‰ª¨ÊîπÂñÑÊ≠§È°µÈù¢</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">‰∏äÊ¨°Êõ¥Êñ∞:</span> <span class="time">2025/02/10, 17:02:33</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/45880/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">GPU WARP Mangement Papers</div></a> <a href="/qishao-notes/pages/45883/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">GPU MultiTask</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ‚Üê
        <a href="/qishao-notes/pages/45880/" class="prev">GPU WARP Mangement Papers</a></span> <span class="next"><a href="/qishao-notes/pages/45883/">GPU MultiTask</a>‚Üí
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="ÂèëÈÇÆ‰ª∂" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="Êú¨Á´ô‰∏ªÈ¢ò">Vdoing</a> 
    | Copyright ¬© 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="ËøîÂõûÈ°∂ÈÉ®" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="ÂéªËØÑËÆ∫" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="‰∏ªÈ¢òÊ®°Âºè" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          Ë∑üÈöèÁ≥ªÁªü
        </li><li class="iconfont icon-rijianmoshi">
          ÊµÖËâ≤Ê®°Âºè
        </li><li class="iconfont icon-yejianmoshi">
          Ê∑±Ëâ≤Ê®°Âºè
        </li><li class="iconfont icon-yuedu">
          ÈòÖËØªÊ®°Âºè
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.7fcedfe3.js" defer></script><script src="/qishao-notes/assets/js/2.7441cbb4.js" defer></script><script src="/qishao-notes/assets/js/60.fe0350ce.js" defer></script>
  </body>
</html>
