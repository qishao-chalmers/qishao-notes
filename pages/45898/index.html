<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>GPU Multiworkload scheduling | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.685d6d32.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.dd2b9bf7.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.4d9050ab.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/84.6ceb15a3.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.023fac80.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.be99f241.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.dc419179.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.6706c6d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.86619558.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.2ad0cee3.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.f6d47cde.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.8521d8a7.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.b0ddbee5.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.cf18dc67.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.be07d04a.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.b001321c.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.b23b3733.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.1c9a25c8.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.9ed4f148.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.c5f12574.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.6f580a89.js"><link rel="prefetch" href="/qishao-notes/assets/js/115.dd524179.js"><link rel="prefetch" href="/qishao-notes/assets/js/116.fae4e6d2.js"><link rel="prefetch" href="/qishao-notes/assets/js/117.12dd3672.js"><link rel="prefetch" href="/qishao-notes/assets/js/118.1b2aae36.js"><link rel="prefetch" href="/qishao-notes/assets/js/119.1129bb6c.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.dfe024be.js"><link rel="prefetch" href="/qishao-notes/assets/js/120.6d50419b.js"><link rel="prefetch" href="/qishao-notes/assets/js/121.3811f82f.js"><link rel="prefetch" href="/qishao-notes/assets/js/122.d9873a4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/123.b0270000.js"><link rel="prefetch" href="/qishao-notes/assets/js/124.d5375f61.js"><link rel="prefetch" href="/qishao-notes/assets/js/125.828287b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/126.65535593.js"><link rel="prefetch" href="/qishao-notes/assets/js/127.36dd15c2.js"><link rel="prefetch" href="/qishao-notes/assets/js/128.7be5e93a.js"><link rel="prefetch" href="/qishao-notes/assets/js/129.b9bb6277.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.5efab44e.js"><link rel="prefetch" href="/qishao-notes/assets/js/130.a243a343.js"><link rel="prefetch" href="/qishao-notes/assets/js/131.54f42aec.js"><link rel="prefetch" href="/qishao-notes/assets/js/132.aeb97b6e.js"><link rel="prefetch" href="/qishao-notes/assets/js/133.3a4234ba.js"><link rel="prefetch" href="/qishao-notes/assets/js/134.c759d591.js"><link rel="prefetch" href="/qishao-notes/assets/js/135.ddeb320a.js"><link rel="prefetch" href="/qishao-notes/assets/js/136.4942cafe.js"><link rel="prefetch" href="/qishao-notes/assets/js/137.7a99dd56.js"><link rel="prefetch" href="/qishao-notes/assets/js/138.e568e87e.js"><link rel="prefetch" href="/qishao-notes/assets/js/139.d4cfd6e2.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.c969a0b3.js"><link rel="prefetch" href="/qishao-notes/assets/js/140.271f177f.js"><link rel="prefetch" href="/qishao-notes/assets/js/141.870b5fb2.js"><link rel="prefetch" href="/qishao-notes/assets/js/142.f95dab0f.js"><link rel="prefetch" href="/qishao-notes/assets/js/143.1ca0052d.js"><link rel="prefetch" href="/qishao-notes/assets/js/144.7e715292.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.df53a168.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.67008b70.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.861e1d5d.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.c9558048.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.880240a3.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.61609b8a.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.72b53498.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.a98326f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.acc5896d.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.88ca611d.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.37e52c50.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.331f17d4.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.914a6398.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.b0085c20.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.d8572433.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.99819a65.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.bbe8b7b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.021c72f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.b6eb8bb3.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.f2770d51.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.3e885138.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.09bff73a.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.c2f281c9.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.fe8a50b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.577877cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.68da95f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.c90ef165.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.f4483d20.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.4afd8139.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.be88c93d.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.18008ea4.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.ad822e3a.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.8b6602f3.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.79799614.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.ba1e5730.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.5afe5ead.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.79517f64.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.e4bdf3ee.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.383cf514.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.267ac7b3.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.1bb5adf4.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.e63d9095.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.92826847.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.87dcb106.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.c59442b1.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.ea5b377a.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.03d7c65b.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.8745291b.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.897efcf6.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.c17451bf.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.f19094e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.c5a54419.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.198a3d86.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.84a4e203.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.af5651e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.82293b3c.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.141cb180.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.76a443ae.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.e05fe896.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.40baa929.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.07170ed6.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.358dd4b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.070db250.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.30ad9c0b.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.4e6127f9.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.5fb56a87.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.f83e87e1.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.0b200c98.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.0075faa9.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.010789b5.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.b83bf513.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.34c778cf.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.4125c387.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.268c27b3.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.5bfd4e1a.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.1fd6a95d.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.1adcaec9.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.d1761f22.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.a09ec872.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.a79fe340.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.9ff8f5de.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.4b493fac.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.0bbcf4ff.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.0ec9e400.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.5ae969aa.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.7f1a14f3.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.866a7ec5.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.bc25b0f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.d5e7980d.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.bef1ed73.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.5375a66d.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.685d6d32.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/cc7034/" class="sidebar-link">Operand Collector</a></li><li><a href="/qishao-notes/pages/2476ae/" class="sidebar-link">GPU WARP Scheduler</a></li><li><a href="/qishao-notes/pages/14769f/" class="sidebar-link">Precision Exception</a></li><li><a href="/qishao-notes/pages/44771e/" class="sidebar-link">Unified Memory Paper List</a></li><li><a href="/qishao-notes/pages/44871e/" class="sidebar-link">TensorCore Paper List</a></li><li><a href="/qishao-notes/pages/45871e/" class="sidebar-link">Memory Behaviour Paper List</a></li><li><a href="/qishao-notes/pages/45871f/" class="sidebar-link">GPU Virtualization Paper List</a></li><li><a href="/qishao-notes/pages/458720/" class="sidebar-link">Large Language Model Paper List</a></li><li><a href="/qishao-notes/pages/458721/" class="sidebar-link">GPU Simulator</a></li><li><a href="/qishao-notes/pages/458722/" class="sidebar-link">Architectural Survey</a></li><li><a href="/qishao-notes/pages/458724/" class="sidebar-link">Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper</a></li><li><a href="/qishao-notes/pages/458725/" class="sidebar-link">Understanding GPGPU-SIM 1 How to get Instruction</a></li><li><a href="/qishao-notes/pages/458726/" class="sidebar-link">Understanding GPGPU-SIM 2 Instruction Execution</a></li><li><a href="/qishao-notes/pages/458727/" class="sidebar-link">Understanding GPGPU-SIM 3 How is the simulation started</a></li><li><a href="/qishao-notes/pages/45872/" class="sidebar-link">Understanding GPGPU-SIM 4 Microarchitecture</a></li><li><a href="/qishao-notes/pages/45874/" class="sidebar-link">Understanding GPGPU-SIM 5  Memory Interface</a></li><li><a href="/qishao-notes/pages/45873/" class="sidebar-link">Warp Related Memory Optimization</a></li><li><a href="/qishao-notes/pages/45875/" class="sidebar-link">GPU Cache Coherency</a></li><li><a href="/qishao-notes/pages/45876/" class="sidebar-link">GPU Cache &amp; Memory Hirerarchy</a></li><li><a href="/qishao-notes/pages/45877/" class="sidebar-link">GPU TLB</a></li><li><a href="/qishao-notes/pages/45878/" class="sidebar-link">GPU Page Table Walk</a></li><li><a href="/qishao-notes/pages/45879/" class="sidebar-link">GPU Cache's Papers</a></li><li><a href="/qishao-notes/pages/45880/" class="sidebar-link">GPU WARP Mangement Papers</a></li><li><a href="/qishao-notes/pages/45882/" class="sidebar-link">GPU Unified Memory Innovations</a></li><li><a href="/qishao-notes/pages/45883/" class="sidebar-link">GPU MultiTask</a></li><li><a href="/qishao-notes/pages/45884/" class="sidebar-link">GPU Training Notes</a></li><li><a href="/qishao-notes/pages/45885/" class="sidebar-link">GPU Paper with Code</a></li><li><a href="/qishao-notes/pages/45886/" class="sidebar-link">GPU Driver &amp; Runtime &amp; Compliation</a></li><li><a href="/qishao-notes/pages/45887/" class="sidebar-link">Accel-Sim Simulator</a></li><li><a href="/qishao-notes/pages/45889/" class="sidebar-link">Understanding GPGPU-SIM 6 Memory Space</a></li><li><a href="/qishao-notes/pages/45890/" class="sidebar-link">GPU Insturctions</a></li><li><a href="/qishao-notes/pages/45892/" class="sidebar-link">GPU GEMM</a></li><li><a href="/qishao-notes/pages/45893/" class="sidebar-link">GPU Compiler Optimization Pass</a></li><li><a href="/qishao-notes/pages/45894/" class="sidebar-link">GPU GEMM</a></li><li><a href="/qishao-notes/pages/45895/" class="sidebar-link">GPU Workload Scheduling</a></li><li><a href="/qishao-notes/pages/45896/" class="sidebar-link">GPU Workload Scheduling</a></li><li><a href="/qishao-notes/pages/45897/" class="sidebar-link">GPU Cache Management</a></li><li><a href="/qishao-notes/pages/45898/" aria-current="page" class="active sidebar-link">GPU Multiworkload scheduling</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/45898/#_1-gslice-controlled-spatial-sharing-of-gpus-for-a-scalable-inference-platform-comprehensive-summary-analysis" class="sidebar-link">1. GSLICE: Controlled Spatial Sharing of GPUs for a Scalable Inference Platform — Comprehensive Summary &amp; Analysis</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45898/#_1-problem-statement-the-gpu-inference-bottleneck" class="sidebar-link">1. Problem Statement: The GPU Inference Bottleneck</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45898/#_2-gslice-the-solution-intelligent-dynamic-spatial-multiplexing" class="sidebar-link">2. GSLICE: The Solution — Intelligent, Dynamic Spatial Multiplexing</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45898/#_3-the-critical-architectural-choice-why-mps-streams" class="sidebar-link">3. The Critical Architectural Choice: Why MPS &gt; Streams</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45898/#_4-the-foundational-insight-the-knee-point" class="sidebar-link">4. The Foundational Insight: The &quot;Knee Point&quot;</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45898/#_5-the-engineering-masterpiece-shadow-if" class="sidebar-link">5. The Engineering Masterpiece: Shadow IF</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45898/#_6-synergy-the-triad-of-brilliance" class="sidebar-link">6. Synergy: The Triad of Brilliance</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45898/#_7-architecture-overview" class="sidebar-link">7. Architecture Overview</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45898/#_9-conclusion-a-paradigm-shift" class="sidebar-link">9. Conclusion: A Paradigm Shift</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/45898/#✅-final-takeaway" class="sidebar-link">✅ Final Takeaway</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/47871e/" class="sidebar-link">TO READ</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/gpu/#gpu" data-v-06225672>gpu</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-09-16</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">GPU Multiworkload scheduling<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[164] GSLICE: Controlled Spatial Sharing of GPUs for a Scalable Inference Platform — Comprehensive Summary &amp; Analysis</li></ol> <hr> <h2 id="_1-gslice-controlled-spatial-sharing-of-gpus-for-a-scalable-inference-platform-comprehensive-summary-analysis"><a href="#_1-gslice-controlled-spatial-sharing-of-gpus-for-a-scalable-inference-platform-comprehensive-summary-analysis" class="header-anchor">#</a> <strong>1. GSLICE: Controlled Spatial Sharing of GPUs for a Scalable Inference Platform — Comprehensive Summary &amp; Analysis</strong></h2> <p><em>Summarized by Qwen</em></p> <h3 id="_1-problem-statement-the-gpu-inference-bottleneck"><a href="#_1-problem-statement-the-gpu-inference-bottleneck" class="header-anchor">#</a> <strong>1. Problem Statement: The GPU Inference Bottleneck</strong></h3> <p>Modern GPUs (e.g., NVIDIA V100) offer immense computational power (~125 TFLOPS) and memory bandwidth (~900 GBps). However, most Deep Neural Network (DNN) inference models (e.g., ResNet-50, VGG-19) are computationally lightweight in comparison and cannot fully saturate this hardware.</p> <ul><li><strong>Traditional Solutions Fail:</strong> <ul><li><strong>Temporal Sharing (Time-Slicing):</strong> Runs one model at a time → Low utilization, low throughput.</li> <li><strong>Batching:</strong> Improves throughput but increases latency → Violates Service Level Objectives (SLOs) for real-time apps.</li> <li><strong>Default CUDA MPS (Multi-Process Service):</strong> Allows concurrent execution but provides <strong>uncontrolled spatial sharing</strong> → Heavy models monopolize the GPU, starving lighter ones → Unpredictable performance.</li> <li><strong>Static GPU% in MPS:</strong> Static allocation → Wastes resources when workload is light or starves models when demand spikes.</li></ul></li></ul> <blockquote><p>✅ <strong>Core Challenge</strong>: Achieve <strong>high GPU utilization</strong>, <strong>low latency (SLO compliance)</strong>, <strong>performance isolation</strong>, and <strong>scalability</strong> simultaneously across diverse, concurrent inference functions (IFs).</p></blockquote> <h3 id="_2-gslice-the-solution-intelligent-dynamic-spatial-multiplexing"><a href="#_2-gslice-the-solution-intelligent-dynamic-spatial-multiplexing" class="header-anchor">#</a> <strong>2. GSLICE: The Solution — Intelligent, Dynamic Spatial Multiplexing</strong></h3> <p>GSLICE is a <strong>DPDK-based, framework-agnostic inference platform</strong> that overcomes these limitations by introducing <strong>controlled, adaptive, and isolated spatial sharing</strong> of the GPU via enhanced CUDA MPS.</p> <h4 id="key-innovations"><a href="#key-innovations" class="header-anchor">#</a> <strong>Key Innovations</strong></h4> <table><thead><tr><th style="text-align:left;">Innovation</th> <th style="text-align:left;">Description</th> <th style="text-align:left;">Why It Matters</th></tr></thead> <tbody><tr><td style="text-align:left;"><strong>Self-Tuning GPU Resource Allocation</strong></td> <td style="text-align:left;">Dynamically adjusts each IF’s GPU % based on real-time metrics: <br> • <code>Residual Latency Capacity = SLO - Observed Latency</code><br> • <code>Residual Throughput Capacity = Achieved Throughput - Request Arrival Rate</code><br> • <strong>Logic</strong>: Increase GPU% if either metric is negative; decrease if both are positive and large (&gt;5%).</td> <td style="text-align:left;">Replaces static, wasteful provisioning with intelligent, feedback-driven resource management. Ensures every IF gets “just enough” to meet its SLO and demand.</td></tr> <tr><td style="text-align:left;"><strong>Max-Min Fairness Algorithm</strong></td> <td style="text-align:left;">When reallocating resources, GSLICE prioritizes the IF with the <em>lowest</em> current GPU% relative to its need.</td> <td style="text-align:left;">Prevents starvation of lighter or less demanding models. Guarantees fairness while maximizing aggregate system throughput.</td></tr> <tr><td style="text-align:left;"><strong>Shadow IF + Overlapped Execution</strong></td> <td style="text-align:left;">For every active IF, GSLICE maintains a <strong>shadow (hot-standby) IF</strong> on the same CPU core. When GPU% needs adjustment:<br> 1. Configure shadow IF with new GPU%.<br> 2. Load model onto GPU on shadow.<br> 3. Perform seamless switchover (&lt; 100 µs) from primary to shadow IF.<br> 4. Terminate old primary IF.</td> <td style="text-align:left;">Solves the critical problem of <strong>~2–15s restart overhead</strong> in CUDA MPS. Enables dynamic re-provisioning with <strong>near-zero downtime</strong>, making adaptation practical.</td></tr> <tr><td style="text-align:left;"><strong>Self-Learning Adaptive Batching (SLAB)</strong></td> <td style="text-align:left;">Dynamically adjusts batch size based on observed latency, arrival rate, and SLO headroom. Uses EWMA for smoothing.</td> <td style="text-align:left;">Balances the trade-off between batching’s throughput gains and latency costs. Converges to optimal batch size <strong>10x faster</strong> than systems like Clipper.</td></tr> <tr><td style="text-align:left;"><strong>Parameter Sharing Across IFs</strong></td> <td style="text-align:left;">Model weights (the largest component of GPU memory) are loaded <strong>once</strong> onto the GPU. Multiple instances of the <em>same</em> model share these identical parameters.</td> <td style="text-align:left;">Reduces per-IF GPU memory footprint by 20-50%. Enables multiplexing <strong>5–54% more IFs</strong> on the same GPU. Dramatically reduces model load times (8–10x faster).</td></tr> <tr><td style="text-align:left;"><strong>Zero-Copy Data Transfer</strong></td> <td style="text-align:left;">Uses DPDK’s pinned memory and GPU DMA to scatter-gather network packets directly into GPU memory, bypassing the CPU.</td> <td style="text-align:left;">Eliminates costly CPU data copy overheads, especially critical for high-throughput streaming data.</td></tr></tbody></table> <h3 id="_3-the-critical-architectural-choice-why-mps-streams"><a href="#_3-the-critical-architectural-choice-why-mps-streams" class="header-anchor">#</a> <strong>3. The Critical Architectural Choice: Why MPS &gt; Streams</strong></h3> <p>This is where GSLICE makes a <strong>foundational, deliberate, and highly impactful decision</strong>.</p> <h4 id="the-two-paths-to-spatial-sharing"><a href="#the-two-paths-to-spatial-sharing" class="header-anchor">#</a> <strong>The Two Paths to Spatial Sharing</strong></h4> <table><thead><tr><th style="text-align:left;">Feature</th> <th style="text-align:left;">CUDA Streams</th> <th style="text-align:left;">CUDA MPS</th></tr></thead> <tbody><tr><td style="text-align:left;"><strong>Scope</strong></td> <td style="text-align:left;">Within a single process.</td> <td style="text-align:left;">Across multiple processes.</td></tr> <tr><td style="text-align:left;"><strong>Mechanism</strong></td> <td style="text-align:left;">Multiple command queues within one app. Kernels from different streams can overlap <em>if</em> they don’t compete for the same resources.</td> <td style="text-align:left;">Multiple independent applications (processes) run concurrently on the same GPU. Each has its own context.</td></tr> <tr><td style="text-align:left;"><strong>Resource Control</strong></td> <td style="text-align:left;">No built-in mechanism to limit compute/memory usage per stream.</td> <td style="text-align:left;">Can enforce <strong>GPU% limits</strong> (SM allocation) per process via environment variables.</td></tr> <tr><td style="text-align:left;"><strong>Isolation</strong></td> <td style="text-align:left;">❌ Poor. Streams from the same app can interfere. One slow kernel blocks others.</td> <td style="text-align:left;">✅ Good. Processes are isolated by design. Resource limits provide hard boundaries.</td></tr> <tr><td style="text-align:left;"><strong>Use Case Fit for Inference</strong></td> <td style="text-align:left;">Mixing different models? Impossible. Only suitable for batching <em>within</em> one model.</td> <td style="text-align:left;">Perfect. Enables running AlexNet, ResNet-50, and VGG-19 as separate processes on one GPU.</td></tr></tbody></table> <h4 id="gslice-s-experimental-findings-streams-are-not-the-answer"><a href="#gslice-s-experimental-findings-streams-are-not-the-answer" class="header-anchor">#</a> <strong>GSLICE’s Experimental Findings: Streams Are Not the Answer</strong></h4> <p>The authors rigorously tested both approaches:</p> <ol><li><p><strong>Streams Don't Enable True Heterogeneous Multiplexing</strong>:</p> <ul><li>Experiments showed that when running <strong>ResNet-50 and AlexNet concurrently using streams</strong>, their kernels <strong>did not overlap meaningfully</strong> (Figure 6).</li> <li>The heavier ResNet-50 kernels dominated the SMs, forcing AlexNet to wait. This was effectively <strong>time-sharing disguised as spatial sharing</strong>.</li> <li>Result: Increased latency, no real throughput gain, and poor isolation.</li></ul></li> <li><p><strong>Streams Harm Latency</strong>:</p> <ul><li>Adding more streams beyond 2 consistently increased latency without improving throughput for most models (Figure 5a).</li> <li>Even for Alexnet, which saw some throughput gain with more streams, latency still increased significantly.</li> <li>The overhead of managing many streams and context switches negated the benefits.</li></ul></li> <li><p><strong>MPS with Resource Limits Is the Right Tool</strong>:</p> <ul><li>Figure 7 demonstrates the key insight: By running AlexNet and ResNet-50 as <strong>separate MPS processes</strong>, each limited to a fixed GPU% (e.g., 40% and 60%), their kernels <strong>could execute concurrently without interference</strong>.</li> <li>The GPU's SMs were partitioned, ensuring each model had guaranteed access to its allocated compute units.</li> <li>This provided true <strong>performance isolation</strong> — the latency of one model was unaffected by the other, as long as its allocated GPU% was above its &quot;knee point&quot;.</li></ul></li></ol> <blockquote><p>🚫 <strong>Conclusion from the Paper</strong>:<br> <em>“Streams are problematic while multiplexing models with unequal compute requirement... streams do not provide any meaningful overlap... resulting in increased inference task completion time... Therefore, in GSLICE we prefer using MPS with resource provisioning instead of CUDA streams, whenever possible.”</em></p></blockquote> <h4 id="why-this-decision-was-revolutionary"><a href="#why-this-decision-was-revolutionary" class="header-anchor">#</a> <strong>Why This Decision Was Revolutionary</strong></h4> <p>Choosing MPS over streams wasn't just a technical preference — it was an <strong>architectural commitment to isolation and control</strong>.</p> <ul><li><strong>Streams</strong> optimize for <em>parallelism within a single application</em>.</li> <li><strong>MPS with limits</strong> optimizes for <em>fair, multi-tenant, heterogeneous resource sharing</em> — the exact problem cloud inference platforms face.</li></ul> <p>By choosing MPS, GSLICE could leverage NVIDIA’s built-in mechanism for <strong>resource partitioning</strong> (GPU%) and build its self-tuning logic <em>on top of it</em>, rather than fighting against the inherent limitations of streams. This allowed them to deliver <strong>guaranteed SLOs</strong> — something streams fundamentally cannot provide in a mixed-model environment.</p> <h3 id="_4-the-foundational-insight-the-knee-point"><a href="#_4-the-foundational-insight-the-knee-point" class="header-anchor">#</a> <strong>4. The Foundational Insight: The &quot;Knee Point&quot;</strong></h3> <p>Through extensive profiling of models (AlexNet, ResNet-50, VGG-19, GNMT, Jasper), the authors made a profound discovery:</p> <img width="564" height="544" alt="image" src="https://github.com/user-attachments/assets/c08c68d8-075d-4272-bc88-410b6f7cb533"> <blockquote><p>✅ <strong>Every DNN model has a &quot;knee point&quot;</strong> — a specific percentage of GPU resources (e.g., 60% for ResNet-50, 70% for VGG-19) beyond which increasing allocation yields only <strong>diminishing returns</strong> in throughput or latency improvement.</p></blockquote> <h4 id="why-this-changes-everything"><a href="#why-this-changes-everything" class="header-anchor">#</a> <strong>Why This Changes Everything</strong></h4> <table><thead><tr><th>Traditional Assumption</th> <th>GSLICE's Discovery</th></tr></thead> <tbody><tr><td>Give each model 100% GPU to maximize its performance.</td> <td>Even the heaviest models plateau at ~70% GPU.</td></tr> <tr><td>Static allocation: “We’ll give VGG-19 80%, ResNet-50 20%.”</td> <td>The optimal allocation is <strong>model-specific</strong>, <strong>non-linear</strong>, and <strong>not intuitive</strong>.</td></tr> <tr><td>Resource waste is inevitable.</td> <td>You can <strong>save 30–40% GPU per model</strong> without hurting performance.</td></tr></tbody></table> <p><strong>The Impact:</strong></p> <ul><li><strong>Massive Efficiency Gain</strong>: If you run 5 models on one GPU, and each only needs 60% instead of 100%, you’re effectively saving <strong>200% worth of GPU capacity</strong>.</li> <li><strong>Enables Multiplexing</strong>: Without knowing the knee point, spatial sharing would be guesswork. With it, you know exactly how much “headroom” each model has — making controlled multiplexing <em>scientific</em>, not heuristic.</li> <li><strong>Foundation for Self-Tuning</strong>: The knee point isn’t just a number — it’s the <strong>baseline demand</strong> used by GSLICE’s self-tuning algorithm to determine if an IF is over- or under-provisioned.</li></ul> <blockquote><p>✅ <strong>Think of it like this</strong>:<br>
You wouldn’t give a sedan the full engine power of a Formula 1 car — even if it could handle it. The “knee point” tells you <em>exactly</em> how much engine power each car needs to reach its top speed — no more, no less.</p></blockquote> <h3 id="_5-the-engineering-masterpiece-shadow-if"><a href="#_5-the-engineering-masterpiece-shadow-if" class="header-anchor">#</a> <strong>5. The Engineering Masterpiece: Shadow IF</strong></h3> <h4 id="the-problem"><a href="#the-problem" class="header-anchor">#</a> <strong>The Problem</strong></h4> <p>In CUDA MPS, changing a process’s GPU% requires <strong>restarting the entire process</strong>. Restarting a PyTorch/TensorFlow/TensorRT inference process takes <strong>2–15 seconds</strong> — completely unacceptable for real-time systems.</p> <p><strong>Result:</strong> Dynamic resource allocation was a theoretical dream.</p> <h4 id="gslice-s-solution-shadow-if-overlapped-execution"><a href="#gslice-s-solution-shadow-if-overlapped-execution" class="header-anchor">#</a> <strong>GSLICE’s Solution: Shadow IF + Overlapped Execution</strong></h4> <p>Here’s the genius:</p> <table><thead><tr><th>Step</th> <th>What Happens</th></tr></thead> <tbody><tr><td><strong>1. Active IF</strong></td> <td>Running normally, serving requests, using 30% GPU.</td></tr> <tr><td><strong>2. Shadow IF</strong></td> <td>A <em>hot standby</em> instance running on the same CPU core, with identical buffers, framework, and code — <strong>but NOT accessing GPU</strong>. Zero overhead.</td></tr> <tr><td><strong>3. Need to Increase GPU%?</strong></td> <td>GSLICE configures the <em>shadow</em> IF with the new target (e.g., 70%). It begins loading the model weights onto the GPU… <strong>while the active IF keeps serving requests</strong>.</td></tr> <tr><td><strong>4. Switchover</strong></td> <td>When the shadow IF is ready, the IF Manager does a near-instantaneous (&lt;100µs) handoff: <br> - Stops the active IF after its current batch. <br> - Promotes the shadow IF to active. <br> - Terminates the old active IF.</td></tr> <tr><td><strong>5. Result</strong></td> <td>GPU% changed from 30% → 70%. <strong>No downtime. No dropped requests. SLO intact.</strong></td></tr></tbody></table> <h4 id="why-this-is-so-impressive"><a href="#why-this-is-so-impressive" class="header-anchor">#</a> <strong>Why This Is So Impressive</strong></h4> <table><thead><tr><th>Feature</th> <th>Why It Matters</th></tr></thead> <tbody><tr><td><strong>Zero-Downtime</strong></td> <td>Achieves what was thought impossible: dynamic reconfiguration of GPU resources without service interruption.</td></tr> <tr><td><strong>Hidden Latency</strong></td> <td>Masks the 2–15s model load time by overlapping it with live traffic.</td></tr> <tr><td><strong>Minimal Overhead</strong></td> <td>Shadow IF uses only CPU memory and cores — no extra GPU, no network, no data copies.</td></tr> <tr><td><strong>Scalable</strong></td> <td>Can be applied to any ML framework (PyTorch, TF, TensorRT) with minimal changes (&lt;30 lines).</td></tr> <tr><td><strong>Elegant Simplicity</strong></td> <td>No kernel-level hacking. No CUDA driver modifications. Just smart software orchestration.</td></tr></tbody></table> <blockquote><p>✅ <strong>Think of it like upgrading a jet engine mid-flight:</strong><br>
You don’t land the plane. You have a second, identical engine already spinning up on the wing. At the perfect moment, you flip the switch — and the plane continues flying, now with 2x thrust.</p></blockquote> <p>This is <strong>systems engineering at its finest</strong>: solving a hardware limitation with pure software ingenuity.</p> <h3 id="_6-synergy-the-triad-of-brilliance"><a href="#_6-synergy-the-triad-of-brilliance" class="header-anchor">#</a> <strong>6. Synergy: The Triad of Brilliance</strong></h3> <p>These three innovations don't just coexist — they <strong>amplify each other</strong>:</p> <ol><li><strong>Knee Point</strong> tells you <strong>how much</strong> GPU each model <em>needs</em>.</li> <li><strong>Shadow IF</strong> lets you <strong>dynamically give it that amount</strong>, instantly and safely.</li> <li><strong>MPS with Resource Limits</strong> provides the <strong>underlying, controllable infrastructure</strong> that makes #1 and #2 possible.</li></ol> <p>Together, they enable <strong>true elasticity</strong> for GPU inference:</p> <ul><li>A sudden spike in image requests? → GSLICE detects latency pressure → increases VGG-19’s GPU% from 60% → 75% via shadow IF → SLO maintained.</li> <li>Traffic drops? → Reduces GPU% → frees capacity for a new IF.</li> <li>New model added? → Load its weights once → share parameters → spin up shadow IF → deploy with zero delay.</li></ul> <h3 id="_7-architecture-overview"><a href="#_7-architecture-overview" class="header-anchor">#</a> <strong>7. Architecture Overview</strong></h3> <img width="645" height="439" alt="image" src="https://github.com/user-attachments/assets/b83bca76-6759-41f4-9902-a6a1daa6fafd"> <ul><li><strong>IF Manager</strong>: DPDK process handling network I/O and coordinating IF lifecycle.</li> <li><strong>libml</strong>: Lightweight C/C++ library providing core services (batching, zero-copy, resource mgmt) abstracted from ML frameworks.</li> <li><strong>Orchestrator</strong>: Manages creation/destruction of IFs.</li> <li><strong>Shadow IFs</strong>: Hot-standby instances enabling zero-downtime re-provisioning.</li> <li><strong>Supports</strong>: PyTorch, TensorFlow, TensorRT, CNTK, MXNet, Darknet (minimal changes required).</li></ul> <h3 id="_9-conclusion-a-paradigm-shift"><a href="#_9-conclusion-a-paradigm-shift" class="header-anchor">#</a> <strong>9. Conclusion: A Paradigm Shift</strong></h3> <p>GSLICE is not merely an incremental improvement—it represents a <strong>paradigm shift</strong> in GPU inference infrastructure.</p> <p>It successfully solves the long-standing trilemma:</p> <blockquote><p><strong>High Utilization</strong> + <strong>Low Latency (SLO Compliance)</strong> + <strong>Performance Isolation</strong></p></blockquote> <p>By combining <strong>self-tuning resource allocation</strong>, <strong>zero-downtime re-provisioning</strong>, <strong>adaptive batching</strong>, <strong>parameter sharing</strong>, and <strong>zero-copy I/O</strong>, GSLICE achieves unprecedented levels of efficiency and scalability. It transforms the GPU from a static, underutilized asset into a <strong>dynamic, intelligent, multi-tenant, high-performance computing engine</strong> capable of serving hundreds of diverse, real-time inference applications concurrently.</p> <p>Its impact lies not only in its performance numbers but in proving that <strong>intelligent, software-defined control</strong> can unlock the full potential of commodity hardware for the next generation of cloud AI services.</p> <h3 id="✅-final-takeaway"><a href="#✅-final-takeaway" class="header-anchor">#</a> ✅ <strong>Final Takeaway</strong></h3> <blockquote><p><strong>The Knee Point</strong> revealed a hidden truth about DNN efficiency — <strong>less is more</strong>.<br> <strong>The Shadow IF</strong> solved a seemingly unsolvable systems problem — <strong>zero-downtime reconfiguration</strong>.<br> <strong>The MPS &gt; Streams Choice</strong> was the foundational architectural decision that made all of this possible — <strong>true isolation for heterogeneous workloads</strong>.</p></blockquote> <hr> <p>These three elements together form a masterpiece of systems research: elegant, practical, transformative, and deeply insightful.</p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/03.gpu/38.gpu_scheduling.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/09/16, 21:03:00</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/45897/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">GPU Cache Management</div></a> <a href="/qishao-notes/pages/47871e/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">TO READ</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/45897/" class="prev">GPU Cache Management</a></span> <span class="next"><a href="/qishao-notes/pages/47871e/">TO READ</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.dd2b9bf7.js" defer></script><script src="/qishao-notes/assets/js/2.4d9050ab.js" defer></script><script src="/qishao-notes/assets/js/84.6ceb15a3.js" defer></script>
  </body>
</html>
